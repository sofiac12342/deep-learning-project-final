chunk_id,text,token_count,start_sentence_idx,end_sentence_idx,video_id
0,hey everyone welcome to my deep q learning tutorial here's what to expect from this video since deep q learning is a little bit complicated to explain i'm going to use the fren lake reinforcement learning environment it's a very simple environment so i'm going to do a quick intro on how the environment works or also quickly answer the question of why we need reinforcement learning on such a simple environment next to navigate the environment we need to use the epsilon greedy algorithm both q learning and deep q learning uses the same algorithm after we know how to navigate the environment we're going to take a look at the differences of the output between q learning and deep q learning in q learning we're training a q table in deep q learning we're training a deep q network we'll work through how the q table is trained then we can see how it is different from training a deep q network in training a deep q network we also need a technique called exp experience replay and after we have an idea of how deep q learning works i'm going to walk through the code and also run it and demo it just in case you're not familiar with how this environment works quick recap so the goal is to get the learning agent to figure out how to get to the goal on the bottom right the actions that the agent can take is left internity we're going to represent that as zero down is one right is two up is three in this case if the agent tries to go left,358,0,0,EUrWGTCGzlA
1,or up it's just going to stay in place so in general if it tries to go off the grid it's going to stay in the same spot internally we're going to represent each state as this one is zero this one is 1 2 3 uh 4 5 6 all the way to 14 15 so 16 states total each attempt at navigating the map is considered one episode the episode ends if the the agent falls into one of these holes or it reaches the goal when it reaches the goal it will receive a reward of one all other states have no reward or penalty okay so that's how this environment works at this point you might be wondering why we would need reinforcement learning to solve such a simple map why not just use a pathfinding algorithm there is a twist to this environment there is a flag called is slippery when set to true the agent doesn't always execute the action that it intends to for example if the agent wants to go right there's only a oneir chance that it will execute this action and there's a two3 chance of it going in an adjacent direction now with slippery turn on pathf finding algorithm will not be able to solve this in fact i'm not even sure what algorithm can that's where reinforcement learning comes in if i don't know how to solve this i can just give the agent some incentive and let it figure out how to solve this so that's where reinforcement learning shines in terms of how the agent navigates the map both deep q learning,358,0,0,EUrWGTCGzlA
2,and q learning uses the epsilon greedy algorithm basically we start with a variable call epsilon equal and set it equal to one and then we generate a random number if the random number is less than epsilon we pick a random action otherwise we pick the best action that we know of at the moment and at the end of each episode we'll decrease epsilon by a little bit at a time so essentially we start off with 100 random exploration and then eventually near the end of training we're going to be always selecting the best action to take before we jump into the details of how the training works between qq learning versus deep q learning let's take a look at what the outcome looks like for q learning the output the output of the training is a q table which is nothing more than a two-dimensional array consisting of 16 states by four actions after training the whole table is going to be filled with q values for example it might look something like this so in this case the agent is at state zero we look at the table and see what the maximum value is in this case it's to go right so this is the prescribed action over at deep q learning the output is a deep q network which is actually nothing more than a regular feedforward nur network this is actually what it looks like but i think we should switch back to the simplified view the input layer is going to have 16 nodes the output layer is going to have four nodes the way that,358,0,0,EUrWGTCGzlA
3,we center input into the input layer is like this if the agent is at state zero we're going to set the first note to one and everything else zero if the agent is at state one then the first noe is zero and the second noe is one and everything else is zero so this is called one hot encoding just do one more if it's at the last if we want to put in state 15 then everything all zeros and state 15 is one so that's how the input works with the input the output that gets calculated are q values now the q values are not going to look like what's in the q table but it might be something similar let me throw some numbers in here okay so same thing for this particular input the best action is the highest q value when training a neural network essentially we're trying to train the weights associated with each one of these lines in the neural network and the bias for all the hidden layers now how do we know how many hidden layers we need in this case one layer was enough but you can certainly add more if necessary and how many nodes do we need in the hidden layer i try 16 and it's able to solve the map so i'm sticking with that but you can certainly increase or decrease this number to see what happens okay so that's the differences between the output of q learning versus deep q learning as the agent navigates the map it is using the q learning formula to calculate the q,358,0,0,EUrWGTCGzlA
4,values and update the q table now the formula might look a little bit scary but it's actually not that bad let's work through some examples let's say our agent is in state 14 and it's going right to get to state 15 we're calculating q of state 14 action 2 which is this cell the current value of that cell is zero because we initialize the whole q table to zero at the beginning plus the learning rate the learning rate is a hyperparameter that we can set as an example i'll just put in 0.01 times the reward we get a reward of one because we reach the goal plus the discount factor another parameter that we set i'll just use 0.9 times the max q value of the new state so the max q value of state 15 since state 15 is a terminal state it will never get anything other than zeros in this table max is zero essentially these two are gone subtracted by the same thing that we had here okay so work through the math this is just 0.01 so we get 0.01 here now let's do another one really quickly agent is here take a right q of 13 going to the right 13 is also all zeros this is the one we're updating here is zero plus the learning rate times a reward there is no reward plus the discount factor times max of the new state max of state 14 max of state 14 is 0.01 subtracted by again it's zero this is equal to 0.9 okay it's actually pretty straightforward now how the heck does,358,0,0,EUrWGTCGzlA
5,this formula help find the path so if we train enough the theory is that this number is going to be really close to one and then the states next to it it's going to be 0.9 something and then the states adjacent to that it's probably going to be 0.8 music something and if we keep going we can see that a path is is actually two paths are possible so mathematically this is how the path is found over at deep q learning the formula is going to look like this we set q equal to the reward if the new state is a terminal state otherwise we set it to this part of the ke learning formula let's see how the formula is going be used in training for deq learning we actually need two neural networks let me walk through the steps the network on the left is called the policy network this is the network that we're going to do the training on the one on the right is called the target network the target network is the one that makes use of the dq learning formula now let's walk through the steps of training step one is going to be creation of this policy network step two we make a copy of the policy network into the target network so basically we're copying this the weights and the bias over here so both networks are identical step number three the agent navigates the map as usual let's say the agent is here in state 14 and it's going into state 15 step three is just navigation now step four we,358,0,0,EUrWGTCGzlA
6,input state 14 remember how we have to encode the input it's going to look like this 0 1 2 3 12 13 state 14 15 the input is going to look like this as you know neural networks when it's created it comes with a random set of weights and bias so with this input we're actually going to get an output even though these values are pretty much meaningless so we might get some stuff that looks like i'm just putting in some random numbers okay so these q values are meaningless and as a reminder this is the left action down right and up okay step five we do the same thing we take the exact same input state 14 and send it into the target network which will also calculate the exact same numbers because the target network is the same as the policy network currently step six this is where we calculate the q value for state 14 we're taking the action of two is equal to since we're going into state 15 it is a terminal state we set it equal to 1 step seven step seven is to set the target input state 14 output action two is this node we take the value that we calculated up on step six and we replace place the q value in the output step number eight we take the target q values we use it to train the policy network so this value is the one that's really going to change as you know with neural networks it doesn't go straight to one it's going to go toward that direction so,358,0,0,EUrWGTCGzlA
7,maybe maybe it'll go to 0.01 but if you repeat the training many many times it will approach one step nine is to repeat the whole thing again of course we're not going to create the policy network we not going to make a copy of it so what we're repeating is steps three through 8 step 10 after a certain number of steps or episodes we're going to sync the policy network with the target network which means we're going to make them identical by copying the weight and biases from the policy over to the target network after syncing the networks we're going to repeat nine again and then repeat 10 until training is done okay so that's generally how deep que learning works um that might have been a little bit complicated so maybe rewind the video watch it again and make sure you understand what's happening to effectively train a neural network we need to randomize the training data that we send into the neural network however if you remember from steps three and four the agent takes an action and then we're sending that training data into the neur network so the question is how do we how do we randomize the order of a single sample but that's where experience replay comes in we need a step 3a where we memorize the agent's experience as the agent navigates the map rest storing the state that it was in what action it took the new state that it reached if there was a reward or not and and if the new state is a terminal state or not we take that,358,0,0,EUrWGTCGzlA
8,and insert it into the memory and the memory is nothing more than a python deck how a deck works is that as the deck gets full whatever at the end gets purged we need a step 3b this is the replay step this is where we take say 30 random samples from the deck and then pass it on to step four for training so that's what experience replay is before we jump into the code if you have problems installing gymnasium especially on windows i got a video for that i'm not going to walk through the q learning code but if you are interested in that jump to my q learning code walkthrough video after watching this one also if you have zero experience with neural networks you can check out this basic tutorial on neuron networks you o need pytorch so head over to pyt to.org and get that installed the first thing that we do is create the class to represent our deep q network i'm calling it dqn as mentioned before a deep q network is nothing more than a feed forward node network so there's really nothing special about it here i'm using pretty standard way of creating a node network using pytorch if you look up any pytorch tutorial you'll probably see something just just like this so since this is not a py torch tutorial i'm not going to spend too much time explaining how py torch works for this class we have to inherit the neuron network module which requires us to implement two functions the inet function and the forward function in the init function,358,0,0,EUrWGTCGzlA
9,"i'm passing in the number of nodes in my input state hidden layer and output state back at our diagram we're going to have 16 input states as mentioned before i'm using 16 in the hidden layer that's something that you can adjust yourself and in the output layer four notes we declare the hidden layer which has 16 going into 16 and then the output layer 16 going into four and then the forward function x is the training data set and we're sending the training data through the neuron network again this is pretty common pytorch code next we need a class to represent the replay memory so it's this portion that we're implementing right now in the init function we'll pass in a max length and then create the python deck in the append function we're going to append the transition the transition is this tupo here state action new state reward and terminated these sample function will return a random sample of whatever size we want from the memory and then the link function simply returns the length of the memory the fen l dql class is where we're going to do our training we set the learning rate and the discount factor uh those are part of the q learning formula and these are values that you can adjust and play around with the network sync rate it's the number of steps the agent takes before syncing the policy and target network that's the setting for step 10 where we sync the policy and the target network we set the replay memory size to 1,000 and the replay memory sample size",358,0,0,EUrWGTCGzlA
10,"to 32 these are also numbers that you can uh play around with next is the loss function and the optimizer these two are py torch variables for the loss function i'm simply using the mean square error function for the optimizer will initialize that at a later time this actions list is to simply map the action numbers into letters for printing in the train function we can specify how many episodes we want to train the agent whether we want to render the map on screen and what do we want to turn on the slippery flag we'll instantiate the frosen lake environment and then create a variable to store the number of states this is going to be 16 inst store the number of actions this is going to be four here we initialize epsilon to one and we instantiate the replay memory this is going to be size of uh 1,000 now we create the policy deq network this is step one create the policy network we also create a target network and copy the weights and bias from the policy network into the target network so that is step number two making the target and policy network identical before training we'll do a print out of the policy network just so we can compare it to the end result next we initialize the optimizer that we declared earlier and we're simply using the atom optimizer passing in the learning rate we'll use this rewards per episode list to keep track of the rewards collected per episode we'll also use this epsilon history list to keep track of epsilon decaying over time",358,0,0,EUrWGTCGzlA
11,we'll use step count to keep track of the number of steps taken this is used to determine when to sync the policy and target network again so that is for step 10 the syncing next we will loop through the number of episodes that were specified when calling the train function we'll initialize the map so the agent starts at state zero we'll initialize the terminated and truncated flag terminated is when when the agent falls into the hole or reaches the goal truncated is when the agent takes more than 200 actions on such a small map 4x4 uh this is probably never going to occur but we'll keep this anyway now we have a while loop checking for the terminated and truncated flag keep looping until these conditions are met this part is basically the epsilon greedy algorithm if a random number is less than epsilon we'll just pick a random action otherwise we'll use the policy network to calculate the set of q values take the maximum of that extract that item and that's going to be the best action so we have a function here called state to dqn input let's jump down there remember that we have to take the state and encode it that's what this function does this type of encoding here okay let's go back with p torch if we want to perform a prediction we should call this torch.,310,0,0,EUrWGTCGzlA
12,no grads so that it doesn't calculate the stuff needed for training after selecting the either a random action or the best action we'll call the step function to execute the action when we take that action it's going to return the new state whether there is a reward or not whether it's a terminal state or we got truncated we take all that information and put it into our memory so that is step 3a when we did the epsilon greedy that was the step three after executing the action we're resetting the state equal to the new state we increment our step counter if we received an award put it on our list now we'll check the memory to see if we have enough training data to to do optimization on also we want to check if we have collected at least one reward if we haven't collected any rewards there's really no point in optimizing the network if those conditions are met we use memory.,218,1,1,EUrWGTCGzlA
13,sample and we pass in uh the batch size which was 32 and we get a batch of training data out of the memory we'll pass that training data into the optimized function along with the policy network and the target network let's jump over to the optimize function this first line is just looking at the policy network and getting the number of input noes we expect this to be 16 the current qist and target qist so this is the current cus this is the target cus what i'm about to describe now is step 3b rep playing the experience so to replay the experience we're looping through the training data inside the mini batch let's jump down here for a moment so this is step number four we're taking the states and passing it into the policy network to calculate the current list of q values step number four the output is the list of q values step number five we pass in the same thing to the target network step five we get the q values out here which should be the same as the q values from the policy network step number six is up here so reminder of what it looks like step six is using this formula here so that's what we have here if terminated if terminated just return the reward otherwise use that that second formula so now that we have a target we'll go to step seven step seven is taking the output of step six and replacing the respective q value and that's what we're doing here we're replacing the q value of that particular,358,2,2,EUrWGTCGzlA
14,action with the target that was calculated up above step number eight step eight is to take the target values and use that to train the current q values so that's what we have here we're using the loss function pass in the current set of q values plus the target set of q values and then this is just standard pi torch code to optimize the policy network now step nine step nine is to repeat steps 3 to 8 starting from navigation to optimizing the network basically that's just continuing this inner while loop of stepping through the states and this outer for loop of stepping through each episode step 10 is where we sync the policy network and the target network and we do that down here if the number of steps taken is greater than the network sync rate that we set then we copy the policy network into the target network and then we reset the step counter also after each episode we should be decaying the epsilon value after syncing the network we repeat step n again which is repeating basically the navigation and training again so it's just going to go back up here and do this all over again all the way until we have finished the number of episodes so that was training after training we close the environment we can save the policy or the weights and bias into a file i'm hardcoding it here you can definitely make this more dynamic here i'm creating a new graph and i'm basically graphing the rewards collected per episode also i'm graphing the epsilon history here after graphing,358,2,2,EUrWGTCGzlA
15,i'm saving the graphs into an image all right so that was the train function we have the talked about the optimize function let me fold that we already looked at that now the test function is going to run the fren lake environment with the policy that we learned from the train function we can also pass in the number of episodes and whether we want to turn on slippery or not so the rest of the code is going to look pretty similar to what we had in the train function we instantiate the environment get the number of states and action s 16 and 4 here we declar the policy network load from the file that we saved from training this is just p torch code to switch the policy network to prediction mode or evaluation mode rather than training mode we'll print the train policy and then we'll loop over the episodes we set the agent up top you've seen this before keep looping until the agent gets terminated or truncated here we're selecting the best action out of the policy network and executing the action and then close the environment so ideally when we run this we're going to see the agent navigate the map and reach the goal however if slippery is on there is no guarantee that the agent is going to solve it in one try it might take a couple of tries for the agent to solve the map all right down at the main function we create an instance of the fen lake dql class first first we're going to try non-slippery we'll train it for,358,2,2,EUrWGTCGzlA
16,"a th times or 1 th000 episodes we'll run the test four times just because the map pops up and goes away when the agent gets to the goal so um we want to run it a couple times so we actually can see it on the screen okay i'm just hitting contrl f5 all right training is done and looks like the agent is able to solve the map we also have a print out of the policy that it's learned i print it in a way that matches up with the grid let's jump back to this in a second let's go to our files so after training we have a new graph on the left side that's the number of rewards collected over the 1,000 episodes as you can see over time it's improving and finally at the end it's getting a lot of rewards on the right side we can see epsilon decaying starting from one slowly slowly down to zero also another file that gets created is it's a binary file so we can't really display it but it contains the weights and biases of the policy network so if we want to do the test again we don't need the training we can comment out the training line and just run this again and we can see the agent solve the map we can look at what policy the agent learned the way this is printed it matches up with the map so at state zero the best action was to go right which is state one at state one the best action is going to go right again at",358,2,2,EUrWGTCGzlA
17,state two go down at st six go down again at st 10 go down again at state 14 go to the right okay so it was right right down down down right so remember earlier that there were two possible path the one that we actually learned plus the one going down because epsilon greedy has a randomness factor whether the agent learns the top path or the bottom path is just somewhat random if weet train again maybe it'll go down the next time now let's turn the slippery factor on uncomment the training line now with slippery on we expect the agent to fail a lot more often or to fall in the holes a lot more often let me just triple the number of um training and let me do 10 times for testing okay i'm running this again again with slippery turn on there's no guarantee that after training the agent is going to be able to pass every episode let's make it come up you see it trying to go to the bottom right but because of slippery it just failed right there but there you go it was able to solve it let's look at the graph the results compared to the non-slippery surface is significantly worse also you want to be careful getting stuck in spots like these which means if i was unlucky enough to set my training episodes to maybe 2200 and it gets stuck here which means it learned a bad policy it won't be able to solve the map so in your training if you're finding that the agent is not able to solve,358,2,2,EUrWGTCGzlA
18,the map when slippery is turned on it might be because of things like these okay that concludes our deep q learning tutorial i love to hear feedback from you was the explanation easy to understand what can i improve on and what other topics are you interested in,63,2,2,EUrWGTCGzlA
0,before reinforcement learning after reinforcement learning what's happening guys my name is nicholas and in this video we're going to be going through a bit of a crash course on reinforcement learning now if you've ever worked with deep learning or machine learning before you know the two key forms are supervised and unsupervised learning now reinforcement learning is a little bit different to that because you tend to train in a live environment now there's a really easy way to remember the core concepts in reinforcement learning all you need to remember is area 51.,124,0,0,cO5g5qLrLSo
1,now you're probably thinking what the hell does area 51 have to do with reinforcement learning well the area in area 51 stands for action reward environment and agent these are the four key things you need in any reinforcement learning model now in this video we're going to be covering all of those key concepts let's take a deeper look as to what we're going to be going through so in this video we're going to cover everything you need to get started with reinforcement learning we're going to start out by creating an environment using open ai gym we're then going to build a deep learning model using tensorflow and keras this same model will then pass to kerasrl in order to train our reinforcement learning model using policy-based learning now in terms of how we're going to be doing it we're going to be largely working within python and specifically we're going to be working inside of a jupyter notebook we'll start out by building our environment using open ai gym we'll then build our deep learning model again using tensorflow and keras and then once we've built that model we're then going to train it using kerasrl we'll then be able to take that same model save it down into memory and reload it for when we want to deploy it into production ready to get to it let's do it so there's a couple of key things that we need to do in order to build our deep reinforcement learning model so specifically we need to first up install our dependencies then what we're going to do is build an environment,358,1,1,cO5g5qLrLSo
2,with open ai gym with just a couple of lines of code so this is going to allow us to see the environment that we're actually using reinforcement learning in later on then we're going to build a deep learning model with keras so we're specifically going to be using the sequential api there and then what we're going to do is train that keras model using keras reinforcement learning and last but not least we're going to delete it all and reload that agent from memory so this is going to allow you to deploy it into production if you want to later on so first up let's install our dependencies so what we're going to need here is tensorflow keras kerasrl as well as open ai gym so what we've done is we've installed our four key dependencies so we've used pip install and specifically we've installed tensorflow 2.3.0 we've installed open ai gym so that's just gym we've installed keras and we've also installed keras rl2 so those are all our dependencies now done and installed now what we can actually go and do is set up a random environment with open ai gym now open ai gym comes with a bunch of pre-built environments that you can use to test out reinforcement learning on so if we actually head on over to gym.openai.com you can see there's a bunch of random environments so here we've got some algorithms we've got atari games so if you wanted to build atari or video game style reinforcement learning engines you could we're going to be working on these classic control ones and specifically we're going,358,1,1,cO5g5qLrLSo
3,to be using cartpol and so the whole idea behind carpol is that you want to basically move this cart down the bottom here in order to balance the pole up there so the whole idea is that for each step you take you get a point with a maximum of 200 points so ideally what we're going to see when we start off is with our random steps we're not going to get anywhere near 200 but once we use deep learning and reinforcement learning we ideally should get a much closer to actually hitting our final result now we've got two movements we can either go left or right so what we're going to see is when we create our environment we're going to have two actions available either left or right if you work in different reinforcement learning environments you might have a different number of actions that you can take so for example you might go up or down left or right if you're working with other things so now what we're going to do is set up this environment so you can work with it within python so if we go back to our jupyter notebook let's start setting that up so the first thing that we need to do is import our dependencies so in order to do that we're going to import openai gym and we're also going to import the random library so we can take a bunch of random steps so those are our two key dependencies imported so and this is specifically for our open ai gym so we've imported gym and we've also imported random,358,1,1,cO5g5qLrLSo
4,now what we can go and do is actually set up that environment so that's our environment set up so what we went and did there is we used the open ai gym library and specifically we used the make method to build our carpol environment so remember that was the carpol environment that we saw here we then extracted the states that we've got so this is available through env which is our environment that we just set up observation space dot shape so we're taking a look at all the different states that we've got available within our environment and we've also extracted the action so if you take a look we're getting that from our action space and we can see that we're going to have a specific number of actions so if we take a look at our states we've basically got four states available and if we take a look at our actions we've got two actions so basically those are left or right moving our carpal left or right now what we can actually go and do is actually visualize what it looks like when we're taking random steps within our carpol environment so ideally what we'll see is that our carpals just sort of moving randomly because we're taking random steps in order to get a specific score so remember with each step that we take where our carpol hasn't fully fallen over we're going to get one point with a maximum of 200 points so let's build our random environment all right so we've written a bit of code there now what we're actually going to do is,358,1,1,cO5g5qLrLSo
5,start by breaking this down from here so the first thing that we're going to do is render our environment so this is going to allow us to see our cut in action when it's moving left and right then what we're doing is we're taking a random step so we're either going left or right so zero or one basically represents one of those steps so we're just taking a random choice to see how that impacts our environment then what we're doing is we're actually applying that action to our environment and we're getting a bunch of stuff as a result of that so we're getting our state we're getting our reward we're getting whether or not we've completed the game so whether or not we've failed or whether or not we've passed and we're also getting a bunch of information then based on our step we're going to get a reward so remember if we take a step in the correct direction and we haven't failed we get one point this basically allows us to accumulate our entire reward now if we fail or if we get to the end of the game then done is going to be set to true so what we're doing is we're continuously taking steps until we're complete so we reset the entire environment up here and then we're also printing out our final reward so ideally what we'll get is the episode number as well as our score so let's go on ahead and run that and see our episodes live and in action actually it looks like we've got a bug there episode all right,358,1,1,cO5g5qLrLSo
6,so you can see our carts moving and it's moving randomly and you can see that our pole is sort of flailing about now what we're actually logging out is the score each time so it looks like we're surpassing a specific threshold and we're failing so we're only getting up to a maximum of about 38 so that's our maximum score now ideally what we want to be able to get is all the way up to 200 and this is where reinforcement learning comes in so basically our deep learning model is going to learn the best action to take in that specific environment in order to maximize our score now this all starts with a deep learning model so let's go ahead and start creating a deep learning model now in order to do that we first up need to import some dependencies and these are largely going to be our tensorflow keras dependencies so let's go ahead and import those so we've imported our dependencies so we've specifically first up imported numpy so this is going to allow us to work with numpy arrays then we've imported the sequential api so this is going to allow us build a sequential model with keras then we've also imported two different types of layers so specifically we've imported our dense node as well as our flatten node and last but not least we've imported the atom optimizer so that's going to be the optimizer that we use to train our deep learning model now what we can go and do is actually go and build that model so we're going to build this wrapped,358,1,1,cO5g5qLrLSo
7,inside of a function so we can reproduce this model whenever we need to so that's our build model function defined so what we've basically gone and done is created a new function called build model and to that we're going to pass two arguments so specifically our states so these were the states that we extracted from our environment up here and we're also going to pass through our actions so these are going to be the two different actions that we've got in our carpol environment in order to build our deep learning model we're first instantiating our sequential model then we're passing through the flatten node and specifically to that we're going to be passing through a flat node which contains our different states so remember our four different states that we had then we're adding two dense nodes to start building out our deep learning model with a relu activation function and last but not least our last dense node has our actions so this is basically going to mean that we pass through our states at the top and we pass through our actions down the bottom so ideally what we should be able to do is train our model based on the states coming through to determine the best actions to maximize our reward or our score that we can see here so let's go ahead and create an instance of that model just by using that build model function and we can also visualize what the model looks like using the model.summary function so you can see here that we're passing through our four different states we've got 24,358,1,1,cO5g5qLrLSo
8,dense nodes 24 dense nodes so these are going to be our fully connected layers within our neural network and then last but not least we're going to be passing out our two different actions that we want to take within our environment now what we can go and do is take this deep learning model and actually train it using keras rl so first up we need to import our keras rl dependencies so let's go ahead and do that so those are our dependencies imported so we've imported three key things here so we've imported out a deep queue network agent so basically there's a bunch of different agents within the keras rl environment so you can see we've got a dqm agent a naffa agent ddpg sasa sem so all of these are different agents that you can use to train your reinforcement learning model we're going to be using dqn for this particular video but try testing out some of the others and see how you go now what we also have is a specific policy so within reinforcement learning you've got different styles so you've got value-based reinforcement learning and you've also got policy-based reinforcement learning so in this case we're going to be using policy-based reinforcement learning and the specific policy that we're going to be using is the boltzmann q policy which you can see here now the last thing that we've gone and imported is sequential memory so for our dqn agent we're going to need to maintain some memory and the sequential memory class is what allows us to do that so now what we can go,358,1,1,cO5g5qLrLSo
9,and do is set up our agent and again we're going to wrap this inside of a function so we can reproduce it when we want to reload it from memory so let's go ahead and build that function so that's our function defined now what we've basically done is we've named our function build agent and to that we pass through our model so this is our deep learning model that we specified up here and we're also passing through the different actions that we can take so those were the two different actions left or right that we had available within our environment then we set up our policy we set up our memory and we set up our dqn agent and to that dqn agent we actually pass through our deep learning model and memory our policy as well as a number of other keyword arguments so then what we do is we return that dqn agent so let's go on ahead and actually use this dqn agent to actually now go and train our reinforcement learning model so first up we want to start out by instantiating our dqm model then we're going to compile it and then we're going to go ahead and fit all right and there you go so you can see that our dqn model is now starting to train so what we actually did is we instantiated our or we used our build agent function to set up a new dqm model and that was that up here and we passed through our model as well as our actions we then compiled it and we passed through,358,1,1,cO5g5qLrLSo
10,our optimizer so this was that atom optimizer that we imported right at the start and we also passed through the metrics that we want to track so in this case it's mean absolute error then we use the fit function to kick off the training and to that we pass through our entire environment the number of steps we want to take whether or not we want to visualize it so we'll take a look at that in a second and we also specified verbose as one so we don't want full logging we want a little bit of logging now what we can do is just let that go ahead and train to take a couple of minutes and then we should have a fully built reinforcement learning model five minutes later sweet so that's our reinforcement learning model now done dusted and trained so all up it took about 256 seconds to go and train and you can see in our fourth interval that we're accumulating a reward of about 200 now what we can go and do is actually print out and see what our total scores were so remember when we started out up here so just taking random steps we were getting about a maximum score of about 51 but that's not all that great considering that the total maximum score for the game is about 200.,302,1,1,cO5g5qLrLSo
11,so what we did there in order to test that out is we accessed our dqn model and we use the test method to that we pass through our actual environment the number of games that we want to run so in this case they're called episodes so we ran 100 games and whether or not we want to visualize it then what we did is we outputted our mean result now if we wanted to actually visualize what the difference is we can do that as well and you can see our model is performing way better so you can see it's actually able to balance the pole a whole lot better than what it was before when it was just randomly sort of flailing about we can test that out again so this time rather than doing five episodes say we wanted to um 15 for example so you can see that our model again it's performing way better than what it was initially so it's actually able to reiterate itself and resort to balanced it and make sure that that pole stays straight brings a tear to my eye so good sweet so that's all done now what happens if we actually wanted to go and save this model away and use it later on say for example we wanted to go and deploy it into production well what we can actually do is we can actually save the weights from our dqm model and then reload them later on and to try to test them out so we can do that using the save weights method from our dqm model so let's,358,3,3,cO5g5qLrLSo
12,go ahead and save our weights then what we'll do is we'll blast away all of the stuff that we just created and we'll rebuild it by reloading our weights so we've now gone and saved our weight so if we actually take a look in our folder you can see that we've gone and generated two different h5f files so these basically allow us to save our reinforcement learning model weights now if we wanted to go and rebuild our agent first up let's start by deleting our model deleting our environment and deleting our dqn agent and then what we can do is rebuild it using all the functions that we had and reload those weights to test it out so if we go and do that so you can see if we go and try to use our dqn.test method there's nothing there because we've then gone and deleted it but what we can do is we can go and rebuild that environment and test it out so let's go and do that perfect so we've now gone and reinstantiated all of our models so we first up we built our environment we extracted our actions and our states just like we did before then we used our build model and our build agent functions to go and rebuild our deep learning model and reinstantiate our dqn agent and then last but not least we compiled it now what we can do is actually reload our weights into our model and then test it out again so in order to do that we can use the dqn dot load weights method so before,358,3,3,cO5g5qLrLSo
13,up here we use save weights now we can load our weights in order to re-test this out and the file that we're going to pass to our load weights method is the one that we exported out here so we can copy that in and paste that here and now we've gone and reloaded our weights we can actually go and test out our environment again so ideally what we should get is similar results so again you can see it's performing well it's performing just as well as what it did before we deleted our weights and now we went and reloaded them and that about wraps up this video so we covered a bunch of stuff so specifically we went and installed our dependencies we then created a random environment using open ai gym and we got about a maximum score of about 51 we then built a deep learning model using keras and then use keras rl to train that using policy-based reinforcement learning and then last but not least we went and reloaded that agent from memory so that allows you to work with this inside of a production environment if you want to go and deploy it and that about wraps it up thanks so much for tuning in guys hopefully you found this video useful if you did be sure to give it a thumbs up hit subscribe and tick that bell so you get notified of when i release future videos if you do have any questions or need any help be sure to drop a mention in the comments below and i'll get right back to you,358,3,3,cO5g5qLrLSo
14,and all the course materials including the github repository as well as links to documentation are available in the description below so you can get a kickstart and get up and running with your reinforcement learning model thanks again for tuning in peace,55,3,3,cO5g5qLrLSo
0,greetings fellow learners in this video we are going to talk about q-learning but let's start this with an inquisitive question can you think of a scenario in your daily life where a computer or an ai could benefit from learning on its own just like humans do now this could be anything from optimizing your morning routine to tackling problems at work in this video we're going to explore how q-learning aligns with these real world scenarios so don't be shy to share your thoughts down below we're going to divide it into three passes starting with pass one where we just start with highle definitions and then just get into more details for future passes pay attention because i'm going to be quizzing you later on let's get to it what is q-learning in simple terms well to help me explain this we have our trusty robot frank frank say hi he hello what a cutie frank here is going to navigate this environment and in order to navigate this environment it uses a q table you can think of this q table as frank's conscience that tells frank what to do in every situation all right i see where you are frank so what do you want to do here my q table is telling me to go right gotcha gotcha all right now that you're there what about now my q table is telling me to go down you go for it buddy now this is a simple world of just nine grid squares where there's a finite number of states but what if there were a lot of states well that,358,0,0,x83WmvbRa2I
1,would mean that this q table becomes really big too big to fit in frank's tiny little head it would literally run out of memory so instead of having this table as a conscience it's probably better to just have a function as frank's conscience and this function would just take in a state and an action and it would output some value that emphasizes how good that action was taken for the state this value is known as a q value by the way quiz time have you been paying attention let's quiz you to find out what role does the q table play in frank's decision process in the simplified world of nine squares a it directly tells frank what action to take in each state b it serves as a backup memory for frank in case the q function fails c it's used for aesthetic purposes to decorate frank's environment or d it has no impact on frank's decision making and he relies solely on the q function comment your answer down below and let's have a discussion now at this point if you think i deserve it and you love frank please do consider hitting that like button that'll do it for quiz time for now but keep paying attention cu i'll be back music again so we replac this giant q table with a function in the context of ai this function is typically a neural network so q-learning is to learn values in a q table deep q learning is used to learn the parameters of a q network or a nural network now this q network is randomly initialized,358,0,0,x83WmvbRa2I
2,and represents frank's conscience we have a target network that represents the ideal conscience and this is the same architecture as the q network the goal now is to compare frank's current conscience with frank's ideal conscience and from this comparison we can compare them get a loss and use this loss to update the parameters in the q network with back propagation and then the cycle repeats in the next pass we're going to get into more details but for now in order to train this network we actually first need to take a step back and collect this data so let's just talk about that very briefly so this data collection first of all is done using this q network it's first randomly initialized and some state is chosen the q network will generate the action to take this action is taken and then reward is gained and the agent goes to the next state so the quadruple of the state the action the reward and the next state is then taken and stored in some memory called the experience replay buffer and we repeat this process of just choosing some state producing these quadruples and storing them so this data is now in our experience replay and then can be later used to train our q network which we just discussed before quiz time it's that time of video again have you been paying attention let's quiz you to find out why is experience replay considered beneficial in deep q learning a it helps the q network generate actions more quickly b it stores the q values for future reference c it breaks,358,0,0,x83WmvbRa2I
3,the temporal correlation and improves learning stability or d it ensures that the q network is always initialized with the ideal parameters comment your answer down below and let's have a discussion that'll do it for quiz time for now but once again i will be back so pay attention music now there are two phases that we discussed in past two that is the data collection phase and the training phase so let's talk about both in detail starting with the data collection phase so first we randomly initialize the q network we choose an initial state pass it through the q network and the number of neurons by the way in the output layer of this network is the number of possible actions so each neuron outputs a q value for the input state at a specific action so for example if we could only go left right up or down there would be four neurons in the output layer and each of these neurons would correspond to a q value for each of those actions so we then choose an action in an epsilon greedy fashion we take the action in the environment or in some simulated environment and once the action is tanken frank will get a reward and frank will also be in a new state so now we take that quadruple that we just described that is the state the action the reward and new state and store it into the experience replay buffer now we can randomly choose a new state and repeat the process of collecting the data in the experience replay buffer and this is the data,358,0,0,x83WmvbRa2I
4,collection piece now on to training now we have this experienced replay which has a lot of data and we can use this data to train the q network so let's take a batch of this data now in this case just for explanatory purposes let's say the batch size is just one example so that we can see what's going on throughout we feed the current state of the quadruple into the q network we get the q value corresponding to the action in the quadruple and then we also take the the same current state pass it through the target network and we get the highest q value there now we take the reward from the quadruple and add it to this target q value so now we have two main values one from the target it's an ideal q value and then we have the current q value now we're going to compute the mean squared loss using the q network's q value that represents frank's conscience and the target final q value that represents the optimal decision this loss is then back propagated into the q network and the parameters are updated so frank learns but the target network remains the same now we repeat the process for a few batches where frank's conscience continues to learn but the target network still remains the same and then after a few batches we will update the parameters of the target network to then match the q network and then continue and eventually frank's conscience and decision making improves and he's a champ quiz time i'm back have you been paying attention let's quiz,358,0,0,x83WmvbRa2I
5,you to find out why is a target network used in the training process of deep q learning a to randomly initialize the q network b to represent frank's current conscience c c to provide an ideal reference for parameter updates and improve learning stability or d to store experiences in the experience replay buffer comment your answer down below and let's have a discussion now that'll do it for quiz time for now and unfortunately i'm not going to be back again so before we go let's get a summary of the video a q table represents an agent's conscience the deep q networks combine q learning with neural networks dqn construction has two phases a data collection phase and a training phase the data collection collects independent quadruples of experiences in data stores called the experience replay buffer in the training phase trains the q network using data in the this experienced replay buffer and that's all we have for today but if you want to know information more about the raw q learning algorithm do check out my video right on screen here and as always thank you all so much for watching please consider liking the video if you do think i deserve it and i will see you in the next one bye-bye,283,0,0,x83WmvbRa2I
0,hello my name is luke and welcome to this video on deep reinforcement learning this video is a follow-up to two previous videos where we cover the basics of reinforcement learning itself we we cover the concepts of reinforcement learning why we need it how we can use it and some algorithms that have been created using the concepts of reinforcement learning in this video we're going to be utilizing deep neural networks to really supercharge those reinforcement learning algorithms and make them more useful in a wider array of situations including some more complicated environments so if you're unfamiliar with basics of reinforcement learning i recommend you check out those videos first if you're unfamiliar with the concepts around neural networks and deep learning i have a series on deep learning with pytorch which is the deep learning framework we're going to be using here so if you're all familiar with those then we can proceed with this video so deep reinforcement learning so some background if you watched my previous two videos on reinforcement learning well at least the last one at the very end we talked about how we can expand upon the ideas of those discrete grid worlds that we were using those discret states and we could potentially try and discretize our world take our world and chop it up into a discrete number of chunks and use those tabular cu tables or value tables or action probability tables or what we might want to do especially if we have a very complicated environment or we're not able to neatly chop up our world we might want to use some other,358,0,0,WxjEZmIiRQU
1,way of representing our q value or q function or our probability action functions um even if our world is discret if there's a huge number of potential states then we can sort of treat it as continuous and go from there so what neural networks um going to be used for in terms of reinforcement learning is that it's going to be used in place of those hup tables so they're going to act as our continuous function so our continuous q function continuous value function etc etc um in place of those tables so as we've already discussed here we need some sort of continuous function if we want to treat our state or our environment as a continuous thing right we can't chop it up into des states and have a lookup table neural networks are a continuous function approximator so we can use the concepts from deep learning to act as a continuous function rather than a discrete lookup table and we'll go into exactly how we do that a bit later on but first uh let's address some of the issues with using neural networks in reinforcement learning we covered a lot of the issues of reinforcement learning itself and why it's difficult and all the sort of things we need to be aware of but there are quite a few challenges unique to reinforcement learning that don't particularly mesh well with deeper learning concepts and neural networks and how they work uh i've got a few here and a couple of slides the first one here the reinforcement learning training signal is often very spar so we covered that in,358,0,0,WxjEZmIiRQU
2,the previous videos we train our agent or encourage our agent to take certain actions by giving it a reward via some reward signal some reward function um and ideally we don't want to have to handhold our agents too much and so that reward signal is uh relatively sparse if our reward signal was dense then that's just kind of similar to us just labeling every state with what the act the agent should do the idea behind reinforcement learning is that we should just be able to give our agent some rewards and it should work out the sequence of actions to take to get those rewards uh what that means is that the actual ual signal we use to update our agent at different states of has to be inferred uh so for neural networks and deep learning if you were trying to train like a convolutional classifier for every input you would have a label of that image or that input or whatever it is uh you have a ground truth for every input and you know what every input should be labeled as for every input reinforcement learning we don't have that and often what the actual action we should do um we don't actually have that information the agent tries to learn that through the learning process um from an inferred and spar signal neural networks don't really like that training deep learning models is already quite a noisy process that just introduces some additional noise second point here training signals are often dependent on each other what i mean in this point is that uh again comparing to something,358,0,0,WxjEZmIiRQU
3,like training a convolutional class classifier within your network you have your data set which you define at the start it's all labeled and that's the data set you'll perform epo over to train your neural network with reinforcement learning you can kind of think of it as we're building our data set as our agent is learning so we get exposed to newer states and newer parts of our environment as our agent improves what that means is that the data that we train on is actually changing constantly through the learning process so the example here is if you're playing a video game uh to get to sort of the second enemy in a level you have to defeat the first enemy and if you never defeat the first enemy you'll never see the second one so might seem obvious but it's very different to how something like a traditional learning problem with with neural networks is is framed so our agent our neural network never gets trained on the states the observations those frames of the second enemy if it's never actually seen the first one so the data that you train off of is dependent on what your agent is doing so it can be sort of potentially hard to predict um and it kind of leads into the next point or the final point on this slide the training distribution is non-stationary so the sort of data your agent sees is changing dependent on how good your agent is and uh again dependent on the behavior of your agent the actual policy itself at the start of training we sort of,358,0,0,WxjEZmIiRQU
4,take random actions which means we do a lot of exploration at the start of say a level in a video game then as our agent learns sequence of actions to take then we start to explore the start of the level uh less than we normally would and neural networks right if you're trained in this way where you are trained on certain state inputs at the start and then slowly you're only trained on a different set of data and your networks can sort of forget what it was supposed to be doing in those other states um so example again to get to the second level in the video game you need to complete the first level right and if you're training in a sequential way then you could potentially forget what you've done early on which means that you might take bad actions and this leads to uh the forgetting leads into this first point on this slide which is that unlike a lookup table that we used in the first two videos with our discreete states a neural network uh you can't just change the values say or the q value or update that q value for a single state and neural network is a parameterized model and if every input every output uses the same parameters then if i adjust the parameters for one state i adjust the q value for all states so again that introduces a bit of noise we can't like a lookup table just neatly update only a single vi value for a single state action change the parameters which change changes all of our q,358,0,0,WxjEZmIiRQU
5,values or our values the last point on this slide is that neural networks require a lot of data to train and can easily overfit to a small data set so this is problem that you know we might not face too much if we're say training a convolutional classifier we can always just get more data but because our agent uh creates the data set as it learns if our agent is not doing enough exploration potentially and is only seeing the same stat over and over again it could overfit and potentially um just take the same states over and over again take the same actions over and over again and even if we're um making random actions well it doesn't help us too much because as we saw in the previous videos um using a policy like epsilon greedy where we're taking random and non-random actions is optimal especially for complicated environments where you know a platform game like you're playing mario or something like that if you're only taking random actions you wouldn't progress very far in the level so you need some non-random actions but if our network is too big and overfits maybe it overfits to a particular sequence and kind of gets stuck so doesn't generalize i showed this uh picture at the end of the last video it's just a it's a bit old now but it's a very high level overview of some deep reinforcement learning algorithms split up into the sort of different categories so our value model base and our model which we've seen policy optimization and q learning as well okay so we've gone,358,0,0,WxjEZmIiRQU
6,through a lot of the problems with trying to use neural networks um especially for complicated environments in reinforcement learning and it was really this paper um using q learning that was the sort of first one to really succeed in using neural networks in place of like a lookup table or other methods in order to use reinforcement learning concepts on a complicated as i say here high dimensional sensory input using reinforcement learning so high dimensional sensory input basically they were training on images from a video game specifically the atari suite of games so there was quite a few games actually that were playing right this was the first instance of um someone actually being able to do it in a generalized way so they were actually able to use the same methodology to train an agent on different video games as i said they were effectively using q lining okay so as i mentioned at the very start what we're doing with deep reinforcement learning is effectively taking that q value table that we may have seen for and replacing this discrete table with a continuous neural network and this neural network will give us our q values for our given input states so similar to like a classifier or a regression model our q network will take in some state observation and output a q value for each action specifically for q learning in the example for the atari game they don't just give you they don't just give the model the current frame they actually give it the previous few frames so it gets an idea of sort of the,358,0,0,WxjEZmIiRQU
7,motion that's useful for some video games so this is all the transition from reinforcement learning deep enforcement learning is we're just replacing those lookup tables and you could use other other um objects as well to represent our q function but specifically we're replacing it with a neural network okay so now that we're using neural network and deep learning we need to define some objective so we need to change that sort of update step that we saw for our q table into an objective into a loss function that we can then train our model with gradient descent on so we can back propagate from and train with gradient descent and really it's pretty similar the actual objective to just the update step we just need to frame it in terms of some loss so as you'll see here the loss for our q learning is the expectation over some batch of yi subtract current q value the state action pair so we we in some state took some action like before here theeta is just representing the parameters of our newal network and you can see this is basically just the mean squared error between some target and the output of our network for a particular q value right and then this target yi is just equal to the reward plus gamma max q value of the next date so you remember back to our update step we had the current q value is equal to so the new q value is equal to the current q value plus some learning rate times the difference between this and this if you put,358,0,0,WxjEZmIiRQU
8,this here in place of yi then that expression there is basically what was inside the brackets of that update step all we're doing is saying that you have to minimize the difference between this new q value and the old q value right um the last point here note that we have a bit of a problem that our target for our q value is constructed from the output of our neural network that can lead to instabilities in some cases uh it is common to well firstly note that when we do our back propagation this q value here is detached and we're not actually back propagating through this next state q value there are other uh you can also use a different or an old version of the parameters of our q network but here we're updating this one but maybe the parameters of this q network are actually an older q network so we do an epoch of training which we'll get to in a moment but the q value we use to get to the next state q values are actually an old version before we started updating um in our example i'm not doing that because it it doesn't um doesn't make much of a difference and it simplifies things but um just to make you aware okay so we saw a lot of those problems with using neural networks for arq value function um how does this paper that we've just seen attempt to solve some of those problems well a lot of the problems um are actually solved by using what is called an experienced replay buffer so,358,0,0,WxjEZmIiRQU
9,this non-stationary data having this data shifting noisy data not getting enough rewards sparse rewards and these dependent training signals right um all of these problems again you can think of as being an issue because we're using neural networks because i'm doing an update to my model and that suddenly changes the q values for every single state input right because of this because it's a parameterized model um and we don't sort of and if our data is shifting then we're sort of training on different parts of our data set and causes issue hopefully hopefully that makes sense so what is the experience replay buffer well it's basically a fifo buffer first in first out buffer of a fixed size that basically just collects um and holds on to each state transition experience so basically we create a data set so it's a fifo um data set with a fixed size so we keep adding these state transition experiences and and we pop out the old ones the oldest ones when it becomes full so the state transition experience is the current state the action we take the reward we got and the next state because that's all we need in order to actually do this q update step that's all we need to do so we sort of package that up into a tuple a tuple and we log that as one data point into our data set this fifo buffer can be quite large up to millions of these st transitions and note that these are collected over multiple trajectories so what we do is we perform a number of roll,358,0,0,WxjEZmIiRQU
10,outs number of trajectories we collect our data either wait till we filled up our 5o buffer which is what we'll do or you can start at some point once you've got enough data and then you basically do batched gradient descent on this fifo buffer of data you take a batch you calculate for that batch um the loss defined here of that and then you perform creating descent and do an update step and you do some number of iterations over your data set you don't need to do the entire data set but you perform an epoch over your data like normal a training time perform a batch gr descent over the average loss and the spar rewards overfitting they basically just say they use a relatively small network toe prevent overfitting and um yeah okay this is basically a very simplified outline of the algorithm as we've said initialize some replay buff buffer to some fixed size randomly initialize the parameters of our model and then we perform our trajectory roll out we append to our replay buffer and then we sample from that and do some number of training steps so using these uh methods they were able to i said train different agents i'm going to clarify they trained a new agent on each of these games from atari the atari video ser game series and were in some cases able to outperform humans and it also says here double dqn which is a slightly different version of dqn so they were able to in some cases outperform humans no cases not um since then of course um further advances,358,0,0,WxjEZmIiRQU
11,has uh not only allowed us to play even better at these atari video games but as we'll see excel at a bunch of other far more complicated games but for now let's have a look at a basic code example uh of deep q learning implementing and using some of those techniques that we've just talked about so the environment we're going to be using is called cart pole from the gym or gymnasium now environment so c pole is a very basic environment um we're not going to be using an image as an input we're going to be using some states of the actual um of the environment of the game as i said uh it used to be called gym it's now called gymnasium the actual i think i've got the same thing here it's actually called gymnasium now so if you want to download yourself and install it search gymnasium and basically gym or gymnasium is a set of environments you can also find all of the actual atari game environments if you want to train on those we'll have to change the network and whatnot that um basically structures the game in such a way that it makes it really easy to train for reinforcement learning they have a bunch of different um games they also have a apis that are useful for constructing your own environment so if you have some problem some environment that you want to train an rl agent in gy can be useful for constructing the actual infrastructure around that environment which in many cases is actually the hard part how you can take your,358,0,0,WxjEZmIiRQU
12,environment or whatever thing you want to act in or have your a agent act in and turning that into something that can be trained using reinforcement learning can be quite difficult but we're not going to go into that in this video but just pointing out that gym or gymnasium now uh has tools to help you do that what i do want to point out is that as i said we're not going to be using video frames we're going to be using this observation space which is just four values and those four values are cart position the cart velocity so the position along this axis here it's velocity the pole angle and the pole angular velocity so the game is basically how long can you uh keep this pole standing upright so we have a cut that moves forward and backwards and we've got a hinge point here and we're trying to balance this pole on this frictionless hinge here actually think i've got a video here link to quickly of a real life version of this and someone he has trained an actual rl agent real life version of this so we can see it ring there i won't watch this for too much longer but you can click that if you want and see an actual agent learn a real version of cart pole okay let's go and to our code so like usual you can find all this code on the github repository link in the description so um if you don't have gym or gymnasium installed you need to do this that well as well as torch so,358,0,0,WxjEZmIiRQU
13,we're using p torch um for deep learning part of this like i said i have a series if you're unfamiliar so let's look at the key parts of this code so the first thing we've got here is our q network kept this pretty simple again like i said having our network be relatively small not only helps it uh stop from overfitting it also helps it learn a bit faster not having too many layers so i've just got a two layer mlp first of course is our four inputs and then we have some hidden size 256 and then the number of actions so i might point out as well we have two actions which is just go left or go right a bit right so we take our input first fully connected ra you f connected and then we have a q value per action okay so that's our simple neural network so the next thing we have here is the class for our replay buffer this object is as i've mentioned in charge of logging all of those um obervations those state transitions the state action rewards next state and i'm not going to go through this too much you can go into your own time i don't have a huge amount of time um but basically that's all it is is also responsible for during training time this it function here this generator will randomly shuffle and spit out a batch of data so that we can use this and iterate over this at training time so we use data log to log the data it will split that up,358,0,0,WxjEZmIiRQU
14,into our um the different dictionaries for our state action reward next state and then the generator function here will randomly shuffle and batch that up for training test agent functions similar to before um except now we just do a forward pass of our observation state uh through our qet and get our q values and take the arax like we saw before take the max q value right so i might pause here as well and sort of explain how the gym environments work so they're very simple you just need to import gym define environment name here i've only created a single environment to train with it also supports um multiple environments at the same time so in parallel you can run multiple simulations of the game in parallel and you do that during training so you can collect more roll outs at once in this uh for this environment it's relatively simple and quick to learn so i'm only doing one at a time so all we need to do is reset the environment that gives us the first observation right i put that through my q net get the action i should take and then we do an m.,265,0,0,WxjEZmIiRQU
15,step i pass in the action that i'd like to take into uh the gym environment and then we get the observation the reward a done signal so these environments also have a terminal state for cart pole the terminal state is when that pole basically falls over or um goes past a a certain angle the episode terminates if the pole angle is not in the range plus orus 12 so basically if the pole looks like it's going to fall can't recover from it we terminate and we get and that's it the reward for the environment is basically plus one reward for every step every time step that we stay alive for basically so as you can imagine to get more rewards you need to stay alive for longer which means balance the poll for longer so but hopefully as you can see the gym environment is quite simple to use okay so let's look at our dqn update so this basically runs a single epoch over our replay buffer so you can see all we need to do is iterate over our replay buffer and it will return a batch of data um for iteration here we're iterating over the entire data set so we get our next states so that's the next state of our state action reward next state pass that through our q network like i said i'm using the same q network for our next q values and our current q values i'm detaching so i don't back propagate through this one at next stes the current q value approximation we need to use gather to select the,358,1,1,WxjEZmIiRQU
16,q values corresponding to these actions that gives us the index of the q values and then what we do is we get to the max q values for the next state so we have the index of the q values the max q values um so we need to index the q values because we don't want to back propagate through both of the q values only the one that we actually took the action through the expected q value this is our target we basically calculate what the q value should be now it's the reward plus gamma times the next q value and multiply by masks masks here is from the gym environment basically this is telling us if we hit a terminal state so whether or not this is a terminal state so if i'm at a terminal state and i grab the next state jy will give me the same observation again unless we've reset the environment with for.,213,1,1,WxjEZmIiRQU
17,"reset here so this index q value is the q value of the action we took again that's just our mean squ error we get our loss and we do zero gr back propop optimization step okay so we define a few rer parameters we have our learning rate gamma which is our gamma here for our q the buffer size so this is the size of my replay buffer i've got two 2,000 observations mini batch size and the maximum number of steps this is the maximum number of interactions our agent will have with the environment and this is an important uh parameter in terms of the efficiency of reinforcement learning as we'll talk about a bit later on so data names here is basically the key values for my data dictionary i want to define a state next state action rewards what we saw and also the masks so i create an instance of my qet my optimizer and my repay play buffer so i'm going to start off the epsilon for our q learning at one which means we're going to be taking all random values at the start but what i'm going to do is i'm going to decay that epsilon over time and have a minimum value of 0.2 for the epsilon so we're going to start with high randomness to explore a lot and then we're going to slowly take more and more of the optimal or greedy steps so this here is our roll out trajectory loop so while not done so done is basically the mask that i talked about before which flags when we've hit a",358,2,2,WxjEZmIiRQU
18,terminal state so while we haven't hit a terminal state we go through here we do our epsilon greedy action so either take a optimal or a greedy action or a random action do a single step of our gym environment we'll get our next observation our reward and that done flag convert the reward to a tensor and we'll append that to a list the rewards actions the masks and the observations i'm appending at the start so this is the observations of the current state and then at the end of that loop when we've hit a terminal state so there is a max of 200 steps as well i should mention we log the observation again um this will always be in this environment a terminal state we either done or um we've failed because we''re fallen over we then log each of those into our replay buffer specifying what particular data it actually is okay so once our replay buffer is full so i'm going to perform a whole bunch of trajectories first and then update our model so we call the dqn update update function and for every second roll out we're going to update the learning rate so we're doing learning rate decay and decaying epsilon as well and we're going to perform some tests of our agent so here i'm performing 10 roll outs one after the other and taking the average score so we can track the performance of our agent over time okay and then closing just for cleanup so let's run this and see how our agent performs so here i'm also just using the um,358,2,2,WxjEZmIiRQU
19,ipython notebook function clear output to just uh plot and then remove the plot so we can get this little animation of our agent training like i said there are 200 a max of 200 steps so if we get plus one reward for per time step then we should expect a perfect agent to be able to achieve 200 uh points of reward and you can see that our agent is is converging nicely to that value so it's a bit of noisy there um so one thing you'll notice is that the actual performance of the agent changes every single time you run it so sometimes we get some weird behavior like this so it's we may have decayed epsilon too quickly or something like that so it sort of dropped off after the optimal point let's run it again and see if we get something slightly different so maybe i'll pause it and see what we get okay so i've run it again and we got a slightly different result you can see still see there was a dip for some reason in the actual performance but it's converged back up to 200 so there we go we've trained our agent and it was able to successfully balance the pole in this simulated cart pole for the full 200 time steps okay so there we go we've trained our first deep reinforcement learning agent using some of the principles of you learning so hopefully you can see um at a high level all we've done is changed that q value table to a uh neural network and that has allowed us to use,358,2,2,WxjEZmIiRQU
20,a continuous environment so we've got those continuous position of the actual box on this wire whatever you want to call it and the continuous angle of this bar here we couldn't do that with a discrete uh vi table maybe we could like i said maybe you could chop up the position of this box discrete way and you can chop up the angle of this pole in some discrete way and use an actual lookout table but um we are able to use more information from the environment the full continuous range by using a neural network a simple nonlinear neural network okay so as i sort of mentioned before this was quite old and there has been since then a whole range of improvements to deep q learning and um modifications to q learning specifically uh improving various aspects of it uh improving performances making improving stability and those sorts of things as well as sort of new classes of algorithms so you can see here's a little bit of a summary of some uh examples we're not going to cover those ones but if you're interested here you can look up some of these again they all use the same or similar concepts from basic reinforcement learning usually just improvements to stability and the efficiency of the actual algorithm so that max steps parameter that we uh put into our code to define the maximum number of training experiences or experiences from the environment um for reinforcement learning as i mentioned at the start uh neural networks require a lot of data deep reinforcement learning agents require a lot of interactions and,358,2,2,WxjEZmIiRQU
21,experiences with the environment and if your agent is interacting with the real world that can be very uh time expensive right the real world runs at 1 second a second you can't like a simulation run at faster than that so yes okay so let's look at some of those other algorithms we saw in the first two video videos so we saw some other class of methods called policy methods policy based learning and now we've got neural networks and they're usually called policy gradient methods so we're learning our policy network directly instead of some value function as well um i point out here as well we're actually using model free learning this whole video those modelbased learning methods while you can use them still with neural networks you can parameterize your value table as a neural network which we'll see when we get to acor critic methods um but again you need that model of the environment something like alpha zero which was played it was the chess um or alpha go which was played go or shogi as well um in those environments you do have a perfect knowledge of the environment right the board game is deterministic it's just that the actual state space is massive but they use neural networks for say the value function but let's get back to policy based learning so again like q learning were replacing that lookup table with a neural network so we put our states in and instead of a q value we'll get an action probability okay so here is the objective for say an algorithm like reinforce the loss function,358,2,2,WxjEZmIiRQU
22,you can see we want to maximize this value point that out we want to maximize the log probability of our policy for a state action multiplied by r which is our returns here so like i said the returns are positive then we're going to increase this a lot if they're negative we're going to decrease this or if the returns a small will only increase it or decrease it a small amount here this is basically what we're saying is is that for a given state action so for a given output of our neural network we're going to increase it we want to increase or decrease that output just the outside magnitude of the output based on whether or not our returns were positive or negative how much we increase it will depend on the magnitude as well so an important difference here though note that the loss is the expectation of the current policy so expectation over pi here pi as i mentioned is our policy parameterized by theta so unlike q learning where we batch up we batch up the state action rewards next state and we train off of that because the returns are based on our policy then this loss is based on the policy that got these returns so if we were to create a replay buffer of experiences over multiple different agents then those returns would have been uh gotten by a different agent right and that's important because um we could be potentially be taking actions that this agent would never take uh and that can cause well that causes problems with updating say a um,358,2,2,WxjEZmIiRQU
23,probability that's very small with a log of that probability um but again what we're doing here is we're increasing the log probability of an action for that agent based on it trajectory and we can you can get away with taking some of the trajectories of agents that were know similar to so as long as your replay buffer wasn't too big you get away with that because technically while you're training your agent is changing while you're doing iterations but if it strays too far you start to get off policy okay so let's have a look at an example of reinforce so you might be thinking well well if you don't have the experienced replay buffer then aren't just going have problems with instability well because we're updating our network directly and our network is our policy we don't have to learn this value based um this value function or this q value function uh which is sort of a bootstrapped process it can take a bit of time so the policy based learning isn't as sensitive to not having enough data or having shifting data so let's have an look at an example of reinforce so that's our q learning have a look at our reinforce so our network is very similar to before fact it's identical we have two linear layers for our mlp with our rue activation function you see here the output of our network is a categorical distribution so a pytorch categorical distribution so we have a discrete distribution here we have two actions and we're going to get a distribution we're going to assume the output of,358,2,2,WxjEZmIiRQU
24,our network is the log odds the unnormalized probabilities and we tell categorical that's what it is and it will normalize those probabilities and sample from a discrete categorical distribution of two values and and return the distribution so it's going to return the distribution object then to get the actual action we'll sample from that distribution and we can um drain it from there so we have our simple function to calculate the return turns similar to what we've seen before our test agent again we've already covered how the gym environment works so we pass in our observation into our model we get that distribution that categorical distribution that's a ply torch object sample from that distribution that'll give us one of the actions zero or one because we've got two put that on the cpu and get item to get that as a python integer because that's what uh the gym environment needs and we do everything else the same okay so we create an instance to find our learning rate and our atom optimizer you can see here that we're training with quite a large number of steps while we don't need a replay buffer specifically the fact that we have a replay buffer in q learning uh means that we can train over the same data multiple times with reinforce here we're actually only going to train over a set trajectory once and effectively throw it away again because if we were to train on older and older trajectories that actually um were from an agent using a policy that was very different to the current one it actually impacts performance,358,2,2,WxjEZmIiRQU
25,quite a lot if it's too old what that means is that to train it for long enough we need to get more newer experiences because again to train our agent we need to collect experiences using the current policy which means we need to do more trajectories and more rollouts can't use old trajectories at least with this basic implementation of reinforce so we actually need to train probably doing the same number of iterations and update steps but we need more interactions with the environment so in that case worth pointing out that it's not as inefficient as basic uh as efficient as basic dqn though uh later versions of policy grading methods are okay so the roll out again is very similar similar to the grid worlds we saw instead of epsilon greedy we're just sampling from the action distribution which will hopefully be pretty flat at the start so pretty random at the start you can make it so that this action distribution is very flat by say multiplying it by a uh learnable parameter very close to zero or by initializing the parameters of the final layer to be very small so that the outputs are very close to zero the the log probabilities are very close to zero so the distribution is flat just to ensure that it's random at the start if you're having problems again we're going to log the observations the rewards and the log probabilities as well note that um here i'm doing it in the replay buffer we detach the actual outputs of our model so sorry we don't actually collect the outputs of our,358,2,2,WxjEZmIiRQU
26,model here we put them into the replay buffer here however we're collecting the output of our model the log probability is just i get that from the distribution object the normalized log probability and i'm logging that we're actually going to train our model uh on these we're going to update these so the actual path of the gradients will actually go from here into the actual trajectory so we're doing a single roll a single trajectory collecting those log probabilities and updating those based on our returns quite a different approach than our q learning so i log all the rewards log probabilities i log all the rewards sorry i pass the rewards to our returns calculation i get the returns um i just concatenate those as they were a list i'm going to normalize them just by dividing them by the max because it goes up to like 200 so it gets quite large can be a bit unstable so just making sure you only goes up to one um there's better ways of normalizing it but that's just what i've done here concatenate the log probabilities because they're a list so it's a single tensor here and my loss is just i'm doing the sum here as well um just to play around with the actual magnitude of the gradients you can play around with the learning rate and as well so my loss is just those log probabilities which is normalized outputs of my model multiply by those returns i get my action loss and i perform a single optim single update step you can see here i've done an entire,358,2,2,WxjEZmIiRQU
27,trajectory and i'm doing one iteration that's one of the reasons why i'm taking the sum here instead of the average so the magnitude of the gradient is a bit larger you could also have a larger learning rate as well so one single update for a single roll out is quite inefficient so we need to do a lot of steps but let's have a look at how this goes okay so i'll pause this and get back when it's done all right so training's over you can see we've learned our agent has improved but it hasn't actually converged at the 200 step mark so hasn't quite performed as well as our dqn you can see training is a bit noisier than our q learning well relatively noisier and we haven't converged yet though you can probably fine-tune things a little bit with the hyper parameters and how long you're training for but in general reinforce is a pretty simple algorithm and as we talked about in the reinforcement learning basics we have problems when we're using just the returns and um just the just the returns as our advantage multiplying by the log probabilities the returns are noisy as well um and again it's not very efficient we have to do a lot more steps um and we'll talk a bit about how we can improve this further conceptually b in a bit okay so let's move on to our actor critic method so this is what we saw again at the end of the um reinforcement learning basics so instead of using our returns which are no we can use an approximation,358,2,2,WxjEZmIiRQU
28,that is less noisy but biased which is our value function actor critic methods are methods where we have both our policy and our critic or our value network so actor acts our critic works out how good our current situation is and we can use various methods to uh learn our value function we're going to use temporal difference learning which we covered in the reinforcement learning basics so you don't have to have a separate or you could have a separate actor and policy network however it is most common to share most of the parameters of your acor critic network and just have different heads this could be a single layer or multiple layers so maybe share a bunch of the cnn layers and then split off for your fully connected layers for your action probabilities and your value function so for our actor critic network again our policy um act critic methods we're going to our advantage now which was just the returns before our advantage now is the returns subtractive value so our advantage here is going to be how much better was this current roll out from the average or the expectation and we're going to increase the probability of taking an action if we performed better than average decrease if we performed worse than average so we can learn the value function with either multicolor updates or td updates so we're going to use td updates so again in terms of code we're just swapping out what was here so let's have a look at our acor critic method so here i've actually got two separate networks you don't,358,2,2,WxjEZmIiRQU
29,need two separate networks i've got two mlps one for the actor and one for the critic again distance distribution uh is categorical for our actor and then critic we've just got one single output for our value function so calculate our returns um sorry we're not using td learning here we're just using the returns for our value function so you can swap this out for td learning if you want temporal difference but we're just going to use um the straight i guess monti carlo update method so similar to before in uh similar to the montio update we're just basically going to update our model with the difference between its current prediction and the calculated um returns so here our critic loss is just the difference between the values which are the predicted values during the roll out and the calculated returns so our model should be the expectation of the returns so that's what our value function is and then our advantage is going to be the returns subra the values we're detaching from here we don't want to back propagate from our agent into the values from our value function so we need to detach here keeping track of these sort of things is very important to make sure we don't um do anything funny so we're using the values but we don't want to back propagate from our agent into the value function okay of our advantage we swap out our returns for our advantage and multiply that by our log probabilities and our total agent loss is the action loss plus the credit loss so it's all part of,358,2,2,WxjEZmIiRQU
30,the same uh by torch module network but they are technically two separate mlps that don't share any parameters so we only need to do one instance of that same number of steps same learning rate uh and pretty much everything else should be the same so let's now see how we go with that simple change of adding a value network instead of just the pure returns so i'll pause again and get back when it's done okay so we finished training and you can see we again haven't quite converged to 200 but we've performed a bit better than just our reinforced algorithm so that suggests that those returns quite a bit noisy and having our function there smooth things out a bit like i said you could go further and change how updating the critic to use td learning make it again even smoother hopefully would improve performance there so still a bit inefficient and we're not going to look at more complicated versions of algorithm in this video if you're interested let me know but there have been since um the simple act critic methods a lot of improvements around policy gradient methods uh improving the training efficiency and speed and stability as well um one point that you might think is an issue is the fact that we're doing one iteration per whole trajectory in roll out uh wouldn't it make sense to potentially do multiple iterations over a single trajectory or perform many trajectories with the same policy bat those into a data set and perform potentially multiple epoch over that you can one problem with performing multiple epochs over,358,2,2,WxjEZmIiRQU
31,the same batch of data is that our policy gradient method or objective here is just increase the probability or decrease if the advantage is positive or negative you do an iteration over the same batch of data over and over again so if this is positive we're just going to keep increasing the probability up higher and higher and higher and high and up to infinity or down to negative infinity over and over again and again our advantage is noisy our advantage just says it's better than average is it the best action to take is it the only action to take no it's just it always better than average but again our advantages based off a calculation of our value function which is going to be bad at the start so we can't treat our advantages actually good and again our advantage is just saying it's better than average so you don't want to just keep iterating over the exact same um set of experiences right so there's um you need to think about that when you do updates and later more advanced algorithms like po box policy optimization um really implement a lot of the fixes and updates to simple actor critic methods to make them more efficient and more stable and train a lot faster if there's interest i can go make a video on uh something like po okay so we've implemented a few deep learning deep reinforcement learning algorithms and seen how we go from reinforcement learning and seeing where deep learning fits into the picture so you can really take these concepts and scale them up quite a,358,2,2,WxjEZmIiRQU
32,lot so there's not only more advanced algorithms like i've said but there are slightly different algorithms uh using different techniques as well the field of reinforcement learning and deep reinforcement learning has really exploded since that first atari deep vi learning so there's been competitions and environments where you can train an arl agent in minecraft of course there's been alpha zero and alpha go and also is now alpha mu which is a uh off sorry a model free learning so alpha go and alpha zer are actually model based but alpha mu is a model free version you may have heard of the open ai 5 which was a um reinforcement learning agent using po so policy gradient method i believe uh trained on dota 2 so video game and out competed the top human competitors in that case so reinforcement learning is quite powerful u when implemented correctly so there's even been statements saying right these are by some of the original authors of the deq paper and some of the original creators of a lot of the reinforcement learning algorithms richard sutton who go as far as to say that reinforcement learning is all you need in terms of machine learning and this paper reward is enough uh which which i recommend you having a read of if you have the chance and they're basically just saying here that given a suitable reward function you can train an rl agent to do anything so you don't need any other methods you just need a suitable reward function and you can theoretically ch train an agent to do anything and they go,358,2,2,WxjEZmIiRQU
33,on and say that's how we learn that's how intelligence systems learn and that's how they should learn and that's how you get to something like agi um however we are yet unable to to know how to create this optimal reward function and as we'll see there's a few uh problems with reinforcement learning at the moment that are some of which are have been improved upon but not yet completely solved so i'm going to close out this video with going through some of the problems issues um and further work that's been done in reinforcement learning that you might be interested in so reinforcement learning deep reinforcement learning sounds like the perfect solutions to many problems however there are still a lot of problems as i've mentioned um deep reinforcement l doesn't work yet this blog post here by alex is really really good just outlining a lot of the academic research and open uh work that's been done showing the issues with reinforcement learning some of which we'll talk about here but that goes into a lot of depth about it so this uh meme is very apt if you can get it to work it works really well but the scope is actually far more limited than some will hope leed to believe so training time as i've sort of alluded to already deep reinforcement learning agents need a lot of experiences with the environment that dota 2 plane rl agent which was the open ai5 so was basically five copies of it uh was trained with 10 or at least the initial one i'll uh be specific the initial one,358,2,2,WxjEZmIiRQU
34,"was trained with over 10,000 years of game experience so it would take a human player playing for 10,000 years of dota 2 to get as good um as it was right so you can imagine a human does not take 10,000 years to get good at dota too but our rl agents take 10,000 years of game experience to um get that good so how do they actually get that well they had many many many many versions of the agent playing the same game at the same time and collected that experience and did some complicated algorithms to train and consolidate many different agents into one super agent so 10,000 years sure we can run multiple games at the same time but we can't run the world multiple times right and we can't run the real world any faster than you know a second per second so you could have a really fast accurate simulation of the real world training that and try and transfer to a robot however um there is quite big problem in that uh you're never going to get the simulation really really accurate and so the generalization problem for reinforcement learning and deep reinforcement learning is quite large a big problem that you may have noticed in pretty much every description i've given of reinforcement learning training procedures is that we train and evaluate our model in the exact same environment right if you experience with machine learning and deep learning you know that we always have a withheld test set that we don't train our agent with to see how well our model has learned this solution how",358,2,2,WxjEZmIiRQU
35,"will it's actually generalized how will it actually understands what's it's doing we don't tend to really do that with reinforcement learning and so um a lot of our agents in a lot of situations are actually just overfitting for a particular environment in some situations that might be fine because we don't actually train our agents with ground truth um overfitting and working out the circum of actions to take might be the objective and so a little and so overfitting might be fine um but if you're trying to say train a robot to um come up with a solution to some task or to perform some task in a changing environment then that's going to be a big problem then you're going to have to end up training for 10,000 years in every s sort of possible permutation and variation of your environment so a lot of recent research not so recent now it's a bit old now has shown that deep rl agents are really really bad at generalizing to even slightly different environments even though the task is the same the uh structure the rules and of the game everything is exactly the same you might have slight changes such as the background color slight texture changes and that just basically takes your agent from performing perfectly performing completely randomly so uh open ai i believe yes open ai released some time ago now a paper um initially launching one environment called coin run which was basically this game on coin run here which was basically a procedurally generated so proen environments where you had theoretically an infinite number of permutations",358,2,2,WxjEZmIiRQU
36,"of the same game so you can think a platform game but you have an infinite number of levels so they're just randomly generated levels and the textures and colors and backgrounds and the enemies textures and shapes and locations changes for every level um and using this environment and then subsequently the entire proc gen benchmark which was 16 different games specially generated levels as well they showed how you don't train with enough levels then um your agent does not actually generalize to any level so you can see here for coin run you needed to play with at least 10,000 different variations of the same game in order for the test train performances to converge so and some of them was even more so 10,000 you know close to 100,000 for the maze game you know less for some other games so you need a huge amount of variation and diversity when training your rl agent like training any neural network in order for it to perform well the problem is if you're trying to train an agent in the real world getting that amount of variation um is hard and if you're training in a simulation making sure the simulation doesn't have any um sort of problems or unrealistic elements that when you transfer to the real world you're not just going to have a sudden collapse of your performance of your model right so even just a small thing such as the texture not being quite right can just completely ruin your agent's performance so some other problems rl agents and rl algorithms especially in deep learning can be very sensitive",358,2,2,WxjEZmIiRQU
37,to as we said random initialization hyper parameters environment experiences um it's often joked that the random initial ization of a rl agent is itself a hyperparameter so maybe you'll run your training of your rl agent on a particular environment everything is exactly the same you run it once does really well it converges to a really nice performance you run it a second time from scratch and it does terribly um so the random seed of your actual um model weights is like a hyper parameter for your training um you can see in even this here for procgen you you can see for some of them the sort of shaded areas are sort of the um the variance so you can see the variance is quite large some of them they running the exact same agents you can see the plots here showing the different score range so running the exact same code just with a different initialization can get you very different results unlike something like classification and regression with neural networks during training agents can completely collapse just unstable the deep reinforcement learning we saw our q network would just randomly start to reduce performance infrastructure for training is usually more complicated the actual algorithm so i kind of talked about this before wrapping up your environment into something that can be trained with reinforcement learning is actually a whole effort itself so a lot of the work for reinforcement learning is uh or the advancement is done by large companies who have basically the resources developers engineers to actually construct the environment in a way that you can actually train,358,2,2,WxjEZmIiRQU
38,uh again for example to use the open air doa 2 the agent did not actually get just the screen um it basically got some visual information but it got some game information as well that a normal player wouldn't get they tried to make it fair um in terms of specifically what it got but it wasn't just screen information um it's basically just simplify it okay other problems uh reward signal so going back to that um article reward is enough it can be very very difficult to define a good reward signal one that again we don't want to hand label every single state but one that gets across to our agent again going back to the very very start of the first rl basics video the reward function the reward signal is how we encourage our agent to get the task done it's how we communicate what the task is is and coming up with a reward function that actually accurately uh sort of gets that across um is very difficult right optimizing your agent to get a high reward you assume may correlate to achieving some task but it's actually very hard to get those two things aligned and you can end up with misaligned agents so this is a very um bit old now very very well-known video of an rl agent playing coast runners which is just a sort of boat racing game um and it gets points for how fast it completes or um the actual lap and also gets points and rewards for hitting these little turbo power boosters so what it's leared or what the agents,358,2,2,WxjEZmIiRQU
39,learned is that sure i could complete the race but if i just spin around in circles as we'll see in a second these turbo reward boosters uh it gets rewards for they respawn and so it can just keep spinning around and around around and around in circles and just collect those turbo boosters uh over and over again and not actually bother completing the race so you've said maximize um the number of rewards the speed time and also collect the turbos well it's just found that collecting the turbos is the easier thing to do so it's just going to do that so reward hacking is we call that okay so finally the very last thing i just want to talk about briefly is ai safety so this gets talked about a lot in uh in ai in general um it's kind of a more in some ways obvious issue in reinforcement learning things like misaligned agents and reward hacking it's more obvious to see how if you don't structure your reward signal correctly you can get an agent that is trying to do something to maximize the rewards but the solutions come up with isn't exactly what you want it to do and you have that misaligned agent and in fact some have said it's impossible to actually create a reward function that is only interpretable one way so there's only one way to maximize that reward and you know you assume it's the way you thought of but maybe your agent comes up with a different way so the field of ai safety tries to come up with potential ways in,358,2,2,WxjEZmIiRQU
40,which ai systems can avoid doing what we want changing its own own goals becoming misaligned so ai safety grid worlds is an environment that sort of tries to quantify that so becoming misaligned potentially changing how it gets rewards changing what rewards are itself and maximizing some alternative objective so that's all i've got for this video hopefully that was interesting to you you've got a bit of an insight into reinforcement learning if you'd like to see more on some more advanced or specific deep learning algorithms deep reinforcement learning algorithms let me know and i can go into more depth there but for now that's it and i'll see you next time,149,2,2,WxjEZmIiRQU
0,what's going on everybody and welcome to part five of the reinforcement machine learning series in this video in the subsequent videos we're gonna be talking about deep q learning or dq ends or deep q networks and to start the prerequisites if you don't know deep learning stop pump the brakes you got to learn deep learning so if you want you can go to the homepage of python programming dinette click on the machine learning here and then do this deep learning basics with python x flow and chaos at least do like the first two the first two tutorials i want to say yeah getting it loading in your data probably the first three especially since we're using confidence do the first three and then come back to this cuz otherwise you're gonna be so lost so okay deep cube learning basically it got its start with this the following paper this human level control through deep reinforcement learning if you've ever looked up reinforcement learning first you found that all the tutorials suck and hopefully not this one and then and then you've seen the following image so this is how the deep deep query network or deep q network is gonna learn so what's gonna happen is you've got the this input in this case it's an image you've got some convolutional layers they don't have to be convolutional layers some fully connected layers and you don't have to have those but you've got some sort of deep neural network and then you've got your output layer and your output layer is going to map directly to various actions that you,358,0,0,t3fbETsIBCY
1,could take and it's gonna do so with a linear output so it's a regression model with many outputs not just one now some people did try to do just one output per se so it basically to have a model per possible action but it doesn't really work well and that's super gonna take a really long time to train so anyways here's another example oh it's beautiful really beautiful you got input values so again it doesn't have to be an image you know this could be delta x delta y for the food delta x delta y for the enemy for example that could be your input boom then you've got your hidden layers again they could be convolution they could be dense they could be current whatever you want there and then here you've got your output again with the linear activation so it's just gonna output these scalar values well these are your q values they map directly to the various actions you could take so in this case let's say this was your output you would take the arg max well the bard map or the max value here is 9.5 and the values are you know you're basically if we were to map this and get the index values it would be 0 1 2 3 so arg max would be 1 so whatever action 1 is that would be the move that we would make ok so we've replaced this cue table with a deep neural network the benefit here is we can learn way more complex environments first of all we can learn more complex environments just,358,0,0,t3fbETsIBCY
2,because a deep neural network is capable of actually mapping that also a deep neural network can take actions that it's never seen before so with q learning if a certain scenario presented itself and it was outside of any of the discrete combinations that we've ever seen well it's gonna take a random action so it got initialized randomly a deep neural network on the other hand is not it can actually recognize things that are similar but it's never seen this thing before and it can act accordingly so first of all a deep neural network is going to do better in that case it so in that way it can solve for way more complex environments but also as we saw as you just even barelly increase that that discrete size of your q table the amount of memory that's required to maintain that cute a fool just just explodes right and it both in in terms of your observation space size or your discrete observation space size but also in your actions so in our case up to this point we've only taken four i'd like to introduce a few more actions moving forward you know so what we're gonna do is we're gonna keep the diagonal moves but also allow for just straight cardinal like up-down left-right as well as don't move so so we're gonna introduce all of those as well anyway so so for those two reasons networks are way better the downside is normal networks are kind of finicky so we've got a we're gonna have to handle for a lot of things that are finicky about neural,358,0,0,t3fbETsIBCY
3,networks also it's gonna take a lot longer to train so on an identical model or an identical environment like in the blob and where it took our cue table minutes to fully populate basically it's just a brute-force operation basically so if it's small enough you know your cpu can handle it where it took you know minutes for a cue table it's gonna take hours for cue learning but the benefit is where it takes i'm sorry it's gonna take hours for deep cue learning but the benefit here is for certain environments where it takes yes a long time like weeks for deep cue learning to learn an environment it would require you know petabytes of memory for a cute able to figure it out okay so so that's that there's your difference so really they're gonna solve different types of environments and honestly q-learning is pretty much useless you can use it for cool novel little niche things for sure but you're not gonna find too much use for it in the real world i don't think we're as deep q-learning you can actually start to apply it to really cool things so anyways enough jibber-jabber the last concept i want to do before we get into a actually writing code is right here this learned value change so before the new cue function basically was this whole thing where as the neural network kind of solves for this like learning rate and all that and just updating values this is solved through back propagation and fancy things but we still want to retain the future values because normal networks don't care,358,0,0,t3fbETsIBCY
4,they don't give a heck about future they care about right now what is this thing what is this exact value they don't care about well what does this chain of events do other than a recurrent neural network i suppose but really a recurrent neural network cares about the history not the future so anyways in this case we still want to retain this so we are still going to use this and basically the way we're going to do this is every step this agent takes we still need update q value so what we're going to do is query for the q-value we take that action or a random one depending on epsilon then we know we re sample the environment figure out what our you know what the next reward would be your and then we can calculate this new q value and then do a fit operation so people who are familiar with the neural networks are already like wow that's a lot of fits yep sure is also that's one fit at a time so as you're gonna see when we go to write this code we actually have have to handle for that as well because that would make for a very unstable neural network so there's two ways that we're handling for that but with that i think we're just gonna jump in the code because i think it'll make more sense use code coding it and covering it as we get to those points okay so hopefully that was enough information like i said all the tutorials have ever seen on deep hue learning have been terrible,358,0,0,t3fbETsIBCY
5,and there's like so much information that's left out that you know to get the overarching concept honestly this picture is enough but then when it comes time to actually look at code and how it really will work like can you sit down and code it after you read someone's tutorial my hope is you really can after this one but otherwise i don't think a tutorial exists for doing it so so anyways let's get started so the first thing we're going to do is i'm going to create we're gonna at least hopefully code our agent at least the model and talk about some of the parameters and then probably in the next tutorial we'll do fitment and or training basically and all that so anyway class dqn agents and this agents let's just do define create model first this should be fairly basic it's just going to be a continent we don't have to do a continent i'm just gonna do one just so you can more easily translate this to something else so the first thing you should do at any time you learn something is go try to apply it to something else so try it and then you'll mediate because you it might make sense to you as you're listening to me talk about it but then you go to try to apply it and then suddenly it doesn't work or you're confused or you realize oh i don't really actually know how to do this so anyway the first thing you should do try to apply it someone complained recently about my drinking on camera saying they don't,358,0,0,t3fbETsIBCY
6,want to hear me gulp i'm drinking because my mouth is dry and the most annoying thing ever is listening to someone talk with a dry mouth so you're welcome jerk so anyway create model okay so we'll start with model equals a sequential model and now let's go ahead and make some imports so the first thing that i need to bring in is from careless models let's import import sequential and then we're gonna say from charis layers let's import dense drop out conv to d-max pool to d activation activation flatten i think that's everything sorry that ran over my face my bad anyway there we go dense dropout comp doody doody max pull to d and actually that's max pooling to d activation and flatten okay those are all of the imports also that you can always go to the text-based version as you toriel's if code runs over my ugly mug you can check those at office all actually i don't even know maybe at the very end it'll be there anyway by the time you need it it'll be there anyways okay so we've got that stuff the other thing i'll go ahead and import to is from kaos duck call back so let's import tensor board we need other things but i want to cover them when we get there so model is equal to a sequential model then we're going to say model dot add and then we're gonna start adding conv to capital d let me fix that as well cool calm 2d which will give us 256 convolutions and the convolution window will be a 3x3,358,0,0,t3fbETsIBCY
7,and then input underscore shape and this is going to be equal to it will say n dot observation space values and then close this off and this isn't quite yet exist we have to create the environment so a little uncertain if i'm going to actually rewrite the environment so we it's converted to object-oriented programming now i may not actually like we might just copy and paste that the updated environment i'll just talk about it because otherwise it'll take like an hour to to go for that so anyways but we will this will exist at some point in the near future then once we've done that i guess let me zoom out a little bit since we're running a space here the next thing that we're gonna do is we'll say model dot add so it's our conf layer we're gonna add an activation and the activation here we'll use rectified linear and then model dot add max pooling to d then we'll use a two by two window and again if you don't know what max pooling is or convolutions check out that basics tutorial because i cover it and i also have beautifully drawn photos if you really like my other photo you'll love that those photos then after the max polling we're just a model that and and we're gonna add a drop out layer and we'll drop out 20 and then we're just gonna do the same thing again so this will be two by 256 so copy-pasta we don't need to include the environment shape again or the input shape rather this so two by two fifty-six,358,0,0,t3fbETsIBCY
8,then we're gonna say model dot add we're gonna do a flattened here so we can pass it through dense layers and then we'll say model dot ad we'll throw in a dense sixty-four layer and then finally model dot add a dense layer and it'll be m dot action space size and then the activation activation will be linear and then model dot compile and we'll say loss is mse for mean squared error optimizer will be the atom optimizer with a learning rate of 0.001 and then metrics we will track accuracy okay so that is our model again sample code is in there will be a link in the description to the sample code so if you've missed anything you can check that out okay so and then atom we don't actually have atom and ported so let's go ahead and grab that as well so from charis dots optimizers import atom just for the record to if anybody's watching this in the future this is still tensorflow like 1.15 i don't really know actually version i'm on but it's not 10 serve low to yet so keep that in mind so some things might change by then and if it has changed check the comments and then when it finally actually does truly matter what version i'm tensorflow i'm on i will let you guys know it's also kind of expected that you guys will know how to install tensorflow and karos i'm not gonna cover that again for that go to the basics tutorial so anyway go model compiled okay then we will return that model okay so that's our model,358,0,0,t3fbETsIBCY
9,then define in it so and then now we're gonna do the init method for this model so we're gonna say self dot model equals create underscore model and then so that is going what is your problem why are you unhappy what what the heck am i missing oh self dot karima more coffee is definitely necessary okay self dot model is create model so that is that's gonna be our main model but then we need what's going to be called our target model what the heck's at dec so let's write that first so self dot will call this target model equals self dot create model but then we're gonna say self dot target model dot set underscore weights and we want to set the weights to be exactly the same as self dot model get underscore weights okay so what's going on here so we are gonna wind up having two models here and the reason why we want to do that is kind of there's there's a few reasons but mostly it's because this model is gonna be going crazy so first of all we initial the model itself is initialized randomly as all neural networks are but we also are gonna initialize with an epsilon likely of up one so though the agent is also going to be taking random actions meaningless so initially this model is going to be trying to fit to a whole bunch of random and that's gonna be useless and but eventually as as its explored as it's gotten rewards it's going to start to hopefully figure something out but the problem is we're doing,358,0,0,t3fbETsIBCY
10,a dot predict for every single step this agent takes and what we want to have is some sort of consistency in those dot predicts that were doing because besides doing a dot predict every single step we're also doing a dot fit every single step so this model especially initially is just going to be like all over the place as its attempting to figure things out randomly so the way we're going to compensate for that is we're gonna have two models so we've got self dot model this is the model that we're not fitting every step and then we have self dot target model this model we're this is the one that we're doing a dot predict every step and i would even say this is this is what we not predict against every step and then this is the one that gets you know gets trained every step make note of that because you'll probably forget so so so then what happens is after every some number of steps or episodes or whatever you'll re-up date your target model so you'll just set the weights to be equal to to model again so this just keeps a little bit of sanity in your predictions right so in the predictions that you're making this is how you'll have a little bit of stability and consistency so your model can can actually learn something because it's just gonna be there's so much randomness it's gonna be very challenging initially and so long as you're doing things randomly okay so that's one way we are handling for the chaos that is about to ensue,358,0,0,t3fbETsIBCY
11,"next we're going to have self dot replay underscore memory and that is going to be a dequeue or day q i don't i always forget how to pronounce that to use that we're gonna say from collection collections import dequeue and if you don't know a dik-dik you is it is a set length think of it as ikat an array or a list that you can say i want this list to be sighs so we're gonna say max len equals replay memory size and let's go ahead and just set that real quick we'll just say boom and we'll set this to 50,000 also a cool little trick i recently learned was you can use underscores in place of commas so so python sees this as 50,000 but it's a little more human readable like this number is a little harder but then especially like once you have like a number like this the underscore suddenly becomes very useful so cool so don't do a plus though underscore cool so that's our replay memory size so what the heck is this so the replay memory so we talked about how we keep the predictions so how we keep the agent taking consistent ish so at least the agent is taking steps consistent with with the model over time okay try to think of a great way to word that but so we've got the prediction consistently consistency sort of under control it's still gonna be crazy it's at the start period but this is that we've got that settled now we need to handle for the fitment craziness that is going to ensue",358,0,0,t3fbETsIBCY
12,because because we saw we have you know we've got two models so we've slowed down one one way of working with that model but the other ones still gonna be crazy so self dot model it's still gonna be dot fit every single step but not only is it getting a dot fit every single step it's getting a dot fit of one value every single step so again assuming you're not new to normal networks you know that we trained neural networks with a batch and and part of the reason why we do that is one it's quicker but it also tends to have better results not necessarily the higher the batch you use at some point the batch size gets too big but to have some sort of batch size used it generally will result in a more stable model and just learning over time so if you just fit with one thing it's gonna adjust all those weights in accordance to that one thing and it's gonna get to the new thing do another fit and it's going to adjust all the weights in accordance to that one thing as opposed to if you throw in a batch of 60 four things it's gonna adjust all the weights in accordance to all 64 things so it's not going to overfit to one sample and then go to the next sample over fit to that sample then go to another set you know i'm sam so so we want a handle for that too and the way that we do that is we have this replay memory so we take this replay,358,0,0,t3fbETsIBCY
13,"memory and that can be let's say 50,000 steps okay and then we take a random sampling of those 50,000 steps and that's the batch that we're gonna feed off of so train the neural network off of so that's how we're getting stability in both the training network that's getting updated every single step but hopefully we're smoothing that out a little bit and then we're smoothing out the predictions because we're also not allowing the prediction model to be updated every step instead it's being updated let's say every five episodes or whatever we intend to go with they these are all hyper parameters or really constants that we're gonna set at the top of our program okay replay memory okay so the next thing mmm is gonna be self dot tensor tensor board and that's going to be equal to modified tensor board yeah we're gonna set log underscore dirt to be logs var - var and we're gonna make this an f string and we're going to import import time if you're want to say 3.5 or younger python you can't do f strings you'll have to do a dot format or whatever and so for the logs we're gonna say model underscore name and then we're gonna say int time dot time this is just so we can kind of keep track of what's what and then the next thing i want to do i thought it was gonna piss you at me for this anyway let's go ahead and set model name before i forget and for now just it's whatever you want i'm gonna say it 256 by -",358,0,0,t3fbETsIBCY
14,you might try a 32 by two and a 64 by 2 it anyways i'm by 256 by - i'm een the neural network it's a 256 by 2 content so we've got tensor board done obviously this doesn't yet exist i'll talk about that in a moment and then finally self dot target update counter equals zero so we're gonna use this to track internally when we are ready to update that target model so this you know this thing here that we've explained why we want to do that so the next thing we want is i'm not gonna write out that so go to the text-based version of 0 and we're gonna grab this updated this 10th this updated tensor board class did we already pull i think we already pulled in tensor board but what i really want is this right here this whole class i'm like oops i want that comment too i'm gonna copy that i don't really see any benefit for us writing that but i'll explain why we want that here in a moment paste that in cool so what this is doing is it's just modifying the tensor board functionality from tensor flow and karos so by default karis wants to update this this tensor board file this log file every time we do a dot fit well we're doing a dot fit uh you know up you know about 200 times in episode and then we're gonna do like 25 thousand episodes so first of all i'm sorry i think i use the word update log file it wants to create a new log file per,358,0,0,t3fbETsIBCY
15,dot fit well we don't want that we want just one log file so so having this is gonna solve quite a few things as was written by daniel and though basically one the i o was taking a lot of time so just simply writing the new file was sometimes taking longer than a dot fit but also we've got all these extra files each one was 200 kilobytes not necessary and yeah so just a bunch of it was just not constructive to do this i guess that's the behavior because most neural networks are just gonna fit once on sort of cycle and it's not a big deal but for deep queue learning where you're fitting just constantly if something needed to be done okay so that is our modified tensor board now what we're gonna do is go back to our dq n agent so that's our init method let me just kind of clean this up a little bit i want these kind of separated out i don't want that separated out i don't know i can't really do that what i want so i'll just leave that so then create model let's go ahead let me see if i go hopefully well we're not gonna finish this but let's add in one more method so we're gonna say define and in this one we're gonna say update underscore replay underscore memory self and then transition train transition can we type so then we're gonna say self dot replay underscore memory dot append trant wow i really want to call it transition transition so transition is observation is just going to,358,0,0,t3fbETsIBCY
16,be your observation space action reward new observation space and then whether or not it was done so we need to do that so we can do that new cue formula so that's that's all we're doing there you'll hopefully that'll probably make more sense when we actually get to actually use self dot update replay memory or agent update replay memory finally yeah we'll do we'll do this one last method and then we'll save train for the next next tutorial so that's gonna take a while so define get cues so we will get q self terminal terminal state and then step and then so this will be returned a self dot model underscore predict and we're gonna say the numpy array i think we've imported numpy we'll grab that soon of the state dot reshape negative one and then we're gonna say asterisk state dot shape all that's doing is unpacking state and oh did i i sure did terminal state that's for train right now we're just passing state here so state shape and and then we're gonna do div whoops div by 255 and then zero so model dapper ticket always returns a list even though in this case we're really only predicting against one element it's still going to be a list of one element urn array of one element so we still want the zero with element there div by 255 is because we're passing in this rgb image data so you can very easily normalize that data by just simply dividing by 255 okay so that's it let's go ahead and go up to the very top here,358,0,0,t3fbETsIBCY
17,let me import numpy as np so let's say import numpy as in b let me go ahead and save that and i think i'm gonna save train for the next tutorial because that's gonna take a while so quick shoutout to the recent we're not recent actually these are my long-term channel members people who have been sticking around for a while reginald roberts seven months luiz fernando seven months stefan our scoot harwood eight months ampere hammer eight months ever william sam's nine months and abbaji ten months that's quite a while you guys thank you guys all for your support and if you guys have any questions comments concerns whatever feel free to leave them below if anything up to this point is confusing or you want to get a little more information again comments below or come join us in discord gg's slash syntax if you think i've done something wrong or said something wrong i'm sure i have feel free to let me know and yeah i am hoping that you guys can get through this tutorial series here and actually be able to use this stuff i think right now i just like i said i could not find a tutorial or a information to actually do this kind of stuff that actually made sense or actually explained everything so just a lot of copying and pasting going on so my hope is that you guys can actually get through this and know how to do this yourself so that's my goal so if you're having trouble getting to that point let me know because i'll i need to fix,358,0,0,t3fbETsIBCY
18,that so anyways i will see you guys in the next video where we will do the train method and then we might only do the train method but hopefully we can do the environment i'm thinking i might just copy the environment we like we we made some tweaks to the environment and we made that object-oriented and so we mmmm i might just like copy that because the the environment honestly has not changed we just made an object-oriented just to make it easier so you know you know there's like little tweaks made i think i'm just going to talk about the tweaks that were made and that'd be it that way hopefully in maybe in the next tutorial we can actually do the train method here talk about that deep q learning stuff and hopefully get a model training by the next video so i'll think about that you can leave your opinion below but probably by the time you've seen this i will have already made my decision so anyway yeah till next time everybody,234,0,0,t3fbETsIBCY
0,i'm brian thornbury and today i'm going to teach you how to code the algorithm deep cue learning released by deepmind in their 2015 paper human level control through reinforcement learning this video is part of a series where i'm going to teach you how to code the more advanced algorithm rainbow released in 2017.,71,0,0,NP8pXZdU-5U
1,this video is primarily intended to be technical so we can understand the algorithm and implement it in code also i highly recommend you have experience using both python libraries numpy and pytorch which is what we're going to be using to implement deep cue learning today however this is not a requirement because technically you could just copy the code that i'm writing and have a working implementation of deep cue learning to get the most out of this video i highly recommend you take the time to understand what's going on in the code pause if necessary go read or do little experiments so you can understand what each part of the code is doing okay before we dive into the code let's do a brief overview of the paper and algorithm here we have the paper human level control through deep reinforcement learning released by deepmind in 2015 this paper introduced the deep q learning algorithm we're going to go over a few different parts of this paper but before we do let's take a moment to understand what is q learning i found this great graphic courtesy of analyticsvideo.com that gives a basic overview of what we're talking about q-learning was an algorithm invented sometime in the 90s and up until recently actually up until deepmind releases paper people believe that it did not work properly with neural networks so deep q learning is the fusing of q learning with neural networks to estimate what's called the q function the q function is very simple it takes in any given state of the environment and returns what's called the q value for,358,1,1,NP8pXZdU-5U
2,each possible action the agent can take in that environment the q value is the expected reward from all future states in that environment given that we take that action to understand what that means let's take a look at how the neural network interacts with the rest of the reinforcement learning system so the neural net predicts the q value for each particular action given the state in this case there's three different actions that we can take in this environment speed up no change speed down we use what's called an epsilon greedy policy to select an action with the highest q value or sometimes just a completely random action to facilitate exploration and learning about the environment we take the action by sending it to in this case the simulator and the simulator updates its state based on that action and returns a reward you can think of this as the score in a game but in practice it can be many different things at the same time the simulator returns a new state that the neural network takes back in and repredicts the new q values and the loop begins all over again now how the neural network updates its parameters based on this reward is the crux of the q learning algorithm and what we're going to go over in the paper okay back to the paper now we're going to briefly go over this we're not going to take the time to go through the whole paper i'm going to go through a couple of the highlighted portions here that i think are important and then we're going to take,358,1,1,NP8pXZdU-5U
3,some time to understand the algorithm before we code it so right here this is the core equation of q learning this is how it learns from the rewards returned by the environment if this looks like greek to you don't worry about that it did the first time i ever saw it too and we don't need to really understand this to implement the algorithm so let's go ahead and go down and keep going so right here what i want to show is a couple different examples of why it's important to actually read the paper before you implement these algorithms so right here you can see that they detail the neural network architecture that they use additionally you can see that they clipped all positive rewards at one in all negative rewards at negative one and they also use frame skipping so that the agent only sees and interacts with every four frames these are just a couple examples of the many different implementation details in this paper we are actually not going to use this neural network architecture or a lot of these implementation details in our implementation because we are not going to be training the agent to play atari games in this video at least scrolling down we come to the most important part of the paper training algorithm for deep queue networks this section gives us all the details we need to actually implement the algorithm before we dive into the algorithm let's take a moment to examine this part right here where they talk about how they equip the error term between 1 and negative 1.,356,1,1,NP8pXZdU-5U
4,exactly what they say they do but if we look at newer implementations of the algorithm such as the one from openai baselines we can see that they use a huber loss which changes from the squared error to the absolute error when loss is greater than or less than negative one in our implementation we're going to use the huber loss i've tested it and it works well this is just an example of how papers can be ambiguous and interpreted differently so it's important to do your research if something strikes you as ambiguous now onto the algorithm i'm going to explain what each line means right now and then we're going to dive straight into the code first we initialize the replay buffer to its full capacity with random experiences then we initialize what's called the online q function followed by the target q function the differences between these will come more apparent in a moment then we step into our loop reset our environment to get our initial raw state and then pre-process that state with our environment wrappers we actually won't need to do any pre-processing in our implementation since we won't be training on atari games then we step into our core training loop we select the action with the highest q value given the current state or with probability epsilon select completely random action this is the epsilon greedy policy i mentioned earlier just a reminder all this will become more clear once we start coding it then we send that action to the environment which returns a reward in a new state we store that state inside of,358,3,3,NP8pXZdU-5U
5,the replay buffer and we're actually going to need to add an additional element to this tuple here telling us whether or not it's a terminal state at this point we get a random mini batch of transitions from the replay buffer we compute our targets based on those transitions and then perform the gradient descent step on those targets and right here where it says squared this is another great example of where papers can be ambiguous since they don't actually include the air clipping inside of this algorithm so as we implement it it will actually be slightly different since we won't be squaring the error term then every c steps we set the target network parameters to the online network parameters the online network parameters are what have been being updated by the gradient descent step okay now let's dive straight into the code you see i'm using pycharm you can go ahead and use whatever ide you're comfortable with although i highly recommend pycharm but in the past i used to use vs code and it worked just fine so i already set up my project here since it took a little bit to install the dependencies but the primary thing you're going to want to do is create a virtual environment and then you're going to want to run pip install torch gym and that's going to go ahead and install pi torch and open ai gym which are our two dependencies for this project now as you can see i already have them installed in mine so we're going to go ahead and get started create a new file dqn,358,3,3,NP8pXZdU-5U
6,dot pi okay so just to start off we're going to create a few hyper parameters here okay now that we have those typed up let me quickly explain what each of these are gamma is our discount rate for computing our temporal difference target and we'll see what that is a little bit later in the code batch size this is how many transitions we're going to sample from the replay buffer when we're computing our gradients buffer size this is the maximum number of transitions we're going to store before overwriting old transitions min replay size this is going to be how many transitions we want in the replay buffer before we start computing gradients and doing training epsilon start this is going to be the starting value of our epsilon this is going to be the ending value of our epsilon and this is the decay period which the epsilon will linearly anneal from this value to this value over this many steps target update frequency is the number of steps where we set the target parameters equal to the online parameters now we're going to see exactly what all that means as we get a little further in the code now before we dive into the rest of it let's get our imports typed out okay so if you're following along a video just go ahead and pause it and get those imports typed out and we're going to continue on here so the very first thing that we need to do is create our environment and we're going to be using the cartpole environment because it's an environment where we can,358,3,3,NP8pXZdU-5U
7,iterate quickly and find out whether or not we have a correct implementation of q of deep q learning okay so we create our replay buffer which is just the deck standard deck from python the max length buffer size and we're also going to create a reward buffer where we store the rewards earned by our agent in a single episode and we do this to track pretty much the improvement of the agent as it trains let's keep track of the reward for this specific episode and if you guys are familiar with jim then you'll know that there are definitely better ways to do this with the monitor but we are pretty much doing a vanilla implementation here without too many fancy things okay for the next step we need to come back up here and create our network class and in pytorch you do that by creating a class which inherits from an nn.module and we initialize music the class calling the superclass we compute the number of inputs to this network by taking the product of the observation space shape now in this particular environment cartpole taking the product is actually not necessary since we already have a one-dimensional space but in other environments such as the atari games this is going to be a three-dimensional shape for the images or in other environments possibly even more dimensions so that's why we have this generalized product here so this is essentially how many neurons are in the input layer of our neural network the rest of the network we're going to use a standard two layer sequential linear network with 64,358,3,3,NP8pXZdU-5U
8,hidden units right there so separated by a tan h lin non-linearity and the number of output units in our network is going to be equal to the number of possible actions that we can or that the agent can take inside of that environment which is this env action space in now quick note here cue learning can only be used with action spaces that have a finite number of actions called a discrete action space other environments with continuous action spaces require different algorithms okay we do our forward function which every single nn.module is required to implement and we're going to come we're going to just put this placeholder here for now we're going to come back and fill it in in a minute and that's the act function we'll see what that does in a moment okay now we can come and create our networks we're going to create our online net and our target net and i forgot to pass the env in here now that's good okay now we set the target net parameters equal to the online network parameters and they both have been initialized differently so that's why we have to do this and this is part of the algorithm as specified in the paper so now the first step is to initialize our replay buffer and what we're gonna do remember that constant min replay size we are going to put that many transitions inside of the replay buffer before we even start so first thing we reset our environment get our first observation we loop min replay size times and then we select a random action,358,3,3,NP8pXZdU-5U
9,by calling the action space dot sample method looks like i made a little mistake here i forgot the end next thing we need to step the environment with that action and as a result we get out the new observation which is the state of the environment in case i didn't explain that a moment ago and we get out this other information which is the reward returned by the environment for that action whether or not the environment episode is done and the environment needs to be reset and the info dictionary which we're not going to use at all so we'll just go ahead and replace that with the underscore now we create our transition tuple which is the obs the action the reward done and the new observation and we stick that inside of our replay buffer just like that so now we have that experience inside of our replay buffer which we can use to train on later never forget set obs equal to the new obs and if the environment needs to be reset reset it and always get the new ops out of that reset okay now we're getting to the exciting part which is the main training loop do the same thing here reset our environment get the observation and we're going to count which step we're on this is pretty much like a wow true loop but instead we get the count using this itertools.count method okay so the first thing we need to do here is select an action to take in the environment but remember we're using the epsilon greedy policy so we need to,358,3,3,NP8pXZdU-5U
10,compute epsilon for this step since it interpolates between the epsilon start and epsilon end values over epsilon decay steps and numpy has this really handy function np dot interp which we can use to do just that and what this is going to do this is going to start out at epsilon start and end up at epsilon end after epsilon decay steps and for every step after that it will still be epsilon end okay so remember this is 1.0 and this is 0.02 so that means we're going to go from 100 random actions to 2 random actions and this is done to facilitate exploration by the agent in the environment so we get a random sample if the random sample is less than or equal to epsilon then we take a random action using the axis space.sample method once again however if it's not less than equal to epsilon we need to intelligently select an action using our network so we're going to call onlynet.act we're going to pass in the observation here so we're going to go back and fill that in in just a moment before we do let's go ahead and copy this entire code right here from the initialize replay buffer section and put that right here since it's exactly the same remember what we're doing we're stepping the environment we're getting all the new data the reward whether or not it's done the new observation setting up the transition tuple putting that in the replay buffer always setting obs to new obs and resetting the environment if it needs to be reset but we need to do,358,3,3,NP8pXZdU-5U
11,a couple other things here besides resetting the environment we need to append the episode reward to the reward buffer and we need to reset the episode reward which reminds us that we need to come right up here and add the reward for this step to the episode reward now let's go back up and fill in that act function before we move on any further first thing we need to do we need to turn obs into a pi torch tensor just like this called torch dot as tensor make sure to pass in the d type and i always put this underscore t so i know that this is a torch tensor next we need to compute the q values for this specific observation and a quick note here we have to put the unsqueezed zero and that's because every single operation in pi torch expects a batch dimension and since we're not using any batched environments we don't have a batch dimension so we just create pretty much a fake batch dimension of size one with the unsqueezed zero okay now we have the q values for every single possible action that our agent can take we need to get the action with the highest q value and we do that using torch.argmax just like this and that's it in the action we need to turn this pi torch tensor here into an integer so that's pretty simple we just detach the tensor and then called the item and now we have the action indicee which is just a number between 0 and 1 minus the number of actions and that's what,358,3,3,NP8pXZdU-5U
12,we can return as the action so that's how we select an action in q learning now we move on to the next part come all the way down here and we're going to start our gradient step now i'm going to type this all out real quick and then i'll explain it okay so what we've done here is sample batch size number of random transitions from our replay buffer that we added in earlier and remember the transitions are all structured as this kind of tuple but we need each of these elements individually in their own arrays so that's what we've done here we get each observation and put it in a list from these transitions we sampled and then we call np dot as array and the reason for doing this even though it might seem like a waste as you'll see a little bit later is because pi torch is much faster for making a torch tensor from a numpy array than it is from a python array as i have known from my own experience and mistakes okay now we need to convert each of these into a pi torch tensor it's going to be very simple to do especially if we have multi-line edit go back to the beginning here music or underscore t and one more thing actions is actually an in 64 since it's an index really okay now we have all these pi torch tensors we're ready to start computing our targets for our loss function so first thing we need to get the target q values for the next or the new ops's and this,358,3,3,NP8pXZdU-5U
13,is the first time where we're using the target net now imagine that for each of these new observations we have a set of q values but we need to do is we need to collapse this down to one highest q value per observation and we do that like this okay so let me conceptually explain what this is doing right here we have a set of q values for each observation and we can say that each observation is essentially the batch dimension and the q values are dimension one so what we're telling this is get the maximum value in dimension one and discard all the rest and keep that dimension around even though there's only one value in it and then we do the zero index because max returns a tuple where the first element is the highest values and the second element are the index of those values which is equivalent to argmax now we're ready to compute our targets i'm going to type this out and explain it afterwards now to quickly show what we've done computing the targets essentially computes this function right here from the paper back to the code what we've done is we take the rewards plus gamma times the max target q values except if that particular step is a terminal state then we will zero out this entire portion and it will be only the reward so that's a clever way of computing this piecewise function in one step now we're ready to compute our loss and apply our gradients we get the q values from our online net for the ops and then,358,3,3,NP8pXZdU-5U
14,we get the q values for the action we actually took and let me explain this after i'm done typing it so what we've done here is once again we have a set of q values for each observation except this time we're not getting the max what we're doing is we need to get the q value for the actual action that we took in that transition we do that using this torch.gather method which applies this index in this dimension to all of the elements of this tensor and effectively gives us the currently predicted q value for the action we took at the original time of the transition so now we compute our loss and we use the huber loss like i mentioned before and that is this called smooth l1 loss in pi torch and it's done just like this and by the way the order of these does supposedly seem to matter i haven't really tried switching them around but just a heads up there we do our actual gradient decept descent step first things first we zero out the gradients and it looks like we forgot to create our optimizer so we're going to come back all the way up here right after we create the networks and we're going to create our optimizer so that's tort dot optim dot atom using the atom optimizer we optimize on the online network parameters and we set our learning rate five e negative four and this really should probably be a constant up at the top here but for the moment we're just going to leave it now we're back down here,358,3,3,NP8pXZdU-5U
15,we you call loss.backwards to compute our gradients and optimizer.step to apply those gradients a couple more steps here we need to remember to update the target network every target update frequency steps very simple to do using the modulo and this load state dict function will set the parameters of the target net to the same exact values as those in the online net let's not forget to do our logging so we can see if things are actually improving print out the current step music and we'll print out the average reward from our reward buffer over the last 100 episodes okay now let's go ahead and just do a smoke test i'm just going to run it and see whether or not it seems to be improving it looks like we made a mistake forgot two underscores on the init function here we made another mistake we accidentally typed 65 here instead of 64.,204,3,3,NP8pXZdU-5U
16,these little mistakes are extremely common when you're programming ai and they'll probably happen to you as well just try not to get bogged down by them and it looks like we have another mistake and that's right down here where we are doing the torch.gather and something wrong with the index and i realize that's because we forgot to put the batch dimension on a couple of our arrays here our tensors and notice that we're using unsqueezed negative one instead of unsqueeze zero as we did up here in the act function and the reason for that is because up here there's no batch dimension we only have one observation so we want to pretty much create a fake batch dimension but down here we have already that size number of actions and that effectively is the batch dimension and what we want to do is we want to get each of these values inside of its own dimension that's why we call unsqueezed negative one to add a dimension at the end rather than at the beginning now let's go ahead and run it again and it appears to be going it looks like tough to tell if it's improving we'll let it run for a few steps here and this is when it's always useful to have a baseline when you can go and run someone else's code with the same exact type of parameters running the same exact supposed algorithm and just see what's the pattern of how it improves or doesn't improve and it's extremely valuable because while you're sitting here watching this you can get these extremely frustrating results,358,4,4,NP8pXZdU-5U
17,where it seems to be improving and then doesn't and in my case once i actually had the first time i implemented dqn i had a tiny bug in the program that would sabotage the results about halfway through the training and it would just go all the way down to zero and it took me about a week to find seven days and that was it was terrible but you know that's part of ai is kind of finding these tiny little bugs and part of getting better okay this appears to be improving just fine if we let this run it will solve the program so you can see here we have a working implementation of deep q learning now i want to go ahead and add in one more component before we let that solve and that's going to be right here and this is a really simple component after it's done solving it we just want to watch it play instead of just staring at the numbers and this is really cool really um rewarding after you know you finally got this working and you can actually see it playing the game so what we do here if we have at least 100 episodes and the mean var reward buffer is at least 195 and the maximum score is 200 in carpool v0 so just to clarify that we just go ahead into this infinite loop here to ignore the rest of the code select an action we ignore most of this but we still need the done step the environment render the environment which is the fun part and remember to,358,4,4,NP8pXZdU-5U
18,reset the environment now let's run it and when it's done i'm going to speed this up here when it's done we're going to get to see it play okay wow look at that it's playing and it seems to be solving the game and we can come and take a look at what it does randomly to see how different it is so if we just go ahead and exit out of this and stop our training here and we just go straight into this loop just for demonstration's sake to see exactly how much it has learned we can see here it just ends the game every single time because once this thing gets up to a certain angle it assumes it has lost so you can see it's it's not doing anything at all it's completely random actually okay and that's that now we have the working implementation of deep q learning trained on the cartpole environment now i want to come back up here and make a quick note before i end and that is about these hyper parameters these hyper parameters are going to be different especially in this variant of deep q learning for every single environment and i got these exact type of parameters from the openai baselines example of their solution of carpool and if you go ahead and look in the paper and you scroll down scroll down they have a list of all the hyper parameters they used in their solutions of the atari games here so that's all i hope you tune in for the next video where we're going to be continuing on with,358,4,4,NP8pXZdU-5U
19,deep cue learning and we're going to take it all the way up until we've implemented the popular rainbow algorithm one of the most sophisticated variants of deep q learning today thank you for watching and making it this far remember to like and subscribe got a lot of great videos and more algorithms besides deep q learning coming even after this have a great day,85,4,4,NP8pXZdU-5U
0,what is going on guys welcome back in this video today we're going to learn how to train a reinforcement learning agent in python using q learning so let us get right into it not music a all right so we're going to do reinforcement learning in python today to be precise we're going to implement q learning in order to teach an agent to play a game without giving any concrete instructions now i want to show you what this looks like in the end so the end result so that you know what we're aiming for uh the game that we're going to play today or that the agent is going to play today is this taxi driver game so we have a taxy we have a customer spawning at one of these random locations here at one of these colored squares we have a hotel at one of the other colored squares and a taxi has to pick up the customer and bring them to the hotel now as you can see this taxi does this successfully but it doesn't do that because we hardcoded or because i hardcoded any logic into it but because of reinforcement learning or q learning so the idea is that the taxi can in the beginning take random actions uh explore the action space in the different states it can be in and then it's rewarded for good actions and not rewarded for bad actions or maybe even punished depending on the game that you're playing and by doing that by just letting the taxi do whatever it wants and rewarding it properly it's going to learn,358,0,0,MSrfaI1gGjI
1,to play the game properly so this is what we do with q learning this is what we're going to implement from scratch in python today all right so let's get started with the coding we're going to implement the q learning process from scratch so we're not going to rely on any external python packages to do this for us we're only going to use numpy to work with aas efficiently but we're going to use the open ai gym package in order to use the already existing environment there so the taxi game instead of coding it ourselves so the open aai package or the gym package will provide uh the animations and the rewards and the observation space so the environment and the current state and also the actions and so on all of this will be done using the gym package but the actual learning the actual q learning process will be implemented from scratch using only numpy so let's get started by opening up a terminal and installing the two packages install numpy and gy and once you have them installed we can start by importing first of all random from core python and also gym and numpy s p all right so what we're going to do first is we're going to initialize the environment and we're going to set a couple of parameters for our q learning process now for those of you who don't know all what reinforcement learning or q learning is the basic idea is you have an agent in this case the taxi driver and you have an environment and the agent can interact with,358,0,0,MSrfaI1gGjI
2,that environment in this case it can um drive left right up down and it can interact with the environment by going to certain fields and if there's a customer the customer will enter the taxi and then the taxi can drive around and hopefully it ends up at the hotel location in this case it gets a reward and the idea is that instead of telling the taxi or the agent what to do with instructions we just let the agent do whatever it wants and we reward good behavior so if the agent accidentally by random behavior ends up taking the customer and bringing them to the hotel uh we reward that with with a reward um and by doing that repeatedly we teach the taxi in all the different states what to do and how to drive uh in order to succeed at the game that's the basic idea and q learning does that with a so-called q table so the q table is a table that contains values for all the actions in all the possible states so you have a certain number of states uh that you can be in this is all states that are possible in the game so all the positions of the taxi all the positions of the customer all the positions of the hotel and for each of these states you have four actions you can take up down left right and for each of these values in all of these states the q table contains um a value that predicts how rewarding this action is now in the beginning obviously the q table is stupid,358,0,0,MSrfaI1gGjI
3,it doesn't know anything so so it's completely random and the values in there are completely random or zero depending on how you initialize it um and by doing by interacting with the environment and getting rewards and by applying a certain formula for q learning uh we're going to update these values so that over time the agent is going to use the values in this table to make decisions and will play the game properly that's the basic idea of q learning for this we're going to say now environment is equal to gym.,124,0,0,MSrfaI1gGjI
4,make and we're going to use the taxi - v3 environment and then we're going to define a couple of parameters here the first parameter and these are parameters that we're going to use in the formula and in the training loop uh the first parameter is going to be alpha which could also be called the learning rate uh it's a number between zero and one and it basically uh determines how much or to what extent new information overrides uh old information now theoretically in a deterministic environment one is the optimal number i will just go for for the sake of having some nuance here i will go with 0.9 basically this means that new information new rewards and updates and so on are going to or new rewards are going to update the value uh way more radically than just keeping the current value so one minus alpha is you could say the inertia keeping the already existing value and alpha is the the uh proportion or the uh percentage of how important new information is so reward and also what you expect to get from the next state so that is our learning rate we're also going to have a gamma the gamma is a so-called discount factor it's basically how important future rewards are if you set this to one you're basically saying that the longterm rewards are as important as the immediate rewards so you have a very long-term focused model uh or agent so i'm going to go here with 0.95 then we're also going to use epsilon epsilon is going to be our randomness or our exploration,358,1,1,MSrfaI1gGjI
5,rate basically meaning that you have the q table and you can either use the q table which is going to be a deterministic result you have a certain q table and you maximize the q value so you want to take the action that has the maximum q value you want to take the action that is the most promising one that would be just applying the q table the knowledge that you already have or you could just take a random action and this has the benefit that you could end up in a space um where you end up having a reward even though you didn't necessarily know that there's going to be a reward you just took random actions and you end up um at this reward so if you set epsilon to one it means that all your actions are going to be always random so it doesn't matter what's part of the q table all of it will be random uh if you set epsilon to zero never will you take a random action so all the actions are going to be based on the q table on the knowledge in the q table now zero makes sense if you have the perfect q table one makes sense if you don't have any information in the q table um what we're going to do is we're going to start at one so all the behavior in the beginning is going to be random um but we're going to decay this epsilon value so we're going to say epsilon decay is going to be equal to 0.9995 and this is going,358,1,1,MSrfaI1gGjI
6,"to be a number that we multiply uh with epsilon to decay it so to make it smaller and smaller over time until we get to a certain minimum value this is going to be our min epsilon we always want to leave room for randomness so even if it decays a lot we want to have at least 0.01 uh as epsilon so at least a 1 chance of taking a random action just so we can explore the environment in the possible states a little bit more uh then we are also going to set the number of episodes this is going to be how many epoch we basically have let's go with something like 10,000 so how many time uh how many times the agent will play the game and per episode we want to have a max number of steps so let's say this is 100 so that if the taxi just goes around in circles for example um 100 times or takes 100 steps to go around in circles then uh the episode terminates even if it doesn't end up uh getting to the desired location so we have a max number of steps and then we're also going to initialize our q table the q table is going to be a nump array full of zeros so it's going to be initialized with zeros uh and it's going to have the shape of environment.",312,1,1,MSrfaI1gGjI
7,observation space n and and environment doaction space in the idea behind this is that you have a certain number of observations so or observation space so basically how many possible states can the game be in and how many actions can i take per state and this is what we want to model here so um for this particular example and this depends on the environment but in a taxi environment you have 25 positions because it's a 5 time 5 grit uh so you have 25 positions for the taxi for the taxi um you have the four different colored squares where the customer can be located so the customer can be in four different um squares so that would betimes 4 and we would have uh four different locations where the hotel can be located so times four that results in i think 400 so 25 4 4 is 400 different states um but actually no this is actually not correct because uh no sorry we have five different states we have five squares for the um for the customer and we have four for the hotel because the one that the customer at is not the one that the hotel is at so we have 25 5 4 uh which is uh 500 so we have 500 different states that the game can be in and for all these 500 states we have four actions that the car can take up down left right this is our table so for each state each action will have a q value so you take some state and you take the four actions and each,358,2,2,MSrfaI1gGjI
8,of them will have a value telling you how good this action is how how much reward is being anticipated by taking this action in the beginning all of them will be zero over time we will adjust them um we're going to define a function choose action and this function is going to determine our action um given a certain state so the function takes a state and depending on the state we're going to take take a certain action now if a certain random value that we generate random uniform a value between 0 and 1 if that value is below epsilon we're going to take a random action so if epsilon is one all of the values will be below epsilon so we will take a random action which means we're going to return environment.,179,2,2,MSrfaI1gGjI
9,action space sample we're going to just take any action from the action space and we're going to return it otherwise um we're going to return the best action according to the q table so return np arcmax um q table uh at the current state so state with all the actions what this does is it goes to the section of the q table where this state is relevant and it goes to all the actions and gives us the index of the action that maximizes or that has the maximum q value so we're going to take this action uh so if it's not random we take the best action according to the table if it's random we just take any action and this is our function so let's apply this in a training loop we're going to say for episode in range uh num episodes what we're going to do is we're going to say that we want to start with an empty environment or with a with a new environment which means we're going to reset the environment uh which will give us a random starting point so environment.,250,3,3,MSrfaI1gGjI
10,reset will return a state and something that we don't need um and this state now will be the input for our first decision so we're going to say done equals false and then we're going to say four step in range max steps so we go 100 steps max um what we do is we say that we want to take an action the action will be chosen by our function so either randomly in the beginning since epsilon is 1 it's going to be a random action over time with the epsilon decay we're going to go more and more into the direction of using the q table um but i'm going to just choose an action based on the state and this action will then be applied so now i just chose the action to actually do the action i need to do environment.,191,4,4,MSrfaI1gGjI
11,step and i need to pass the action now this is going to return a bunch of values this is going to return first of all the next state so when i am in a state and i take an action i'm in a new state because now the taxi if i go right has moved one block to the right um so i have this new state and i also have a reward now most of the time this will be zero sometimes it might be a punishment depending on the game uh if you are at the goal with a passenger you're going to get a reward of 20 i think in this particular game uh and then it also returns done truncated uh and info i don't think that we're going to use info and done and truncated are just values that we're going to check for to terminate the loop if they are um true and it's also going to overwrite this one here um all right so this is how we get the next state and this is how we apply an action so the interesting part happens now the learning process is updating the q learning table or the q table so we're going to say here that the old value is the q table at the specific state for the specific action that we took so we took an action in a specific state and there already is a q value for this particular action in that particular state in the beginning zero over time it changes what we're going to do now is we're going to update this,358,5,5,MSrfaI1gGjI
12,by applying a formula and part of this formula will be to get the maximum q value of the next state so this is just another variable in in the formula so what we need to do is we need to go to the next state so we need to go to the q table next state not the current one but the one that we're now in because we took the action go there see all the uh actions i can take see all their um the q values and take the maximum one now we're not taking the in we're not interested in which action it is we only want to know what's the most rewarding action from the next state so what reward can i expect from the next state and that is going to be our next max and now we're going to update the q table we're going to say that the q table at the state that we were in for this action will now be the result of the following formula 1 minus alpha so this is what i initially called the inertia uh meaning we're going to keep the old values so one minus learning rate times old value so how much do you want to focus on the already existing knowledge in so it's it's going to be 1 minus alpha and to this we're going to add the opposite alpha times and we're going to also update this value based on the reward if we got any plus the discount factor so how much do we want to take into account future predictions of reward gamma,358,5,5,MSrfaI1gGjI
13,times the next max so we already have a value in the beginning as i said z we already have a certain q value for an action in this state what we do then is we say um give me for the next state the maximum q value i have in the table and then we update the q value for the state that we were in and for the action that we took by keeping a part of the old value to a certain degree 1 minus alpha and adding new stuff to it alpha learning rate times the reward we got if we got any plus the discount factor how much do i want to focus on not just immediate rewards but expected rewards this value times the expected reward that's what we're doing here to update the q table and then we say state is equal to next state and if we have a done or truncated we're going to just say break out of the loop all right um so that is our training process the only thing that we don't have yet is the epsilon decay so outside of this uh inner full loop we want to do after each episode or at the end of each episode we want to say epsilon is going to be equal to the current epsilon um times the epsilon decay but we want to also keep the minimum epsilons epsilon so we're going to say max min epsilon and this calculation so decay epsilon but if it is already below the minimum don't decay it anymore use the minimum that's the basic idea all,358,5,5,MSrfaI1gGjI
14,right so that is our training process and what we're going to do afterwards is we're going to use the q table that we create or that we learn by um by going through this process by updating all these q values we're going to initialize the environment again we're going to say environment is equal to gy.,75,5,5,MSrfaI1gGjI
15,make again taxi v3 but we're going to say render mode equals human so i want to see what's happening during the training i don't need to see what's happening just do it behind the scenes but once it's done i want to see the results i want to see them in action and here i'm going to do five test runs i'm going to say four i in or actually let's say four step in range or actually let's do it properly four episode in range five so we want to do five episodes we're going to say state and something we don't need is equal to environment reset uh done equals false and then we're going to print episode episode and we're going to say four step in range max steps we want to see how well the agent now performs so we're going to render the environment environment.,196,6,6,MSrfaI1gGjI
16,render to visualize it we're going to say action is equal to npr ar max now here we're going to have no randomness we're going to always go according to the q table because we're not training anymore we're using the knowledge that we now have we're using the q table which should now be pretty good and we're just applying it we don't need random actions now so np mark max q table for the given state um with all the actions so pick the action that results in the maximum q value for the given state that we're currently in take that action so we're going to say again next state is equal or next state reward done truncated info is equal to environment step action then state is equal to next state if we reach done or truncated we're going to just say environment render uh finished episode episode uh with reward uh reward like this and then we're going to break out of the loop and in the end we want to close the environment so environment close like this so that is exactly what we need to do here now let me just run this to see if this actually works and to see if i didn't make any mistakes then i'm going to explain it again but there you go you can see the taxi goes to the customer and brings him back to the hotel um okay this was kind of weird now but it should still work there you go yeah seems to work quite well always picks up the customer brings them to the hotel that is,358,7,7,MSrfaI1gGjI
17,what a trained model looks like or a trained agent looks like again so what's the idea we have a q table full of zeros containing enough space for all the possible states the game can be in with all the possible actions that i can take in each state and then for each action in each state i have a certain value now you need to keep in mind that this here is not applicable to any other version of the game this is so to say hardcoded now it's not hardcoded because it learned it by reinforcement learning but this information is very very very specific or this q table is very specific to this particular problem on this particular map uh with all the information so it just has the information for the exact thing that we're looking at here uh it cannot be generalize to any other example here so this q table will not help you if you have a different game uh even if it's similar so it's hyper optimized for this specific game but it's one way to implement a simple reinforcement learning agent by just updating these q values looking at the rewards looking at the expected rewards and updating them with the different factors that we defined in the beginning so this is how you can implement q learning in python so that's it for today's video i hope you enjoyed it and hope you learned something if so let me know by hitting the like button and leave a comment in the comment section down below and of course don't forget to subscribe to this channel,358,7,7,MSrfaI1gGjI
18,and hit the notification bell to not miss a single future video for free other than that thank you much for watching see you in the next video and bye,39,7,7,MSrfaI1gGjI
0,hello world and welcome today we'll talk about really interesting topic about dueling dqrns my name is harris and let's get started so if you like this channel please hit the subscribe button or leave a like comment there and you will receive seven years of good luck of course so let's start with the very first um picture here to just recap what we are talking about the last time so tqns and basically deep q networks here are um on the very left side you can see the agent which is a deep q network or a deep network here um on the very right you can see the environment where the agent is taking actions into the environment he's observing the environment what he's what's going on and then he's receiving backs the states he's going to left or right or something like this and as well he's receiving a reward which means if he's doing something good or he's turning to the right side he's receiving um reward so the target is here to receive the most max reward and that he's ending the game with the max reward right so to just recap what we are we're talking about or what is the problem of dq ends or how you can improve the dql to be more accurate especially due to the estimators and the stability and everything double dqns were proposed to just fix the bias and to fix the estimated problems which you can see here and there are four different games where we have on the value estimators and you have on the x axis you have the,358,0,0,n4xFJGlsCy4
1,training steps and you can see it the red line here represents the normal dqn and you can see the true value depending on the estimated value this gap is really high so the estimators and dq ends are overestimating to make it a little bit better double ducants were proposed to make this more advanced and then to to improve everything how um music double q network looks like you can imagine this like on this on the next picture so we don't have only one um q network we have two q networks we have a normal q network and a target queue network on the left you can see the environment in green where the environment is storing all the actions rewards and everything in the states in the reply memory and the q network is basically interacting with the environment right so there's um action and the state and then he's copying everything updates everything to the target network where a q value is passing by to the target q network and then the goal is to music yeah to um decrease the dq n loss that's how the double dkan was working with q with two q networks so the next step why we are here or where you're talking about today even more improvement of this is um the dual dualing dqans which are in many cases even outperforming double dqns that's why we're talking about this topic today so during the currents from the architecture side looks a little bit different which means they don't have two q networks they have two estimators at the very end as you,358,0,0,n4xFJGlsCy4
2,can see here the vanilla iq network just arbitrary q network here on the um top and below you can see the dualing q network so the difference is that on the button you have to input picture from the environment with three q music convolution neural nets and or um it's input and two convolution nor nets then you have a flattening layer at almost end and then the red vertical lines are estimators so two in parallel estimators the top estimators represents the value of two states and the bottom estimators is representing the advantage of the state depending on the actions these are both of them are fully connected layer and then at very end everything got sum up like aggregation layer and then you are calculating actually the q value from the state action pair to here received the max q value in this case so today we are going to use not the cut pole from the open ai gym we are using the acrobot to just show the example of how you can deal with dq ends here we have three actions here and six states and if you start acrobat here to just show how the performance looks like you can do so and you can see that he's basically not reaching the horizontal line which is like a threshold so he's really bad he has a score minus 500 and so on so after some episodes you can see this the second episode he's still on the minus 500 so the goal is that lower part of the acrobat should um go over this threshold that he's reaching,358,0,0,n4xFJGlsCy4
3,this threshold right of course in best case it's it should be in stay vertical state as long as possible but the goal is of course to reach this threshold this black line that you saw so to do so to improve this acrobat you're using kerasrl and i set up here a model with almost 5000 000 parameters some two dense layers and then i'm building the agent where i'm using the boltzmann policy sequential memory and so on and so on and um with the dqn agents comes the magic right so you can see i'm setting up gamma to 0.99 and i'm enabling dual link network i'm setting it to true to enable the dual network and in this type i'm using the dueling type average you can use max and so on but in this example i'm doing the average so this basically how you are calculating the two estimates together you are using the average of them or you are summing up everything so this is basically what the agent is telling you so afterwards i'm compiling everything and fitting everything without within 100 000 steps and you can see an improvement of the rewards so it's not minus 500 but around about -160 and if i'm visualizing now after the dueling dqn how the performance looks like then you can see actually improvement here you can see that he's hitting and reaching the target and the threshold um we are now at minus seven with respect to the reward you can see here now stopping at minus 94 but he was reaching the threshold of course you can improve this,358,0,0,n4xFJGlsCy4
4,network and you know to make it a little bit more performant and choose even more episode and so on and so on that you're really reaching this vertical um state of the acrobat but in this case i just want to show that he was reaching the threshold it's pretty fine you can see that um he's doing a better job so now you can of course save everything to reuse it in another example and you can also see i was i'm plotting here the rewards or the episodes and you can see that from around about minus 500 that he was improving to almost minus 60 after only 10 app episodes you can see a really improvement here so if you are let's say calculating now 100 200 episodes you really see that he's reaching his goal and yeah but he was doing really great job i'm really happy with this and so thanks a lot for watching and hopefully you liked it leave a like a comment there and see you soon,228,0,0,n4xFJGlsCy4
0,hi everybody welcome back to archive insights so lately we've seen a lot of new emerging algorithms in deep reinforcement learning and in this episode i want to dive into one specific algorithm called proximal policy optimization that was designed at opening eye and has proven successful on a wide variety of tasks going all the way from robotic control to atari and even playing complicated video games like dota 2 now in this episode i'm gonna dive into some pretty technical terrain so i think it's good if you're a little bit prepared i've made a few previous videos with an introduction to reinforcement learning and the problem of the sparse reward setting so i think if you're kind of new to the field of reinforcement learning i would suggest to watch those videos first and then come back to this video as we're going to dive pretty deep into the rabbit hole and being well-prepared definitely as a must for this video but if you think you're ready for it grab a cup of coffee and get ready to dive in deep because this episode is on proximal policy optimization my name is xander and welcome to our kevinsites laughter music all right so let's start by sketching some surroundings first so if we're doing supervised learning on a data set like image net for example then we can have a static training data set we can run a circus in gradient descent optimizer in that data and we can be pretty sure that our model will converge to a pretty decent local optimum the road to success in reinforcement learning however isn't that,358,0,0,5P7I-xPq8u8
1,simple so one of the problems that reinforcement learning suffers from is that the training data that is generated is itself dependent on the current policy because our agent is generating its own training data by interacting with the environment rather than relying on a static data set as is the case in supervised do and so this means that the data distributions of our observations and rewards are constantly changing as our agent learns which is a major cause of instability in the whole training process an apart from having this problem with varying training data distributions reinforcement learning also suffers from a very high sensitivity to hyper parameter tuning and things like initialization for example and in some cases it's kind of intuitive to understand why this happens because imagine that your learning rate is too large well then you could have a policy update that pushes your policy network into a region of the parameter space where it's going to collect the next batch of data under a very poor policy causing it to never recover again and so to address many of these annoying problems in reinforcement learning the team and opening i designed a new reinforcement learning algorithm that's called proximal policy optimization or ppo and the core purpose behind ppo was to strike a balance between ease of implementation sample efficiency and ease of tuning now the first thing to realize about ppo is that it is what we call a policy gradient method and this means that unlike popular key learning approaches like dqn for example that can learn from stored offline data proximal policy optimization learns online and,358,0,0,5P7I-xPq8u8
2,this means that it doesn't use a replay buffer to store past experiences but instead it learns directly from whatever its agent encounters in the environment and once a batch of experience has been used to do a gradient update the experience is then discarded and the policy moves on and this also means that policy gradient methods are typically less sample efficient than queue learning methods because they only use the collected experience once for doing an update and our general policy optimization methods usually start by defining the policy gradient laws as the expectation over the log of the policy actions times an estimate of the advantage function okay so what is that only well the first term pi theta is our policy it's a neural network that takes the observed states from the environment as an input and suggests actions to take as an output and the second term is the advantage function a which basically tries to estimate what the relative value is of the selected action in the current state so let's take apart what that means so in order to compute the advantage we need two things we need to discounted sum of rewards and we need a baseline estimate so the first part is the discounted sum of rewards or the return and this is basically a weighted sum of all the rewards the agent gaad during each time step in the current episode and then the discount factor gamma which is usually somewhere between 0.9 and 0.99 accounts for the fact that your agent cares more about reward that is going to get very quickly versus the same,358,0,0,5P7I-xPq8u8
3,reward it would get a hundred times that for now and this is exactly the same idea as interest in the financial world in the sense that getting money tomorrow is usually more valuable than getting the same amount of money say a year from now and so notice that the advantage is calculated after the episode sequence was collected from the environment so in other words we know all the rewards so there is no guessing involved in computing the discount or return because we actually know what happened okay so that was the first part of the advantage function the discounted sum of rewards and then the second part of the advantage function is the baseline or the value function and basically what the value function tries to do is give an estimate of the discounted sum of rewards from this point onward so basically it's trying to guess what the final return is going to be in this episode starting from the current state and during training this neural net that's representing the value function is going to be frequently updated using the experience that our agent collects in the environment because this is basically a supervised learning problem you're taking states as an input and your neural net is trying to predict what the discounted sum of rewards is going to be from this state onwards so basic supervised learning and notice that because this value estimate is the output of a neural net this is gonna be a noisy estimate there's gonna be some variance because our network is not going to always predict the exact value of that states so,358,0,0,5P7I-xPq8u8
4,basically we're going to end up with a noisy estimate of the value function okay so now we have the two terms that we need we have the discounted sum of rewards that we computed from our episode rollout and we have an expectation an estimate of that value given the state that we're in and if we then subtract the baseline estimate from the actual return we got we get what we call the advantage estimate and so basically the advantage estimate is answering the question how much better was the action that i took based on the expectation of what would normally happen in the state that i was in so basically was the action that our agent took was it better than expected or was it worse and so then by multiplying the log probabilities of your policy actions with this advantage function we get the final optimization objective that is used in policy grading and if you think about what this objective function is doing it's intuitively satisfying because if the advantage estimate was positive meaning that the actions that the agent took in the sample trajectory resulted in better than average return what we'll do is we'll increase the probability of selecting them again in the future when we encounter in the same state and if on the other hand the advantage function was negative then we'll reduce the likelihood of the selected actions which makes total sense right and as i've already mentioned one of the problems is that if you simply keep running gradient descent on one batch of collected experience what will happen is that you'll update the,358,0,0,5P7I-xPq8u8
5,parameters in your network so far outside of the range where this data was collected that for example the advantage function which is you know in principle a noisy estimate of the real advantage is going to be completely wrong and so in a sense you're just going to destroy your policy if you keep running gradient descent on a single batch of collected experience and i'll to solve this issue one successful approach is to make sure that if you're updating the policy you're never going to move too far away from the old policy now this idea was widely introduced in a paper called trust region policy optimization or t rpo which is actually the whole basis on which ppo was built and so here is the objective function that was used in t rpo and if you compare this we the previous objective function for vanilla policy gradients what you can see is that the only thing that changed in this formula is that the log operator is replaced with the division by pi theta old now the slide here shows that optimizing this tr pio objective is in fact identical to vanilla policy gradients i'm not going to go into the derivation details here but if you want you can pause the video or check out lecture 5 of the deep rl bootcamp which will take you deep down the rabbit hole link in the description now to make sure that the updated policy doesn't move too far away from the current policy trp row adds a kl constraint to the optimization objective and what this cal constrained effectively does is it's,358,0,0,5P7I-xPq8u8
6,just going to make sure that the new updated policy doesn't move too far away from the old policy so in a sense we just want to stick close to the region where we know everything works fine the problem is that this cal constraint adds additional overhead to our optimization process and can sometimes lead to very undesirable training behavior so wouldn't it be nice if we can somehow include this extra constraint directly into our optimization objective well as you might have guessed that is exactly what ppo does ok so now that we have a little bit of surroundings let's dive into the crux of the algorithm the central optimization objective behind ppo hold on to your heads it's about to get a little technical the first let's define a variable r theta which is just a probability ratio between the new updated policy outputs and the outputs of the previous old version of the policy network so given a sequence of sampled actions and states this r theta value will be larger than 1 if the action is more likely now than it was in the old version of the policy and it will be somewhere between 0 and 1 if the action is less likely now than it was before the last gradient step and then if we multiply this ratio r theta with the advantage function we get the normal trp or objective in a more readable form and with this notation we can finally write down the central objective function that is used in pp oh here it is look surprisingly simple right well first of all you can,358,0,0,5P7I-xPq8u8
7,see that the objective function that ppo optimizes is an expectation operator so this means that we're going to compute this over batches of trajectories and this expectation operator is taken over the minimum of two terms the first of these terms is r theta times the advantage estimate so this is the default objective for normal policy gradients which pushes the policy towards actions that yield a high positive advantage over the baseline now the second term is very similar to the first one except that it contains a truncated version of this r theta ratio by applying a clipping operation between 1 minus epsilon and 1 plus epsilon where epsilon is usually something like 0.2 and then lastly the min operator is applied to the two terms to get the final result and while this function looks rather simple at first sight fully appreciating all the subtleties at work here takes a little bit more effort so bear with me here i promise we're almost there firstly it's important to note that the advantage estimate can be both positive and negative and this changes the effect of the main operator here is a plot of the objective function for both positive and negative values of the advantage estimate so on the left half of the diagram where the advantage function is positive or all the cases where the selected action had a better-than-expected effect on the outcome and on the right half of the diagram we can find situations where the action had an estimated negative effect on the outcome now on the left side notice how the loss function flattens out when r,358,0,0,5P7I-xPq8u8
8,gets too high and this happens when the action is a lot more likely under the current policy than it was under the old policy and in this case we don't want to overdo the action update too much and so the objective function gets clipped here to limit the effect of the gradient update and then on the right side where the action had an estimated negative value the objective flattens when r goes near zero and this corresponds to actions that are much less likely now than in the old policy and it will have the same effect of not overdoing a similar update which might otherwise reduce these action probabilities to zero remember the advantage function is noisy so we don't want to destroy a policy based on a single estimate and finally what about the very right hand side well the objective function only ends up in this region when the last gradient step made the selected action a lot more probable so are as big while also making our policy worse since the advantage is negative here and if that's the case then we would really want to undo the last gradient step and it just so happens that the objective function in ppo allows us to do this the function is negative here so the gradient will tell us to walk the other direction and make the action less probable by an amount proportional to how much we screwed it up in the first place and also notice that this is the only region where the unclipped part of the objective function has a lower value than the clipped version,358,0,0,5P7I-xPq8u8
9,and those gets returned by the minimization operator pretty clever right and if you're wondering how on earth the authors from the ppo paper managed to design this specific reward function well it's quite likely that they had an intuitive idea of what they wanted the objective function to do so they probably sketched a bunch of diagrams that satisfied the behavior that we just discussed and then came up with the exact objective function to make it all work out and don't worry if you didn't fully get all the little details involved basically the ppo objectives does the same as a trp all objective and that it forces the policy updates to be conservative if they move very far away from the current policy the only difference is that ppo does this with a very simple objective function that doesn't require to calculate all these additional constraints or kl divergences and in fact it turns out that the simple ppo objective function often outperforms the more complicated variant that we have in t rpo simplicity often wins all right nice now that we've seen the central objective function behind ppo let's take a look at the entire algorithm end to it so as mentioned before there are two alternating threads in ppo in the first one the current policy is interacting with the environment generating episode sequences for which we immediately calculate the advantage function using our fitted baseline estimate for the state values and then every so many episodes a second thread is going to collect all that experience and run gradient descent on the policy network using the clips ppo object and,358,0,0,5P7I-xPq8u8
10,as was done in training the opening i-5 system these two threats can actually be decoupled from each other by using thousands of remote workers that interact with the environment using a recent copy of the policy network and a gpu cluster that runs gradient descent on the network weights using the collected experience from those workers note that in this case each worker has to refresh its local copy of the policy network pretty often to make sure that it's always running with the latest version of the policy network to keep everything nicely balanced now importantly the final loss function that is used to train an agent is the sum of this clips ppo objective that we just saw plus two additional terms the first additional term of the loss function is basically in charge of updating the baseline network so this is the part of the network graph that is in charge of estimating how good it is to be in this state or more specifically what is the average amount of this counted reward that i expect to get from this point onwards so even though the value and policy outputs form two separate heads of the same network because they are part of the same computation graph you can actually combine everything in a single loss function and the auto differentiation library will just figure out where to send all the gradients and the reason that these two lost terms are actually part of the same objective function is that the value estimation network shares a large portion of its parameter space with the policy network and the intuition is that,358,0,0,5P7I-xPq8u8
11,whether you're trying to you know estimate the value of the current state or you simply want to take the best current action well you're likely going to need very similar feature extraction pipelines from the current state observation so these parts of the network are simply shared and then finally the last term in the objective function is called the entropy term and this term is in charge of making sure that our agent does enough exploration during training so in contrast to discrete action policies that output the action choice probabilities the people your policy head outputs the parameters of a gaussian distribution for each available action type and when running the agent and training mode the policy will then sample from these distributions to get a continuous output value for each action head now if you want to fully understand why the entropy term encourages exploration i really recommend to check out ugly angelo's video on the ideas behind entropy and kl divergence in machine learning the link is in the description but basically the entropy of a stochastic variable which is driven by an underlying probability distribution is the average amount of bits that is needed to represent its outcome it is a measure of how unpredictable an outcome of this variable really is and so maximizing its entropy will force it to have a wide spread over all the possible options resulting in the most unpredictable outcome and so this gives some intuition as to why adding an entropy term will push the policy to behave a little bit more randomly until the other parts of the objective start dominating and,358,0,0,5P7I-xPq8u8
12,as always we have a couple of hyper parameters c 1 and c 2 that way the contributions of these different parts in the loss function now for people that want to take a deeper look at peo in terms of python code i really recommend to check out this implementation in pi towards from our l adventure trust me even though you've never worked with pi torch this implementation is as clean as it gets and if you're looking for a more production proof implementation i would recommend to check out opening eye baselines which has a full implemented tensorflow version that runs on different environments like atari mojo co and others both links are in the description all right so that's it congratulations if you've made it this far we've covered all you need to know about proximal policy optimization now in the paper you can find a bunch of graphs that compare ppo to other benchmarks in deep rl so don't hesitate to have a look the link is in the description the important thing to remember though is that ppo wasn't specifically designed for sample efficiency but rather to address the really complicated code that was needed for a lot of other algorithms and also you know making it relatively easy to tune in terms of high performers and because ppo achieves both of those objectives while also yielding close to or above state-of-the-art performance on a wide range of tasks it has become one of the benchmarks in deep reinforcement learning so in summary ppo is a state of the art policy gradient method the algorithm has the stability and reliability,358,0,0,5P7I-xPq8u8
13,of t rpo while much simpler to implement requiring only a few tweaks to vanilla policy gradient methods and it can be used for a wide range of reinforcement learning tasks great or before i end this video i would really like to thank all the people that support this channel on patreon i mean even if it's only 1 a month those contributions really mean a lot to me they are a big motivation because they show that the people out there really care for the content that i'm making and it's a really good motivation to keep going so thanks a lot all my great amazing patreon supporters if wha that was it for this episode thank you very much for watching i hope you learned something about proximal policy optimization don't forget to like subscribe and share and i'd love to see you again in the next episode of archived insights music,201,0,0,5P7I-xPq8u8
0,in today's video you're gonna learn how to code a deep q learning agent from scratch in the pi towards framework you don't need any prior exposure to deep learning you don't need any prior exposure to reinforcement learning you just have to follow along let's get started so first a couple announcements first of all this is a repeat of an earlier video the earlier iteration of this project i'm not quite happy with i in particular i did a silly move of storing actions as a one-hot encoding and then turning them back into integer representation which really makes no sense and just cause for some confusion and the code is written in a way that's really not up to my modern standards so i'm redoing it and then cleaner much more concise and much more probably i guess you could say a more better way second if you'd like to learn the why behind all of this check out my course deep you learning from paper to code on sale right now on you to me for 9.99 what better way to pass the time while you are sheltering in place all that said let's go ahead and get started we begin with our imports you're going to need the base torch packaged as tea torch a tenon to handle our layers in this case since the lunar lander environment is just a simple eight vector eight element vector observation we're only going to use linear layers we're not going to be needing any convolutional layers and we'll need an end functional for the value activation function for our deep neural network and,358,0,0,wc-FxNENg9U
1,of course opt-in for our optimizer and we need numpy to do various and numpy type things so we're gonna use a reduced kind of a reduced solution from my course and my course i break everything into highly modular chunks everything is kind of like a bunch of legos you place together that you can mix and match between different projects along with the command-line interface very clean solution here we're just kind of kind of stick stuff into two main classes and kind of mix up some functionality because it's just youtube video but it's gonna do more or less the same thing and our two classes are gonna be the class for the dq network and the other class will be the class for the agent region the reason behind this is that an agent is not a deep to network rather an agent has a deep to network agent also has a memory a way of choosing actions as well as learning from its experience whereas a dbq network is precisely that is just a neural network that takes a set of states as input and outputs the agents estimate of action values for each given state in the input and if you are unfamiliar with pi torch the convention is that every class that extends the functionality of the base neural network layers derives from an image this gives you access to a number of goodies in particular the parameters to perform the optimization as well as the backpropagation function so we don't have to write backpropagation ourselves this will take a learning rate input dims fc-1 dims fc2 dims and,358,0,0,wc-FxNENg9U
2,n actions as input and you want to call the super constructor right away this basically calls the unless i'm mistaken calls a constructor for the base class can we save all the appropriate variables in our class now this is a little bit overkill you could just pass these as inputs to the various functions but you know if we want to extend the functionality later we're gonna have to add this stuff in later so you may as well have it now and we arrived at our first layer of our deep neural network and then i'll take star self dot input dims as input a little output self dot fc one dims and the star itself input dims idiom if you're unfamiliar with it is a way of unpacking a list in this case so we're gonna pass in a list of the eight basically of course mind to eight elements of the observation vector it's a way of making a little bit more extensible so that if you want to extend this to say a convolutional neural network that will facilitate that or rather more specifically if you want to extend it to an environment that has a two-dimensional matrix as input and so the second linear layer will take fc-1 dims as input and output fc two dims and fc three is the output of our deep neural network i'll take out c2 dibbs's input and output what number of actions we can go ahead and call itself dot n actions so the reason is that a dbq network is an estimate of the value of each action given some set,358,0,0,wc-FxNENg9U
3,of states so if i do the agent performs what action what is the expected feature reward given it is in that state all of its policy we also have an optimizer in this case we use adam we're gonna pass in the learning rate and we have a loss function that's mean squared error loss because q-learning is basically kind of like a linear regression where you're gonna try to minimize the distance or rather you're gonna fit a line to the delta between the target value and the output of your deep neural network oh and something i didn't state before is that this is a simple representation of a dbq learning agent and this isn't the full representation that we cover in the course where we have both a replay network and a target network we're just gonna use the replay network for this we're not gonna use the target network because it turns out in this case the target network isn't really required however a replay memory is almost certainly always required which you learn in the course and i have a free preview here on the channel where you can see the results of running a dq network without a replay memory just using regular temporal difference learning and surprise surprise it's highly unstable it kind of learns for a few moves and then forgets everything it learned and i cover all of this in the course as well as a little bit in that video on the free preview we also need a device and this takes advantage of a gpu should you have one and if you do not,358,0,0,wc-FxNENg9U
4,it simply defaults to the cpu teta cuda is available and then you want to send your entire network to the device and this is part of the reason why we derive from the base pennant module class as i said earlier we don't have to handle back propagation but we do have to handle forward propagation it's not yet implicit in the framework how to handle for propagation because you need you know to define your say activation functions so we want to say sell thought fc-1 state so we want to pass the state into the first you know let's blow up the text a little bit for you guys so we want to pass the states into the first fully connected layer and then activate it with a value function and then we want to pass that output from that layer into the second fully connected layer and yet again activated with a value function and finally we want to pass that output to the third fully connected layer but we don't want to activate it and then we want to return that value and we don't want to activate it because we want to get the agents raw estimate we don't want to use say a rho u function because the estimate of the present the present value the future rewards could very well be negative so we don't want to well you it could very well be greater than one so we don't like a sigmoid or a tan hyperbolic something like that so we don't want to activate it we just want to get the raw number out and that,358,0,0,wc-FxNENg9U
5,is in for a dbq network class the main functionality lives in the agent class and that doesn't derive from anything so we have the pass in a hyper parameter gamma that determines the weighting of future rewards the epsilon which is the solution to the epsilon the excuse me the explore exploit dilemma you know how often does the agent spend exploring its environment versus taking the best known action and that has to be some finite value because you never know if you have a complete model of your environment you know how can you ever be sure that what you think is the best action actually is answer is you can't so you got to keep exploring and then it's determined by a parameter called epsilon learning rate to pass into our deep neural network input dims a batch size because we're gonna be learning from a batch of memories number of actions and max memory size as i said earlier we're going to be using a memory leave something like a hundred thousand and at salon end will default to 0.01 and an epsilon decrement of 5 by 10 to the minus 4 so this is the parameter that tells us by what to decrement epsilon with each time step you can do a multiplicative dependence you can do 1 over square roots you can do any type of decrement you want i'm just gonna use linor linear we're gonna subtract off a constant factor with each time step all the way down to the minimum it doesn't really matter it's one of the least important hyper parameters the main thing is,358,0,0,wc-FxNENg9U
6,that you give it some time of acting randomly and then decay it over time to a minimum value and leave it at a minimum which is nonzero some finite value so of course we want to save all these variables and we also have what's called an action space i in range in n range that won't work and actions and this is just a list comprehension that tells us the integer representation of the available actions and the reason is because we're gonna use this later in the epsilon greedy action selection just makes it easy to select an action at random as you will see in a moment so we'll say max mem size is our memory size say our batch size we need a memory counter and the point of that is to keep track of the position of the first available memory for storing the agent's memory and we also need the it can i scroll down a bit let's do that we also need the evaluation network so we'll say self dot q eval equals deep q network and we'll pass in the learning rate a number of actions we need the input dims we need fc-1 dims we're gonna default this a 256 c2 dims 256 just add that over a bit and you can play around with this it's not super critical i mean it to some extent is but it will function for a wide range of neurons in the fully connected layers in addition to the deep neural network you also need some sort of mechanism for storing memory now many people will use something called,358,0,0,wc-FxNENg9U
7,a deck or a dq which is a basically a list in python a linked list so it's dynamic memory structure and you pop things off one end and insert them in the other i don't like doing that because then you're gonna store basically a collection of arrays anyway and then you have to dereference it so you have to say you have to know which position of the of the dq corresponds to which array so i just like saving them as named arrays anyway because it's much cleaner and more more easy to read so what i mean specifically is state memory equals np zeroes seldom msi's might start input dims and we're gonna use np float32 as our data type the data type here is quite important 2 pi torches rather particular about data types it enforces some degree of type checking which is a good thing it can be a little bit annoying at first but once you're aware of it it really saves your bacon later on because you lose some stuff when you go from one level of precision to another so make sure that things are nice and consistent it's quite handy and we also have a memory to keep track of the new states the agent encounters so what the agent wants to know for the temporal difference update rule is what is the value the current state the value of the next state the reward it received in the action it took and to get that you have to pass in a memory of the states that resulted from its actions because remember deep q-learning if,358,0,0,wc-FxNENg9U
8,you're not aware is a model free bootstrapped off policy learning method what that means is we don't need to know anything about the dynamics of the environment how the game works we don't need to know anything about that we're gonna figure it out by playing the game that means model free bootstrapped means that you are going to construct estimates of action value functions meaning the value of each action given you're in some state based on earlier estimates you're using one estimate to update another in other words you're pulling yourself up by the bootstraps off policy means that you have a policy that used to generate actions which is epsilon greedy meaning that uses hyper parameter that we defined a peer epsilon to determine the proportion of the time that you spend taking random versus greedy actions and you're going to use that policy to generate data for updating the purely greedy policy meaning the agents estimate of the maximum value function action by your function sorry so it is off policy in that sense so a lot of buzzwords there more that is covered in the course but that is the gist of it so we also need an action memory and that is an array of numbers eros self that memphis eyes and we use compete in 32 just a set of integers because our this is a discrete environment deep learning it doesn't really work for discrete environment so you can do sorry discrete action spaces so you can do some tricks to kind of get around that but you know they don't work as well we need a,358,0,0,wc-FxNENg9U
9,reward memory and that is shape mm size and d type of mp float32 and so these will be floating-point numbers because they can be fractional numbers they can be decimal point numbers we also need a terminal memory and the point of this is sorry the point of this is that the value of the terminal state is always zero so the reason behind that is simply put if you encounter the terminal state the game is done right and so there are no feature actions until you reset the episode but when you reset the episode you're in the initial state at the terminal state so the future value of the terminal state is identically zero and so we need some way of capturing that when we tell the agent to update its estimate of the q value the action value function q and that is facilitated through the terminal memory and we're gonna be passing in the done flags from our environment and it is therefore type and p dot bool and that allows us to use it as a mask for setting the values of the next states to zero later on which you'll see in the learning function that is it for our constructor next we need an interface function to store one of those transitions in the agents memory so that'll take a state action reward new state and done flag as input iocation call this terminal or so time is done but the first thing we wanna know is what is the position of the first unoccupied memory so index cells mm counter modulus men sighs now using the,358,0,0,wc-FxNENg9U
10,"modulus here has the property that this will wrap around so once we go from memory zero up to 99999 that 100,000 memory we go to store will go all the way back in position 0 so we rewrite the agents earliest memories with new memories and that's because the memory is finite if you're using a deck or a tq then you're just going to be popping stuff off the end once you get beyond the limitation the size of the agent's memory so now that we know where to store it let's go ahead and store it in action memory now this is part of the reason i am redoing it but first you've got to increment the memory counter by 1 to let yourself know that you've filled up a memory and this is part of the reason i am redoing this so this action memory here originally i had gone from a one hot encoding where you take an integer and represent it as an array of zeros with a 1 corresponding to the position of the integer so you have 4 actions you have an array of 4 elements is 0 1 2 3 and if the agent took action for the 100 coding representation that is 0 0 0 1 in an array if it's action 1 it'll be 0 1 0 0 so on and so forth and then i went from that point hot encoding back to the integer encoding in the learn function and you know this was done way back in october of 2018 and i can't really remember the reason for doing that suffice it",358,0,0,wc-FxNENg9U
11,to say it's dumb and led to some issues with people's understanding you know and so i no longer want to use that way of doing things and in fact i've moved on beyond that so we're not going to do that anymore just keep that in mind so the next thing we need to do is a function for choosing actions so the agent has to have some function to choose an action and that is based on the current state of the environment sorry we'll call the call it observation and you'll see why in a second so the observation of the current state of our environment and the first we want to do take a random number and save is less than epsilon sorry greater than epsilon epsilon then if it's better than epsilon we want to take the best known action so what we have to do is take our observation turn it into a pi torch tensor and send it to our device because everything the entire network lives on our device so we have to send the variables we want to perform computations on to our device and we need this bracket around the observation because of the way the deep neural network is set up and so then we pass that state through our dq network through the for function and then we want to get the arc max to tell us the integer that corresponds to the maximal action for that set or a state and yet to dereference it with not item because it returns a tensor and we're gonna be passing this back to our environment,358,0,0,wc-FxNENg9U
12,which only takes integers or numpy arrays as input now if it is not greater than epsilon meaning of it's less than or equal to then we want to take a random action and we'll just do a random choice from the agents in action space and regardless of how you select the action you want to return it one moment the kitty cat is scratching at the door let me let her in say hello kitty brown okay so now that we have a mechanism for choosing actions we can't think about how the agent is going to learn from its experiences so sorry i can't you got to go she's in the way so we do that through a function called learn and that does not take any inputs and right away we're faced with a bit of a dilemma and the dilemma is if we you know we have this memory and it's filled up with a bunch of zeros which we can't really learn anything from zero so it's kind of stupid to trim' so how do we deal with that well there's a couple ways one way is you can let the agent play a bunch of games randomly you know until fills up the whole entirety of its memory and then you can go ahead and start learning that's one possibility you know we're not actually selecting actions intelligently you're just doing it at random that's one possibility and another one is to simply not do that but to start learning as soon as you filled up the batch size of memory and so we can facilitate that that's what,358,0,0,wc-FxNENg9U
13,"i'm going to choose to do by saying if our memory counter is less than the batch size just return so we're gonna call the learn function every iteration of our game loop and you know if we have not felt up or at least the batch size of remember you just go ahead and return don't bother learning no point in it so the first thing we want to do in the event that we are going to try to learn is 0 the gradient on our optimizer and this is a big particular two pi torch you don't have to do this in say karros so the next thing we need to do is to calculate a the position of the maximum memory because we want to select a subset of our memories but we but as i said we only want to select up to the last filled memory and so we want to take the minimum of either amendment counter or the mem sighs sighs is that what i called it me double check self thought man sighs yep so it's the minimum of um m counter or the mem sighs i say bat she goes mp random choice maximum self dup batch sighs new place equals false now we want the replace equals false because we don't want to keep selecting the same memories more than once now this isn't a problem if you've stored 50,000 memories or a hundred thousand memories but it is a problem if you stored you know 32 memories or 64 whatever you know bat size or something small multiple thereof so you want to make",358,0,0,wc-FxNENg9U
14,sure that once you select a memory you take it out of the pool of available memories to select again and then we need just something for bookkeeping basically my batch index and this was also a point of confusion on the github for with my previous video you most certainly need this batch index and you most certainly need the action indices to perform the proper array slicing if you don't use them you don't get the right thing and i made a video detailing that in earlier on maybe a month ago something like that but yeah you need this 100 percent certain on that so i'm when i upload this code to github don't give me you know issues on this because this is the way you do it so state bench equals t dot tensor self dot state memory dot 2q eval dot device so what we're doing here is we're converting the numpy array subset of our agents memory into a pi torch tensor and we also have to do the same thing for the new states new state memory is sub batch to sell q eval device we need the same thing for the reward bench will say reward batch give al dot device and we all see the same thing for a terminal batch and we do need an action batch but that doesn't need to be a tensor it can be an umpire array so that is yourself done action memory is sub batch alright so now we have to perform the feed forwards through our deep neural network to get the relevant parameters for our loss function,358,0,0,wc-FxNENg9U
15,so as i said we want to be moving the agents estimate for the value of the current state towards the maximal value for the next state so in other words we want to tilt it towards selecting maximal actions we do that by saying q eval equals self dot q eval forward state bench but we have to do the dereferencing we had to do the array slicing a batch in next action batch and the reason is that we want to get the values of the actions we actually took those are the only ones were really interested in you can't really update the values of actions you didn't take because you didn't sample those anyway so we want the values for the actions we actually took in each set of our memory batch and we want the agency's estimate of the next states as well forward new state bench and we don't need to do any dereferencing there we're going to handle that momentarily we're going to get the max actions and if you were doing a target network this is where you would use it you would use here as i said that is for more advanced stuff we're not going to bother with that right now the next thing we want to do is say the the values of the terminal states are identically 0 and then we want to calculate our target values this is where we want to update our estimates towards and that's the reward bench plus self dot gamma that's our discount factor times the max over q next along the it's dim sorry not access along,358,0,0,wc-FxNENg9U
16,the action dimension and the zeroth element because the max function returns the value as well as the index so it's returns a to point on the zeroth element which is the value so this is the maximum value of the next state and that is the purely greedy action and that is what we want for updating our loss function so when lost then it's just the q even loss q target tune eval dot to self that cute eval device and you want the back propagated and step your optimizer eval optimizer step and that is it for the nuts and bolts of the learning the next thing we have to handle is the epsilon decrement so each time we learn we're going to decrease epsilon by one so we'll say self dot epsilon by one unit of epsilon decrement the decrement that is self dot epsilon minus epsilon decrement if greater than eps in otherwise set it equal to epsilon function and this is the simplest possible implementation of deep curating you can really do and it is only you know about a hundred lines of code you know less than that if you count just significant lines of code but this is pretty much it folks so we are done with the agent and now we have to worry about fixing our syntax errors it's on this line here for insert i and range there we go and so let's go ahead and clear all of that so now we're ready to handle the main file so licious vim and main torch dqn lunar lander 2020 because this is the new version,358,0,0,wc-FxNENg9U
17,and we start with our imports what do you gem now let's i can increase one more so we need jim we will also need our agent from simple dq and torch 2020 and the port agent excuse me from utils import plot learning and this is certainly gonna get me in trouble because of my incessant renaming of this function we'll see what happens it'll be different on the github in fact i'm almost certain in this directory that has the wrong name plot learning curve i'll do that just keep in mind if you're doing you get pull its plot learning and i'll i'll have it correct in the github but if you're following along you may have a slight difference there so our main loop is pretty straightforward the first thing i want to do is make our environment chip make lunar lander v2 we need our agent and with the gamma of a 0.99 epsilon that starts out at 1.0 so the agent takes fully random actions at the beginning a batch size of 64 number of actions is for eps and equals 0.01 and put bim's equals 8 and the learning rate of 0.003 we also need a couple lists to keep track of our agents scores as well as the history of epsilon so that we can plot the scores over time versus the decay in epsilon over time and we'll play for 500 games so the top of each game we want to reset our score our done flag and our environment and we want to play each game and the first thing we want to do is choose,358,0,0,wc-FxNENg9U
18,an action based on the current state of the environment then we want to take that action get the new observation reward done flag and debug info from the environment you want to increment the score for the episode by reward we want to store that transition observation action reward and done store all of those go ahead and call our learn function and very important set the current state to the new state i forgot this and it took me forever to find it i was debugging the deep neural network i was scratching my head you really need that line otherwise it does not really work at the end of each episode you want to append the score and the epsilon at the end of the episode so long this average score equals np not mean scores last hundred game decisions this is just to get an idea of is our children is our agent learning so we're going to take the average with the previous 100 games and then we're gonna just print some simple information to the terminal so we know that it's running the score the average score and the epsilon at the end of all we're going to want to go ahead and plot our learning curve and this is just the x-axis a file name our x-axis the scores the epsilon history and file name and if you're new to the channel just do a clone of my github repo it may be called it may be called plot learning there i've switched from camel case to underscore case which causes some confusion just keep that in mind so,358,0,0,wc-FxNENg9U
19,let's do a right quit and let's check out the utility i know it is plot learning here okay scores x i see so this to the this version is for blue buddy this is for policy gradients algorithms and i know that because there is no epsilon so this one here plot learning curve is for the epsilon and i also have a bunch of other stuff for you know pre-processing frames and stuff from the course alright so let's go ahead and try to run that main torch dqn lander 2020 see how many typos i made lunare oh that is a problem doing air that is an easy one to fix easy easy typo and that's incredible there was only one typo so i'm gonna go ahead and let that run for 500 games and then we're gonna come back and check out the results and we are back now i didn't time this i kind of wandered off to play with my kid for a little bit but you can see that the performance is a little erratic here now i believe the reason behind this is twofold first and foremost i think the learning rate of is a little bit too high so i would recommend that when you play this for yourself you go ahead and lower that as well and the second reason is that we do not have a target network so it does learn so let's check out the learning plot here you you can see that the agents performance overall increases over time and around like 250 to 300 games or so it reaches an average,358,0,0,wc-FxNENg9U
20,score of about 200 which is considered solved so it does solve the environment but then due to the fact the learning rate is a little bit high the performance starts to degrade over time meaning that the agent is kind of wandering off from that local minima where it has achieved relatively good performance other thing of note is the oscillations in the performance this is due to the lack of a target network so a target network tends to help with model stability over time but in this case it doesn't seem to be a huge super big deal so that is the agents performance and a nutshell - some tweaking zero zero threes a little bit too high when you go and run it for yourself use zero zero one or something a little bit lower and you'll get better performance so everything else conceptually is the same you don't have to tweak anything other than learning it's maybe some model parameters such as the number of neurons and the hidden layers but that's up to you that is how you do deep q learning from scratch in the simplest possible case that is a dbq learning agent with just a replay memory no target network and this can be employed in a wide range of very simple one-dimensional environments meaning you don't input a matrix you just input a a single vector and so i hope this has been helpful any questions comments leave them down below i'll try to get them to them i have been getting inundated with questions and comments at least so my ability to answer is compromised,358,0,0,wc-FxNENg9U
21,but i will do my best nonetheless if you made it this far please consider subscribing and don't forget my new course to you learning from paper to code where you learn the how and the why of all this stuff as well as how to turn deep reinforcement learning papers into code is currently on sale for the remainder of the munch mo remainder of the month of march hit that subscribe button and i'll see you in the next video,106,0,0,wc-FxNENg9U
0,patrick lober is a popular python instructor and in this course he will teach you how to train an artificial intelligence to play a snake game using reinforcement learning hey guys today i have a very exciting project for you we are going to build an ai that teaches itself how to play snake and we will build everything from scratch so we start by creating the game with pygame and then we build an agent and a deep learning algorithm with pie torch i will also teach you the basics of reinforcement learning that we need to understand how all of this works so i think this is going to be pretty cool and now before we start let me show you the final project so i can start the script by saying python agents dot pi now this will start training our agent and here we see our game and then here i also plot the scores and then the average score and now let me also start a stopwatch so that you can see that all of this is happening live and now at this point our snake knows absolutely nothing about the game it only is aware of the environment and tries to make some more or less random moves but with each move and especially with each game it learns more and more and then knows how to play the game and it should get better and better so the first few games you won't see a lot of improvements but don't worry that's absolutely normal i can tell you that it takes around 80 to 100 games until our ai,358,0,0,L8ypSXwyBds
1,has a good game strategy and this will take around 10 minutes also you don't need a gpu for this so all of this training can happen on this cpu that's totally fine okay so let me speed this up a little bit music music all right so now about 10 minutes have passed and we are at about game 90 i guess and now we can clearly see that our snake knows what it should do so it's more or less going straight for the food and tries not to hit the boundaries so it's not perfect at this point but we can see that it's getting better and better so we also see that the average score here is increasing and now the per the best score so far is and to be honest for me this is super exciting so if you imagine that at the beginning our snake didn't know anything about the game and now with a little bit of math behind the scenes it's clearly following a strategy so this is just super cool don't you think all right so let me speed this up a little bit more music all right so after 12 minutes our snake is getting better and better so i think you can clearly see that our algorithm works so now let me stop this and then let's start with the theory so i will split the series into four parts in this first video we learn a little bit about the theory of reinforcement learning in the second part we implement the actual game or also called the environment here with pygame then we,358,0,0,L8ypSXwyBds
2,implement the agent so i will tell you what this means in a second and in the last part we implement the actual model with pytorch so let's start with a little bit of theory about reinforcement learning so this is the definition from wikipedia so reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward so this might sound a little bit complicated so in other words we can also say that reinforcement learning is teaching a software agent how to behave in an environment by telling it how good it's doing so what we should remember here is that we have an a chance so that's basically our computer player then we have an environment so this is our game in this case and then we give the agent a reward so with this we tell it how good it's doing and then based on their reward it should try to find the best next action so yeah that's reinforcement learning and to train the agent there are a lot of different approaches and not all of them involve deep learning but in our case we use deep learning and this is also called steep q learning so this approach extends reinforcement learning by using a deep neural network to predict the actions and that's we're going to use in this tutorial all right so let me show you the rough overview of how i organized the code so as i said we're having four parts so in the next part we implement the game,358,0,0,L8ypSXwyBds
3,with pie game then we implement the agent and then we implement the model with pie torch so our game has to be assigned such that we have a game loop and then with each game loop we do a play step that gets an action and then it does a step so it moves the snake and then after the move it returns the current reward and if we are game over or not and then also the current score then we have the agent and the agent basically puts everything together so that's why it must know about the game and it also knows about the model so we store both both of them in our agent and then we implement the training loop so this is roughly what we have to do so based on the game we have to calculate a state and then based on the state we um calculate the next action and this involves calling model predict and then with this new action we do a next play step and then as i said we get a reward the game overstate and the score and now with this information we calculate a new state and then we remember all of this so we store the new state and the old state and the game over state and the score and with this we then train our model so for the model i call this linear q net so this is not too complicated this is just a feed forward neural net with a few linear layers and it needs to have the these information so the new state and,358,0,0,L8ypSXwyBds
4,the old state and then we can train the model and we can call model predict and then this gets us the next action so yeah this is a rough overview how the code should look like and now let's talk about some of those variables in more detail for example the action or the state or the reward so let's start with the reward so that's pretty easy so whenever our snake eats a food we give it a plus 10 reward when we are game over so when we die then we get -10 and for everything else we just stay at zero so that's pretty simple then we have the action so the action determines our next move so we could think that we have four different actions so left right up and down but if we design it like this then for example what can happen is if we go right then we might take the action left and then we immediately die so this is basically a 180 degree turn so we don't allow that so a better approach to design the action is to only use three different numbers and now this is dependent on the current direction so um 1 zero zero means we stay in the current direction so we go straight so this means if we go right then we stay right if we go left then we go left and so on then if we have 0 1 0 this means we do a right turn and again this depends on the current direction so if we go right and do a right turn then we,358,0,0,L8ypSXwyBds
5,go down next if we go down and do a right turn again then we go left and then again we would go up so this is the right turn and the left turn is the other way around so if we go left and do a left turn then we go down and so on so with this approach we cannot do a 180 degree turn and we also we only have to predict three different states so this will make it a little bit easier for our model so now we have the reward and the action then we also need to calculate the state and the state means that we have to tell our snake some information about the game that it knows about so it needs to know about the environment and in this case our state has 11 values so it has the information if the danger is straight or if it's ahead if the danger is right or if the danger is left then it has um the current direction so direction left right up and down and then it has the information if the food is left or right or up or down and all of these are boolean values so let me show you an actual example so in this case if we are going right and our food is here then we see um danger straight right and left none of this is true so for example if our snake is over here at this end and it's still going right then danger straight would be a one so this again also depends on the current direction,358,0,0,L8ypSXwyBds
6,for example if we move up at this corner here then danger right would be a1 then for these directions only one of them is one and the rest is always zero so in this case we have danger right set to one and then for this in our case our food is right of the snake and also down of the snake so food right is one and food down as one all right so now with the state and the action we can design our model so this is just a feed forward neural net with an input layer a hidden layer and an output layer and for the input it gets the state so as i said we have 11 different numbers in our state 11 different boolean values zero or one so we need this size 11 at the beginning then we can choose a hidden um size and for the output we need three outputs because then we predict the action so this can be some numbers and these don't need to be probabilities so here we can have raw numbers and then we simply choose the maximum so for example if we take 1 0 zero and if we go back then we see this would be the action straight so keep the current direction so yeah that's how our model looks like and of course now we have to train the model so for this let's talk a little bit about this deep q learning so q stands for this is the q value and this stands for the quality of the action so this is what we want,358,0,0,L8ypSXwyBds
7,to improve so each actions should improve the quality of the snake so we start by initializing the q value so in this case we initialize our model with some random parameters then we choose an action by calling model predict state and we also sometimes choose a random move so we do this especially at the beginning when we don't know a lot about the game yet so and then later we have to do a trade-off when we don't want to do a random move anymore and only call model predict and this is also called a trade-off between exploration and exploitation so this will get clearer later when we do the actual coding so then with this new action we perform this action so we perform the next move and then we measure the reward and with this information we can update our q value and then train the model and then we repeat this step so this is an iterative training loop so now to train the model as always we need to have some kind of loss function that we want to optimize or minimize so for the loss function we have to look at a little bit of math and for this i want to present you the so-called belmont equation so this might look scary so don't be scared here i will explain everything and actually it's not that difficult when we um understand this and then code this later so what we want to do here we need to update the q value as i said here so according to the belmont equation the new q value is,358,0,0,L8ypSXwyBds
8,calculated like this so we have the current q value plus the learning rate and then we have the reward for taking that action at that state plus a gamma parameter which is called this count rate so don't worry about this i will also show this later in the code again and then we take the maximum expected future reward given the new state and all possible actions at that new state so yeah this looks scary but i will simplify that for you and then it's actually not that difficult so the old q value is model predict with state 0 so if we go back at this overview so the first time we say get state from the game this is our state 0 and then after we took this place step we again measure or calculate the next state so this is then our state one so with this information again our first queue is just model predict with the old state and then the new queue is the reward plus our gamma value times the maximum value of the q state so again this is model predict but this time we take state one and then with these two information our loss is simply the q new minus q squared and yeah this is nothing else than the mean squared error so that's a very simple error that we should already know about and then this is what we must use in our optimization so yeah that's what we are going to use so we have to implement all of these three classes and in the next video we start by,358,0,0,L8ypSXwyBds
9,implementing the game in the last part i showed you all the necessary theory that we need to know to get started with deep q learning and now we start implementing all of the parts so as i said we need to have a game so the environment then we need an agent and we need a model so in this part we start by implementing the game and we use pytorch for this so let me actually start by creating a environment and we install all the necessary dependencies that we need so in this case i use conda to manage the environments and if you don't know how to use conda then i have a tutorial for you that i will link here but yeah if you don't want to use connor you can also just use a normal virtual and but i recommend to use a virtual and and now let's create a virtual and with conda create minus n and then give it a name for example pi game n and i also say i want python equals 3.7 all right so now this was created so now we want to actuate it with conda activate and then pie game n and hit enter and then we see the name of the environment in the front so this means that we activated it successfully and now we can start installing all what we need so the first thing we want to install is pie game for our game so pip install pie game and hit enter so this is done the next thing we need is pytorch for our model later so,358,0,0,L8ypSXwyBds
10,for this we can go to the official home page and on install and then here you can select your operating system so i use mac and i actually i want to say pip install and we don't need cuda support so only a cpu is fine and we don't need torch audio because we don't work with audio files so we can only grab this pip install torch torch vision and then paste it in here and hit enter and now this installs pytorch and all the dependencies all right so this is done and then we need two more things for plotting later so for this i say pip install much plot lip and we also want i python and then hit enter all right so this was successful as well and now we have everything we need so now we can start implementing all the codes and as a starting point i want to grab the code from another tutorial that i did so you can find this on github and then on my um account and then in the repo python fun and here i actually have two snake games so and then we need this one snake pie game and download this so you can do this and i already did this and have this here so if we open up the editor here i'm visuals using visual studio code then we can see we have exactly those two files and then um the first thing i want to do is i want to run this file and test if this is actually working so right now this is just a,358,0,0,L8ypSXwyBds
11,normal snake game that you have to control yourself so you have to use the arrow keys so let's say python snake game dot pi and then let's hope that this is working so yeah so now i can control the snake and i hope that i can eat the food yes and now if i hit the boundary then we are game over so this is working our environment is set up and now we can start implementing our code so we can change this so that we can use this as a ai controlled game so let me show you the overview from last time so last time i told you that we need a play step in our game and this gets an action and based on this action we then take a move and then we must return a reward the game over state and the current score so first let's write down all the things that we need to change here so first we want to have a reset function so after each um game our agent should be able to reset the game and start with a new game then we need to implement the reward that our agent gets then we need to change the play function so that it takes takes an action and then um returns a or computes the direction then we also want to keep track of the current frame or let's call this game iteration and for later we also need to have a change in the if is collision function to check if this is a collision so first let's let me go,358,0,0,L8ypSXwyBds
12,over this code quickly so what we do here is we use pi game then for the direction we use an enum then for the point we use a named tuple and then here i created a class snake game and here we initialize the things we need for the game so here we initialize the game state for example for the snake we use a list with three initial values and the head is always the front of this list then we keep track of the score and here we have a helper function to place the food and yeah and we already have a function that is called play step and then if we go down to the very end so here we have our game loop so while this is true we take a game or a play step and we get the game over state and the score so this place the function is the most important one so here first we right now we grab the user input so the key we press then we calculate a move based on this key and then we update our snake and check if we are game over and if we can continue we place the new food or check if we eat the food and we update our ui with this helper function update ui then here we have this helper function is collision where we check if we either hit the boundary or we run into ourself and then we also have this helper function move where we get the current direction and then based on this direction we simply um calculate,358,0,0,L8ypSXwyBds
13,calculate the new position of the new hat so yeah that's all um what is done here and now let's change a few things though so the first one i want to change the class name to say snake game ai to make it clear that this is a agent controlled game and now so the first thing we want is the reset functionality so in here um i already have this comment where we in it the game state so now we want to refactor all of this into a reset function so we create a new function define and then let's call this reset and it only gets self and no other arguments and here we can grab all of this code and then simply paste it in here and in our initializer we then call self dot reset so this is the first thing we need additionally we want to keep track of the um game iteration or frame iteration so let's call this self dot frame iteration and in the beginning this is just zero then this define place food can stay as it is and now we need to change the play step function so first of all if we have a look at the overview here i already told you that now we need to give this the action from the agent and we need to return a reward so let's start by um using this action parameter and here we grab the user input so actually right now we can get rid of this so the only thing we still check if we want to quit the game and,358,0,0,L8ypSXwyBds
14,now here um we already have this helper function where we move in the current direction so actually what we change here now this move function doesn't get the direction from the user input so now here it gets the action and then we have to determine the new direction so we do this in a second but first let's only change this and then here we call the self.move with the action and then we update the head then we check if we are game over or not and we actually now we also need the reward so we simply say reward equals zero and let's go back to the slides from last time so the reward is really simple whenever we eat a food we say plus 10 when we lose or when we die then we say our reward is -10 and for everything else we just stay at zero so we initialize the reward with zero then if we have a collision and game over then we say our reward equals to -10 and we want to return this as well so return the reward game over and self.score and here we check only if we have a collision so here i actually want to do another check so if nothing happens for a long time so if our snake doesn't improve and doesn't eat the food but also doesn't die then we also want to check this and if this happens for a too long time then we also break here so we can say or and then here we say if self dot frame iteration and if that this gets,358,0,0,L8ypSXwyBds
15,too large without anything happening then we um stop here so here i use this little formula if this is greater than 100 times the length of our snake so remember this is a list then we break so this is also like this then it's dependent on the length of the snake so the longer our snake is the more time it has so but then if it gets larger than this value then we break and of course we have to update the self.frame iteration and we can simply do this here at the beginning so for each play step we say self dot frame iteration plus equals 1 and when we reset it then we reset it back to zero so this is here and then yeah if we stop we have the reward -10 then here if our hat hits the food then we eat the food so our score increases and our reward is set to plus 10 then we place a new food and say otherwise we remove the last part so we simply move here then this can stay as it is the update function and at the very end we also want to return the reward then for the is collision function we need a slight change so here i only check for self.head but later um to calculate the state or the danger which i told you about so if we have a look at the state so here we calculate the danger so if we are for example if we are here at the corner then we have a danger at the right so for this,358,0,0,L8ypSXwyBds
16,it might be handy if we don't use self.head inside here but if we give this function a point so this gets the point argument and let's say by default this is none and then here we simply ch check if the point is none then we set the point equals to self dot head so inside this where we call this with no argument it can stay as it is and then here of course we have to change self.head to this is now our point so here if we hit the corner point here and point here and point here then we have a collision and here if our point is in the snake body then we also have a collision and otherwise we don't have a collision all right so the update ui function can stay like this and now for the move function here we need to change something so now we get a action and now based on this action we want to determine the next move so if we go back to the slides so here we designed the action like this so it has three values um one zero zero means we keep the current direction and go straight 0 1 0 means we do a right turn and 0 0 1 means we do a left turn so this is dependent on the current direction so if we go right and do a right turn then we go down next if we go down and do a right turn then we go left next and so on and left turn is the other way around so now um,358,0,0,L8ypSXwyBds
17,we want to determine the direction based on the action so let's write a quick comment here we have straight right turn or left turn so to get the next direction first i want to define all the possible directions in a clockwise order so we say clockwise equals and then a list and here we start with direction dot right so here remember for the direction we use this enum class so it has to be one of those directions so our clockwise directions should start with direction right then from this on the next one is direction dot down then we have direction dot left and as last thing we have direction dot up so right down left up this is clockwise and then to get the current direction or the current index of the current direction we say index equals and then we can say clockwise dot index and then the index of the self dot direction so we are sure that this has to be in this array we because the self direction must be one of those enum values and then we check that different um possible states so these ones so for this we can use numpy and i guess we have to import numpy first as np and then we can use it here we can say if numpy and then we use this function array equal and then here we put in the action and the array that we want to compare so if this is equal to one zero zero then we go straight or we keep the current directions so we simply say our new direction,358,0,0,L8ypSXwyBds
18,equals and then clockwise of the index and then remember the index is just the index of the current direction so here we basically have no change then we say l if if our array if numpy array equal if the action equals to 0 one zero then we do a right turn so this means we go clockwise so if we go right then the next direction would be down if we go down then the next direction would be left and if we go left then the next direction would be up so here we say index equals or this is our next index actually and here we say this is the current index plus 1 but then modulo 4 so this means if we are at the end up and then do the next one if we have index so this is index 0 1 2 3 and then if we have index 4 modulo 4 is actually again index zero again so from this we do a turn and then come back at the front again so this is our right turn so now this is the next index and now our new direction is clockwise of the next index and then otherwise we can simply use else here and actually change this to an l if so now this is the last case so it has to be here it has to be zero zero one and if this is the case then let's copy and paste this in here then our next index is the current index minus one modulo four so this actually means we go counter clockwise so,358,0,0,L8ypSXwyBds
19,we do a left turn so if we start with right then the next move would be up and then the next would be left and then the next would be down and then right again and so on so now this is our new direction and then simply we say self direction equals new direction and then we go on so here we extract the head and then here we have to check if self dot's direction now is right then we increase the position of x and so on if we um have the left direction then we decrease x and if we go down then we actually increase y so for so the y starts at the top at zero and then increases if we go down so if we go down then we have to increase y and if we go up then we have to decrease y so if self direction equals up then y minus equals the block size and by the way the block size is just here a constant value of 10 so that's how big our one block of the snake should be in pixels so yeah this is everything we need here in the move function and now here we don't need this anymore so this is actually no longer working with a user input so you can just delete this and then later we control this class from the agent and call this play step function so yeah for now this is all we need to implement the game so i already talked about the theory of deep q learning in the first part in,358,0,0,L8ypSXwyBds
20,the last part we implemented the pi game so that we can use it in our agent controlled environment and now we need the agent so let's start and so here um if you haven't watched the first two parts then i highly recommend to do so so this is the starting point from last time and i actually want to make one more change that i forgot so here the is collision function should actually be public because then our agent should use it so just remove the underscore here and then also remove it in this class itself when we call this so then we have our snake game and i also want to rename this to just be game and now we create a new file agent dot pi and then start implementing this so first here we import torch from pi torch then we import random because when later we need this then we also need import numpy snp and from our um implemented class we need the snake game so we say from game import snake like snake game a i so i think that's what we call this class snake game a i so yeah that's the right name then we also hear at the beginning we defined this enum for the direction and this named tuple for the point which has an x and a y attribute so we also want to import these two um things so we import direction and we import point and then we also say from collections we want to import deck so this is a data structure where we want to store our,358,0,0,L8ypSXwyBds
21,memories so um if you don't know what a deck is then i will put a link in the description below so this is really handy in this case and you will see why this is the case later and then here i want to define some parameters as constants so we have a maximum memory of let's say 100 000 so we can store 100 000 items in this memory then we also want to to use a batch size that you will see later and here i will set this to 1000 so you can play around with these parameters and i also want a learning rate later and i want to set this to 0 0 1 and yeah feel free to change this and then we start creating our class agent and it gets of course an init function with self and no other arguments and then let's have a look at the slides from the first part where i explained the training so we want to create a training function where we do all of this so we need to get the state calculate the state where we are aware of the current environment then we need to calculate the next move from the state and we need to um then we want to update or do the next step and call game.playstep and then calculate the new state again then we want to store everything in memory and then we also want to train our model so we need to store the game and the model in this class so first of all let me create the functions that we,358,0,0,L8ypSXwyBds
22,need first so we need a function get state which gets self and this this gets the game and then we calculate the state that i showed you with these 11 different variables then we want to have a function that we call remember remember and it has self and here we want to put in the state then the action then we want to remember the reward for this action and we want to calculate or we want to store the next state next state and we also want to store done or bit or you can also call this game over so this is the current game overstate then we need two different functions to train and we call this defined train on the long memory and it only needs self so i will explain this later and we also let's copy and paste this i also have a function define train on short memory so this is only with one step you will see this later then we need a function and we call this get action to get the action based on the state so it gets self and the state and first we only say pass and these are all the functions we need i guess and then i want to have a global function that i call simply train and here we say pass and then when we start this module h and dot pi so we say if name underscore equals equals main then we simply call this train function and then we can start the script by saying python agent dot pi like i did in the very,358,0,0,L8ypSXwyBds
23,first tutorial so let's start implementing the agent and the training function so let's start with the init function of the agent so here what i want to store is first i want to store some more parameters so self.number of games so i want to keep track of this so this is zero in the beginning then self.epsilon equals um zero in the beginning this is a parameter to control the randomness so you will see this later then we also need self dot gamma equals zero so this is this is the so-called this count rate which i briefly showed in the first tutorial i will explain this a little bit more in the next tutorial where we implement the model and the actual deep q learning algorithm then we want to have a memory so we say self.memory equals and for this we use this stack and this can have a argument max leng equals and here we say max memory and what then happens if we exceed this memory then it will automatically remove elements from the left so then it will call pop left for us and that's why this deck is really handy here and then later here we also want to have our model and the trainer so i will leave this for a or s to do for the last part in the next video and now this is all for the init function and now we can go back and now let's do the training function next so again let's have a look at these slides so we need these functions in this order so let's first,358,0,0,L8ypSXwyBds
24,let's write some comments of first let's create some lists to keep track of the scores so this is an empty list in the beginning and this is used for plotting later so then we also want to keep track of the mean scores or average scores this is also an empty list in the beginning then our total score equals zero in the beginning our record our best score is zero in the beginning then we set up a agent so agent equals an agent and we also need to create a game so the game is a snake game ai object and then here we create our training loop so we say while true so this should basically run forever until we quit the script and now here let's write some comments so we want to get get the old state or the current state so here let's say state old equals and then we call agent dot get states and this gets the game so we already set this up correctly we only have to implement it then then after this we want to get the move based on this current state so we say the final move equals agent dot get action so we actually called this action and the action is based on the state then with this move we want to perform the move and then and get new state so for this we say reward um done and score equals and here we call game dot play step from last time so i think game dot play step with the action yes game dot play step and this gets,358,0,0,L8ypSXwyBds
25,the final move and then we get the state old or the new now the new state state new state new equals agent and again gets state now with the new game then after that we want to train the short memory of the agent so only for one step so for this we say agent agent dot train short memory and this gets if we have a look here um actually uh this short memory should get some parameters so exactly the same as we put in the remember function so train short memory gets all of those variables and then here when we call this now we should get some hints strain or let's save this file and then say agent dot train short memory and now we should get the hints no we don't get this but actually we want to have the state action reward next state and done so here let's do this so say let's say state old then the action which was the final move then the reward then the state new and adds last thing the done or game over state variable so now we have this then we want to remember all of these and store this in the memory so we say agent dot remember and then here it gets the same um variables so we want to store all of this in our deck and then this is all we need so now we check if done or if game over then if this is true then what we want to do is um we want to let's write a comment train the long memory,358,0,0,L8ypSXwyBds
26,and this is also called replay memory or experience replay and this is very important for our agent so now it trains again on all the previous moves and games that it played and this tremendously helps him to improve itself and we also here want to plot the results so first of all we want to reset the game so we can simply do this by saying game dot reset we already have this function here so this initializes the game state and resets everything so the score the snake the frame iteration and places the initial snake and the food so now we have this then we want to increase agents dot number of games so this plus equals one then we want to say agent dot train long memory and this doesn't need any arguments then we want to check if we have a new high score so if score greater than the current record then we say record equals our new score and we will also want to leave this as a to do here so here we want to say agent dot model dot save later when we have the model and so here in the here we want to store this as self.model and now what we also want to do here um let's print some information so print the game and then the current number and then the score and the record so here let's say our game is agent dot n games then we also want to plot the or print the score so this is just the score and we want to print the current record so,358,0,0,L8ypSXwyBds
27,the record equals record and then here we want to do some plotting so i will implement this in the next tutorial so i will leave this s8 to do so this is all for our training function so what i showed in the slides and now of course we have to implement those functions so for the get state function um let's go back to this overview and here as i said we store 11 values so if the danger is straight right or left then the current direction so only one of these is one and then the position of the food if it's left of the snake right of the snake up or down of the snake so these are the 11 states and now let me actually copy and paste the code in here so that i don't make any mistakes but we will go over this so first let's grab the head from this game so we can do this by calling game dot snake zero so this is a list and the first item is our head then um let's create some points next to this head in all directions that we need to check if this hits the boundary and if this is a danger so for this we can use this named tuple so we can create a point with this location but minus 20 so the 20 is hard coded here so this is the number that i used for the block size so like this we create four points around the head then the current direction is simply a boolean where we check if the current,358,0,0,L8ypSXwyBds
28,game direction equals to one of those so only one of those is one and the other one is zero or false and then um we create this array or this list with this 11 um states so here we check that if the danger is straight or ahead and this is dependent on the current direction so if we are going right and the point right of us gives us a collision then we have a danger the same or or if we go left and our left point gets a collision then we also have a danger here and so on so this is dangerous straight and then danger right means if we go up and the point right of us would give a collision then we have a danger for a right turn basically and so on and the same for the left so this might be a little bit tricky so i recommend that you pause here and go over this logic for yourself again so yeah these only have give us three values in our state so far then we have the move direction where only one of them is true and the other one is false and for the food location we simply check if food if game food x is smaller than game head x then we have food is left of us and the same way we check for right up and down and then we convert our list to a numpy array and say the data type is in so this is a nice little trick to convert this true or false booleans to a zero or,358,0,0,L8ypSXwyBds
29,one so yeah now this is the get state method now let's move on to the remember function so here we want to remember all of this in our memory so this is a deck and this is very simple so here we say self dot memory and then the deck has also the append function where we want to append all of this in this order so the state the action the reward the next state and the game over state and as i said if this exceeds the maximum memory then pop left if max mem memory is reached and yeah this is the remember function then let's start implementing the train long and short memory functions so for this so we actually we store a model and a trainer in here so let's actually say self dot model equals let's say this is only none in the beginning and leave a to do and self dot trainer equals none in the beginning and this is a to do so these are objects that we create in the next tutorial and then here we call this trainer to actually do the optimization so let's start here so for only one step we say self.trainer and then this should get a function that we call let's call this train step and then it gets all of these variables so the state the action the reward the next state and the game overstate and this is all that we need to train it for only one game step and we design this function um so that it takes either only one state like this but it,358,0,0,L8ypSXwyBds
30,can also take a whole tensor or a numpy array and then uses multiple as a so-called batch so let's do this here so for this we take the variables from our memory so here we want to grab a batch and so in the beginning we defined the batch size is 1 000 so we want to grab 1 000 samples from our memory but first we check if we um already have a thousand samples in our memory so we say if lang and self dot memory if this is smaller then the batch size then we simply or actually let's say if this is greater so if this is greater than we want to have a random sample and say mini sample equals and then we want to get a random sample so we can use random dot sample so we already imported the random module random dot sample from self dot memory and as a size it should have the batch size so this will return a list of tuples and this is because here i forgot one important thing so when we want to store this and append this we want to append this as only one element so only one tuple so we need extra parenthesis here so this is one tuple that we store and then here we get the batch size number of tuples and otherwise else if we don't have uh a thousand elements yet then we simply take the whole memory so we say mini sample equals self dot memory and then we again want to call this training step and for this so let's call,358,0,0,L8ypSXwyBds
31,this here again self.trainer.trainstep but here we have multiple states so let's call this states actions rewards next states and done and right now so now we have it in this format that we have one tuple after another and now we want to extract this from the mini sample and then put every states together every action together every reward to it together and so on and this is actually a really simple with python so we can say we want to extract the states the actions the rewards the next states and the duns game overs and here we simply use the built in sip function and have to use one asterisk and then the mini sample argument so yeah check that for yourself if you don't know how the sip function works but again it now it puts every states together every actions and so on if this is too complicated for you then you can also just do a for loop so you can iterate over this mini sample and basically say for action or for state action rewards next state and done in one mini sample and then again you call this here for only one for only one argument so yeah you can do it both ways but actually i recommend to do it this way because then you have this as only one argument and then you can do this faster in pytorch all right so now we have both the training functions now we only need the get action function so here in the beginning we want to do some random moves and this is also called a,358,0,0,L8ypSXwyBds
32,trade-off between exploration and exploitation in deep learning so at some point or in the beginning one we want to make sure that we also make random moves and explore the environment but then the better our model or our agent gets the less random moves we want to have and the more we want to exploit our agent or our model so yeah this is what we want to do here so for this we use this epsilon parameter that we initialized in the beginning so for this let's implement this first so we say self dot epsilon equals and this is dependent on the number of games so here i hard code this to 80 minus self dot number of games you can play around with this and then let's get the final move so in the beginning we say zero zero zero and then one of those now has to be true so here first let's check if random dot rent int and here between 0 and 200 if this is smaller than self dot epsilon then we take a random move so we say move equals random dot rant ins and this must be between 0 and 2 so the 2 is actually included here and this will give us a random value 0 1 or 2 and now this index must be set to one so we say final move of this move index equals one and yeah so so the more games we have the smaller our epsilon will get and the smaller the epsilon will get the less frequent this will be less than the epsilon and when this,358,0,0,L8ypSXwyBds
33,is even this can even become negative and then we don't longer have a random move so again if this was too fast here then feel free to pause and think about this logic again so now we have that and otherwise else so here we actually here we want to do a move that is based on our model so we want to get a prediction prediction equals self dot model dot predict and it wants to predict the action based on one state so we call the state zero and we get this here but we want to convert this to a tensor so we say state 0 equals torch dot tensor and as an input it gets the state and we also give it a data type equals let's use a torch dot float here then we call self.model predict with the state this will give us a prediction and this can be a raw value so if we go back to this slide so this can be a raw value and then we take the maximum of this and set this index to a1 so here we say our move equals and we get this by saying torch arc max and the arc max of the prediction and this is a tensor again and to convert this to only one number we can call the item and now this is an integer and now again we can say final move of the smooth index is one and now we have this so now we return the final move here return and yeah this is all we need so now we have this,358,0,0,L8ypSXwyBds
34,and can save it like this and now we have all that we need for our agent class and now in the next one so what we must do here is implement the model and the trainer and then also the plotting so let's go back to the code and here i left this essay to do so we need a model and a trainer so let's create a new file and let's call this model dot pi and then here let's first import all the things we need so we need import torch then we want to import torch dot n n s n n then we want to import torch dot optim s optim and also import torch dot n n dot functional s capital f and we also want to import o s to save our model and now we want to implement two classes one for the model and one for the trainer so let's create a class and let's call this linear underscore qnet and this has to inherit from nn dot module module and by the way if you are not comfortable with pytorch and want to learn how to use this framework then i have a beginner series here on this tutorial for free and i will put the link in the description so this will teach you everything to need to get started with pytorch so right now let's start implementing this linear qnet function so we need the init function define init and we need to have self and this gets an input size input size a hidden size and an output size and then the first,358,0,0,L8ypSXwyBds
35,thing we want to do is to call this super initializer so we call super in it and here um this is very simple so if we have a look at the slides then our models should just be a feed forward neural net with a input layer a hidden layer and an output layer um feel free to extend this and improve this but it works fine for this case and it's actually not that bad here so let's create two linear layers so let's call this self.linear1 equals nn.linear and this gets the input size as an input and then the hidden size as the output size then we have self.linear2 equals nn.linear and now this gets the hidden size as the input and the output size as the output then as always in pi torch we have to implement the forward function with self and it gets x so the tensor and here what we want to do is first we want to apply the linear layer and we also use an actuation function here so again if you don't know what this is then check out my beginner tutorial series there i explain all of this so we say x and then we can call f dot reloose we use this directly from the functional module and here we say self dot linear one with our tensor x as the input so first we do the linear layer then we apply the actuation function and then again we apply the second layer so we call self dot linear 2 with x and we don't need an actuation function here at the end,358,0,0,L8ypSXwyBds
36,we can simply use the raw numbers and return x so this is our forward function then let's also implement a helper function to save the model later so let's call this self safe and this gets the file name as an input and i use a default here so we say model dot pth is simply the file name and then the last time i think i already called this function um so not yet but now we can comment this out so if we have a new high score then we call agent dot model dot save and here let's create a new folder in here so let's say this is the model folder path equals and let's create a new folder in the current directory and call this model so dot slash model and then we check if this already exists so the file in this folder so we can say if not os dot path dot exists and then we say our model folder path then we create this so we say os dot makers and we want to make this model folder path then we create this final file name so we say file name equals os dot path dot join and we want to join the model folder path and the file name that we use here as the input now this is the file name for saving and then we want to save this and we can do this with torch dot save and we want to save self dot state dict so i also have a tutorial about saving the model we only need to save this state,358,0,0,L8ypSXwyBds
37,dictionary and then as a path we use this file name so now this is all we need for our linear q net and now to do the actual training and optimization we also do this in a class that i call q trainer q trainer and now here what we want to do we want to implement a init function which gets self then it also gets the model then it should get a learning rate and it should get a gamma parameter and here we simply store everything self.lr equals lr self dot gamma equals gamma and we also store the model so we say self dot model equals model then to do a pie charge optimization step we need a optimizer so we can create this by calling self.optim or let's call this optimizer equals and we get this from the opt-in module and here you can choose one optimizer so i use the atom optimizer and we want to optimize model.parameters and this is a function and then it also needs the learning rate so lr equals self dot l r and now we also need a criterion or a loss function so let's call this self dot criterion equals and now if we go back to these slides at the very end we learned in the first part that this is nothing else than the mean squared error so that's very simple so we can create this here by saying self.criterion equals so this is nn.mse loss and now this is what we need in our initializer and then we also need to define a we call this train step function,358,0,0,L8ypSXwyBds
38,which gets self and then it needs to have all the stored um parameters from last time so it needs to have or let's have a look at this so here when we call this it needs the state the final move the reward the new states and done so let's copy and paste this in here and rename this slightly so this is just the state this is the action this is the reward so this is the new state this can uh let's call this next state here and then done can stay as it is and for now let's simply do pass here and before we implement this let's go back to the agent and now set this up so here we say from and we call this model and we want to import the linear i think we call this linear q net and q trainer and then here in the initializer we want to create an instance of the model and of the trainer so self.model equals our linear qnet and now this needs the input size the hidden size and the output size so here i use 11 256 and three so remember if we have a look at the slides again um the first one is the size of the state so this is 11 values and the output must be three because and we have three different um three different numbers in our action and you can play around with this hidden size but the other ones have to be eleven and three so this is the model and the trainer equals the q trainer and this gets,358,0,0,L8ypSXwyBds
39,the model so self.model then it gets the learning rate equals the learning rate which we specified here and we also pass on the gamma value so gamma equals self dot gamma and the gamma is the discount rate so i this has to be a value that is smaller than 1 and usually this is around 0.8 or 0.9 so in this case let's set this to 0.9 so you can play around with this as well but keep in mind that it must be smaller than one so now we have this and then i made one error in the last tutorial so this is very important that we fix this right now so here in the get action function i actually i called this self.model predict but actually pythog doesn't have a predict function so this would be the api for tensorflow for example so in pi torch we simply call self.model like this and then this will execute this forward function so this is actually the prediction then so yeah please make sure to fix this okay so now we have everything and if we have a look and go back then we see we call this self.trainer train step with only one parameter but also with multiple ones so we want to make sure that we can handle different sizes so now let's start implementing this function and now the first thing we want to do so right now this can be um either a tuple or a list or just a single value so let's convert this to a pi torch tensor so let me copy and paste this in,358,0,0,L8ypSXwyBds
40,here so we do this for the states the next state the action and reward and we can do this by calling torch.tensor and then the variable and we specify the data type to torch dot float and we don't have to do this for the done or game over value because we don't need this as a tensor and now we want to handle multiple sizes so we want to check if the length and then we can check state dot shape and if this is one then we only have one dimension and then we want to reshape this so right now we only have if this is the case then we only have one number but actually we want to have it in the form one and then the values so this is the number of um batches so if this is already if this has already multiple values then it's already in the in the size n and x so then it's already correct and now here we want to append one dimension and we can do this with the torch unsqueeze function so we can say state equals states dot or sorry not state but torch dot un squeeze squeeze and then the states and we want to put it in dimension zero or axis zero so this means that it appends one dimension in the beginning and this is then just one then i also wanted to do this for the other um tensors so for the next state and for action and reward and the done value we also want to convert this right now this is only a,358,0,0,L8ypSXwyBds
41,single value and we want to convert this to a tuple so we can do it like this so now we have a done so this is how you define a tuple with only one value and now um we have it in the correct shape so now what we have to implement is um from last time or from the very first tutorial where i showed this bellman equation and then we simplified this so we have the old queue where we simply call model predict with the old state and the new queue with this formula so let's do this so first let's um write a comment here so as first thing we want to get the predicted predicted q values with the current state and this is simply by doing let's call this prediction equals self dot model and then we want to do this with state 0 or we just call this state here and then for the second part we need this formula the reward plus the gamma value times the maximum of again model predict with state one so first let's write this as a new uh comment so the first thing is we want to apply this formula reward plus gamma times and then the next predicted q value and then we want to have the maximum so the maximum of this so maximum and then um this is a little bit tricky so the maximum of this um sorry let's do it like this maximum of next predicted q value so this is only one value but um if we do it like as first um parameter the,358,0,0,L8ypSXwyBds
42,predictions this has actually this is the action this is actually three different values so what we do to get the same here is we do a clone of this and then we set the index with this action to the new q value so this is let's call this q new like i showed you in the formula and then we set the let's call this predictions and then the index of the arc max of the action we set this to our q new value so again this might be tricky so again we want to calculate the new q value with this formula that i showed you but then we need to have it in the same format and for this we simply clone this so then we have three values again and two of the values are the same but the value with the action so the action is for example one zero zero so um the index of the one is then set to the new q value so this is what we want to do here so for this let's first let's create a clone target equals prediction dot clone so we can do this with a pi torch tensor and then um we want to iterate over our tensors and apply this formula so for this we say for index in range and then the length of the let's call this done and here everything should have the same size so then this works so now we iterate over this and then one thing that i didn't mention so far is that we only only want to do this,358,0,0,L8ypSXwyBds
43,only do this if not done um otherwise we simply take the whole reward so we say q new equals reward of the current current index and now we check if we are not done so we say if not done and the done is of the current index then we apply this formula so now we say q new is actually um the reward so the reward of the current index plus self dot gamma and then times torch dot max the maximum value of the next prediction so here's self dot model of next state of this index so this is exactly what we have written here and now we need to set the target of the maximum value of the action to this value so here we get the let's we call this target so the target of the current index and then of the arc max of the action so for this we can again say torch dot arc max of the of the action and we want to have this as a item so as a value and not as a tensor and now this is our q new value so this might be a little bit tricky to understand so i recommend that that you pause here and go over this again and now we have everything that we need so let's have a look at the slides again we have our q and our q new and then we apply the loss function so the mean squared error and in pi torch so what we have to do here we can simply use this optimizer and do a step,358,0,0,L8ypSXwyBds
44,and first we have to call this zero grad function to empty the gradient so this is just something that we have to remember in pi torch and then we calculate the loss by calling self dot criterion and here we put in the target and the prediction so this is q new and q and then we call loss dot backward and apply back propagation and then update our gradients and then we call self.optimizer.step and this is all that we need in this training step and now this is actually all that we need in this model file so now again let's go back to the agent and i guess we already set up the q trainer and then when we train this we call this train step function either for only one of those parameters or for a whole batch and now this function can handle different sizes and now the only thing left to do here is to actually to plot the results so for this let's create a new file and let's call this hell helper dot pi and then here let me actually copy and paste this in here so this is just a simple function with matplotlib and i python and yeah here we want to plot the scores so this is a list and we want to plot in the plot the mean score so let's create them so here in the agent we say from helper import the plot function and then down here in the training function so we already created an empty list for the scores and for the mean scores and now after each,358,0,0,L8ypSXwyBds
45,um game we want to append the score so let's remove the to do and implement this so we say plot scores dot appends and then the current score then let's calculate the new mean or average score so for this let's say total score plus equals the score and then let's call this mean score equals the total score divided by the number of games so agent and games and then we append this to plot mean scores dot append the mean score and then we simply call the plot function with the plot scores and then the plot mean scores and now let's save this file and also let's save this file and then let's try it out so in the terminal let's call agent dot pi and let's cross fingers so we have a syntax error in the model.pi file so um here we actually here we have two equal signs so let's fix this and save this and run it again and then we made another mistake name error so here this is actually called prediction.clone so again let's save this and run this and now it starts training without crashing and it also plots so let's let this run and see if this is improving music all right so as we can see the algorithm works and the snake is getting better and better and the scores are getting higher and higher and also the mean or average score is getting higher so i forgot one important thing which i show you in a second but for now um so the snake is not perfect and the main issues are that,358,0,0,L8ypSXwyBds
46,it traps itself sometimes and also sometimes it gets stuck in an endless loop sequence so this is something that you can improve as a homework so yeah like this it now it trapped itself so yeah let me stop this actually and then show you what i forgot so in the game we can actually um set the speed so for the human controlled game when i want to play this i set this to 20 but now i recommend to set this to a larger number so that the training will be faster so for example you can use 40 here or even higher so i go with 40 and yeah i think that's the whole code you can also find this on github and yeah i hope you really enjoyed this little series about reinforcement learning and if you enjoyed this then please hit the like button and consider subscribing to the channel and then i hope to see you next time bye,215,0,0,L8ypSXwyBds
0,in the previous video we went over how to set up an ai environment suitable for reinforcement learning applications for the dinosaur game so if you haven't checked that out already i highly recommend doing so if you're not already familiar with the ai gym setup so in a reinforcement learning agent environment a reinforcement learning task is to train an agent that interacts with its environment a specific scenario is considered as state and with each scenario the agent predicts an action to perform in the dinosaur game the action will either do nothing or jump and the main objective of the agent is to maximize its reward across an episode the reward is a distance and an episode is everything between the initial state and the terminal state within the environment we continuously reinforce the agent to learn to perform the best actions and this is known as a policy q learning is where the agent will perform a sequence of actions which will presumably maximize the total reward the q value can be determined by the perceived reward at the current state and performing action plus a gamma term multiplied by the highest q value by the next state the gamma term is adjustable and this determines the contribution of reward in the future now a deep q network uses the logic of q learning in conjunction with a three-layer convolutional network let's go over the high-level steps for our dqn demonstration first you want to feed a game screen which is considered to be the state to the deep q network which will return q values for all possible actions in the,358,0,0,nCgd9lrmYwE
1,current state you will select an action using the epsilon greedy policy and initially the action will be random but based on the random action you will choose the action that has a maximum q value you'll then perform the action and move to the next state to obtain the reward you're going to store this transition to a memory buffer as well you'll then sample random batches of transitions from the memory buffer to calculate the loss okay so i'm going to be going through on how to implement a dqm in relation to the reinforcement learning process related to the ai dinosaur game note that these links are attached with this particular script provided a lot of inspiration and a lot of details that i actually use throughout this entire script over here so do make sure to check them out these links will still be attached to the script when i push all of my changes to my github repository which is attached down into the description now let's go over the main function really quickly so the only thing i'm really doing here is that i'm just initializing my t-rex environment in this case and then i'm calling the run function uh note that all these other variables that you know just commented out are related to just recording each one of the given i guess like law scores actions or q values but i did not really i didn't actually implement it but due to the complexity but you could do so by just you know passing in the data frames a part of the function and just incorporate each of,358,0,0,nCgd9lrmYwE
2,the roaches that append the rows to its associated data frame and you can have somewhat of a logging structure related to your data frames before we go to the run function let's go ahead and go over the initialization part so this is just the batch size related to however many images that we want to extract from the overall memory array that we have going on um and this is we're just going to be calling out 32 random images and then we're going to be training uh the predictive network um and you know updated weights we have 10 000 games over here uh where each game is this is also in sequential order so it's like with each game being the dinosaur starting and then it ending on some obstacle because it failed the game uh we initialize our environment we have our action size and then we initialize our agents where our agent is essentially our dqm model our action size is two uh it's going to be initialized uh well you're gonna be calling it zero or a one depending on what action it's going to be where zero is doing nothing and a one is where the dinosaur is actually jumping over the obstacles so let's go over the agent really quick um this is the agent class so we have some additional initialized parameters here we have the storage of where our model exists we have two for action size and this is our memory array where i mentioned earlier our batch size will be purchasing 32 random images from this memory and it'll retrain the prediction network,358,0,0,nCgd9lrmYwE
3,uh we have our epsilon fundament and gamma which is primarily used to update the q values we also have our decay over here so the epsilons are pretty much just used to update or decide whether or not we should be using a random value right here to you know predict what the dinosaur should be doing or should it rather go ahead and you know utilize the actual model itself and predict uh based on what state is currently at uh but we use that epsilon value to determine if we should just use random actions or so okay so after that uh we have our dimensions of our given image in this case 300 300 by one where one is the number of channels and then this is the rows and this is the height so it really depends on how large your dimensions of your specific image you would want it to be but in this case i just made it so that's 300 by one we have of course our update rate and this is for every 1000 iterations where we want to update the weights of our predictive network to our target network so our predictive network is the one that's continuously being trained right and it's like slowly diverging from the original model which is just uh right here our model portion right here now actually no it's going to be our target model i should say our target model is our original model and our predictive model is just our model in this case so uh the primary reason behind this is that it's just the dq1 process,358,0,0,nCgd9lrmYwE
4,of where it has like two neural network models is training one and then updating the old one um after you know after in this case 1000 iterations for the update rate here uh and primary reason behind this is to sort of keep the models in check and you're going to have two different q values based on the predictive model and the target model itself and then the loss function will be you know subtraction of those two and square those values that is going to be the loss uh nonetheless after that after you initialize the values and update its weight i just print out the summary over here so what the model looks like over here is that we're just i'm using a keras by the way keras back end but it's gonna be three convolutional layers and connect it with a fully connected layer of 512 and then go ahead and create the number of outputs with whichever number of actions you can be using in this case i have two so this could be ending with two output layer output nodes i should say uh based on doing nothing or jumping um and then just activating with linear and of course use the atom optimizer and then you have your model and then the other functions are just you know update the target model based on the weights uh based on the predictive model weights i should say you have a safe model function and then this is our act function which is as i mentioned earlier this determines whether or not the dinosaurs can be you know jumping or,358,0,0,nCgd9lrmYwE
5,doing nothing based on a zero or a one value and that's what these logics are doing over here where it's using the epsilon to determine whether knowledge can be using a random state or a predictive state either or is going to have one of those values and then we have a remember function and then this is just appending images and it's associated action reward next date and done we'll go a little bit deeper into this uh but this is what the deck of 5000 images are going to be appended by using this function over here and then our replay this is just going to be going through our given uh memory um based on this memory over here it's going to be iterating through the memory uh and it's going to be in batch sizes of 32 images based on the state action reward and next day done et cetera and then it will decide you know whether or not it's going to be updated in target q value or just be using the regular q value if it's not done or done and then you just go through the process over here of where it's going to be iteratively training its model um based on the predictive network model okay and then once the model has been fit and everything like once it's done entering through the mini batch we will then update the epsilon value if it's greater than our epsilon minimum which in this case the epsilon minimum was a value of 0.1 over here so if it's greater than that it will go ahead and decay the,358,0,0,nCgd9lrmYwE
6,epsilon by multiplying by the f on the k which is 99.5 percent as noted there okay so let's go to the run function that is essentially underneath the dyno initialization part and it's over here and so very high level i have a try finally statement going on so if i want to mainly get out of this particular program it will save the model inside of the model's dyno runner.h5 file and i will automatically have the most up-to-date model uh if i want to exit out this particular program alright so we're gonna be iterating through 10 000 games over here and then we have total reward game score these are going to just for like you know um understanding purposes on our given output for our print statements uh we're going to have you know a decimal value or so if the geyser is continuing on his way uh as a increment of 0.1 or if he loses those could be a decrement of one so we're gonna have like a positive or negative value reward system going on here we're also having gonna have a game score pretty much to determine you know how far the dinosaur has dropped now at the very beginning of each game we'll be resetting our environment so that we're at the very beginning of the game that's pretty much what that is happening here now this is very important uh we have a deck of a length four determined by blend over here um and i'm just gonna be appending the states to this deck now primarily what this uh deck is gonna be doing,358,0,0,nCgd9lrmYwE
7,is that it's gonna be an amalgamation of four images specifically using this blend images function which is essentially just taking the average of all the pixels that exist within that deck if there's less than four values or four pictures i should say we do take that into consideration and we just take the average of whatever else is in there so that's pretty much happening there and the reason behind that is that it's gonna get pretty much just four frames all together and then you know uh to generate some form of a predictive outcome for the next state uh and then we'll be plugging in that predictive state into the action function to retrieve some form of an action whether or not the dinosaur should be jumping or you know doing nothing essentially so we plug in that action to the step function over here and then we get four additional values that will be the next image that comes along the associated reward with that with the previous action that we have already taken uh the associated score you know how well uh the uh dinosaur has done so far you know just get the the roadmap score and how much distance has the dasher traveled and whether or not the dancer has actually hit an obstacle and then we should be appending that that new image to the image deck and then we're going to do another blending sessions to get the next appropriate measure we're then going to be plugging that into our memory buffer over here um and this is where we are actually going to be mini,358,0,0,nCgd9lrmYwE
8,batching from so we're going to be sampling 32 images or i should say 32 values from the array that this remember function is appending to and of course we have you know um some additional reward systems are you know incrementing or decrementing and the old state becomes a new state and it keeps on iterating through once this for loop has run its course and of course if the dinosaur has crashed we will then want to train based on his previous experiences and hopefully we can get all of the all the failures and successes based on that one game in order to you know hopefully improve upon you know its own understanding of how the game operates so let's do a really quick demo on what that might look like let's go over here and all you do just do python and then dyno run go ahead and run that and then a selenium web browser just popped up over here that's good this is our model that was initialized that we printed out using the self.model this is our uh it is our target model and for the very first 100 or so iterations that's going to be going through it will somewhat like you know like hang up on this however within each of the episodes i'll be going through it will become more streamlined as time goes on so this is pretty much my demonstration on the reinforcement learning aspects of the dinosaur games and after about 9 000 or so iterations that i've ran through the highest score i actually got from this reinforcement learning process of this,358,0,0,nCgd9lrmYwE
9,dinosaur doing its thing was about 150.,9,0,0,nCgd9lrmYwE
10,there are definitely a few ways that this process can be improved first i can use an entirely different deep cue model such as a source model or a or a double deep q model for that instance i can also switch up the policies that determine which action the model decision used initially this particular policy i was using was a epsilon greedy model and this policy can't change furthermore i can also adjust how frequent the prediction network updates the target network weights for faster learning and lastly to speed up training time i could also run the selenium web application in parallel to have multiple games running for faster results nonetheless i ran this process for about 24 hours and only achieved a score of 148 so there is definitely room for improvement,176,1,1,nCgd9lrmYwE
0,what's up guys welcome back to this series on reinforcement learning in this video we'll finally bring artificial neural networks into our discussion of reinforcement learning specifically we'll be building on the concept of cue learning we've discussed over the last few videos to introduce the concept of deep cue learning and deep cue networks this will move us into the world of deep reinforcement learning so let's get to it from everything we've discussed over the last few videos we should now be comfortable with the idea of cue learning now while it's true that the key learning algorithm that we use to play frozen lake may do a pretty decent job in relatively small state spaces its performance will drop off considerably when we work in more complex and sophisticated environments in frozen lake for example our environment was relatively simplistic with only 16 states and four actions giving us a total state action space of just sixteen by four meaning we only had 16 times four or 64 cue values to update in the cue table given the fact that these q-value updates occur in an iterative fashion we can imagine that as our state space increases in size the time it will take to traverse all those states and iteratively update all the q values will also increase think about a video game where a player has a large environment to roam around in each state in the environment would be represented by a set of pixels and the agent may be able to take several actions from each state the iterative process of computing and updating q values for each state,358,0,0,wrBUkpiRvCA
1,action pair in a large state space like this becomes computationally inefficient and perhaps infeasible due to the computational resources and time this may take so what can we do when we want to step up our game from a simple toy environment like frozen lake to something more sophisticated well rather than using value iteration to directly compute q values and find the optimal q function we instead use a function approximator to estimate the optimal q function well you know what can do a pretty darn good job at approximating functions artificial neural networks will make use of a deep neural network to estimate the q values for each state action pair in a given environment and in turn the network will approximate the optimal q function the act of combining q learning with a deep neural network is called deep q learning and a deep neural network that approximates the q function is called a deep q network or d queuing let's break down how exactly this integration of neural networks and q learning works will first discuss this at a high level and then we'll get into all the nitty-gritty details suppose we have some arbitrary deep neural network that accepts states from a given environment as input for each given state input the network outputs estimated q values for each action that can be taken from that state the objective of this network is to approximate the optimal q function and remember that the optimal q function will satisfy the bellman equation that we covered previously with this in mind the loss from the network is calculated by comparing the outputted,358,0,0,wrBUkpiRvCA
2,q values to the target q values from the right-hand side of the bellman equation and as with many standard networks the objective here is to minimize this loss after the loss is calculated the weights within the network are updated via stochastic gradient descent and back propagation again just like with any other typical network this process is done over and over again for each state in the environment until we sufficiently minimize the loss and get an approximate optimal q function so take a second now to think about how we previously use the bellman equation to compute and update q values and our q table in order to find the optimal q function now with deep q learning our network will make use of the bellman equation to estimate the q values to find the optimal q function so we're still solving the same general problem here just with a different algorithm rather than making use of value iteration to solve the problem we're now using a deep neural network alright we should now have a general idea about what deep q learning is and what at a high level the deep q network is doing now let's get a little more into the details of the network itself so we discussed earlier that the network would accept states from the environment as input thinking of the game frozen lake that we played last time we could easily represent the states of this environment using a simple coordinate system from the grid of the environment if we're in a more complex environment though like a video game for example then we'll use images,358,0,0,wrBUkpiRvCA
3,as our input specifically we'll use still frames that capture states from the environment as the input to the network the standard pre-processing done on the frames usually involves converting the rgb data into grayscale data since the color in the image is probably usually not going to affect the state of the environment additionally will typically see some cropping and scaling as well to both cut out unimportant information from the frame and shrink the size of the image now actually rather than having a single frame represent a single input we usually will use a stack of a few consecutive frames to represent a single input so we'd grab say four consecutive frames from a video game we then do all the pre-processing on each of these four frames we mentioned earlier the greyscale conversion the cropping in the scaling and then we'd take the pre-processed frames and stack them on top of each other in the order of which they occurred in the game we do this because a single frame usually isn't going to be enough for our network or even for our human brains to fully understand the state of the environment for example by just looking at this single frame from the atari game breakout we can't tell if the ball is coming down to the paddle or going up to hit the block we also don't have any indication about the speed of the ball or which direction the paddle is moving in if we look at our four consecutive frames though then we have a much better idea about the current state of the environment because we now,358,0,0,wrBUkpiRvCA
4,do indeed have information about all these things that we didn't know with just a single frame so the takeaway is that a stack of frames will represent a single input which represents the state of the environment now that we know what the input is the next thing we need to address is the inner workings of the network the layers well really we're not going to see much more than we're used to seeing with any other network we've already come like seriously many deep queue networks are purely just some convolutional layers followed by some nonlinear activation function and then the convolutional layers are followed by a couple fully connected layers and that's it so the layers used in a dq n are nothing new and nothing to be freaked out about if you do need a crash course or a refresher on convolutional neural networks or on neural networks in general then be sure to check out the deep learning fundamental series the last piece of the network to discuss is the output the output layer will be a fully connected layer and it will produce the q value for each action that can be taken from the given state that was passed as input for example suppose in a given game the actions we can take consists of moving left moving right jumping and ducking then the output layer would consists of four nodes each representing one of the four actions the value produced from a single output node would be the q value associated with taking the action that corresponds to that node from the state that was supplied as,358,0,0,wrBUkpiRvCA
5,input to the network we won't see the output layer followed by any activation function since we want the role non transformed q values from the network all right so now we know what dq learning and deep q networks are what these networks consist of and how they work i know when i was originally learning about dq ends i was prepared to learn some type of new and mysterious neural network and was pretty surprised when i found out that really and there was nothing new about the network at all instead we were just utilizing a cnn to solve a different type of problem let me know in the comments if you're feeling the same way and stay tuned because in the next video we'll continue discussing dq ins and dissect the training process step by step to prepare ourselves to create and train our own dq in encode please leave a thumbs up to let us know you're learning and be sure to check out the corresponding blog to this video as well as the depot's art i've mine for exclusive perks and rewards thanks for contributing to collective intelligence and i'll see you in the next one one particular game i can show to you is this game it's an atari game called break out those of you that are a child of the 70s or 80s may remember it very well may be new to others of you and an agent learns to play this game is by seeing the pixels on the screen then making mathematical calculations which propagate through those millions of connections layer by layer in the,358,0,0,wrBUkpiRvCA
6,network producing an action which is a command on the joystick now if that action is good and results in the agent getting more points more reward then the learning algorithm reinforces that action by adjusting ever-so-slightly all of the millions of connections in that neural network we can see how the behavior of the agent changes over time with its experience of the game so at first even after a hundred games of experience it's still pretty rubbish it misses the ball all too often but if it keeps on playing after a few hours 300 games then it gets better and better at this point it's at about human ability it can return the ball keep keep alive for a long time if we let it keep on learning then after a few more hours and a lot of play and a lot of games of experience then it gets really good it gets better than a human because it learns how to how to do this trick called tunneling that is systematically send the ball to the sides of the wall so that it bounces around on top and the agent has less work and more reward the learning algorithm doesn't just work on breakout it works on most of the 57 games that we've tried it on and it achieves superhuman level of play at most of them music,302,0,0,wrBUkpiRvCA
0,hello world and welcome today we are going to talk about deep reinforcement learning especially dqn so deep q networks uh i'm using today a catpool as environment to train uh the agent so far that he is able to take the award of 200 and fin finalize his um environment and his aim in this environment my name is harris and yeah let's get started so for more tutorials hit the subscribe button or leave a like comment there to stay tuned to just tell you uh something about deep reinforcement learning so as you know a normal agent here on this picture you can see on the very left on the right side you can see the environment where he is in and if he's doing some good states or some good action he's receiving of course a reward for this he's taking observation all the time to just check his state if he's on the right or wrong track and where he's going for instance in a maze uh and then for this after finalizing some environment he's receiving rewards or during this as well uh dqn are so far really good because you just don't have arbitary agent that is training after a while maybe takes hours or days to train the environment and to finalize it finally um tqn as you can see here are using um noral nets to train agent to receive really good awards so um to just start here we are importing open gym where you have a lot of uh environments i took here um i chose the c pole environment where you can see here,358,0,0,1o8O-L6Agms
1,"there are three actions and uh four states which can be um applied here so if you start the environment and the cuto game you can just see here that he's basically have a lot of problems he's just after only 10 episodes he receives a score around about 20 or 30 which is okay but um he's not reaching his final goal right so for this you need to um learn a model to create model with tens of flor caros caros rl and i set up here a just normal model um with only two 700 parameters so it's pretty fine for this model you can re really reach the 200 for goal so um the 200 um rewards in this case i'm building the agent here and the noret and i'm taking the dqn agent the balsman policy because he needs to train also his policy this is um something that he really needs to know and this kind of some you can imagine policies something like a rule um where he can you know just learn after a while to just receive the max reward here so i'm trying the dq ag in here i'm um choosing the model above the memory the policy and so on and i'm setting up and building the agent i'm compiling everything with adam optimizer and training him only with uh 50,000 steps and after this you can really see so you not the agent before now if you are studying the dqn agent so after he was learning with the neonet to get better and better you can really see that after only 10 epo",358,0,0,1o8O-L6Agms
2,you can really test his abilities and you can see he's reaching really the 200 steps which is really nice in this case so this is something really good so he's getting a real expert pretty fast uh which is really nice in this case and yeah so this basically is this is the power of dqn which you can really apply in daily life and yeah thanks a lot for watching stay tuned and see you soon bye-bye,102,0,0,1o8O-L6Agms
0,what's going on everybody welcome to part 6 of the reinforcement learning tutorial series as well as part 2 of the deep q learning as well as dqn and deep q networks tutorials where we left off we're basically ready to add the train method to our agent and then basically incorporate our agent to the environment and start that iterative training process so i've switched machines i'm on a paper space a boon to machine now and then i've pulled up the exact versions that i'm using also people ask a lot like why am i on paper space it's just a nice virtual machines in the cloud kind of cloud desktop but also high-end gpus it's great for doing machine learning in the cloud if you want i'll put a link in the description it's a referral link it's like a ten dollar credit you can definitely check them out this wasn't really meant to be a sponsor spot and like i really do use paper like i use the heck out of paper space so anyway and i've got plenty of high end gpus locally it's just very convenient to use paper space but anyways i've pulled up the exact versions of things i'm using tensorflow 2.0 is on the way it's just not quite ready so anyway still using tensorflow one these are the exact versions you can feel free to follow along on a different version it's just if something is going wrong and you want to match my exact versions of things you can do this also in case you didn't know what's not all caps please in case you,358,0,0,qfovbG84EBg
1,"didn't know like to install exact versions it would be like pip 3 install a tensorflow - gpu double equals one point one three point one four examples so that's how you install an exact version of something ok let's go ahead and continue so i am in actually i think i've got it up already yeah cool thanks harrison any time bro so so we're going to do is come down to our agent class of the dqn agent and i'm just going to go to the very bottom here i'm gonna make a bunch of space and come up here and now what we're gonna do is just add the new train method so define train and then we're going to pass here cell terminal underscore state and then step whatever step run okay so the first thing we want to do is do a quick check to see should we actually train so recall we're gonna have this replay memory and then from the replay memory which should be a quite a large memory we're gonna grab a mini batch that's relatively small but also is a batch size that is of decent size so typically the neural networks feel like to train in batches of like 32 or 64 or something like that so we're gonna do the same thing here so we want 32 or 64 to be pretty small compared to the size of our memory so in this case i want to say our max memory size is 50,000 and then the least amount that we're willing to train on is 10,000 and the reason why we want to",358,0,0,qfovbG84EBg
2,do that is we don't want to wind up overfitting so we want to use this replay memory principle i guess but what we don't want to do is have replay memories so small that every step is training on the exact same thing so we're effectively doing like you know hundreds of epochs on the exact same data that's not what we want so anyway what we're gonna say is if len of self dot replay underscore memory is less than the min replay memory size if that's the case we'll just return we actually don't wanna do anything here otherwise if it's enough then let's go ahead and get our mini batch which will be a random sample of self dot replay memory and the sample size that we want is mini batch underscore size and we need to do both we need to import random and set the mini batch size so i'm going to go up to the tippy-top here i'm going to import random and then i'm gonna come down here mini batch size we'll set that to 64 and i'll go back to our bottom here awesome so now that we've done that we want to get our cue values so don't forget as well that one of the things get out of my face let's go here python programming net let me type the dqn we're actually going to use some stuff from the other one too so i just pull it up but recall the following image so we actually want to get so this bit is kind of handled otherwise both learning rate and then just some,358,0,0,qfovbG84EBg
3,of this logic is handled by the neural network but we still need the reward discount in that future value so we still want to use this little bit so to do that we need to know current q values and future q values so so mini-batch okay so what we're gonna say here is current underscore states is equal to the numpad a and then we're going to use a list comprehension here we're gonna say transition 0 for transition in mini-batch okay cool and then what we want to do here is normalize it so again if you don't know why we're normal i think that check out the basics tutorial but basically it's just images so anytime you have rgb images that's like 0 to 255 and onward you can really and not but i had normalized the wrong word i've probably been saying that wrong the whole time anyway instead what we're actually trying to do is scale it because all the images are normalized already pretty much but instead we're actually trying to scale it between 0 1 that's just the best way for convolutional neural networks to learn on image data it's just useful to have and really it's the best for any machine learning you generally want to scale between 0 1 or negative 1 and 1 anyways enough on that so current states then what we want to grab is the current q's list and that is also going to be equal to self dot model dot predict now pay attention that's self dot model so it's at that model that is the crazy model soft model dot,358,0,0,qfovbG84EBg
4,predict and we want to predict on current states and then we want to do the same thing with future so we're gonna say new current states this is after we take steps it's gonna be equal to the number rate and again we're gonna say transition 3 i want to say is that right yes yeah transition 3 for transition in or mini-batch and again we want to do by 255 and i'll explain these index values in a second if you don't understand those or you forgot it should be up here somewhere or is that i think we're gonna have to define that in our environment anyway i'll explain that in a moment yeah so coming back a new current okay so now what we're gonna say is future q's i'll explain it basically bottom line future q's list is equal to self dot target model so now we're using that target model that doesn't so crazily change dot predict against the new current states now we're going to separate things out into our typical x's and y's so these will be our feature sets these will be our labels or our targets so these will be this will be like literally the images from the game and then this will be the action that we decide to take which we still that's gonna be in the environments one will get there anyway it'll be like up-down left-right diagonals and all that stuff okay so now what we're gonna say is for index and then we're gonna have this giant tuple and this is what is consisting in this mini batch of things,358,0,0,qfovbG84EBg
5,so you've got the current state you've got the action the reward the new current state tub don't put the s there and then whether or not we're done with the environment so that's exactly where these indexes are coming from so current states we're grabbing the current state from mini-batch and then down here we're grabbing that new so 0 1 2 3 right we're grabbing that from the mini-batch and then we're predicting on it based on the state itself but then with this information what we can do is calculate that last step of our formula from before right we can calculate this little bit here to create our q's so for index creditor to done apparently my mouse has decided to stop working what do we want to do the first thing we're gonna say is if not done then we still want to actually perform our operations so what we're gonna say is max future q is equal to the np max of future q's lists for whatever index were actually at right now and then we're gonna say the new q is equal to the reward plus the discount discount which i don't think we have because it didn't try to autocomplete that x max future q else we're just going to say new q equals reward so if we are done then we need to set the new q to be whatever the reward is at the time because there is no future or q right we're done okay so now we need to go up to the top and set our discount okay so going up to the,358,0,0,qfovbG84EBg
6,top here let's set discount equals 0.99 then we'll go back to the bottom here and now what we want to do is actually update these q's so recall this is how our neural network is gonna work you've got input then you've got the output which is your q value so let's imagine the scenario where we got this out the action that we would take given no epsilon let's say would be this one right 0 1 at index 1 we take action here for because the q value is 9.5 it was the largest q value but then let's say i we don't like that action we ended up actually degrading it a little bit we actually want that new q value to be 8.5 well to update that for a neural network like in our table we just updated it right you just updated that one value but in the neural network we output these four values so what we end up having to do is we update this 9.5 to an 8.5 like in a list and then we refit the neural network to instead be three point two eight point five seven point two one point three so that's the next thing that we need to do so coming down here what we want to say still in our for loop we're going to say current q's equals the current q's list at the index that we're at as we're iterating over and i miss sublime text we're iterating over yes these two values index and then you should be able to guess what i meant to have here and,358,0,0,qfovbG84EBg
7,that was and numerate and also in wu maybe today is not a day for tutorials for me cheese and then mini batch that's probably going off of the screen here see if i can help out here there we go so for index qurna state action reward new current state done in enumerate mini batch i guess i got stuck there because i was explaining these and then pointing out the transitions anyway now it's a valid loop so if not done anybody knows if adam just has a built-in pepe kind of syntax checker let me know post comment below because this is what comes on the paper space machines and i just can't be asked to always be throwing on sublime text anyway current cues okay so that now we have our current cues we fixed our enumerate now what we want to say is the current cues for the action that we took is now equal to that new cue value once we have that we're ready to append to our features and our labels so x dot append so that would be the current state and then y dot append current cues so again features labels this is the image that we have and then current cues or the q values so cool so just just in case that's not getting through x's wise all right so now now that we've got all that we can pop out of this for loop and then we're going to say self dot model dot fit and we've got another really line here np dot array of our x values divided by 255 numpy,358,0,0,qfovbG84EBg
8,array of y and then our batch size will be equal to exactly what our mini batch size was and then i think i should be valid i want to never do this but i just i'm running off the screen i think that was still run because we're in the parameters here and then we're gonna set verbose to zero then we're going to say and make this pet paid as well then we're going to say shuffle is false because we've already grabbed a random sampling we don't need but it's no benefit to that callbacks will be equal to that custom callback that we wrote so self dot tensor board and then so we're gonna do that fit if terminal state is the well actually if terminal state else none so we will fit all of this if we're on our terminal state otherwise we will fit nothing so now what we're gonna do is come down still in line with what we're just writing there and we're gonna say if terminal state if that's the case we're going to self dot target update counter plus equals one so if we've made it to the end here and what we're trying to do here is determine if it's time to update our target model yet so the next thing we'll just throw in right underneath this so updating updating to the term and if we want to update target model yeah okay so so we'll keep adding to that and then if self dot target our target update counter if that is greater than update target every so this is yet another constant,358,0,0,qfovbG84EBg
9,that we've got to defined if we've hit that number or we vic actually if we've exceeded that number what we want to do is self dot target underscore model that set weights and we just want to set the weights to self dot model dot get weights so we'll just we just copy over the weights from our initial model and then self dot target update counter needs to be reset and we set that to zero so now let's go ahead and update target every and i'm trying to look and see what i win with i think it was a five yeah 5 so we'll go up and we'll say set update that to five okay once we have that now we have to bring in our environment and our blog class so those are two things i'm just gonna copy and paste in i don't think there's any benefit we'll run through them just in case but i'm gonna come down it's a lotta ads bro that's our modified tensor board we already oh wait mother wrong tutorial i was like as soon as i saw a modified tensor board i was like oh hmm i'm just gonna go to the bottom i guess and then let's say where is so that's our agent class that we've already written modified tensor board yes so this we're gonna grab i think we'll just grab all the way here and we'll just paste that above and i'll talk about that here in a moment so i'm grabbing all of blob m and the code that comes right after that so defining basically everything from,358,0,0,qfovbG84EBg
10,blob em to modified tensor board we're gonna grab that i'm gonna come over here and pasta okay so we have our blob environment we're saying the size is 10 really nothing too fancy here i don't really think anything's been updated there there the reset like all these kind of methods here like everything's been kind of turned into a method instead if you still wanted to move in this case we'd also have to defy think we'd need that to be self dot enemy self dot food i'll just throw that in there before before i forget that so enemies self dot food but we're not moving them for the basic one anyways so this is all good whether or not we want to render it and then get image this is just so we can actually pull an exact image from our environment because that's we're going to use the image as input as opposed to before where we were just using the image to display it and look at it now the image is actually the input value so we're not doing the delta of to the food or to the enemy but you could you really could it's just that i couldn't really decide which way i wanted to do it for this tutorial because that's actually it's easier to learn that in this case this model would perform way better if we were doing that but the problem is it's not really what we're using dq ends for most of the time so i decided not to do that but you can and maybe if you want to you should,358,0,0,qfovbG84EBg
11,treat that like change that like make the input be those that was for observation things like see if you can do that that should be a pretty easy change if you understand what's going on anyway so that's our blob m then we're just kind of defining the blob m here we're setting some initial things and then we're setting random seed to one just so as we do this more and more hopefully our results will be good to compare to each other when we change hyper parameters and stuff if you wanted to train multiple models on the same machine you can use this code as well then finally we're gonna make this directory called models if we need it so just a bunch of helper stuff really nothing specific to dq ends so i just didn't really i don't really see any benefit for us to write out blob em you're welcome so the other thing would be let me check your see we've got blob and we're missing tensor board we got the d queuing agent so the other thing we don't have is the actual blob class so i'm also going to take that we'll take that copy that i'm gonna throw that down here again we've already kind of rips i'm actually gonna cut that i mean oh i tried i just tried to copy it from my main computer here so this will take our blob class and then we'll we'll check some of these other values but we still have some code to write at the very end here pasta so this is our blob class again,358,0,0,qfovbG84EBg
12,we already had a blob class so i didn't really see any point to going over this again the only difference here is we have this new equals so we're just doing another operator overloading just so this is how we can check to see if two blobs are over each other now so cool add that and then here i've added all nine movement possibilities so not only the diagonals also don't move and then move up down left right again not complex code don't see any reason to belabor over that here we do movement this keeps it in bounds i don't really like none of this other stuff has really changed so again nothing with dq ends for us to really belabor over so cool now the last things that we need to do is actually do our iteration and all that so what i'm gonna do is come to the bottom here shoot i can't remember if we actually created our agent or not so d cute no doesn't look like we did so come to the very bottom here and i'm going to say agent equals dq and agent so we've got our agent now and then also we did i believe we've already got our environment created should be like right after the environment if i recall right just got a check yeah so we've got our environment we've got our agent now we're ready to start iterating over things and in fact i think maybe even before we do that let's make sure we have all of these and ikana i think i'm just going to copy paste these,358,0,0,qfovbG84EBg
13,"as well and then we'll talk about those i just want to make sure i'm not missing something so in fact i'll just do this so replay memory size 50,000 ok 64 5 got our name discount ok cool so i'm gonna get rid of that and then let's see if we've got anything new here memory fraction i'm actually not i'm not we're not using that at the moment but that has to do with that code that was commented out 20,000 episodes that should be fine let's see here epsilon we start at 1 that's fine this is a decent decay for 20,000 steps minimum minimum epsilon is fine this is for our step aggregation show preview false we can set that to true if we want to actually see the visuals of everything running okay so coming back down to the bottom here it's a lot of scrolling need like a paid end of page button now we're ready to actually iterate over everything so what we're gonna say now is for episode in tq tq diem for the range of 1 to however many episodes we decided we wanted which was 20,000 plus 1 and then we're gonna say ask ascii equals true this is for our windows fellows units our unit is gonna be episode episode cool so now what we're going to say is agent dot tensor board daunt step is going to be equal to whatever epis biz episode we're on then we're going to set the starting episode reward at two i can type episode underscore reward we'll set that to be a zero we're gonna say we're",358,0,0,qfovbG84EBg
14,on currently step number one current state is going to be equal to m dot reset so we're kind of matching the syntax of hope that wasn't off the screen what we're setting we're kind of matching the syntax from opening i jim let me make some more space here before that happens again hopefully that wasn't off-screen i'll go back and check anyway it wasn't really too complex of code at least ok current state we set that to the reset we're going to say done is equal to false and then we're going to iterate over all this so wow we're not done the next thing is if if mp dot random dot random is greater than whatever our epsilon is so this is very similar to everything we've done before so we're just going to say action equals np arg max of agent dot get q's get cubes for whatever our current state is otherwise so else action equals np random dot rand int between 0 and n n dot action space size in all caps okay so once we've done that we're going to continue on our theme of similarly some text things and we're going to say new lips new state reward and whether or not we are done is equal to and don stepp action and then we're going to say yep i really just wonder if i up shrub typo'd episodes somewhere else anyway plus equals whatever the reward is no jerk whatever the whatever the reward is and then what were to say is if show preview so if that is the case and not episode modulo aggregate,358,0,0,qfovbG84EBg
15,steps every will do n dot render so that just shows the environment if we want that then every step we have to add that to the replay memory so we're going to say agent dot update underscore replay memory and we're going to update that with all those what five things current state we've got action reward new state new state and whether or not we're done there so then agent dot train done and then step and then current state current state equals new state step plus equals one okay so now yeah i think now we're gonna copy the rest there's really nothing again nothing dqn related and this is also code that we've already written i'm just trying to keep this as short and applicable to what we're hopefully trying to learn so this is where we are now and then now all we want to do is we're going to add the reward to episode rewards we are going to aggregate various stats so we're gonna grab the average the minimum and the maximum reward and then with those things we're actually going to throw those in a tensor board rather than creating our own you know matplotlib chart we could use matplotlib that's totally fine so like if you are following along and converting this to like i don't know some other machine learning framework first of all how dare you but also you could use matplotlib and then here min reward basically what we're trying to do is find a good reason to save a model so if a model scored really good based on you right now we're,358,0,0,qfovbG84EBg
16,just throwing in one thing but you could change it or you can make it based on multiple multiple things so first let me just copy this over now so i'm going to say copy i'm gonna come over here where we just were pasta and cool so we add that and then so here min reward if we scroll up to the top that was one of the things that we just kind of threw in i think ya so negative 200 so this is like if the so if the model hits an enemy it's a negative 300 and it's taken steps so if the model gets a negative 200 that mostly just means it didn't get to the food but it also didn't hit an enemy so that's still like somewhat positive let's say so so if the worst agent still never hit an enemy we're saying hey that's a good thing but you could also say like you could take min reward you could set it to negative 100 and instead you could say rather than rather than min reward being greater than or equal to that we could actually say average rewards so if the average reward is greater than negative 100 or zero then let's save that model so then we save that model okay otherwise we're just decaying epsilon that's the same as what we did before so okay so we've covered a lot of things i did copy and paste some things and i know someone's gonna be like you made fun of people for copy and get paid what i meant before was that people were copying,358,0,0,qfovbG84EBg
17,and pasting from each other in this case the things i copy and pasted was like the stuff that we've already written or the things that don't matter so feel free to let me know if you like that or dislike that it would have taken probably two more videos for us to rewrite the do the environment thing and to do what was the other thing we copy and paste well also the tents are bored the blob em and we did we only made a couple of changes that be even like all those stupid elif's like come on you're welcome so anyways let me know if you like that or just like that but i think that made the most sense so now we're ready to actually train so what i'm gonna do is i'm gonna start trying see if we hit errors try to debug through those errors i'm sure i made a lot of typos and then i'll kind of just like pause the recording and then i'll show you training through and then we can kind of look at how the model is doing and stuff like that and talk about it and you know just really have a good time together so so now what we're gonna do is first let's open in terminal let's go ahead and run dqn part two dot pi know whoa just cuz there's no way we don't have said of course though the one thing that so we forgot tensorflow so great and i think what i'm going to do actually is because we probably are missing a few of the other imports,358,0,0,qfovbG84EBg
18,i'm actually just going to copy the imports from from here so i'm sure we're missing a few so i'm gonna copy that up here deep reinforcement learning is really complicated guys there's a lot of moving parts in case you didn't know okay so yeah i'm trying to think if there's anything here that might be confusing you know i've explained all that stuff so cool so now let's go back to where i was i'm gonna close that cuz i don't want that getting in my way anymore and let's try and rerun part two yet again okay i don't quite understand activation linear oh it's a typo act deviation okay so that was in our house a long time ago guys i i know you might watch these videos back-to-back or think that like it was like one day apart but it's been like almost a week probably since i filmed that other video i can't find it but carve 2d find it let's go okay activation activation what am i missing here oh activation activation linear okay cool let's try again i wonder how many errors were actually gonna hit here step action equals get q's current state did we actually pass that's totally possible currents get q's for current state mm let me see if that's actually get q's why are we missing i don't think that's true get q self state we shouldn't he shouldn't need a step there if i if i threw that in it's a mistake get q state right okay because that's just to predict so i probably threw in step i was maybe thinking of,358,0,0,qfovbG84EBg
19,train or looking at train maybe anyway all we're trying to do there is make a prediction and to make a prediction the all we eat is state whether it would be known no need for step there so let's try again what are we on four or five that's a lot of errors oh my goodness dq n has no attribute model predict i don't see a typo there self dot model i think probably it's self dot model dot predict though would be my guess self dot model underscore predict let's find where we wrote that okay get cubes i'm guessing that's it let me just check let's see def get q's self dot model wow even in that that can't be right i'm just gonna change this but actually in the in the that's so weird in the text-based version it even says that self dot model dot predict that's got to be that's got to be a mistake i mean i just can't imagine that would be correct who knows them sorry i'm getting pretty tired this stuff is some dense stuff let's try that again let me go over to the end of this there's no way please just start training i beg you self dot model dot predict self dot was that in get q's model predict yeah it wasn't get q's interesting okay so it's actually training thank the lord yeah interesting so that's so funny i wonder how that how that made it in maybe code up to this i guess cuz i just i type it in the previous tutorial and then i copied it to here,358,0,0,qfovbG84EBg
20,i'm surprised nobody picked that up in the previous tutorial to be honest that that that one's already been released for the current channel members was about a hundred people so i'm surprised nobody noticed that one what was like what's model predict but i guess maybe they thought that was coming i don't know anyway okay cool so while that's training shout out to speaking of channel members some of my newly upgraded for different months channel members i'm clicking the wrong thing there we go fuba forty-four you honest gessler both of you guys on working on month number twelve which is crazy frank lloyd jr.,140,0,0,qfovbG84EBg
21,"three months payments i yeah d five months chris olin four months and the cassie wacky for seven months thank you all very much for your support it allows me to do things like this that take me so freaking long in the ad revenue will never pay for this video so i really really appreciate y'all's support because this stuff's super cool but like i said i couldn't could not do it if i didn't have some sort of outside support because advertisers don't care about these videos not enough mostly because not enough people care about these videos it's very difficult as a niche channel doing it like it's already programming as niche nobody wants to watch programming like pretty much every one of my viewers is like super unique like you guys are watching this i assume most of you for fun so you're already a pretty rare individual so anyways it's it's hard out there for a syntax anyway so what i'm gonna do is i'm gonna let this keep training i'm gonna let it i think will let it go through 20,000 steps and we'll see where we are i might run it again so sometimes as the model learns and does like a cycle through epsilon it's useful to do it again so reset epsilon back let it decay again and i found that to be true also with learning rate so just with any neural network so even if you could let it continue training or decay it's lower so let's say you did 20,000 steps you decayed learning rate at a certain rate if you want it and",358,1,1,qfovbG84EBg
22,"then did it again for another 20,000 steps now you've got 40,000 total well if you decayed at a certain rate that would have gotten you to the same ending learning rate often it's actually better to cycle it so do 20,000 reset 20,000 again it's better to do that than to do one kind of pass through so it kind of helps get you out of these local traps so i'm finding the same thing to be true for epsilon so rather than let's say your end goal is to do a hundred thousand steps you could decay have a decay in such a way that it decays smoothly all the way through to a hundred thousand steps so it ends at a pretty low you know let's say less than one percent epsilon but doesn't get there quickly it gets there you know right on time to reach a hundred thousand steps let's say it's i find it's actually better or so far i've found it's actually better to instead do 20,000 steps and do five cycles like you'll end up with higher accuracies higher rewards that kind of thing so anyway kind of cool so maybe i'll get to show you guys that here we'll see so questions whatever if you're feeling fuzzy it's normal ask questions either post them in the comments or come join us in discord gg centex otherwise i'll see you guys for you in a few moments for me and probably a looks like four hours so see you then okay a moment or two has transpired since since they're recording but i don't want to make this",358,1,1,qfovbG84EBg
23,a separate video so here are the results of a 2 by 256 confident and basically what i did was i did like three passes so this would be like our first pass through the data and you can see the average started at negative 180 and it kind of made its way up to negative 73 and then basically i loaded where that model left off ran it again so just kind of read akay the epsilon again so there we go this time getting to a peak of about negative 50 and then i ran it again and this time got it to be an average of about what's this one negative thirty seven point five eight and then we could we could try to keep continuing to do that i just it's taking a lot of time to train these and this isn't necessarily like the coolest model in the world okay it's not the coolest environment so i don't really want to spend too much time going over this i've already taken quite a while to do this series like you guys don't understand like just just doing the last two videos has taken well over a week probably two weeks worth of work especially if you include what what i've had daniel doing as well i mean it's just taking a lot so anyway so i'm gonna stop it here and we're gonna move on to another rl algorithm so but what you can see is it's actually pretty cool some of some of these stats like you can just see as you continue training everything got better like you can,358,1,1,qfovbG84EBg
24,see on the first past even the minimum everything except for pretty much were word max like pretty quickly it starts to max out but here you can kind of see the minimum even improves which is pretty good because it i guess i guess we're still not above negative 300 like if we could get minh to be above negative 300 that would mean we never hit an enemy which would be really cool anyway cool so that's one i have another tensor board and this is a different model and what this one is is with many enemies as well as movement is turned on so i had 10 enemies one food one player and i allowed movement to be on for all of the enemies and this was the training i was really hoping to see something a little better than this mostly because over here i was kind of expecting that we would even we would be better than this so then i started to wonder well you know part of the problem is if you were to really look at this image most of the image is like black and or just nothingness so it's like not useful in any way to the model so then i started to think hmm hmm maybe that's the problem what if i add a lot of enemies then we'd have like more to work with per each convolution that happens in each window but that didn't really solve the problem either so and if anything it just made it harder so you can see here it really never did any better than like probably,358,1,1,qfovbG84EBg
25,the best average was negative 120 which is pretty good i mean that's still out most of the time it doesn't hit an enemy at least and if it doesn't hit an enemy i guess it didn't exhaust steps either so i mean at some point it still eventually on average getting to the food but i don't really think it's it's that great and like i said i'd rather do a cool environment something like starcraft or or we could make blob and m cooler but this is just taking way way too long and i just don't want to necessarily die on this hill i want to explore some other hills and then figure out which one i really want to do battle on so anyway i think that's it i think there's anything else i really want to say other than this if you wanted one thing you could do is try remember now so i have dq and tutorial but these are actually from a different directory i'm sure if this is dq n i think it's in dq n stuff let me check real quick open up dq and stuff let's go to logs yeah ok so so we could kind of watch the model if we wanted so i can go up here the last time i did this daniel made fun of me for this being the way that we test or like we just watch the model but i think it's fine so what we'd want to do is load in the best ish model so past three and then we could load in well the one that's,358,1,1,qfovbG84EBg
26,here is actually ok so the average 25 max 0.35 is the average i like this one better and then the min negative whoa whoa wow thank you to 454 min dang so it almost took seven hundred steps and oh this is a negative seven this is a positive whoo stick with the positives okay so this is probably the one i grabbed so it's fine so we'll load in that model we can set render show preview to true aggregate i think preview is when we aggregate also load model i don't with this we didn't even do in the tutorial but just so you can see let me just save this this is just in the model method so let me just search for load model here let's just go to next next right so if load model is not none so if we set it to none it'll create a fresh model otherwise if we set something there it's gonna load the model and then it just loads the model okay nothing fancy going on there um and i think that should be everything that you would need to know as far as changing to look from what i just covered in the tutorial anyway so we can load model aggregate every show preview true okay let me then come into here open and terminal let's do python blob world cnn nine moves load hi so hopefully this trying to think if i aggregate steps every i feel like i might be missing something as far as getting it to pop up every game because that was 50 what episodes it's been,358,1,1,qfovbG84EBg
27,just like today's a fresh day and i meant looked at this code let's go check so show preview let's just copy paste whoops show preview and go down to the code here ever and not episode thousand so instead let's actually say modulo one so we'll see every single episode thank you very much so this is a bunch of episodes i mean at least he's not hitting the enemy but he's pretty stupid this doesn't oh epsilon is hi okay okay let's fix that real quick sir probably why you want a standalone script for this but yeah you know whatever epsilon we will just set to be zero one more time okay yeah he's much quicker now i cycling through and just getting food he's not even hitting the enemy at all i haven't seen him hit yet that time me at least you didn't get the food he likes the corners boom boom boom boom okay yeah so i mean this one's not terrible i mean it's pretty good he really likes to go into that corner though that's kind of funny nice okay all right so that's all i'm gonna do here we're gonna get into some of the other reinforcement learning models and like i said i'd like to do something more complex and it kind of bums me out that it was taking so long but hey that's that's deep learning for you it does take a while to train models so i would like to do something far more advanced but i want to figure out which thing i want to do which which reinforcement learning algorithm do,358,1,1,qfovbG84EBg
28,i want to use for that so still on the hunt we could do some cool stuff with dq ends for sure but i definitely want to check out a three-c for sure before that maybe even neat stuff like that so anyways if you guys something you want me to look at let me know leave a comment come join us in the discord whatever otherwise it is a new day so i will shout out some of my other channel members erik gomez on five months robo five months alexis wong 7 you day bingley and metanoia all seven months as well thank you guys very much for your support it allows me to spend a week on a stupid algorithm actually it's really cool i like that i can do this i hope that i can continue being able to do tutorials like these that i will never make the ad revenue back on so thank you guys very much for your support till next time,219,1,1,qfovbG84EBg
0,hi everyone and welcome to code and action so this is my version of a self-driving car i got the idea of coding this little project from codebullet who basically built the same thing but since i couldn't find his code on his github i thought it might be a good exercise to learn more about coding in python and more about deep q learning so now i want to give you a short overview of how it's working so the first thing i needed was a gaming environment which not only renders the game but also calculates the movement and calculates if there is a collision and keep track of the the score so i'm using pi game for the basic rendering and stuff and this is a quite popular library to program games in python so there are a lot of classes and objects in in the gaming environment but the main ones are the car the ray i will explain later how um for what we need the race and uh the main game environment let's see here it is and the gaming environment has um a defined set of of functions which i will call using my my machine learning algorithm so the first thing is we have an initiate function as normal we have a reset so if for example the car crashes into the wall i want to reset then we have the step function so we want to hand over a certain action and we want to the car to respond accordingly so for example if we move forward then the car should move forward as well and then,358,0,0,uJKpCfX88A8
1,we have a render function which basically tells pygame how it should draw the certain frame of the game on the screen and then lastly we have a close function so this is the basic idea around the gaming environment so after setting up the gaming environment the next thing that we need is the so called agent and this is basically the machine learning ai that controls the car so let's have a look at it and it's written in this file here so we have the class agent and it has different functions like the act function and it basically gets a certain observation data and then it acts either randomly or it predicts its action by using a neural network and the neural network here is called brain let's have a look at it and here we see the neural network that's behind the brain and yeah it has different functions to be trained and to predict its next action based on its observation and i don't want to go into a lot more details if you are interested please watch the video of coldbullet he goes in a little more detail about how this algorithm works and after setting up the agent let's see how the agent plays the game and trains inside the gaming environment so the basic idea is learning by doing so the agent gets to play in the gaming environment and through its observation it gets better at playing the game basically so now let's have a look what these observations are and now we can see what the the car is seeing basically and it just measures,358,0,0,uJKpCfX88A8
2,the distance to uh the blue points and this is the only thing the car basically sees the car also knows its own velocity to measure the progress of the car these green goals are used and every time the car crosses the red line the next goal gets activated and so on and so forth so that's it for this video i hope you liked it and thanks for watching and since i find it quite mesmerizing to watch the car drive by itself let's see if it can reach 1000 points have fun music music music so music music so music music music so music so music applause music applause music music music music music music music music so music music foreign,161,0,0,uJKpCfX88A8
0,welcome back everyone so i'm really excited to continue this lecture series on reinforcement learning and in this one i'm going to zoom in specifically on deep rl deep reinforcement learning or set another way reinforcement learning using deep neural networks okay this is one of the most exciting trends uh both in control theory and in machine learning over the past decade uh and there's just a ton of new and exciting results out every month uh it seems on deep reinforcement learning just a reminder this is um following roughly a new chapter in the second edition of our data driven science and engineering book so i wrote a reinforcement learning chapter to cap off that control theory part it was also a great excuse for me to learn more about reinforcement learning so that's something actually just a meta point for for all of you uh you know trying to explain things to people is always the best way to learn something and this was really you know i took this as an excuse to i wanted to learn about reinforcement learning so i wrote a book chapter and made this this video sequence um so i hope you are also enjoying it as much as i am okay so this is such a big field i am going to touch on the you know the tip of the iceberg of all of the different techniques of deep reinforcement learning and i actually have another video from you know a number of months ago maybe even a year ago where i talked at a very high level about how you can use deep,358,0,0,wDVteayWWvU
1,neural networks and reinforcement learning and i mostly did this with examples and demonstrations of all the cool stuff you can do so today i'm going to dig deeper into more of the algorithms and how these different flavors of deep reinforcement learning fit into kind of our classic our classic pictures that we've been developing over the last few lectures okay i'm going to jump right in and hopefully this one's kind of exciting all right so the first type of of kind of deep neural network uh embedded into a reinforcement learner kind of the simplest thing you can do is what's called a deep policy network so your policy function pi is presumably a relatively complex function of the state uh the state you find yourself in s and the action you're going to take a so it's the probability of taking an action given that you're currently in state a and this is supposed to be optimized to maximize uh future rewards in your system okay so this policy pie could be pretty complicated it could have there could be a lot of rules or you know physics of how you make decisions and actions optimally given the state that you find yourself in i don't know why i have a terminator here but presumably it's using a deep neural network uh to optimize its policy and so that policy function pi can be parametrized as a neural network with uh weights theta so the input of the neural network is the state the output is some probability of which action to take and you may you know flip a coin and,358,0,0,wDVteayWWvU
2,take an action based on that probability and so what we would be trying to do is to optimize uh over these parameters theta to maximize the future reward to give myself the best policy pi possible okay so there is a brilliant blog by andre carpathy deep reinforcement learning pong from pixels where they essentially walk through and they actually provide a python code with about 150 lines of code that actually codes up this deep policy network so it's really really cool basically how you would take a high dimensional state s this is a high dimensional measurement of your system this is pixel space so you know there are over a thousand pixels in this image that are input into this policy network and then essentially the policy network is optimized to make the best decision of whether or not i should move up or down my my pong paddle okay so you're this character on the right and all that you have to do is based on the current pixels decide what's the probability of moving up or down and then take an action based on that probability and so the idea here is at a very very high level is that we have our policy parametrized by the network weights theta and we're going to optimize over theta to give us the best chance of getting a future reward and i'm being a little bit intentionally vague about you know i'm not writing down a loss function a neural network loss function that we're training over because there are different ways that you can optimize using these deep policy networks and,358,0,0,wDVteayWWvU
3,actually one of the uh the easiest ways essentially you can use back propagation so you know neural network training in general uses this this idea of back propagation so information kind of upstream can be propagated back and eventually used to compute gradients and optimize these these weights and so the basic idea is that i would optimize my weights by computing gradients based on this cumulative expected future reward and so this is partly why i'm being vague is you have a lot of choices on how to represent this future reward so if you represent that that cumulative future reward using a value network so if you have a policy network to make actions to take decision to make decisions on what actions to take and you are estimating your future reward using a value network that would be an actor critic network so there's different ways of kind of embedding information into this this deep policy network but the basic idea is that you you represent your policy as a network and you optimize over its parameters using essentially gradients through gradients of something through back propagation i'm going to talk more about this okay um this is an aside that i think is really important and i want to tell you about but if you don't love you know math derivations you can skip these next five minutes this is kind of the guts of how policy gradient optimization works in general this is this is more general than neural networks if you have a policy function pi that is parametrized by some parameters theta this could be a support vector,358,0,0,wDVteayWWvU
4,machine this could be some other kind of model or it could be a neural network where the weights are parametrized by theta then i am essentially going to derive how if you take the chain rule or if you take the gradient of this this reward function this cumulative reward function you can get a signal to update your policy weights data so again if you don't love math you can skip this next five minutes but i think this is kind of important okay so the cumulative future reward r sigma given a set of parameters theta for my policy so you can think of theta as being a policy if you like is the following so mu sub theta is the uh kind of expected future distribution or probability of finding yourself in a state s at long times given this policy theta so it's kind of the asymptotic distribution if you played this game for a long time this would be the expected probability of finding yourself in any of those states s out of all possible states so this is a probability distribution of where you find yourself in long times this is kind of just to be mathematically precise we need this part here and we would add so uh so we wait by the the probability of finding yourself in a state in long time and then you weight by the quality function the quality of being in that state okay and and taking that action times the probability of taking that action given your weights theta this is a bit of a complicated expression this is you can,358,0,0,wDVteayWWvU
5,kind of convince yourself that this is true and this is outlined in detail in the chapter in the book that this is consistent with how we normally write this reward function as the sum of all the future possible rewards this is just folding it up using this quality function okay so it's the quality of being in a state and an action so i have to add up over all of the probabilities of being in taking any of those actions so you can this is a sum or a weighted sum over actions and similarly i have to do that same weighted sum over states and i'm using the asymptotic probability of being in that state so this is the the cumulative future reward and what you'll notice is that we now we don't have the policy anywhere we just sorry we do have the policy but it's parameterized by theta so essentially what we can do is take uh the gradient of this reward with respect to theta because that's the signal we're going to use to change or to update theta we're going to use gradient optimization presumably using back propagation in neural networks so we need to compute this this gradient we want to move in the direction that in theta that increases r we want to move in the direction of theta that increases the total reward and so if i take the the chain rule here or just basically propagate this this derivative through you'll notice that this term here is a function of theta and so i compute its its gradient with respect to theta and the,358,0,0,wDVteayWWvU
6,next step is really just uh keeping track of of algebra here so what i've done is i've taken this expression here and i divide by the policy theta and so i multiply by the policy pi theta on the left so these cancel each other out i'm just you know dividing by it over here multiplying it by it over here so nothing changed now i'm asking myself this question and i hope you're asking the same question why didn't my mu sub theta why didn't i have to chain rule this this object here and that's a really good question i'm going to have to think about that and maybe you can comment below in the in the video and we can work through uh kind of what went wrong here but this is this is just a sketch of how we can get uh this policy gradient information okay so so you do this and now you'll notice uh that this term here is just the gradient of the logarithm of my policy the gradient of the logarithm so now we have pi my policy times the q function so everything to the left here is the reward is the reward that i had times the gradient of the log of the policy with respect to theta and so now this is the expected value of my quality function times the gradient of the log of my policy with respect to theta and so this can be used essentially as the the signal this i can compute uh so so essentially i can compute the log of my my uh probability pi and,358,0,0,wDVteayWWvU
7,i can weight it by my quality function and that gives me the information i need to update my my weights theta so i just need to compute this using back propagation and this is better conditioned to compute with okay again this is a bit of a mathematical aside but uh if you're interested for example you know in how you actually compute this uh this policy gradient this is at least one way that you would do that so this is where i really wanted to get to is deep q learning.,122,0,0,wDVteayWWvU
8,"most of the most impressive demonstrations of deep reinforcement learning in the last five or ten years many of them have been deep q learning, where essentially you learn a quality function with a neural network. so again we know this is the basic formula for q learning: it is an off policy temporal difference learning algorithm so this is my estimated i'm sorry this is my estimate of my future reward if i'm at state s now and i take action a now and this is my actual reward if i take that action and i actually get a reward, this is the actual future reward. so the error signal between these, i can use to update my quality function as i learn through trial and error. so we've already had a whole lecture on this. so essentially what you can do, is you can parametrize this quality function by some neural network weights theta and this is really really useful because if you think about the kinds of things we use q learning for, or deep q learning for, the state space s might be astronomically big for example if i'm playing a game of backgammon or chess or go the number of board combinations in chess is more than 10 80. it's more than there are nucleons in the known universe. and go has an even larger state space s . so instead of iterating over states, over and over, what i'm going to do is i'm going to parametrize this quality function by a much lower dimensional set of parameters theta and then optimize over theta .",352,1,8,wDVteayWWvU
9,"so instead of iterating over states, over and over, what i'm going to do is i'm going to parametrize this quality function by a much lower dimensional set of parameters theta and then optimize over theta . so essentially you can think of this as extracting low dimensional features of your quality function that are parametrized by theta or some neural network... and we're going to optimize theta to get a better q function. and this is going to really be important to address the curse of dimensionality we have in these very high dimensional state spaces. so with this one, i will write down the actual cost function involved in solving this... so this is the neural network cost function that we use when we build a deep q learner. essentially the loss function that the the network is trying to minimize is the expectation of the square of this temporal difference error... so this is literally just the temporal difference signal here where now my q functions are parametrized by theta and so the neural network is going to use its algorithm, stochastic gradient descent and back propagation to optimize these parameters theta to give me the best possible q function that minimizes this td error, this temporal difference error. and again, we think that there is a strong evidence that's what biological learners are doing... at least at some level of their neurological hardware is minimizing this temporal difference error that we talked about in the last lecture. so this is pretty cool...",332,8,18,wDVteayWWvU
10,"so this is pretty cool... you can actually just take the update, turn this into a loss function and the neural network is going to optimize those parameters theta for you and that's been a very powerful approach. we've seen this in the deepmind atari video game playing... this is one of my favorite examples of block out you've seen this before if you watch my channel where essentially from this pixel space a deep q learner with a convolutional level layer can learn the right strategy, the right quality function, for what actions to take given what state it finds itself in... and it actually learns some pretty interesting things that it should actually eventually drill a hole in one side here and then it'll basically work itself out and that's the most efficient way to increase its reward, which is really cool. that's why they say human level control because this this algorithm, this deep q learner is exploiting the physics of the game to get solutions that only expert humans would find.",227,18,23,wDVteayWWvU
11,and so this is the architecture so it has some convolutional layers some fully connected layers you know from pixel space to joystick you know signal but essentially this is just a big uh kind of deep q learning demonstration convolutional cue learning and this is just the list of all of the video games and i guess you know the ones above the line it is better at or better than human level and the ones below the line it's still not as good okay so that was deep cue learning essentially you can have this this traditional q learning and you can cook up a loss function for your neural network that will essentially through trial and error through experience it will learn the weights theta that give you the best q function possible and there's a variation of this that i really like called deep dueling q networks or dueling deep q networks ddqn which essentially takes your quality function and it actually splits it into two networks a value network which is just a function of the current state and an advantage network which says what is the advantage over just the value of being in that state for taking an action a so this is a good architecture when you're the difference in in quality for different actions is very subtle so maybe i'm in a really good board position in a game of chess and the different actions i take are giving me very marginal increases or decreases of my quality function and so what it does is it splits off the value function of just being in a state,358,24,24,wDVteayWWvU
12,from the incremental advantage of taking an action having been in that state and i mean this is essentially equivalent to the q function it's just another way of writing the q function but now you can train two neural networks and they're kind of dueling with each other the value function is trying to get as much explanation of that q function just from the state and this advantage network a is trying to figure out what the effect of taking actions is and then you can kind of optimize over these actions and do interesting things okay another really really important concept in machine learning sorry reinforcement learning is the actor critic learning and actor critic methods are actually way older than deep reinforcement learning so i'm going to tell you what they are in general and then i'll tell you how we use them for neural networks so the idea of an actor critic reinforcement learner is essentially to take the best of both worlds from policy-based and value-based learning so in the you know a few lectures ago we talked about policy iteration and value iteration and they have these kind of different flavors and different benefits and strengths and so in an actor critic framework we're going to have two um two learning two learners an actor and a critic and the actor is basically trying to learn a good policy and the critic is critiquing that policy based on its estimate of the value so the critic is learning a value function essentially and the actor is learning a policy uh okay good so the actor is trying to represent,358,24,24,wDVteayWWvU
13,this policy pi and the the critic is going to be uh using information from its estimate of the value function and so one really really simple way to do this is remember that policy gradient algorithm i was telling you about you can essentially do an actor critic policy gradient where what you do is you're updating the parameters of your policy uh so this is policy update data but you're using the temporal difference signal from your value learner so your value function is getting updated too you're trying to learn this value function from your critic and your critic is essentially giving you the error signal that you use to update your policy so again this kind of mixes the best of both worlds you're using value information but you're using that to update a policy using this kind of in some cases gradient policy information now in the context of deep neural networks there's some interesting things you can do one of the ones i like is called this advantage actor critic network and it uses that uh deep dueling queue network i told you before where you split the quality function into the value function plus the advantage of taking an action and so essentially now the actor again is a deep policy network so this is a deep neural network for the policy with some weights theta and then the critic is a deep dueling q network uh q that tells you what's the quality of being in a state and taking an action and so here similarly this is that policy uh iteration that policy gradient iteration i wrote,358,24,24,wDVteayWWvU
14,down earlier we derived earlier except now this policy gradient is updating the deep policy network and and gradient iteration is much much faster than the model free techniques i was telling you about in the last lecture using gradient information will converge way faster than traditional q learning or traditional value iteration but it requires you have a model that's parameterized by theta that you can take the derivative with respect to theta but remember in that deep policy gradient we needed our quality function and so this again this is an actor critic network because i'm doing policy iteration a policy gradient iteration in this case but i'm using a q network for that quality function so my q network is learning the quality function and i'm updating my policy using this policy gradient network and presumably you update your your ddqn the way you always do by once you take actions and you get rewards you'll update this this q function using the temporal difference error so i think this is really cool you can kind of mix the best of a value-based formulation and really q networks are kind of value-based and policy-based you know for the optimization so you optimize the policy using value-based information so this is really cool and this is different than q-learning which is purely value-based so in queue learning you are still updating the q function but you're updating the q function based on q information and then at the end of each optimization then you figure out what your best policy is just by reading it off whereas here you're kind of jointly you're optimizing,358,24,24,wDVteayWWvU
15,the policy directly but using this uh this cue information this this value-based information so this is kind of a neat a neat approach okay so that again was just the tip of the iceberg of deep reinforcement learning but we got a little bit more specific into some of these methods so very very popular deep quality function networks you can use those to do deep policy gradients for example in an actor critic so if you like you know there is an actor critic method that uses a deep q network for the critic to do a deep policy gradient uh optimization now you'll notice conspicuously here there is this deep model predictive control block that i didn't really talk at all about it's kind of a different uh a different flavor of optimization and i'm almost certainly going to have a whole video or series on deep model predictive control later but there's some really interesting work in the community where people are learning these kind of really good optimal nonlinear controllers using these these classic dynamical systems optimizations like model predictive control but then they try to learn a policy that mimics that that model predictive controller so this can be a very expensive online optimization it requires a lot of model forecasting it's very very computationally intensive typically and so if i do that expensive computation maybe i'm teaching a quadrotor how to fly through an obstacle field once i learn the right control actions maybe i can embed that in a neural network maybe i can train a policy that very rapidly kind of encodes the information of these deep,358,24,24,wDVteayWWvU
16,model predictive controllers okay so that was again just a very high level overview of some really important topics building on some of the pieces that we've been learning over the last few lectures all right thank you,49,24,24,wDVteayWWvU
0,what is going on guys welcome back in today's video we're going to learn how to do deep reinforcement learning with openai gym and tensorflow so let us get right into it music now before we get into the actual tutorial i would like to mention that this video was sponsored by ip royal and i encourage you to not skip this part because this could be very interesting to a lot of you guys since this is directly related to data science and machine learning oftentimes when we do data science and machine learning we don't work with just the csv file that we load from our file system we have to gather the information the data ourselves using web scraping and web crawling for example and one problem with that is that if you do it from one computer or maybe from five computers you have a very limited amount of ip addresses that you can use and the problem with that is that providers apis websites are just going to block you over time because you're sending too many requests too frequently the solution for that is to use a proxy server and to use multiple different ip addresses to rotate proxy so to say and the problem with this is that of course you can try and use some free proxy lists but there are multiple issues with that first of all you cannot trust those people that provide these servers why would anyone provide the service for free can you really trust them i think not second of all they're unreliable a lot of these uh free proxy servers don't work,358,0,0,YLa_KkehvGw
1,at all they work maybe for a day or two and then they're basically useless if you need something reliable and serious you can use ip royal they provide a hundred percent transparent proxy services with a lot of ips that you can use and basically you can connect from different locations you can do large-scale professional web scraping using their services you can go to their website iprole.com you can look at their services you can look at their pricing and the good thing about this promotion here about the sponsorship is that you can get 30 off with the coupon code neural nine so make sure you check out iproil.com in the link in the description down below alright so we're going to get started by installing a couple of external python packages first and for this we're going to open up the command line and we're going to type pip install followed by the package names that we want to install and the first package that we want to install is the open ai gym package which is going to provide the environment for reinforcement learning so it's going to provide the ecosystem that our agent will be operating in and in order to install it we just say pip install gym and for compatibility reasons we're going to specify an exact version we're going to say gym equals equals 0.25.2 the reason for that is because we need this to be compatible with the version of keras rl2 that we're going to use because we're now going to also install tensorflow and keras dash rl2 so keras reinforcement learning 2 and this,358,0,0,YLa_KkehvGw
2,module here is compatible with this version of openai gym this is why we need to install it that way and if you don't have it installed already you want to install numpy as well in my case all of these are already installed so i don't have to do anything once you have everything installed you can start by importing gym so open ai gym and we're also going to import here from core python the random module because what i want to do first here is i want to show you the environment that we're going to use we're going to use the card pull environment which is just the task of balancing a stick or a pole so by moving left and right the stick should not fall or the pole should not fall and we're going to try to use a random agent so the random agent basically chooses randomly if he goes left or right and then we're going to use an intelligent deep learning reinforcement or deep reinforcement learning agent to see if it performs better so we're going to create an environment by saying gym dot make we're going to make a cart pull dash v1 environment this is an identifier you also have different environments that you can use but this is a very simple one because we can only go left or right so we're going to make that environment and we're going to set the render underscore mode to human so that we can actually see what is happening here and then what we want to do is we want to go through a couple of,358,0,0,YLa_KkehvGw
3,episodes of just trying random action so we're going to say here episodes equals 10 and then 4 episode in range one up until episode plus one episodes plus one like that we're going to just get the state of the environment when it's reset so we're going to say state equals environment reset so everything from scratch we're going to get the the default or the initial state we're going to say done equals false so we're not done with the simulation yet the score is zero which is also just the default and then what we do is we say while not done we want to take an action and the action can be as i said moving to the left or moving to the right to balance the stick or pull um so we're going to say action equals and the action is encoded as zero or one so zero meaning going left and one going right and from this action pool we're going to choose a random action we're going to say action equals random dot choice zero one so we pass a list with zero and one and the action is going to be chosen at random uh and then what we want to do is we want to call the actual step function so we want to say environment dot step and we want to take this action so we want to take either 0 or 1 as an action here and this function returns now four values at least in this version of openai gym in i think in more modern or more recent versions it returns five values,358,0,0,YLa_KkehvGw
4,but we only need four so we need the n underscore step or actually we don't need all of them i think we only need a reward and the done but so basically we could also just leave that as an unnamed variable here so as a placeholder so the first parameter or the first return value is not important to us the second one is the reward this is important to us because we want to know now my system is lagging we want to know if this step that we took made things better or worse and if the rewards positive it means that the action was actually uh good so it was a an action that led to a to a better state so each action basically changes the state and the state that we are now after this action is either better or worse or the same and depending on that we're getting a reward which can also be negative then it would be a penalty so we have this reward here then we also have done which is we're going to name this done because then it will also set the value off this variable here and this basically means okay now the simulation is over we lost in the case of the of the cardpool environment that means that the card pole lost the balance and now we we lost basically game over uh and then we have a last parameter info that we're not interested in so what i want to do here with each iteration we want to add to the score the reward and remember if it's,358,0,0,YLa_KkehvGw
5,negative we're going to subtract it automatically because we're adding a negative reward so we're subtracting from the score um and we want to also render the environment with each iteration so we want to actually see what's happening and by the way for this i think we need to also install an additional package i'm not sure if that's necessary but i think if you want to see what's happening you need to also install uh how is this pronounced piglet piglet p-y-g-l-e-t this is also a package it is necessary for that for this render step here and with each episode we also want to print episode episode number and score whatever the score is and when i run this now or actually in the end we need to also close the environment but when i run this now you're going to see uh that this is basically the simulation you can see how the balancing is happening by going left to right the reason why you don't see the full uh thing is because we end the loop when uh the simulation is done if i don't do this if i say while true instead of while not done you're going to see the complete fail so when i run this here now you can see this is what happens here now we don't stop the simulation because it fails and we just keep going we don't terminate after it's done but you can see that the random actions going randomly left randomly right does not really make much sense here so you basically lose um yeah and the reason we don't see,358,0,0,YLa_KkehvGw
6,all of this when we do done is because at some point it's clear that there's no way to rebalance anything so the simulation is over not well done while not done so this is what we get with random actions score of 22 40 23 16 92 30 not very high so most of the time below 30 or 40.,79,0,0,YLa_KkehvGw
7,occasionally you might get to 90s as you see here so this is what we get with random actions now let's train a deep neural network on this to perform better than this uh for this we're going to just import a couple of more uh modules here or functions and classes we're going to say from tensorflow dot keras dot models we want to import the sequential model now for some reason my pycharm doesn't recognize keras i have it installed for some reason it marks it as red but it still works so from tensorflow keras models we want to import sequential from tensorflow keras layers we want to import the dense layer so a fully connected layer and a flattened layer from tensorflow.keras.optimizers we want to import the atom optimizer and then we want to also using keras rl2 we want to import from rl.agents the dqn agent so the deep i think this was deep q learning agent uh which is basically q learning is reinforcement learning so or one type of reinforcement learning and from rl dot policy we want to import the boltzmann q policy and from rl dot memory we want to import sequential memory by the way for this video today i'm going to show you how to do all of this if you want a video or a video series or separate videos multiple videos on the theory behind all this on the theory behind neural networks i mean for neural networks i have some videos but if you want to have mathematical theoretical explanations of what is actually happening behind the scenes let me know in,358,1,1,YLa_KkehvGw
8,the comment section down below below if there is enough demand i will make some videos but for this video we're going to focus on just the implementation on the practical parts so we're going to import and use now after we create the environment what we want to now have is we want to know how many states are there so how many different states can i have and for this we're going to say environment dot dot observation space dot shape and we're going to get the first dimension of this this is the amount of states that we have here then we also want to have the amount of actions we know that we only have two here but we can also get this dynamically from each environment by saying environment dot action space dot in so that is that and um what we want to do then is we want to say uh or actually maybe first of all let's comment all of this out let's see how many states and actions we have so print states print actions uh what's the problem here okay so one thing that could help is going into the command line and saying pip install and what's the package proto buff we can set it to this version maybe this is also just a compatibility issue proto buff and then equals equals 3.20 point start let's see if that solves the issue let's run this again and seems like it works deprecation warning but there you go okay now it works so this fixed the problem um i don't know if you guys are going to,358,1,1,YLa_KkehvGw
9,encounter the same problem but if yes you just have to install the correct version that the error message is telling you to install so uh let's get back to the code so we have now i didn't look at the states i think it was four states and two actions and what we want to do now is we want to build a neural network that is going to solve this tax so we're going to build a uh deep q learning agent that is going to have a neural network network model behind it so we're going to first start by saying model equals sequential and then we're going to say model dot at we want to add a flattened layer first with the input shape equal to 1 and states so 1 4 in our case then we want to have a dense layer so add dense layer with 24 neurons and an activation and of course you can play around with these values right you can use 32 neurons you can use 100 neurons you can see what happens when you change these values these hyper parameters so action or activation equals relo rectified linear unit we're going to copy this we want to have one more of those layers here and then we want to say model dot at dense layer with two neurons so action space neurons and the activation here should be linear so this is going to be our underlying machine learning model and then we want to take this and turn this into a deep q learning agent by saying the following thing agent equals dq agent,358,1,1,YLa_KkehvGw
10,and the model is going to be set to model the memory is going to be set to sequential memory the limit will be set to 50 000.,36,1,1,YLa_KkehvGw
11,the policy we're going to set to boltzmann q policy nb underscore actions is going to be actions nb underscore steps for warm-up we're going to just use 10 here and the target model update is going to be 0.01 i think that's like the learning rate or something approximating that um so this is the agent that is using this model uh behind the scenes and now what we want to do is want to say agent dot compile with an atom optimizer and a learning rate of uh 0.001 the metrics that we want to evaluate here is the mean absolute error and we want to then say agent dot fit on the environment with a hundred thousand steps we don't want to visualize the training process now you can also visualize the training process if you want to and this is also by the way important we want to remove this render mode human here again so that we don't have to constantly see the training process if you want to see it you can do it you can do that but i'm just going to say false and verbose equals one to get some information and then we're going to say results equals agent dot test in the environment with 10 episodes and this we're going to visualize so that we can see actually the final result and then we want to also get numpy mean results dot history we want to get the episode reward uh the mean of that so we also need to import numpy s and p and in the end again we want to close the environment,358,3,3,YLa_KkehvGw
12,so again let's just briefly recap before we run this we create the same environment we get the amount of states and actions we create a machine learning model a neural network model we create an agent a deep q learning agent with that model with sequential memory with the boltzmann q policy with some other parameters we compile this model using the atom optimizer we fit it onto the environment without visualizing the process of training and then we evaluate this agent we try to see how it performs on the environment and this is something that we display and then we also get the mean reward per episode so let's run this and see what happens it starts up and then it does some training there you go we get a lot of deprecation warning i think because of the versions we're using here but this is now training for uh ten thousand what is it ten thousands steps so training steps essentially and i think this is only going to happen once so we can just wait for this we're not going to have multiple epochs and once this is done it's going to try the testing it's going to start the testing and we're going to see how it performs and the goal is of course to not lose balance at all so this is interval two winds that where did i provide this oh no i think the the thing that we're doing is we're training it for a hundred thousand steps so it's doing 10 000 steps ten times so we're going to skip that part and i'm going to,358,3,3,YLa_KkehvGw
13,come back to you once it's done all right so now it's running and you can see how it performs it balances the pole quite well um it's still going this is i think the first run and it's not failing at all so i think if it's there you go we have a reward of 500 which is the best you can get and if it now would lose balance you would get something around 300 maybe but you can see that it always gets 500 so it mastered the task it doesn't fail it basically keeps the balance and probably you could run this for uh sorry you could probably run this for hours and it would not lose balance most of the time i guess so yeah we're going to stop this here maybe we want to run this uh with one less zero just so we can see that it's maybe not enough to just use 10 000 steps because then it's trained it's not random it will perform uh to some degree but it will not be the same as training it for a hundred thousand steps so this was basically the perfect agent for this simple problem i think if we train it at just 10 000 times it might end up being perfect but most of the time it will probably not have mastered the task it will have to uh to be trained a little bit more to perfectly master the task so let's just wait for this one uh epoch here and then we can see what the performance looks like there you go so it does,358,3,3,YLa_KkehvGw
14,work to some degree but you can see it failed here so it gets 188 which is way better than random but you can see it loses the balance it uh yeah this one was not good at all 128.,52,3,3,YLa_KkehvGw
15,and occasionally it might score 500 i don't know but it is not perfect but the one that we trained for a hundred thousand steps was kind of perfect so yeah this is how you do deep reinforcement learning in python so that's it for today's video i hope you enjoyed it and hope you learned something if so let me know by hitting the like button and leaving a comment in the comment section down below also don't forget to check out ip royal the sponsor of this video today you will find them at the link in the description down below and of course as always subscribe to this channel and hit the notification bell to not miss a single future video for free other than that thank you much for watching see you next video and bye thank you music,185,4,4,YLa_KkehvGw
0,welcome back everybody in today's tutorial you are gonna learn how to code a dueling deep q learning agent in pi torch you don't need any prior experience you don't need to know anything about reinforcement learning you just have to follow along let's get started so of course we begin with our imports will need os to handle some file joining operations will need the base torch package we will need torch 10 n as n n to handle our layers will need n n dot functional as f to handle the rail you function we need the optimizer of course for the atom optimizer and we need numpy to handle numpy type operations so the dueling deep q learning agent benefits from a replay memory just like regular deep q learning so we're gonna go ahead and code up a replay buffer class and that just arise from the base object and that'll take a max size an input shape and number of actions as input and the first thing we want to do is save our relevant parameters and make sure to instantiate our memory counter now the memory encounter is very important for knowing where your last stored memory is what we're gonna do is we have a memory of finite size we're going to start from 0 go all the way up to memory size and then once we try to store memories past that we come back to the beginning and start overwriting our earliest memories of course we need a memory to keep track of the states that we encounter that will be mem sized by input shape and,358,0,0,kjW3Ba4hTYI
1,that will be an umpire float32 so the star input shape idiom if you're not familiar with it this will iterate over a tuple so we're gonna pass in in this case a list not necessarily a tuple but we'll pass in a list of elements in this case for the lunar lander environment it is a list containing eight because there are eight elements of the observation vector the new state memory is pretty much the same thing same shape anyway it's of course the new observations the agent encounters as it takes actions we need an action memory and that is numpy zeros by self that mem sighs and the pi torch framework is a little bit particular about data types so we're gonna call this n 64 that's really overkill you're only gonna pass in you know actions of order you know a dozen two dozen or so so numpy in eight would actually be sufficient but it's it expects long tensors because we're gonna use these to do indexing later so in 64 it is numpy zeros for our reward memory and that gets men sighs and that's float32 and we need a terminal memory to keep track of the terminal flags and that gets mem size shape within you int unsigned int 8-bit integer data type so numpy in you and eight means unsigned 8-bit integer so 0 to 255 exclusive or inclusive rather next we need to store transitions in our memory that takes a state action reward new state and done flag and the first thing we want to know is what is the index of our first available,358,0,0,kjW3Ba4hTYI
2,memory and that is given by mem counter modulus mm size and self dot state memory index you know state and likewise for the action memory reward memory equals reward new state memory index equals state underscore self dot terminal memory index equals done now in other and of course you want to increase the memory counter by one every time you store memory now in other implementations i have done one - done that is great for when you are multiplying by the done vector in that case you want the reward for the terminal state to be 0 identically because once you reach a terminal state you are expect to be to reward a zero because you're done playing the game so the terminal memory must facilitate that but in the case of this particular tutorial we're going to be using these as a mask and just set everywhere done is equal to 1 equal to 0 so you'll see that in a few minutes so there are multiple ways of handling it no one unified right way of doing it but i kind of change around depending on which example i'm working with all it depends on my mood so sample buffer self and badge size and what this will do is you want to find the maximum memory meaning the highest occupied position so the mem counter stuff that man size so it is the minimum of memory counter or mem size and then you want to get a batch of random integers and choice from 0 to maximum in shape batch size and replace equals false that means that you don't,358,0,0,kjW3Ba4hTYI
3,resample used memories so if you sample memory 50 you don't put it back in the pool to sample again now this is great for very small memory sizes if you use a large memory size it's not such a huge concern but i put it in here just for completeness so then states equals self state memory sub batch actions action memory sub batch rewards reward memory sub batch states actions new states states action rewards new states and then done self-taught terminal memory batch then just returned state's actions rewards new states and the dunn's and that is it for our our replay buffer next we get to the class that encapsulate our deep dueling deep q learning agents so class dueling linear deep q network i call it a linear deep q network because this is not going to be suitable for dealing with it the atari library i'm saving that for my course yes i have a course deep learning from theory to code where i'm gonna show you how to go from the paper all the way to the full implementation and not with the lunar lander environment both the atari open ai library so for you to be out to suffice with the more simple stuff of course with pi torch you want to derive all of your deep neural network classes from anand module that way you get access to self dot parameters we want to call our initializer and that will take a learning rate alpha a number of actions a name the name will be for saving files for checkpointing very important in bude dims and a,358,0,0,kjW3Ba4hTYI
4,checkpoint directory temp dq n and then we want to call super dueling linear deep q network network self dot and it it just calls a super constructor self at fc one a linear layer with input dims as input shape and 128 elements for output so we don't need a very complicated deep q network to handle this particular environment so it is fairly straightforward and fairly computationally cheap dot linear and this takes 128 elements as input and another 128 as output now what makes dueling deep you learning different is that instead of computing a single quantity for the q value you compute both a value in an advantage function so selvi will be our value function and that takes one or 28 elements as input and one element is output and our advantage it's also another linear layer that takes 120 elements as input and actions as output and then we take a a nice sum not a direct somebody something related to a sum of these two quantities to get our actual q function next we need an optimizer that is opt into adam self dot parameters and learning rate equals alpha we need a loss function mean square error loss pretty straightforward we need a device so if you have a gpu you definitely want to use it so that's cuda 0 if t dot cuda dot is available else cpu if you have two gpus you can say cuda zero or cuda 1 and then you can use command line parameters to pass in which gpu you want to use in this case i'll just use cout to zero,358,0,0,kjW3Ba4hTYI
5,self dot to self self dot device and this will send your entire network to whatever device you specify and we need a checkpoint directory and we need a checkpoint file that's os path join self dot checkpoint dur with name plus we'll call it dueling dq n so that way we can have a file for saving our model this isn't so important if you're running this on a gpu because it runs pretty quickly for the lunar lander environment but if you're gonna implement this for the atari library then you'll definitely gonna want to save your model file for checkpointing next we need to define the forward operation of our network and then i'll just take a state as input so the first layer is just a value activation performed on the feed-forward of the state through the first fully connected layer l2 is the rail you operation performed on the pass the feed-forward of l1 through the second fully connected layer the value function would just be self dot v acted on l2 no activation we just want the linear output and likewise for the excuse me the advantage function self thought a acted on l2 so here we have two parallel streams value and advantage that both take the output from l2 neither which get neither which get activated and we want to return v and a very very straightforward next we have two functions to handle the bookkeeping the you know the save check point operation and that just says print saving check point just so you know it's doing something oh i can't type tonight all right what a,358,0,0,kjW3Ba4hTYI
6,disaster all right so t dot say self dot state dictionary self dot checkpoint file so this will save our state dict from our model into the checkpoint file next we have the inverse operation loading a checkpoint and we say print loading checkpoint making care to actually type properly self doubt load state dictionary t dot load cell thought checkpoint file so next up we wanted to find our agent class the agent handles the interface with the memory handles the learning functionality as well as a functionality for choosing actions and then it will also handle the functionality of saving the models so this is kind of a introduction to the concepts of object oriented programming an agent is not a deep queue network an agent has a deep queue network and the deep queue network is of course a function approximator for a state action value function based on the given environment so just keep that straight in your mind you know it's always helpful when you're coding stuff to know how you're going to structure stuff i've seen lots of examples where they they kind of stick everything within one object class and it's kind of i don't know i don't particularly like that because it doesn't make for extensible or maintainable code so you want to always want to separate stuff where it makes sense so the agent is gonna take a gamma which is our discount factor for future rewards an epsilon which is the random parameter that determines how often the agent is going to take a random action learning rate alpha number of actions input dims amen sighs,358,0,0,kjW3Ba4hTYI
7,that's just for instantiating the memory a batch size that is for sampling that memory the epsilon min so the agent will decay epsilon over time because you want to take more and more greedy actions over time but you never want it to go directly to zero because you never know if your model is actually accurate so you always want to be testing the model with some amount of random actions and 1 is a good number you can use point 1 percent you know 5 percent it really varies it just all depends next we're going to use a linear decrement of epsilon a factor of five by ten in the minus seven with every learning step a replace target of default of a thousand we use a hundred for this particular environment what this is is for deep q learning you have a target in an evaluation network and you only perform the back propagation on the evaluation network and then update the parameters of the target network every replace it or a number of learning steps so every thousand steps you copy the parameters from q evaluation to q next network next up we need the checkpoint directory temp dq n will be our default that i do something different up here you know what let's call this temp slash dueling something like that just to make sure it is distinct from my regular stuff dueling underscore dq n and we can say the same thing here dqn just for consistency's sake that's just to make sure i'm not overriding any models for the regular dq networks keep everything in the,358,0,0,kjW3Ba4hTYI
8,same directory so next we have to store our member variables self dot gamma equals gamma epsilon of course is the starting back value of epsilon self dot eps min does epsilon min self dot epsilon decrement epsilon decrement we need an action space and this is the set of all available actions and that is just up 0 up to n actions minus 1 and this will be used for random action selection in the choose action function we need a learn step counter and that just keeps tracks keeps track of how many times we execute the learning function that's related to this replace parameter here that's so that we can know when we need to update our target network parameters self dot replace target count equals replace and then finally we get to the memory and that's a replay buffer and size input dims and actions and we need self dot q eval and that is a dueling linear a deep q network that is a mouthful that gets alpha and actions input dims equals input dims let's put this on the next line and we need a name of q eval let's say and that is for of course saving the model and we're just going to pass in the checkpoint dirty goes checkpoint derp and then we can do the same thing for the next network this handles the calculation of the values of the next state and that gets a different name of course q next but otherwise that parameters are the same the learning rate here doesn't matter because we're not going to perform back propagation on this network,358,0,0,kjW3Ba4hTYI
9,it's just there for consistency with the interface next we need a function to store a transition in the agent's memory that takes a state action reward new state and done flag as input self dot memory dot store transition state action reward state underscore and done pretty straightforward doesn't return anything i just want to store value in the array next we come to the choose action function it takes an observation as input and we want to say if mp random random is greater than self dot epsilon then we want to take a random action observation equals observation and p new axis this is necessary because the dq network up here is gonna expect something like batch size inputs so we're gonna pass in a single one with one by eight instead of eight comma so nothing too magical there but we do have to cast the observation from a numpy array into a 10 pi torch tensor and that also has to go to the device of the queue eval network and that is because pi torch differentiates between cuda tensors and regular tensors so if we are performing computations on the gpu everything we pass in must also be on the gpu so then blank we don't care about the value we care about the advantage you go self dot cute eval forward state and then the action equals tia arc max of advantage item we have to call that item because this arg max returns a tensor pi torch tensor in particular and the open idea only takes integers and numpy arrays as input for actions so i i can't,358,0,0,kjW3Ba4hTYI
10,see them back just integers so we have to call that item to get the actual number out from the tensor if we're gonna take a random action we want to say action use mp random choice sell that action space this would just choose an action at random from the available actions and we want to return the action next we have a function to replace the target network and that doesn't take any inputs but we do say if self dot replace target count is not none and self dot learn step counter modulus self i replaced target count equals zero then we want to say self dot q next load state dictionary itself dot cute eval dot state dict and next we want a function to decrement epsilon this just handles the decrease of epsilon over time and we say if self dot no we don't need to do any checks there so we'll handle will handle the check later on so we want to give the agent some warm-up period but we'll handle that in the learning function so so got epsilon equals self dot epsilon minus self eps if self dot epsilon free then epsilon min else self dot eps min so this will subtract off epsilon decrement from epsilon if it's greater than its minimum value otherwise it just sets it equal to epsilon min epsilon underscore min there we go one month typo to debug when we get to the terminal next we come to the most interesting function of the class the learning function and the first thing we want to do is address the question of what,358,0,0,kjW3Ba4hTYI
11,happens if we haven't filled up the batch size of our memory so it's a numpy array of zeros and so learning from zero is really isn't helpful it's not going to give you any useful parameters so we want to say if self dot memory dot counter lessons self-taught batch size return so if we haven't filled up at least batch size memory just you know return don't bother learning and pi torch you have to zero out the gradients on your optimizer at the top of every learning step so that way you don't accumulate gradients from one step to another at the top of the step you also want to deal with the question of do you want to replace the target network whoops too many spaces and then then you can go ahead and deal with the question of how to actually calculate losses and back propagate them through the network so we want to sample our memory so we'll say state action reward new state done equals self dot memory dot sample buffer self that batch size so we'll go ahead and sample our memory and remember that pi torch requires that we deal in pi torch tensors not numpy arrays so we have to say state equals t dot tensor state to self dot q eval dot device it's all had to be cuda tensors new state samael new state to sell by q eval dot device and there's a kind of a very nuanced point here if you say t dot capital tensor it is a different thing from saying lowercase t and the difference that i found is,358,0,0,kjW3Ba4hTYI
12,that if you use capital t it'll it will select some default type based on whatever pi torch things you want whereas if you use the lowercase it will preserve the data type of the underlying variable you are casting into a tensor which is what we want we don't want to go from an integer to a float 32 because then pi torch complains so make sure to use lowercase t for tensor just kind of an advantageous point it isn't immediately clear by reading the documentation so that is the case i had to find that out through trial and error eval dot device of course we don't want nine we want parentheses we want reward egos t dot tensor reward to self i think my keyboard is getting worn out maybe it's time to buy a new one reward and duns t that tensor done dot to self cute eval dot device so this ensures that everything maintains the underlying data type of the numpy arrays and that they all get sent to the appropriate device next we have to perform our feed for it so the value for state s and the advantage for state s you know self iq eval dot forward of state and likewise for the next states underscore self cue next dot forward new state so we have to do that and now we have to handle the calculation of our loss function so in the dueling dbq network we construct the q notice i'm not saying q equals forward i'm saying that the value and the advantage equals the forward propagation of the state and new state,358,0,0,kjW3Ba4hTYI
13,through the network and i had to construct the q function based on this so the action value from this advantage has shaped num actions and this is a scalar quantity so the reason we didn't have to take into account the value function for the choosing actions is because it's just shifting the advantage function by a scalar quantity so it doesn't really matter it affects everything equally doesn't affect the max there are a number of ways in the paper they talk about calculating the q function and you can read the paper for that we're going to use one of the ways we're gonna subtract off the mean of the advantage because and that's related the fact that the this thing is only identifiable up to a scalar constant so you can add a constant to everything in it and it's you know it's kind of like in physics for you and ship the zero point energy anywhere it doesn't really matter so we want to make it nice and identifiable so we'll say q prayer d goes t dot ad bs we don't have the value function to the quantity a s minus a underscore s mean along the action dimension and very important here to say keep dim equals true so that way it doesn't reduce the dimension by one and you want to how many parentheses do i have there three you want to say gather along the batch dimension sorry i gather minus one along the action dimension on the squeeze - one two parentheses squeeze so we're gonna gather up the actions the agent actually took you can,358,0,0,kjW3Ba4hTYI
14,also do a array indexing if you want i think that works sometimes in array slicing can get a little bit confusing so it's easiest to use this gather which gathers along sorry that should be gathered positive one along the first to mention the action to mention the actions unscrews delonge the batch so that way it's just a linear array and then you want to squeeze it back down so that it has the appropriate shape next you do the same thing for queue next t dot add the s underscore a s underscore minus a sub s underscore mean dim equals one keep them is true whoa one two three gather one action dot on squeeze now i take it back sorry we don't want to gather along the actions the agent actually took it's late at night don't know what i'm thinking what we want to do is this we want to say queue target target target equals reward plus self gamma at times t dot max q next along the action dimension sub zero because t max returns the the the index as well as the value and we want to detach that from the network so that way it isn't involved in the back propagation because this is just a target it's just a scalar quantity we don't really want to perform back propagation on anything involved with q next because that's related to the q next network so very important next up equally important is to say hugh target sub duns equals equals 0.0 so what this will do is it'll treat duns as a kind of like a,358,0,0,kjW3Ba4hTYI
15,mask where everywhere that duns for every index of duns is equal to one it will set that index of cue target equal to zero the reason you want to do that is because when the game is over the expected feature reward which this cue target is the approximation of is identically zero because the episode is over and no future rewards follow next is the actual calculation of the lost cue eval loss cue target and cue predicted the order doesn't matter because it's a mean squared error to celtic you eval dot device and then you want to perform the back propagation and very important you want to step your optimizer after you back propagate the loss and increase the learn step counter by one and at the end of the learning step you want to decrement epsilon so what will be done here we have we have effectively done the same thing as in q learning except that we have the added twist of calculating the value and advantage and then when we calculate the when we construct the q function from those two quantities we add them together and subtract off the mean of the advantage along the action dimension very very straightforward it's actually more straightforward than it would seem when you read the paper so i encourage you to do that it's a good paper quite interesting the rationale behind it is pretty cool i'll cover all that in my upcoming course but feel free to handle it on your own beforehand so that you can get a head start next up we want to handle saving the models,358,0,0,kjW3Ba4hTYI
16,and you say self dot q eval save checkpoint q next save checkpoint and looking at this no i didn't i didn't make a mistake a my save checkpoint functions def load models self self dot q eve a load checkpoint and cute next load checkpoint pretty straightforward okay so now let's go to the main function and code up a loop to see that this actually works so of course the first thing we have the handle is our import to an import jim for our environment numpy we want to import that so we can calculate the running mean of the last hundred games we want to import i'm dueling d queuing towards we want to import our agent we don't care about the replay buffer or the network function from utils we want to import plot learning so we can generate learning plot you saw the beginning of the video and of course if name is main then a and b because jim make lunar lander the two of course a discrete version some games 1000 it only takes a thousand game this actually learns faster than the regular dueling the regular deep q learning if you run them back-to-back you can see that it learns about fifty games faster it's quite impressive hello checkpoint because false agent equals agent gamma of 0.99 epsilon one point zero learning rate of five by ten to the minus four and adams equal eight for this environment number of actions is for mm size will say a million doesn't really matter for this particular environment because the you know each memory is so small you can,358,0,0,kjW3Ba4hTYI
17,save a million of them a batch size of 64 and epsilon decrement of 1 by 10 to the minus 3 and a replace iteration of every hundred games if we're loading a checkpoint then load models agent download models that's of course not a function but we do need a colon and a filename is lunar plan illuminator lunar lander dueling dot png scores epsilon history this is so you can see how this score cumulative the score per episode evolves as a function of decreasing epsilon and we're not going to keep track of the number of steps for i in range none we want to set done equal false score for the episode get 0 and the observation is env resent not done action he was agent choose action observation say observation underscore reward done in folio cmv step actions basically here we're just playing the episode say score plus equals reward agent store transition observation action reward observation underscore and done should actually be int done because done is a boolean let's do that and then agent learn at the end of the episode you want to that i call it scores or score history and call the scores append the score for the episode and calculate the average score numpy mean scores last hundred games and the reason that's important is because of the criteria for solving the environment is at the average reward for the past hundred games is 200 or greater and then you want to just say a little print statement episode i score one f we don't need huge precision and average score percent dot one f,358,0,0,kjW3Ba4hTYI
18,and epsilon dot 2f and percent let's go to the next line and we will print score average score and page nf salon whoa there we go two parentheses and then epsilon history dot append agent dot epsilon so that way we can plot it at the end and at the end of all the games you want to say i plus one for i in range games this is just for our axe he's on our plots plot learning x scores eps history and file name pretty straightforward let's go ahead and head to the terminal and see how many typos i made so here we are so i was learning it in a different directory and for my course i had renamed to the plot learning function so the plot was generated in a different directory but whatever it's all the same and you can see this is the end output where it manages to kick ass pretty much for the entire sequence so if we scroll all the way up to the very beginning you can see that within the first 20 games it gets a winning score of 200 points and starts to really pick up steam by i don't know by a hundred and fifty games or so by around a hundred and what is this i would say by about one hundred and thirty five games it is almost strictly positive rewards starting to really really pick up steam so this is a little bit faster than the regular dq network of course there's always an element of randomness in the way the you know parameters are initialized so you,358,0,0,kjW3Ba4hTYI
19,know run to run differences exist but it's a significantly faster you know if it's 20 episodes faster that's not insignificant so let's go ahead and see if i may any typos so let's just get to it so python main torch dueling dq and lander no module name torch at n n dot function so let's go to dueling dq n taurus torch and that is because it is function functional save try it again dueling linear deep q network is not defined that is in the actual file there that's what i'm calling the super constructor that is because there is a typo dueling linear deep q network that's because dueling liner no dueling linear right there okay agent object has no attribute batch size that's probably because i forgot to set it so let's come down to the agent and say yeah i forgot batch size self dot bench size equals batch size save myself not inconsistent users okay that's always the danger of using nano so let's come down here and get rid of that roll x let's try it again there we go and now it is running let's make sure it starts to get positive rewards you know there one positive reward let's see it isn't doing what i would expect let me double check my file real quick to see what's going on so let's stop this and go back to our code editor and see if i forgot something obvious because this is clearly not learning one second okay so let's come down here and loss you target q pred so and i am calling the learning function,358,0,0,kjW3Ba4hTYI
20,and i know it's getting here because it is decrementing epsilon so it is definitely functional look at my cheat sheet and see if anything pops out at me and q pred q next key targets of duns loss q eval applause guitar guy keeper and lost backward we are stepping the optimizer typically that's the first thing i would look forward to make sure i actually stepped the optimizer let's see me s vs ni score yes mean reward gamma t max q next maybe those one detach q targets have done 's equal zero that looks good let me come up to my actual network blank advantage turn v and a optimizer oh i know what i did this is stupid so now this is a trivial mistake so but it is quite damning so at every step you want to make sure that you set the old observation equal to the new one so let's go back to the terminal otherwise you will be you know constantly choosing an action based on the initial state of the environment which is totally useless let's go back to the terminal and run it all right one more time so yeah so it's already starting to dramatically lower the score my game 15 it gets 162 points so it looks like it is learning i'm gonna call that good of course you get some pop offs - 200 or so so here we go so yeah now it's it's really picking up steam so that is dueling deep q learning in the pi torch framework in my upcoming course i'll show you how to go from,358,0,0,kjW3Ba4hTYI
21,the paper to this implementation not just in the lunar lander but in the atari open ai gym so that way we can play games like space invaders enduro atlantis breakout pong you name it we're gonna cover a whole gamut we're also gonna a couple dueling deep key learning with double d q learning as well as the regular deep q learning algorithm so gonna be really cool stuff i'm looking forward to releasing it i should drop in the middle or a third week of september so be on the lookout questions comments leave them down below if you haven't yet subscribe hit the bell icon because i know only 14 of you get my notifications and i look forward to seeing you in the next video,167,0,0,kjW3Ba4hTYI
0,after my deep q learning video some of you have expressed interest in seeing how to modify the code and use it for mountain car that is exactly what we're going to do today if you haven't seen my q learning video on mountain car definitely check that out first and if you haven't seen my deep q learning video on frozen lake it would be very helpful if you watch that one as well quick reminder that my code is in github it's in the gym solutions folder let's start with my diagram from my deq learning video and see what we need to change to make it work for mountain car for mountain car we're given two pieces of information we're given the position of the car along the x ais and it's a value between 1.2 and 0.6 we're also given the velocity of the car it's a value between 0.07 and 0.07 the that the car can take it's either accelerate to the left accelerate to the right or do nothing let's map the observation space and action space to our deq network let's start with the action space the output of the network represents our action space so since there are three actions the action is going to be three nodes for the input we're given position and velocity so those are the two noes that we're going to use c learning can't handle continuous space which means that there could be infinite number of states the same applies to deep q learning deep q learning can't handle continuous space either what we did in the mountain car q learning video,358,0,0,oceguqZxjn4
1,"is to convert the continuous space into discrete space and the way that we did that was to slice this range into 20 divisions so any number that lies between 1.2 and whatever this arbitrary slice is is going to be mapped to position zero we do the same thing to velocity just chop up this range and convert it into an integer now the inputs to the neural network will simply be a combination of two integers 0 0 0 1 02 1 2 etc okay those are the differences the rest of the algorithm stays the same let's jump to the code and see how it works this is the code that i use in my deep q learning video i made a copy of that for mountain car let me open this up most of the code is the same if you want to know the difference hold control select the two right click and do a file compare and you can walk through the differences i'll just go over the notable changes the dqn replay memory those stay the same the learning rate changed the network sync rate replay memory those things change the network sync rate i bump it up by a lot after 50,000 steps we're syncing the target network with the policy network so i increased it because for a mountain car it needs a significantly more number of actions than the fen lake environment for that same reason we need to store a lot more experiences than before the badge i kept the same here's the number of divisions which is the number of segments to divide the",358,0,0,oceguqZxjn4
2,"position and velocity into 20 is a operary number you can try more or less and you should recognize this code from the mountain car q learning video the this is just code to to create the segments this is our main training loop where we go through number of defined episodes similar to the q learning video um also capping the uh training episode to 1,000 steps this is also something you can adjust the loop pretty much states the same let's jump to the function that converts the state into an input for the know network all this does is to convert the position and velocity floats into integers so wherever those floats land that's the integer that's getting returned another big change is the way i'm graphing the progress here i'm accumulating the number of rewards received per episode and appending it to our list every 1,000 episodes i'm plotting the progress let me jump down to the plot function so it's just creating a plot of the rewards per episode and the epsilon history this way we can visualize how the training is going without waiting till the end another thing to note is that if the rewards that we've collected is really good best reward was defined up here minus 200 if we can get to the go in 200 actions or better save the policy right there at that point i can stop the training manually and just keep that policy or i can keep letting it train and see if it can beat itself and then save another model okay so those are the notable changes let's start training",358,0,0,oceguqZxjn4
3,"and see what it looks like after the first thousand episode we should get a png file here mountain car.png that will graph the progress of the training okay we get the graph here after 1,000 episodes let me open it up so in the first th000 episodes we have not found a solution also there's nothing showing up on the epsilon side yet until the agent gets to the goal at least once we're not going to start decreasing epsilon yet so nothing is showing up on the graph here's the graph updated at episode 2000 we can see that ex epsilon has decreased we can see that on the graph as well looks like it's able to reach the goal in maybe 930 actions okay i'm going to let this train and then we'll review the progress all right we're back we're at episode 9,000 and looks like epsilon is around 6 something around 7,000 episodes the agent is able to reach the goal in maybe around 250 actions but it's still not within the target of under minus 200 so let's give it some time and we'll come back finally at 12,000 episodes we can see that the agent is able to reach the goal in 178 actions which means we met the threshold of 200 at episode 15,000 we can see that the agent improved just a little bit from 178 actions down to 172 let's see if it can get any better at 177,000 episodes we got a pretty good bump from 172 down to 145 i did let the training finish and and 145 was the best result so that's",358,0,0,oceguqZxjn4
4,"pretty good after training for 20,000 episodes we have our best solution at 177,000 episodes so the way i named the policy was to tag on the episode at the end here i'm going to comment out the training the call to the training code and comment the test code i'll pass in the policy that we want to run let's run it okay we can see the mountain car getting up to the goal now i want to mention that there are some starting states where the mountain car it just stuck let's see if it happens okay so you see here that it's stuck in this particular location maybe defying into 20 segments is not enough granularity trying more segments might work better uh also maybe changing some other hyper parameters might make the model more solid but at this point i'm pretty satisfied with what we have that is it for this video if you want to support my channel like comment subscribe thank you",218,0,0,oceguqZxjn4
0,welcome in this video we are going to implement the great paper that introduced the dqn paper from deep mind uh these are results we're going to get by the end of this video so you can see an agent playing the game of breakout and destroying the game and this is the average reward per episode um i want to highlight that for this code we are not using to uh we are not going to use a target network because uh it was not yet introduced in that paper it was introduced in a subsequent paper published in nature that i'm also going to implement later uh to be honest when i try to implement this code uh there were very few implementation online that could get so good results with the target network um but yeah let's dive into the code you see the agent learns to dig tunnels and to send the balls behind the tunnels so yeah these are the results you should expect expect by the end of this video so let's dive directly into the code as usual we're going to use py for uh for implementing the code we're going to use a few helpers well a few uh objects from stable baseline tre so let's give for example the replay buffer or some wrapper on top of the uh uh of the gy environment but i'm going to come to that later we could implement all that from scratch but here to keep the uh the implementation con short and concise we're going to use those uh those modules okay so we can start by just implementing,358,0,0,UvElGVu0cvo
1,the newor that will represent the the the q value so yeah it's a standard new nwor cnn uh yeah fre small new network u yeah so yeah basically have network we assume that x that we receive is u in 8 and then we uh so basically with values going from 0 to 255 we're going to discuss that later but the input will be processed so that they are gray scale images uh and then basally we just rescale the the range of the data between zero and one okay now we can move to the most interesting part the dqn algorithm dq learning so we take a few hyper parameters as input the replay memory size okay i'm not going to explain all the details of the algorithm this is very important to remove the correlation between the training samples in machine learning we assume that the samples are i id so that that they are independent uh and that's why we need to add this replay buffer it is a very u it plays a critical role in the success of this algorithm so i'm not going to discuss all the mats on all the details but if you're interested you can look in the description i have a full course on ud about deep reinforcement learning so number of epoch we're going to train for 30 million epochs that should that should take between 20 well between maybe 48 to 72 hours to train the algorithm the update frequency we're going to come back to that later the batch size discount factor yeah i will not introduce that of course that's,358,0,0,UvElGVu0cvo
2,"a really important concept in reinforcement learning the replay start size that means that at the beginning we're not going to we're not going to train directly first we we'll just let the agent uh explore the environment with a um with a random um policy and we are only going to start training when we get 80,000 um samples in our replay buffer um we are we are going to play with an epsilon gitty approach um so bally sometimes taking the best action uh the best action and sometimes just taking random action so basically the uh this value um drives uh the the ratio between best action and between run action at at first it's we just take random actions and at the end we uh in 99 of the case we take the best action so let that a to to play with that trade-off okay exploration steps um well i'm not sure what's the difference between uh expression step uh okay yeah i'm sorry expression steps it's um basically we are going to decay this expression factor from one towards 0.01 by default uh on bally we're going to do that for 1 million steps so that means that for the 29 million last steps this will the the epsilon will be fixed to 1 person okay let's move uh in the u in the implementation basically we're going to stick to the pseudo code of the algorithm very closy uh before going to the before having a look at the p sud code maybe we can have a look um at one figure okay basally we're going to reproduce that",358,0,0,UvElGVu0cvo
3,figure figure two from the paper and you can see that we have similar results with our with our code okay okay awesome so basally let's go back to the pseudo code uh and basally we're going to stick to it very very closely so first thing to do is to initialize the replay memory uh that's what we do again we use the replay before class from stable baseline tre there is nothing crazy there so we could implement it from scratch as well okay then second line in the code we stick to them one by one so basically we initialize the action value function so we initialize the dqn we give as input the number of actions um so yeah we have four actions in that environment so on so that's the environment we try to solve in our case as a breakout game but the way we implement the algorithm is not dependent on the game so you could easily give another environment on it will work fine but yeah basically we fetch the number of action from the environment basically don't only tr returns the q value for each action and therefore if we want to take the best action we need to return the action with the best um the best q value then an optimizer we're going to use adam with a running rate of 1.25 to the power of 10us 4 okay yeah i'm just initializing a few variables to track the the rewards during training so that you can make the plot uh yeah so that you can make this plot so this is just some kind,358,0,0,UvElGVu0cvo
4,of some kind of logging okay and now we can start it rating so basically while epoch is lower equal than number of epoch if we go to theo code basically we itate of a given number of episode in our code we reason in terms of um in terms of training steps rather than episode so we want to do 30 million training steps then uh yeah so basically uh yeah so basically we here basically in this loop if we are here basically we in a given episode so we iterate over episode but uh okay maybe it will become clear later but for now we are iterating over episodes but we will stop training when we reach a given number of epoch so okay so basically we initialize the sequence um so basically we observe basically we have just we just reset the environment again we are starting a new episode so we are um we're not dead so that's why dead is false for now we've not yet accumulated reward so total reward is zero and we are making an observation we're resetting the environment on uh storing the observation the state of the game that we see uh is rgb data okay so what we do they explain that in the paper they uh they experimented that it get uh ital better results so for the uh the at games uh i think it's not just related to recard it's for most at games um but clear at the re beginning uh of the environment when they reset the game they choose between one and 30 um so between one and,358,0,0,UvElGVu0cvo
5,30 time steps so randomly they do they just firey so bally it's a no no up first fring will make sure to start the game because with breakout if you do not fire uh well the game waits for you to fire to start the the game on air also we we we do sometimes a bit more iteration uh so basically it's the same as doing no operation no up because out you need to go left or right um yeah on the they experimented uh they explained that they they they saw that it improve the results okay so basically while we are not dead we are going to iterate so basically this is uh this for loop while we are not dead so in a given episode while we not dead we uh we do all those steps so with the probability epsilon we are going to se select an action a on otherwise we select the best action so basically first we compute epsilon based on initial exploration fin exploration on exploration steps so basically we have a linear formula yeah basically we we compute as a function of those three values what is the current value of epsilon once we are pass those 1 million time steps epsilon will be just equal to final expression so in our case 1 okay so now that we have epsilon we can start uh we can select an action so either we select an action randomly there is a sample action in the gym environment so that's very easy to do we don't need to sample an index ourself we just do a,358,0,0,UvElGVu0cvo
6,we just call the sample function and then convert that into by array on otherwise we use our q network so we get the q value for each action by feeding the observation to the new nwor and then we take the the the index of the of the best action so we use arax on the q values okay now that with the action we can uh ex uh execute the action in the environment so basically we do un.,105,0,0,UvElGVu0cvo
7,step with the action and we get the next observation the reward a flag that tells that tells us if we dead or no and some information about the environment okay so for the breakout game and i think for for a few at games what they do they do not uh so yeah with reinforcement learning you need to um when you uh compute your expected cumulative reward you need to take into account like uh future reward but if you when you are dead there is no future reward that's why you need to uh to consider differently terminal or non terminal states here for the breakout game what they do they do not say i'm dead when the when the game is over but every time they lose a life they say okay this is the end of the environment well the end of the episode so basically done is equal to true if we've lost a life otherwise it's false okay now we can say set the next observation by just uh okay next observation is just go to next ups. copy so we make a copy to make sure that if we modify real next stops it will not affect next stops uh and then we can uh we can just for ling total rewards plus equal reward um yeah that's just for looking and then reward is called np.,305,1,2,UvElGVu0cvo
8,"reward they also explain that in the paper this is to allow to make sure that you don't have like rewards that uh that that explode um it depends on the environment but usually they do reward clipping so all the values greater than one they uh they set back they clipped to one or all the values below the minus one that clip to minus one on the values of zero are stays at zero okay so then we can add the those uh those transitions into the uh the replay before so the the pairs of observation next observation the action we took the reward we received on uh if that led to the end of the episode or no so if we lost a life or no okay and then we can say ups equal to next subs because when we uh when we take an action we need to feed the observation to the new um so yeah our our new obser well the current state of the game now is equal to next sts uh yeah we we did an action we observed something so now the the the state of the game is ups okay so as we said we are not going to start training directly so so we are just going to train if um if there is enough data in the replay buer so in our case 50, uh 80,000 by default and uh we are going only to train every update frequency epoch so in our case every for epoch not not eo three well okay we can see that in our code we've call that",358,3,3,UvElGVu0cvo
9,eo so let's call that epoch but every four iteration so that allows to to have a faster training so that what really means is we do four um for one training step we do four iteration with the uh with the environment so the agent does four action in the environment and then we do one step of training okay that allows to get more data uh on to do a well a way to see that maybe that's not the best way to phrase it to to to get more data or to do less training on the for to reduce the training time okay uh yeah i did explain that basically here compute the target uh value function uh for bally we simple batch of data point so for those state for those observation we compute the expected q value okay we just apply the formula from the paper again here i'm not going to describe that in detail but i have some resources uh that i will put in the description if you're interested um so we compute the expected uh q value and then we compute the current one the current one predicted by by work and then we compute the msc loss between them here we're using the urber loss instead of the msc loss yeah usually it's uh recommended for this algorithm to go towards the her loss rather than mc loss but if you use the msc loss you should have good results as well okay so now we can do a gradient step so with the loss we can backward or undo a gradient step to update,358,3,3,UvElGVu0cvo
10,"the weights of the new network on bas we almost done we need to do a few extra things such as plotting um so yeah basically every 50,000 epoch why every 50,000 it's because you want to reproduce this plot again so basically they do uh okay they do every 50,000 epoch as well so basically every 50,000 epoch we uh take the mean of all the rewards we got u so bally for each episode uh so here every time we are entering that loop we we are entering a new episode uh so so every time we we start a new episode we are going to compute the total reward for that episode on add that in the reward list and then at every 50,000 um epoch we compute the mean of the total rewards we got so in the list rewards okay and then we make the plot and then i think we will be done and we can put everything together so yeah we update the progress bar so basically u here i added the tqdm uh again every time we enter this this this loop we are u every time we enter this loop we entering a new episode but epoch is incremented here okay so bally that means um yeah that means for example okay yeah basically what my my point is here we're not if number epo equal to 30 million we're not going to do 30 million times this loop because epoch is uh increased um here okay so for example for for one iteration of this loop we might do 1,000 epoch okay so to make sure",358,3,3,UvElGVu0cvo
11,we see the progress bar updated in real time what i'm doing every time i'm increasing the epoch so here epoch plus equal to one we update the progress bar so the tqdm bar okay and then if when we are reaching the end the uh the end of the episode we are adding the total rewards for that episode to the rewards okay so now we can put everything together so we are creating the breakout uh the breakout environment from gy then we do a few processing that are described in the paper so for example we resize we resize the the images to 84 by 84 pixels then we transform them from lgb to gray scale then we do a frame stack so that means that instead of taking a single frame as input we're going to take uh the frame at time step t the frame at at time step t minus one tus2 and tus 3 that allows to see the movement okay if i just receive a frame on the ball somewhere i don't know if the ball is moving up or down but when i take into account the four well the the the the current frame plus the last three frames i really see the movement of i see the movement a bit like a video so that allow me to make better decisions okay uh and also max on skip envirment um yeah this is also described in the paper okay great so then we can just call the environment and close the environment and that's as easy as that only 100 lines of code while using,358,3,3,UvElGVu0cvo
12,a few um a few objects to help us such as the replay buffer on those rer but again we could maybe implement all of that it maybe 30 40 lines of code and then our code will be self-consistent and will be short again like this so i will hope this video was helpful to you i'm going to create a a series of video on reinforcement learning we're going to improve this code with the target network and we'll see that we will get a much better average reward per episode although this gr is already super nice on the trend agent is already super nice but we'll see that we will be able to get even better agent then we're also going to implement po for example proximal policy optimization um so yeah a few uh interesting videos coming about deeper in learning so uh please subscribe on uh uh leave a thumbs up if you think this video was helpful thank you,215,3,3,UvElGVu0cvo
0,in this tutorial you will learn how to land a spaceship on the moon using dueling double-deep q-learning you don't need any prior exposure to reinforcement learning you don't need any prior exposure to deep you learning or double deep cue learning you just need to follow along let's get started before we get allow me to address my relatively long hiatus from youtube i was furiously working to finish up the course deep you learning from paper to code that drops on you to me in the near future it is presently under review everything is done for my and i'm just waiting for them to approve the course of course when it does drop i will leave a link in pretty much all of my videos as well as making an announcement video here on the channel so you won't really be able to miss it if you want to see what you will learn in that course you can take a look at my github i've already posted the code there and as a github.com size field tailor dp learning paper to code i'll drop a link here in the description and pin comment as well so you can check it out now if you're interested all that said let's go ahead and get started so our imports are going to be relatively light we're gonna need os for file joining operations to handle model checkpointing numpy to handle numpy operations like random action selection for epsilon greedy action selection the torch base package to get access to the torch tensor function which we will use to cast numpy arrays to pi torch,358,0,0,H9uCYnG3LlE
1,tensors because pi torch is quite particular about the type of data you pass it in particular it doesn't want numpy arrays it wants tensors of specific types we'll cover that in a minute torche a tenon gives you access to the neural network layers in this case we're gonna be dealing with strictly linear layers and the course we'll be dealing with manipulating screen images from the sorry opening i jim but for now we're dealing with the lunar lander environment which is strictly linear layers and in that functional we'll give you access to the activation functions in this case we'll be using rel you and finally optimism iser in this case adam a variation of stochastic gradient descent with momentum and adaptive learning rates so the first thing we have to address is how will the agent store memory so that it can learn from them later so the easiest and most clean way to do this is to create a class a replay buffer class that will store the memories as some type of data structure and a facilitate sampling of those memories later on so let's go ahead and do that so the constructor for the replay buffer class should take a number of parameters the max size because the memory should not be unbounded you don't want to eat up all the ram on your computer and the input shape from the environment which would tell up the shape of the observations to expect from the open a item in this case the lunar lander which is a list of eight elements and we want to save the appropriate member,358,0,0,H9uCYnG3LlE
2,variables in our class the way i construct the memory is that i use numpy arrays where i pre allocate all of the space you can use a dq which is a python object that is basically a linked list and this has a functionality where it will append objects to the end of the list and delete them from the beginning of the list so that it always stays bounded if you do that though the difficulty is that you have to dereference a list of lists you have a dq object which will store the list of the memories which are you know state's actions rewards and new states and done flags this you have to dereference five items from that and they can get kind of confusing that's why i use the separately separately named numpy arrays it's just a cleanest implementation from a perspective of teaching not necessarily the best perspective in general now this quantity mem counter is the the index of the last stored memory since we're not using a dq we have to keep track of where we store the last memory and so we do that by the mentor variable the first memory we want is the state memory and that is shape mem sized by input shape and that needs to be a float32 data type to deal with the peculiarities of pi torch this star input shape allows us to accommodate input variables of just about any shape so it'll d it will unpack a list in this case our environment will have shape 8 a list of 8 elements so that is how we handle,358,0,0,H9uCYnG3LlE
3,that let's do the new state memory you can probably hear my son downstairs having a hoot okay so the new state memory is pretty much the same thing it's you know in terms of the array it's not the same thing but it is in terms of shape and datatype the next thing we want off is the action memory and that gets shaped mm sighs and it should be an mp in 64 now it is not particularly important that it is mp in 64 except for the fact that pi torch is quite peculiar about the data so just know that and the reward memory will be a numpy float32 and the terminal memory will be the memory of the done flags from the environment the reason we need to do this is that the future reward the agent expects to receive it depends on whether or not the next state is terminal so at the end of the episode it reaches the terminal state and the value of all states all future states in that is when you're in the terminal state is identically 0 because there are no more rewards to come so you you know you get no future returns from the terminal state and so it should be 0 that means that when you calculate the reward the agent's expectation of the future reward that is the targets in in the update rule for our deep neural network you must take into account that whether or not the next state is terminal if it is you set the expected vg reward to just the reward from this state if,358,0,0,H9uCYnG3LlE
4,it is not a terminal state then you have to discount the potential feature rewards that come after the current state and we do that with a terminal memory that acts as a type of mask which we'll see shortly next we need a function to store transitions and that just takes all the appropriate variables from the environment so the first thing we have to know is what is the index of our first free memory and that comes from taking the modulus of the memory counter with the memory size and then we take the memories and store them in the indices in the appropriate arrays terminal memory and finally when you're done storing the memories make sure to advance the memory counter so the next thing we need to do is sample the buffer so this comes into play when the agent is learning where it is learning from its prior experiences you want to be able to sample the buffer or the memory of unit of memories the the the the collection of memories uniformly you don't want to sample one segment of memory more likely than any other so we had to facilitate that with a random choice function and then takes a batch size as input now there are other ways of doing this you can prioritize certain experiences based on their potential for future learning but that is a more advanced method of sampling and best left for future content now if you have filled up the memory then you are free to sample anything from memory zero all the way up to mem size if you have not,358,0,0,H9uCYnG3LlE
5,filled up the memory you don't want to sample the memory of zeros right you want to sample the memory of actual memories that correspond to real gameplay and you do that by taking the minimum of either the mem counter or the mem size not the max the minimum the next thing we have to do is get our batch of random memories and this will sample uniformly it will sample from the zero to the maximum from here and it will sample batch size number so in this case 32 or 64 from this range so zero up to maximum minus one and will give you 64 or 32 whatever your batch size integers is and replays equals false tells the numpy random choice that once it selects the memory you cannot select it again you don't want to over sample memories you know it can give it somewhat of an issue then all you have to do is sample the arrays and return their respective quantities and that is it for our sample memory so now we have to deal with the dueling dq network itself so this gets its own special class and we will give it the appropriate name of dueling dq network so let's go ahead and write that so the constructor will take a learning rate number of actions a number of input dims and a checkpoint directory as well as a name so the name is useful for model checkpointing so that way we don't save q eval and q next networks over each other that would be kind of catastrophic and defeat the point of model check,358,0,0,H9uCYnG3LlE
6,pointing so this is where the os import comes in what we want to do is save our checkpoint directory for later use in case we need it when extending the functionality of our class and create a checkpoint file which is just the root directory whatever or rather directory structure you want to save it in plus the name of the network onto the knee actual deep queue network so we need a layer to handle the input of the observations from the environment and i'll transform those into 512 neurons and then for the dueling portion of our network we need a value an advantage stream i go into more detail on this in the class and the course but the basic idea is that the value network tells the agent what is the value of its current state and the advantage tells it the relative advantage of each action in its given state and in the paper they show you that the two streams end up focusing on different parts of the image is actually quite cool the value stream focuses on one part of the image versus the action part of action stream so it actually learns the two streams learn what is important for the relative functionality and so the value stream takes a 512 neurons as input now puts a single body because the state is single by it it's just a scalar quantity and the advantage function takes the same 512 inputs and converts it into a value of the relative value of each action now when we construct the cue function we may be tempted to just add,358,0,0,H9uCYnG3LlE
7,these two together but you can't actually do this while to come up with some other way of handling it when we get to the agent itself more on that later this reminds me i forgot something but the next thing we need is the optimizer and in this case that is what handles the stochastic gradient descent functionality in this case we'll use op tim what do we want sorry we will use adam what do we want to optimize we want to optimize the parameters of our network and where is that come from you might ask and that comes from something i forgot up here all pi torch classes must derive from the base and in that module class and that gives you access to this self dot parameters and it will perform the gradient descent with learning rates lr sorry about that the next thing we have to concern ourselves with is a loss function and in this case will be a mean squared error loss we're gonna take the mean squared error between our target values and the predicted values of our deep neural network where we shift the predicted values towards those target values next thing we have to do is handle the device selection so if you have a gpu you certainly want to use it if you have more than one gpu you can say cuda one or you can say ku to zero and if you don't have one available it'll select the cpu by default finally we want to send our entire network to that device the reason this is important is because pi torch differentiates,358,0,0,H9uCYnG3LlE
8,between tensors that live on the cpu intensive x' that live on the gpu the latter is a cuda tensor and the former is just a regular tensor and so you cannot mix the two types they have to be on the same device and so you have to be very particular about where you send stuff the next thing we have to concern ourselves with is the feed-forward function we know once we have a state what do we do that we send it through the network and get the value and advantage functions out the other side and this takes a state as input so this has to be turned into a tensor when we before we pass it in the first thing we want to do is pass our state through the network and then activate it with a value function then we want to take that flattened input and get the value and advantage functions out notice that the v a the value in advantage streams both share a common input layer this is the case even in the version with the atari library where you don't have a separate convolutional network to learn images for the value and advantage function you simply have one convolutional network in this case one neural one linear layer to learn the input features of the environment and the two separate streams to learn their respective functionality and then you just return the value and advantage functions next we have a couple of bookkeeping functions to save and load checkpoints it's always easy to it's always advisable to print something to the terminal it'll say hey,358,0,0,H9uCYnG3LlE
9,i am saving a check point and what are we going to save we're going to use a torch save function to say the network's state dictionary the states underscore dict function and you will save it in the check point file of course in pi torch if you're not familiar everything in terms of the layers and parameters of the network are saved as a dictionary kind of a neat way of handling that functionality and the inverse function is loading a checkpoint stupid autocomplete so what you want to do then is load a state dictionary from a file alright that is it for the dueling deep queue network functionality so now we have to concern ourselves with the agent now if you're kind of scratching your head and the reason i structure things this way the reason is that an agent is not a deep queue network this is kind of a finer point of software construction so it's if you're coming from a data science background not a computer science background this may be a foreign language to you but the basic idea is that one of the reason to not reason one of the significant advances in computer science was object-oriented programming it helps abstract away functionality into classes and objects and structures and the basic idea is that you haven't a is a or has a relationship in this case an agent has a deep q network or dueling dq network as it were that tells the values of advantage and action functions but an agent is not just the network an agent is also the network plus the ability,358,0,0,H9uCYnG3LlE
10,to learn from his experiences it's also it is also a memory it has a memory it has the capability to choose an actions based on that memory as well as to decrement its epsilon it's epsilon greedy action selection parameter as well as it having a whole host of other parameters that are relevant to its operation so it is an it is an example of an is a has a relationship hence why i structure things this way so let's code the agent class and so this will take a whole host of parameters so the first thing the first parameter we're gonna pass it is something called gamma so gamma is the hyper parameter that tells agent how much should discount future rewards so what this means in practical terms is that the agent knows that future rewards are uncertain because there is some uncertainty around the way it selects actions due to not just not just its own estimates but in some cases even the environment as well some environments are stochastic meaning in the action you choose is not necessarily action you get so there is some uncertainty around future rewards and so it discounts them by some amount between zero and one and there's a power law so 0 1 to the k power words however many k moves in the future in this case we're using rope 9 9 so a value of 1 means it's completely farsighted and values all rewards equally the value of zero means it's totally myopic only values its current reward epsilon is the fraction of the time that it spends taking random actions,358,0,0,H9uCYnG3LlE
11,"so it'll start out at 1 and gradually decay down to the minimum value you never want to go down to zero because the agent can never be quite sure that its model of the environment is correct and therefore must always spend some proportion of the time exploring to test this model against actual experimental data the learning rate of course number of actions input dimensions a memory size a batch size and epsilon min 0.01 epsilon decrement factor of how quickly to reduce the epsilon over time a replace of 1,000 so what this means is how often is the agent going to replace the target network so it has let me scroll down for you it has two networks one of which tells that the value of the current state and one of which which tells it the value of the next states that's the current and evaluation network or eval and next whatever you want to call it but the basic idea is that the agent only learns performs a learning update for one network and the periodically copies weights of that networked over to the target network and they replaces the interval at which that happens and the final variable is a checkpoint directory and i will give it a default attempt slice julian d dq n now you must make sure to do a make dirt on that directory beforehand otherwise it will give you an error when you attempt to save to a directory that does not exist of course we need a colon there and so then we want to save our member variables don't hit the caps",358,0,0,H9uCYnG3LlE
12,lock key that's the epsilon decrement next thing we need to concern ourselves with is the action space so this is a list of integers in the range 0 to n actions that facilitates random action selection that relates to the point of epsilon greedy action selection the epsilon hyper parameter and doing it this way just saves you a little bit of trouble later on next we need the agents memory and the dueling deep cue networks so cue eval will tell us the value of the current state and we need an appropriate name i like to use the environment as part of the name as well as the algorithm you're using you can use whatever convention you want i also include the name of the network cue eval in this case so the other network the cue next is pretty much identical except in the name so let's go ahead and just copy-paste this will tell us the value of the next actions and the reason we want to know is for our update rule and we'll get to that in a few minutes the next thing we want to concern ourselves with is the way chooses actions so as stated it we use epsilon greedy so that means it calculates a random number that right number is less than epsilon it takes a greedy action if that random number is sorry it's right sorry if the random number is less than epsilon it takes a random action if the random number is greater than epsilon it takes a greedy action meaning it takes the best-known action given its current state and so,358,0,0,H9uCYnG3LlE
13,it must take its observation of the environment as input so if we're taking a greedy action then what we want to do is convert our observation to a state tensor a pi torche tensor send it to our device beat it forward through our network and get our advantage function out we don't care about the value function because it's just a constant it doesn't affect anything and then we can use the advantage function to calculate the maximal action and when you actually want to get the maximal action out you have to take the dot idem dereference function the reason being that this automatic function returns a pi torch tensor which the opening item will not accept as input for its step function so you have to pass it an umpire array that you get with the dot item function if we're not taking a maximal action that means we're taking a random one by taking a random choice from the action space and regardless of how you select the action you want to return it next we have a function to handle the interface with the agents memory that is storing a transition and i should have an underscore because that is a new state and done flag next we need a function to handle replacing the target network and you want to make sure that it's time to do so you don't want to do it when you don't need to and that learn step counter adjust how many times the agent has executed the learning function and what we want to do we want to load the state dictionary from,358,0,0,H9uCYnG3LlE
14,the evaluation network onto the q next network the next thing we have to handle is how is the agent going to decrease its epsilon over time how will it converge to a most agree d strategy and you can see here i'm doing a purely linear decrease over time i'm just subtracting all the epsilon decrement if it's greater than the minimum otherwise just set it equal to the minimum you can do any really any type of decrement over time you can do and you know an exponential logarithmic 1 over square root anything you want just linear is the simplest way to do it so that's how i handle it and i i don't know to what extent it really matters because it spends the majority of its time in the greedy state mostly greedy and does the majority of its learning there anyway so it doesn't really matter as far as i'm aware next we need to handle the interface between the agent and it's saving network functionality and loading functionality alright so that is it for the bookkeeping functionality let's go ahead and scroll down a little bit so the next thing we need to do is handle the learning functionality this is the of course the really heart of the problem and so we want to pay the most attention to this so this doesn't take any input because we're going to get everything we need from the memory the first thing we have to address is what do we do if the agent hasn't filled up enough memories to perform learning so if it if we set batch size,358,0,0,H9uCYnG3LlE
15,the number of memories wants to sample each learning set equal to 64 but it's only executed ten steps of learning or even one step for that matter what do we want to do do we want to have it learn from that one step 64 times or do we want to just wait until it fills up its batch size of memory which is what i opted to do so we haven't filled up the batch size of memory go ahead and return in pi torch the first thing you want to do the top of every learning function is 0 the gradients on your optimizer and for this problem the first thing we want to do is figure out if we want to replace our target network and if we do to go ahead and do so the next thing we have the handle is the sampling of memory so let's do that sorry about that so you may be wondering why i don't convert stuff to pi torch tensors in the replay buffer class and the reason is that i reuse that class for all of the frameworks and so it should be framework independent and so we want to take each of these numpy rays and convert them to pi torch tensors and send them to the respective device you notice i have this t dot tensor there was also a t dot capital tensor and that is a different function it does the same functionality however you need to specify a data type and the lowercase tensor will preserve the data type of the underlying numpy array which is the behavior,358,0,0,H9uCYnG3LlE
16,we want or you can use capital tensor and just specify data type here it's just i do it this way because it results in a little bit cleaner code and that is most certainly a typo next we have the handler a little bit of the contentious point here on this youtube channel so by the way if you are getting dimensionality errors in any of my code then i would encourage you to open up a formal issue on github but do so in a way that allows me to track down the issue so when i run the code on my local machine it runs fine when other people occasionally run it a minority it seems they get issues and that tells me it's probably a configuration issue so they're running some different version of numpy some different version of whatever setup packages and so it's difficult for me to trace down what the issue is so if you open up an issue don't leave a comment rather open up an issue on github and specify what your configuration is you know what version of numpy what version of whatever a particular framework we're running and the tutorial is specify anything that might be relevant and also specify that whether or not you've added print statements in to let me know what the dimensionality is because some things will run like in tensorflow it will do a reduce functionality where even if you pass in the wrong dimensionality something that is match size by batch size instead of batch size by a number of actions it will still accept it but it will,358,0,0,H9uCYnG3LlE
17,give you the totally wrong answer so it fails catastrophic ly but in a way that is totally silent and the worst possible case pi torch is a little bit less forgiving on that i don't i think i'm not entirely certain i haven't i haven't tried to break the code recently but as far as i remember pi towards use a little bit more particular and so it may break a little bit more verbose lis and which is a good thing but either way make sure to put in print statements so that way that you can verify dimensionally so that you are actually doing what you think you're doing so to deal with this now just all a long-winded way of saying i need this line of code and this line of code is and all this is is an array from zero to bat size minus one that handles array indexing and slicing later on if you don't have this you get the wrong answer so this is a kind of critical line rather silly but it is necessary the next thing we have to do is pass the states and new states forward through our evaluation and next networks respectively and then we have to handle the next question of what is the value of the future states as according to the q eval network and all of this is explained in the paper on dueling double-deep rather dueling networks more precisely this particular line really comes from the paper on double-deep key learning which we cover in the course of course in the co in the class of course but,358,0,0,H9uCYnG3LlE
18,the basic idea is you need all three of these quantities to perform the update rule if you need clarification what double dbq learning is you can check out a couple of my other videos on this channel on the topic just do a search for it in the search bar and you'll be led to the non deep versions of it but that'll give you the basic idea so the next thing we need to handle is how do we add together the networks so the q learning comes from q which is the state value state action value function and what it tells you is the value of each action given you're in some state and in this case we have not a q function but a v and an s a value sorry vna a value an advantage function the question is how do we combine these two quantities in a way that gives us the q function and that makes sense now you can't just add them together because you have a problem called identifiability and so you have to add them together and then subtract off some quantity you didn't subtract off a max quantity but in the paper they tell you that they settled on the mean of the advantage stream so let's go ahead and do all of that good grief is a stop mean and you want to take them the mean across the action dimension and keep your dimensions three parenthesis and here is where the bit of confusion around dimensionality comes in so this array indexing is absolutely critical you need to take the 0 through,358,0,0,H9uCYnG3LlE
19,31 or zero through sixty-three whichever your batch size 32 or 64 is and you want the values of the actions the agent actually took that's by taking the actions to mention there are actions sub-array and we do the same thing or something similar for the cue next i need underscores here otherwise it will break and i have something wrong here now that i'm looking at it this should be a minus sign so what we're doing sorry is taking the advantage function and subtracting off its mean - a s underscore dot mean then make a 1 keep giving me go true and we don't need any funny indexing here because we want it for all actions the next thing we want is the similar quantity for q eval and in fact we can even do this to make things a little bit prettier copy paste and i will call this q eval and add in eval there and we need to stick that on a new line okay the next thing we want to know for update rule is what are the max actions of the next states according to the evaluation network and we take those across the action dimension has stated earlier in the video the next thing we have to handle is the issue of how do we value rewards for which the terminals the next state is terminal and we value them at identically 0 so we handle that by doing this so in this case dunn's will act as a mask so every we're done is true everywhere where the next state is terminal will give you,358,0,0,H9uCYnG3LlE
20,a 1 and it will set the index of the queue next at that particular state to 0 so it does not buy you future states for which they are terminal so pluck gamma times q next and then we want to take the indices and max actions so the quantity the target value is going to be q next for the maximal actions according to the evaluation at work ooh we are almost done so the next thing you have to handle is the calculation of the loss function and then back propagation and stepping your optimizer and incrementing your learn step counter and decrementing epsilon alright so that wraps up the functionality for our memory network and agent now we need a function to test everything so let's do that so imports for this are also kind of light you need jim to handle our environment will they dump i to handle averaging over the last hundred games so we can get an idea without the agent is learning of course we need our agent and we need the function to plot the learning curve and i call this different things in different directories it's kind of a misstep on my part so i will do a quick check before we when we go to the terminal to make sure i have the right function name alright let's do our main loop first thing we want to do is make our environment set the number of games now when i ran this before i stopped after 250 it takes a little bit of time just because i want to demonstrate learning i don't necessarily,358,0,0,H9uCYnG3LlE
21,care about getting a winning score but we will run for 500 games this time and let's instantiate already no sorry the low checkpoint it just tells us whether or not we were going to do model testing we can load a saved model and then test it using just to see if it actually worked without learning let's instantiate our agent with a gamma of 0.99 starting epsilon of 1.0 learning ready to fly by ten to the minus four input dims is eight from our environment we have four actions memory size of a million epsilon min of 0.01 batch size of 64 epsilon decrement of one by ten to the minus three and replace interval of every 100 moves so we're gonna load a checkpoint we should do so of course i forgot my colon next we need a number of variables - for our running of our games we'll need a filename for saving our file or a plot sorry and i like to put the relevant hyper parameters in the name such as the environment the algorithm optimizer of learning rates and replace interval i don't put gamma because i almost always use 0.99 so it's kind of assumed we also need a an array for the scores we also need an array for the scores the agent received and it's epsilon is overtime and that is for the learning plot so we want to iterate over the number of games setting the done flag to false resetting our environment and setting the score equal to zero at the top of each game so to play each game we say while,358,0,0,H9uCYnG3LlE
22,not done action equals agent choose action observation and take the out take the action and get the new state reward done and debug info back increment your score by the current reward store that transition which you cannot read let me scroll down there we go and put that there and then you want to learn and set the old sate equal to the new state and you have to do this so that you are actually learning from the so that you're actually choosing an action according to the current state of the environment at the end of every episode you want to append the scores and calculate the average score and this just for printing out to the terminal and we'll use a print statement to let us know where we are the score we received the average score the previous 100 games the agent's epsilon s i don't really need those spaces there and that is it for our debug statement every 10 games or so we want to save our model you can use whatever metric eb1 you can say if the score is better than its best known score them save you can do it every 20 games 50 games whatever i use 10 and you want to append the asian epsilon agents epsilon to the epsilon history and finally at the end of all the games we want to do our plotting and this is just the x axis for our plot i believe that is plot learning curve all right so let's head to the terminal and see how many typos i made in the last 45 minutes,358,0,0,H9uCYnG3LlE
23,all right here we are let's give it a try duel and edq and lander that looks right i mean slf is not defined of course it should be self let's correct that that is way back in a line what is that line 18 let's come all the way back up in the other file of course all the way back up here self dot memory size oh okay it doesn't related that elsewhere let's head back to the terminal and try it again once more okay i have an unexpected argument in my initializer for the looks like the agent class alright let's handle that and so of course here is something stupid as always i forgot two underscores good grief alright so let's head back to the terminal and see how it does once more okay so this is straightforward so i know what happens here so let's head back to the code so what's happened here is i've forgotten my super constructor and that is needed because i'm deriving from an end up module alright let's head back to the terminal and see what else i messed up once again ah of course that's simple so of course load checkpoint is a boolean not a function alright once more and i misspelled observation typicall typicall that's in the main function in line 30 observation observation alrighty let's try that again has no attribute learn step counter ok that's simple that's probably because i forgot to edit right there alright once more and there we go finally and here i was hoping i make it through without any errors of course that's too,358,0,0,H9uCYnG3LlE
24,much to ask so i'm gonna let this run and when it's done i'll go ahead and wrap up the video see you then alright it has finished running and you can see that on the terminal it indicates a final score we run 175 points which is a little bit low earlier on it achieved a score of something like 194 points so it's oscillating around a little bit there are two potential reasons for this so one reason could be that the learning rate is a little bit too high the 5 by 10 to the minus 4 could be about a factor of 5 too high we could retry this with 1 by 10 to the minus 4 the other possibility if you take a look at the loading plot is that the oscillations are attend to the oscillations usually due to too small of a replace interval so instead of a hundred games 100 moves you could do every thousand or so that is another hyper hyper parameter to play around with i encourage you to do that and do a do a git clone of this and play around with it and see what type of results you can get i hope that was helpful in the next series of videos we're gonna be looking at all kinds of different stuff we'll probably move on from here to more policy gradient methods stuff as well as some more generalized machine and deep learning content make sure to subscribe so you don't miss that hit the notification bell and i look forward to seeing you in the next video,354,0,0,H9uCYnG3LlE
0,so i'm going to get started on the next section um yeah by way of introduction uh my name is miguel i was introduced to start i also work with uh john uh dell and alex and um the blue prism ai labs and uh today i will be talking about um dpq learning which is a um different approach to q learning than tql uh where instead we'll be using a function but i'm getting a bit ahead of myself here so um as john presented in the previous section tql uses a table to store the q values for our state action pairs as you see in the bottom here we have an entry for each state in action but you know it's not difficult to imagine a scenario where we have a lot of states and a lot of actions so for example if we had 10 000 states and 10 000 actions then we'd have to maintain a table with a hundred million entries and clearly that gets out of hand quite quickly so instead of maintaining a table maybe instead we could learn a function which takes in as inputs a state and an action and then outputs the corresponding q value this would allow us to handle you know state and action spaces of a much larger size and that's exactly the idea behind dqns where for this mapping function we'll be using a deep neural network so as shown in the bottom left here we have our state we have our action it's going to pass through a network and we're going to get a q value for that,358,0,0,By6TYFSIFVE
1,state in action another motivation for dqns is can kind of be seen in this example here so on the left here we have this this agent who's playing the atari game breakout and as he's playing he's collecting training data and filling in his tql table so this is our tql approach here however what happens if during inference we all of a sudden see a state that we didn't see during training so if it's not in our q table what do we do do we maybe randomly pick an action for that state or try to find a similar state well this is kind of one of the big downsides of tql which using dqn can solve with dqn we can still apply our q network on these unseen states as we have on the right here and if our q network is well trained it should be able to generalize to these states um quite well because of the similarity to maybe previously seen states furthermore dqm allows us to handle states which don't really fit nicely into a table so in our frozen lake example we had this grid so it was easy to translate this grid into table entries uh with our actions but in this breakout game example it's not necessarily clear how we would actually take this image and put it into a table you know perhaps we could hash it and there's definitely some workarounds but it doesn't naturally map and even if we did hash it we would be losing some information but since dqns use uh neural network we can kind of build off all,358,0,0,By6TYFSIFVE
2,this work on computer vision um which you know in your network has had a lot of success with um and this allows us to handle much more complex states as well as states we haven't seen before and hopefully if our network has been well trained we can take advantage of the similarities of our states or our network kind of works as a feature extractor and we will potentially perform quite well on states we haven't seen before if they are similar to states we have seen so unfortunately i don't have time to go into the details of neural networks in this tutorial and i'm sure many of you are familiar with them already so i'll give a very quick one slide recap for those who aren't um familiar as the name might suggest they're motivated by the idea of neurons in biology and what we have is we have these different nodes as we see in this diagram here and these nodes um are compo compose different layers which take in information from the previous layer through these arrows sum them up sum up the their inputs and pass them through a non-linearity to the following layer and the way that we train them is simply by training these weights connecting these different nodes and we trade them by minimizing a loss function on the output and this is and this is done via gradient descent it's been shown that neural networks can approximate a wide variety of functions which allows us to do quite well in a lot of settings so one quick twist to what i said previously in practice,358,0,0,By6TYFSIFVE
3,we actually don't take in the state and the action as inputs what we do is we actually simply take in the state and then output a vector of q values for all actions and this is represented in this equation here so we take in the state in our network and we output one q value for each action and the reason we do this is that it allows us to call our network only once per state instead of once for every state action pair and lets us take care of take advantage of the potential parallelism during inference when we're actually using when the agent is actually being evaluated it selects the action which has the highest q value from this this vector and if there's multiple maxima you can simply randomly break ties and then during training we encounter this this issue again that john presented previously this this idea of balancing exploration and exploitation and here what we do is we take we use an epsilon greedy algorithm to to balance it so during training with some small probability epsilon we'll select an action at random and then with probably one minus epsilon we select the optimal action as we are doing during inference and this allows us to exploit actions we know do well during training so for example not falling into a an icy hole um while still allow us to explore new actions which might lead to better rewards in the future so traveling around our lake for example to try to get to the goal so once again we have our q learning equation here where we,358,0,0,By6TYFSIFVE
4,have our new q values which are updated based on a learner rate our td target which is composed of our reward plus our discounted uh future q value here as well as our current our q value for the current state in action and our goal here is we we want to train our q network to to optimize to minimize this td residuals we're going to use gradient ascent to minimize this 3d residual and as john mentioned previously the the intuition here is that our our td target this this term with the reward and and the maximum action is a better estimate of a q value because a of course it already accounts for the reward that we're getting by selecting this action but also it's also closer to the terminal state where we do in fact know the q value since that's where we'll be receiving a reward so we're trying to move our current estimate towards this future more accurate estimate in the simplest setup we can do this q learning update at every time step but we could also potentially batch it batch the updates which might help with the stability and um if you are familiar with neural networks you will be familiar with the concept of batch updates so to summarize a couple of the pros and cons of dqn and this will kind of motivate as well some of the the other approaches in reinforcement learning that we will be talking about later the dqn approach can be kind of seen as compressing the q table down instead of having to maintain this large potentially very,358,0,0,By6TYFSIFVE
5,large true table we can use a network to get our q values which allows us to represent a much larger state action space it allows us to handle states not seen during training as we saw with the breakout example and the q network can also be seen as a feature extractor which allows us to generalize to these q values we haven't seen before and actually do quite well and one more thing to note is that it can leverage we can learn both on and off policy so we'll see this in a couple of slides but we can revisit some of the transitions we saw in previously and use they use those to learn on so we can train on these off policy examples and that allows it to be a lot more sample efficient um and this is a big advantage compared to policy gradient which we'll be discussing later because in policy gradient you can only learn on policy so by allowing us to learn both an on and off policy we can kind of take advantage of all these transitions that we've seen one of the some of the downsides of dqn is that just like tql it still can't deal with stochastic policies everything's deterministic we're taking the the maximum action um when our agent is interacting with the environment and being evaluated it also can't be directly applied to continuous action spaces we could kind of get around this by discretizing the continuous action spaces but um in certain instances that that may not be what you want to do and you'd want to deal with a,358,0,0,By6TYFSIFVE
6,continuous action space directly and dqm simply can't do that and then finally as i mentioned we need to separately have this epsilon greedy algorithm to balance exploration versus exploitation and it would be nice if if you know if we could ever wish this for our ideal model if this exploration exploitation balance was actually implicit in the model and we could take advantage of this concept of like action uncertainty um as as we are as we are learning and interacting with the environment so that's the basic dqn setup there's a couple extensions that i'll briefly discuss here because they do come up in the exercise so one of the um one of the extensions is it's called experience replay and the idea here is you decouple your batch updates of your model from your your experience stream so how that works is that as you interact with the environment you save your your transitions so a tuple of state action reward and next date and you take that tuple and you store it into a memory buffer and then when you want to update your model you'll simply randomly sample from that replay buffer and apply your q-learning update and this can help a lot with the stability of the training because you're reducing the correlation between all the items in your batch because they're sampled from from the memory another improvement is target networks so the idea with a target network if we look at this equation on the bottom here is that in your td target instead of using your q network your normal q network which we'll call the,358,0,0,By6TYFSIFVE
7,online network you're going to use a an additional q network called a target network and the reason you'd like to do that is that in the normal in in the standard approach this target because it also is dependent on this this q network it's it's kind of a moving target right you're updating your q network and that changes both your your your value this right value in the in the residual but it also changes your target so it's kind of you're chasing this moving target which can make it difficult to learning quite unstable so the idea here is you actually create a second tar a second q network here this target network and that kind of stabilizes your your goal and really kind of stabilizes the training so you have a you have two networks you have the target network the online network the online network you update as usual as we saw before and then this target network use you infrequently update it based on on this online network you're going to kind of copy over all all the weight values every once in a while to to update it in that same vein um and this is this this there's this idea of double dqn so double dqn builds off this idea of a target network except that instead of using the target network to select the value you'll be sorry to select the action you'll be using your online network to select the action so you select the action with the online network you pass it through the target network to get your your target q value and,358,0,0,By6TYFSIFVE
8,then you update as normal so this decoupling of the this full decoupling of the action selection and the evaluation um helps kind of reduce this these estimate estimation issues uh with the q values and once again kind of leads to more stable learning so kind of all these extensions are really focused on improving the stability um of of the the dqn approach and then of course there's a bunch more work that's been done in this to to to improve on this this these these basic degree approaches we can use prioritize replay so instead of randomly sampling from our replay memory we can prioritize certain memories so for example if some of the transitions that we saw have a very high td residual we can we could say that those transitions are potentially more useful to learn from than those where we're doing a very good job of estimating dq values already we can also uh there's there's also another extension on top of double uh similar to double dqm where we it's called uh dueling dqn um and there's a little bit sometimes a confusion the ledger literature here because they both get abbreviated to ddqn but the idea here is you split your estimate of the q value into two parts instead of estimating just the q value you estimate the value of this state as well as um the advantage of taking an action given that state and this will actually come up again in uh in the discussion on actor critics where you have this this idea of a state baseline and this helps with the the variance,358,0,0,By6TYFSIFVE
9,of your updates okay so that leads us to the next exercise so you can follow the quick links on the website here or use this direct link which we will post in the chat and similar once again just make sure you open it in collab and the setup should be similar to the tql there's both the basic dqm implementation as well as as well as um the the experience replay is also in there as well as the target network and once again feel free to ask questions in the chat or to unmute yourself and ask questions and i think we'll be setting up breakout rooms as well thank you,148,0,0,By6TYFSIFVE
0,welcome back to this series on reinforcement learning it's finally time to apply everything we've learned about deep q learning to implement our own deep q network in code in this episode we'll get introduced to our reinforcement learning task at hand and go over the prerequisites needed to set up our environments to be ready to code so let's get to it music all right let's jump right into what we're going to be doing in our upcoming project we're going to be building and training a deep q network to learn to balance a pole on a moving cart this is widely known as the carton pole problem we'll be using openai's gem toolkit to set up our cart and pull environment remember gem is what we used to set up our environment in previous episodes to train an agent to play frozen lake via value iteration if you need an overview or refresher about jim be sure to check out our earlier episode where we first introduced it the carton pull problem consists of a cart that can move left and right along a frictionless track the cart has a pole attached to the top of it which starts out in a vertical upright position however by design the pole will fall either to the left or to the right when it's not balanced the goal here is to prevent this pole from falling over so a reward of plus one will be given for each time step that the pole remains upright and an episode will be deemed over when the pole is more than 15 degrees from vertical or when the,358,0,0,FU-sNVew9ZA
1,cart moves more than 2.4 units from the center of the screen so essentially the longer the pole remains upright without deviating too far from the center of the screen the more reward our agent will get now just to get an idea of what would happen with no optimization at all for this carton poll problem here's a quick snippet of code that will run an instance of the cart and pull environment from jim for 1 000 time steps and will take a random action either left or right at each time step we'll render the environment at each step so we can see what this will look like music so yeah as you can see our deep view network will have some learning to do so that it can balance the poll better than this let's now jump into how to get our environment set up so we can get started with building our dqn first things first we're going to be using pi torch to build our deep queue network pytorch is a neural network api written in python and if you haven't worked with it yet don't worry you'll still be able to follow this project completely we'll be sure to cover everything regarding pytorch as it comes up in our coding but pytorch is a great library so i'll encourage you to check it out if you haven't yet we have a full pie torch video series with accompanying blogs on deepblizzard.com that start from the absolute basics and guide you into building and training your own networks from scratch now for my personal environment i'll be using anaconda with,358,0,0,FU-sNVew9ZA
2,python version 3.7.3 and so aside from pi torch and gym everything else we'll need comes included with anaconda to follow along using anaconda yourself you'll first need to install anaconda and you can see exactly how to do that on anaconda's website where they have the command or executable you'll need depending on which operating system you're using after installing anaconda you'll need to install gem by simply using the command pip install gym then lastly you'll need to install pytorch i recommend for this step to check out our video or blog on how to quickly and easily install pytorch it will get pytorch up and running for you in no time i'm going to be using a windows environment without a gpu for this project so this is the command that i used to install pi torch i definitely recommend you check out the episode i just mentioned though for installing pytorch since this command may be different for you depending on your environment everything else we'll need like our jupiter notebook and matplot live numpy and pillow for example will already be installed as they come packaged with the anaconda install all right we're now all set up to get started with our coding we'll jump right into that next time let me know in the comments if you were able to get everything installed and ready to go by the way did you know you can now test your own understanding by taking quizzes after studying the blizzard content head over to the blog for this video now on dblizer.com to check it out and please like this video to let,358,0,0,FU-sNVew9ZA
3,us know you're learning and be sure to check out the deep blizzard hivemind for exclusive perks and rewards see in the next one hey you know there is this quote by nelson mandela that is really cool he said i never lose i either win or i learn this is a powerful statement try to adopt this way of thinking into your own life next time you attempt something and quote lose rather than looking at that attempt as a loss or a failure look at it as a learning opportunity from which you can grow that is after all what our reinforcement learning agent will be doing music do music you,146,0,0,FU-sNVew9ZA
0,hello guys welcome back today we are diving into classic challenge in reinforcement learning the lunar lander problem if you have ever wondered how ai can land a spacecraft on moon you are into the right place in this video we will build an ai agent that learns to safely land a lunar module on the moon surface we will go step by step from understanding the problem to training the agent using cutting edge reinforcement learning techniques so get ready to see our ai agent face a tough journey from crashing on its first try to eventually mastering the art of land with the style so this is our lunar module and our objective is to train the agent to land this module safely on the moon surface the agent actions are controlled by continuous state information and it must balance fuel usage control and landing precision okay so this is the environment information we have action space of size four four observation space of size 8 and this is the import okay so this is a classic rocket trajectory optimization problem according to pontren maximum principle it is optimal to fire the engine at a full throttle and turn it off and that's the reason why environment has actions like engine on and off okay now let's look at the action space so we have four discrete actions 0 1 2 3 0 is do nothing that means no thrusters are fired one fire left oriented engine two fire main engine and three fire right oriented engine okay next let's look at the observation space so the state is an eight dimensional vector okay,358,0,0,SgMAMuHJHpE
1,so what are those eight dimensions the coordinates of lambda in x and y the linear velocities in x and y its angle its angular velocity and two booleans that represent whether each leg in the contact with the ground or not next rewards so after every step the reward is granted so for example the reward is increased or decreased the closer or further the lander is to the landing pad okay so if you land closer to the landing pad you'll get more reward if you land uh further to the landing pad the reward will be decreased and uh uh reward is increased by 10 points for each lag that is in contact with the ground and these are for the firing the side engines main engines the episode will receive additional reward of minus 100 or plus 100 points for crashing or landing safely respectively an episode is considered a solution if it scores at least 200 points okay and episode termination the episode finishes if the lander crash dashes lander gets outside of viewport or the lander is not awake that means not moving so let's get started so for this problem we are going to use uh google collab you can use jupiter notebook or any other editor you would like to so we'll start with installing the gymnasium so install so i already have gymnasium installed if this is the first time for you this will install the gymnasium okay next we will import so import gymnasium as gym let's have few more installs which are needed so next one is uh swig and next we will install the,358,0,0,SgMAMuHJHpE
2,gymnasium box tod okay so the installation is completed next we will have a few imports let's run that and next we'll have the environment so we can get the environment from here okay okay next we will have the state so we dot obs obervation space do shape and the state size so the shape and okay and uh we have action size action space do n let's print these so we have state print action size state size and print action size so let's run this okay okay so we have the state uh so this is the shape this is the uh state size eight and this is the action size four now let's have few hyper parameters okay so these are our hyper parameters learning rate mini badge gma replay buffer size interpolation parameter maximum number of episodes max time steps in each episode abson is starting value ending value the decay value and the scores on 100 episodes so in this tutorial i'm not going to go deep into these parameters i have created a separate video where i have discussed these parameters in detail uh i'll leave the link in the description please visit if need it okay so let's run that next we'll create the brain uh for the ai agent so let's create ann artificial neural network nn.,293,0,0,SgMAMuHJHpE
3,modu next we'll have the constructor we'll call the super okay so this will be action size let's have the seed as well so seed 42 self to okay so we have three fully connected layers 1 2 and three and uh this is the input layer this is the hidden layer we'll have 64 uh neurons in each layer and this is the output layer okay so this looks good let's have the uh forward pass so this forward pass takes the state pass through first fully connected layer apply the reu activation function then pass through second fully connected layer apply the uh same activation function and then pass through third activation function to the output layer okay i have created a separate video where i have discussed artificial neural network in detail i leave the link of that video in description please visit if needed okay so now let's run this okay next we will work on the replay memory class now let's quickly brief about replay memory so replay memory class is designed to manage the agents memory of the game experiences it stores the state action reward next state and whether the episode ended for each step in the game so on the current state agent will take an action based on the action agent will be rewarded agent will land on the next state and this done to notify whether this action has ended the current episode so this entire tle is called one experience i have created a separate video where i have discussed in detail about replay memory i leave the link in the description please visit if,358,1,1,SgMAMuHJHpE
4,needed okay so let's go back okay so first we create a constructor and this will take capacity this will be the capacity of replay memory so this is the memory and this is the capacity now let's have a few functions so first would be push so self event event is nothing but experience and this function is responsible to pushing the event into memory so self dot memory.,91,1,1,SgMAMuHJHpE
5,append event and we need to check if memory exceeds the capacity if that happens we need to remove the oldest experience from the memory so if so if length of memory is greater than capacity we'll remove the oldest experience okay now let's have a sample method so this function is responsible for taking random experiences from the memory for the agent to trade so experiences so this bass size is number of experiences so let's say for example if the bat size is 100 then we'll randomly pick 100 experiences from the memory for the agent to train next we will convert these experiences into p torch tensors so what do we have in experiences it's a tle state action reward next state and done so first we'll get the state so let's understand this so in experiences the first element is a state so we have list of states we we stack them convert them into py toch tensors and then convert them to float so okay so we have list of experiences okay and for example let's say these are the three experiences state action reward next state and done so first we'll extract the states so we'll get the list of states then we'll rest stack them into one npy array then we convert them to pi torch tensors because neural network operates only on tensors and then we'll cause them to float so again we have cre a separate video where we have discussed all of these things in detail uh i'll leave the link in the description please visit if needed okay so now let's create for uh state,358,2,2,SgMAMuHJHpE
6,actions okay now we have all the five elements uh actions can't be float valu so we'll change it to long and the fifth element done is a boolean value so we'll convert convert it to integer so s type okay and at the end we'll return states actions rewards mi state senta now let's run that okay so we have error syntax error uh looks like we have extra bracket here so let's remove that let's run it again okay so we have another error uh this should be delete okay now let's run it again okay so we are good next we will work on the agent class and this will take a state size and action size so we have state size and action size next we'll create two networks one for local and one for target okay so let's let's call it as local q network and target q network and we'll pass the state size and action size so here is our uh network class okay and this takes state size and action size okay let's quickly understand why do we need two networks so one local network and one target network so it's a design choice in deep q learning to improve stability and convergence of training so local q network it updates actively and it is used to predict the q values for the current state okay and the target q network it used to compute the target q values for the next state and it updates less frequently than local q network so in this video i'm not going to dig deep into this concept i have created,358,2,2,SgMAMuHJHpE
7,a separate video where i have discussed in detail uh that why do we need two networks versus target uh i'll leave the link in the description please visit okay so let's go back now next we need the optimizer so we'll be using adam optimizer and that will update the local network parameters the learning rate we have defined uh here and uh and also we'll have replay memory initialized here okay okay what is it complaining about okay okay this should be torch not touch okay next we'll have the uh training step so let's call it as st step initialized to zero next we'll have the step function step and self and let's copy it from here okay so this step function will be called by the agent after each action okay so agent will pass the experience and we'll store this experience in memory so self dot memory.,197,2,2,SgMAMuHJHpE
8,tstep is zero so in order to train the agent first we'll get the random experiences from the memory so experiences do mini batch and our mini batch size is uh 150 so before we select these experiences from memory we need to ensure that memory has at least these many experiences okay so if next we'll call the learn method so we have yet to implement this learn method so this method will take experiences and the discount factor that is gamma so we have gamma here so we'll pass that okay so with that our step function is ready next we will have the get action function so this method decides which action agent should take based on the current state and it also handles the exploration and exploitation trade-off so let's quickly understand that so so exploration exploitation trade-off is a fundamental concept in reinforcement learning that describes the balance between two comparing objectives okay so exploration the agent tries new actions to discover potentially better rewards or strategies and exploitation agent uses the knowledge it already has to maximize reward by choosing the best known action and why this trade-off is important exploration is necessary to ensure that agent doesn't miss out a better option or strategies that hasn't tried yet and exploitation ensures that agent leverages its current knowledge to achieve high rewards efficiently so i have created a separate video where i have discussed in detail about uh exploration exploitation tradeoff with an example i leave the link of that video in the description if this concept is new to you please watch that video okay so let's go back,358,5,5,SgMAMuHJHpE
9,so this get action function takes the state and epsilon value first we need to convert this state which is in nump array to py stencil so state so let's copy it uh from here okay and here we have we don't have list we have state dot uns squeeze so we have a state which is npi array we convert it to pi torch tensor using this function then cast to float and then uns squeeze zero this is used to add a batch dimension since py torch models expect input in batches now put the local network in the evaluation mode so eval so we switch to evaluation mode to avoid unwanted updates next state will be passed through network without calculating gradients for efficiency okay so for that what we'll do with torch do nograd and we'll get action values okay so we have the action values now before we do anything we'll put the network back to the training mode so self dot local q network.,221,5,5,SgMAMuHJHpE
10,train next the exploration exploitation trade-off so if random dot random so if generated random number is greater than current eps value we'll pick the action from these action values okay so these action values would be something like this so these are predicted q values for each action so here we have uh four actions so the size would be four action zero xtion one action two and action three so we'll pick the action with the highest q value the best predicted action okay and for that we'll do return np do max and we'll pass the action values action values dot cpu do data dot numai so this is exploitation part now the exploration part else agent will pick the random move so possible values are 0 1 2 and three okay return okay so this is responsible for selecting a random action from agent's action space when agent decides to explore rather than exploit so action size which is four and this function will produce the array with elements 0 1 2 and 3 so random choice will pick a random element from the array and with that we are done with the get action function next we will implement the learn method and this method will take experiences and gamma as parameters so this learn method is used for training an ai agent in reinforcement learning okay so before we proceed let's understand the bman adaptation for deep q learning bman adaptation for deep q learning so we compute the target q value using this bman equation so this is the q target s and a s stands for current state,358,6,6,SgMAMuHJHpE
11,and a stands for action r stands for the reward this is gamma the discount factor this is the q network s prime is the new state and then a is the action in the new state so on the current state agent takes an action a on that action agent receives the reward r and then agent lands on the new state s prime then again on the new state agent can take all the possible actions and we will pick the action that gives the maximum reward so this is in future in the next state multiplied by the discount factor which determines how much importance is to give in the future reward the gamma value lies between 0 and 1 we generally take the g value is 0.9 so again on the current state s agent takes an action a for that agent receives the reward r and then agent lands on the new state s prime on that new state agent can take any of the possible actions we will pick the action that gives the maximum reward multiplied by the discount factor added to the reward that becomes the targeted q value for the the current state so this is on the target network now let's continue with the learn method so first we'll unpack the experiences okay so experiences has these five elements experiences okay now we'll compute the target q values so let's call it as next q target and this would be self dot we'll use the target q network so target q network we'll pass the next states so next states dot detach dot max and then,358,6,6,SgMAMuHJHpE
12,uns squeeze now let's understand what are we doing here okay so this is the line and first understand target q networks next states so we pass the batch of next states through the target q network to get the q values for all the possible actions okay so we pass the next state to the target q network and the shape is bat size and number of actions so bat size is number of experien samples and num actions is the total number of actions agent can take we'll see that with an example in a moment so let's proceed so we have understood this part now the detach method so target network is used for calculating the target q values and we do not want to update its weight during bag propagation so detach method removes the computation graph from tensors stopping gradients from flowing through it so this way we ensure that during back propagation we only update the local q network not the target q network now the max function so this function finds the maximum q value for each next state across all possible actions the bman equation selects the best possible future q value assuming the agent follows the optimal policy okay so remember the belman equation so on next state s prime agent has all the possible actions available agent picks the action which has the maximum q value okay so let's clear the screen so max of one operates on dimension one the action dimension so max of f return two values the maximum qy value for each state and the index corresponding to the maximum qy value,358,6,6,SgMAMuHJHpE
13,we don't need this we only need the maximum key value so we'll do max of zero that is this one okay now let's understand the uns squeeze method so this method adds a new dimension to to the tensor the expected shape of next q target is this but right now it's only the bat size so this method makes it a column vector okay so let's understand this with an example so let's say these are the next st q values so here our batch size is four okay and these are the q values for all actions in the next state so we have four states 1 2 3 4 and these are the q values okay so this max function picks the maximum q value in each state so here it is 2.5 4.2 2.0 and 3.3 okay so this is what we have here then this uns squeeze method it gets it to the correct shape to ensure it aligns with other tensors like rewards and duns okay so let's go back to the code so we have calculated next q targets that is this part this right part okay now let's use the bman equation so q target so let's call it as q target and uh that is reward plus so we have rewards plus okay so we have next q targets multiplied by gamma that's the discount factor this part and then we multiply it with 1 minus duns so why this one minus duns so let's say if done is equal to 1 that means if the next state is terminal then we'll just add the reward,358,6,6,SgMAMuHJHpE
14,this entire part will become zero okay so we have calculated the q target now we'll calculate the q expected on the local network so we have calculated this part the q target on the target network and now we'll calculate the q expected on the local network so that is self.,67,6,6,SgMAMuHJHpE
15,loal network we pass these states do gather function 1 comma actions so this selects only the q values corresponding to the action taken so we pass the next states to the target q network to compute the target q values and current states to the local q network to compute the expected q value and now we will calculate the loss so we'll use mean squared error functions to calculate the loss between predicted q values and target q values so that is this part okay now we will reset the gradient so optimizer okay so we have initialized the optimizer here and that is using the adm optimizer and here we reset the gradient from previous step to prevent accumulation okay now we'll compute the gradients of loss with respect to model parameter so loss dot backward so we back propagate the loss to the network and then we will update the weights so optimizer do step so let's summarize this we use mean squared error function to calculate the loss then uh we reset the gradient from the previous step then we back propagate the loss and updates the weight okay and now we will update the target q network using soft update method so self.,271,7,7,SgMAMuHJHpE
16,soft update from here and uh target q network comma interpolation parameter so we have interpolation parameter here next we will implement the soft update method but before that let's let understand about this soft update okay so we have a local model the neural network actively learning then we have a target model a more stable network and we use something called interpolation parameter a factor between 0o and one that controls the update speed so we gradually update the target model and that update speed is controlled by this interpolation parameter so if the parameter is one the target model immedi imately copies the local model weights that is hard update and if you use a very small value let's say 01 then the target model updates very slowly blending in only 1 of the local model parameters at a time and we are going to use 01 as the interpolation parameter value because we want our target model to updates very slowly and this will be the equation we'll be using so parameters of local network multiplied by interpolation parameter plus 1 minus interpolation parameter into parameters of target network here we can see if we take the value 1 then 1 minus 1 so this entire part will become zero and the target would be equal to local so that is the hard update and if we take interpolation parameter as1 then this is 01 time local 99 time target so 99 of target parameters 1 of local parameters so we are blending in with only 1 of local model parameters now let's implement the soft update method so soft update and,358,8,8,SgMAMuHJHpE
17,this takes these three parameters okay so this is the soft update method so interpolation parameter into local network parameters this part and one minus interpolation parameter into target network parameters and we'll copy to the target network okay now we are done with the agent class so let's run this okay so we are good and next next we will initialize the agent so agent agent and we'll pass the state size action size so let's run this so we have another issue in an class in it okay so here we have a typo let's run this again let's run the agent again and let's run this okay another issue learning rate is not defined so we are using the learning rate here and we have it here let's run that and let's run the agent okay so we are good now let's run the agent so we have uh number of episodes here so for episode in so at the beginning of each episode the first thing we will do is reset the envirment okay so we'll call env do reset and this returns the initial state and the information okay so let's check that so go to envirment and the reset so the parameters okay so the reset returns the observation the initial state and information that we don't need okay so we'll discard that now let's have the uh score zero now maximum time steps so max time steps okay so for st in range so first we'll get the action for the agent so we'll call agent.,341,8,8,SgMAMuHJHpE
18,get action function and we'll pass the state and epsilon value so epsilon epsilon is not defined so epsilon value would be we'll start with one so epsilon value here we'll say epsilon starting value okay so now we have the action now we need to perform this action okay so that is environment do step and we'll pass this action and this will give us let's check the step function so this is the step function and this gives uh the observation space the next observation that is the next state reward and terminated so that is done and two more values uh truncated and information that we don't need so it gives the next states rewards reward done and uh what else truncated and info and okay this is deprecated so we don't need these so we'll just discard that okay okay okay so now we have next state reward and done now we will train the agent we'll call this step function okay so agent do step and this step function takes this okay so we have state that's a current state action reward next state and done okay now the next state will become the current state so state next state okay now let's update the score so score reward check if episode is done if that's the case we'll break from the current loop current episode now we'll add the score to the q we have created so we'll add the score dot append score and next we will update the epsilon value so max of epsilon ending value okay so we will start the epsilon value where it is so,358,9,9,SgMAMuHJHpE
19,we'll start with the epsilon value one the ending value is 01 and the decay rate is 995 okay so that's how we'll update the epsilon value it will never go down below epsilon ending value that is 01 now let's print these scores we will print after every 10 episodes so if episode st 10 is equal to 0 we will print the number of episodes and average a score after every 10 episodes and if uh average score of last 100 episodes is greater than 200 that means we have solved the problem okay okay so let's check that an episode is considered a solution if it is scores at least 200 points okay so let's run that okay so we have an error module nn is missing the required forward function we have the forward pass method okay indentation let's run it let's run this again let's run the agent again okay another issue so is not supported between instances of int and tle so let's check that okay so we have memory capacity replay memory replay memory replay buffer size let's check that okay so we have a problem here okay now let's run it again okay we did not run that i guess okay we have another issue list object is not callable okay there is an issue so where are we calling that okay so this should be sample okay let's run it and another issue soft update three positional arguments but four were given so where are where is the soft update okay so this should be self let's run the agent again okay okay now it's running without,358,9,9,SgMAMuHJHpE
20,any issues okay after 30 episodes the average score is minus 19224 okay after 100 episodes the average score is minus 159 so we can clearly see that average score is improving okay after 200 episodes the average score is down to - 97.5 after 250 episodes the average score isus 78 so that's good it's music increasing so after episode 380 the first time average score is positive music music so we have crossed 100 after 570 episodes the average score is 110 after 730 episodes the average score is 193 so we are very close to 200 now okay so congratulations after 738 episodes we have crossed the benchmark of 200 with average score 20288 now let's add the code and watch our ai agent successfully land the lunar module on the moon so this piece of code will record the agent video and save it we won't go into the details of the rendering code as our focus is on building and training the ai okay so let's r this okay so we have an issue um okay so there's an argument missing epsilon okay we'll pass 0.0 because our model is already trained okay so now it's running okay so we have a video now okay it's looking good it's looking good okay it's close to landing almost there okay so our ai agent has landed the lunar module successfully let's run it again to capture another video okay this is looking much better than previous time okay so lunar module is between two poles now okay perfect landing so we have missed a one thing here we can save the model,358,9,9,SgMAMuHJHpE
21,once the problem is solved that means the average score of last 100 episodes is greater than 200 so we'll save the model using torch.,32,9,9,SgMAMuHJHpE
22,save method and we are saving with the name model. pth and it will be saved in the same location along with the video file so with that we are done done our ai agent has successfully landed on moon thanks to reinforcement learning i hope this breakdown of lunar lander problem and learning process behind it was helpful if you have enjoyed the video please like and subscribe to our channel the full code of today's ai agent will be in the video description so feel free to check it out and give it a try yourself thanks for watching i'll see you in the next one bye-bye take care,144,10,11,SgMAMuHJHpE
0,hey guys welcome to an exciting new tutorial where we take the snake game from our previous video you can find the link in the description to the next level this time we are diving into the fascinating word of artificial intelligence to train our snake using deep q learning we'll explore how to build and train an agent powered by py torch and artificial neural network to master the game intelligently whether you are curious about ai reinforcement learning or just want to see a make learn some cool moves this video is for you so let's get started so in this tutorial we'll break the process into clear and manageable steps to guide you through the training of ai for our snake game so these are the seven steps so in step one modify the game which we have built earlier to make it ready for the ai in step two we'll make the replica of the s snake game but with no ui so that will be used for the efficient and fast training next step three artificial neural network in this step we'll build the brain for the snake step four the replay memory where snake will learn from its past experiences then in step five we'll build an agent the decision maker that powers the snake next in the step six we'll discuss about the hyper parameters the settings that shape our ai performance and in step seven we'll train the agent bringing it all together to train and see our ai in action so by the end you will not only understand the process but also have the your own,358,0,0,M1TD52VxDsQ
1,trained ai snake to show off so let's get started so we'll start with modifying the game which we had built earlier and make it ready for the ai so we have created a new project pytorch ann snake game ai and we'll start with modifying this game.,62,0,0,M1TD52VxDsQ
2,piy class which i have taken from uh one of our previous videos where we have built the snake game let's run this to make sure everything is working fine yeah so this is working fine so let's modify this class first i would like to create a base class for all of these global variables so let's call it as base. pi and i'll move these two variables from here to the base class let's call it a s do screen size and self. block width okay now let's make changes in our g.p file so snake class will inherit from the base class so let's import this and uh call the super constructor same with apple so this will also extend the base class and call the super constructor and at the end for the game class okay now the block width would be self. block width okay what is the second variable screen size so let's search for screen size and change to self.,217,1,4,M1TD52VxDsQ
3,block width okay what is the second variable screen size so let's search for screen size and change to self. screen size okay uh what else so okay so let's run okay so everything is working fine now let's see what other the changes are required so no changes in the draw function increase move left move right up down move no changes here so we'll make a change here so at present if a snake reaches the end of the screen it will cross over and come out from the other side now we want the snake to die if a snake hits the end of the screen so we don't need this code we'll just remove that and let's see oh in apple class no changes no changes no changes here in the game class in the game class let's change the uh caption snake game ai deep q self.,197,4,5,M1TD52VxDsQ
4,screen size okay uh what else so okay so let's run okay so everything is working fine now let's see what other the changes are required so no changes in the draw function increase move left move right up down move no changes here so we'll make a change here so at present if a snake reaches the end of the screen it will cross over and come out from the other side now we want the snake to die if a snake hits the end of the screen so we don't need this code we'll just remove that and let's see oh in apple class no changes no changes no changes here in the game class in the game class let's change the uh caption snake game ai deep q self.,171,5,5,M1TD52VxDsQ
5,timer is set to 150 we can reduce it to one since ai is playing this snake game now not any human and no changes here we'll start with length one snake draw apple and score is zero cod is zero we'll just remove this ret data we don't need this so play set timer the snake move draw no changes here no changes here it all looks good can remove this we can also remove the record here that the agent will take care so we'll remove all of this next for checking the collision uh i believe we we'll create a new function for this so we'll just remove all of this we don't need save data method here just remove this no retrieve data remove this as well we'll not show the game over screen because ai will continuously play and learn will not show this a screen so we'll remove that what else um let's create a new method here for checking the uh collision let's understand this so in snake body the block at zero with index would be head so we'll get the head coordinates x and y and then we'll check it against the rest of the snake body to see if head is hitting the snake body also we will check if head is crossing the screen right then also the snake should die so in these two cases we'll return true the collision is true else we'll return false and we'll call this is collision function inside the play method we'll call self.,340,6,6,M1TD52VxDsQ
6,is cision now let's go forward we are not showing record on the screen so we can remove the this and we can reduce the font size since we have a smaller screen now and uh let's display it in the corner rather than in the middle of the screen since we have a smaller message now just a score what else we have a reset function no changes here now in the run method we can remove all of this since uh we won't be playing this game ai will so we can remove this and we can remove all of these key events okay we'll keep the quit event this is not needed here here we'll quit the game and call quit we will keep the screen update event here we'll have self.,175,7,7,M1TD52VxDsQ
7,and display update what else let's see if we need anything else i don't think so let's remove all of this we can remove all of this as well this run method now will be called by the agent that we'll see later let's correct the ination okay so this looks correct to me we'll have one more variable called game over this will be set to false and when collision happens we'll set the game over to true and in the reset we'll reset it to false what else yes in the game method this will take action so this action will be passed by the agent and based upon the action the snake will move that will implement later for now we can have a placeholder method for get next direction okay i believe with this our uh game class is ready for the ai we can remove all of this unused imports now we are done with the step one the snake game is ready for the ai now we'll move to the step two that is a snake game no ui so why do we need this step creating a snake without a ui is a crucial for efficient training it allows the ai to process game stage and learn at much faster pace without the overhead of rendering visuals okay so let's go back to ide so what we'll do we'll create a replica of game file let's call it as no ui okay now in this file we'll remove everything which is related to ui so let's start we don't need the parent screen we don't need this we don't,358,8,8,M1TD52VxDsQ
8,need block just length x and y coordinates direction we do not need a draw method increase move left right we don't need to call this stw method here we don't need parent screen in apple remove this no image for apple no draw method for apple move method so in the game class um we don't need any of this no user events no timer nothing here no surface we'll just pass the length and nothing here no draw method okay we don't need this we'll remove this as well anything else will not be displaying any score since there is no screen will not be passing any of this game over no event type and just simple play method here let's double check uh okay we forgot to remove this and uh yeah everything looks good we can remove the ut of by game so with that we have completed the step two so now we have game with ui and game without ui now in step three we'll build the brain for the snake using artificial neural network in short ann so let's understand this in detail so this is artificial neural network that powers the decision making for the snake game now let's understand these individual layers let's start with the input layer so input layer represented by these red nodes where each node represents one feature okay and these features encode the state of the snake and its envir so basically state of the game so in a game we'll have walls we'll have food and we'll have the snake so all of these like walls snake and food will be,358,8,8,M1TD52VxDsQ
9,represented by the features and for this snake game we'll have 16 features why 16 features that we will see when we implement the agent class for now just understand that this input layer represents the state of the game and each state will have 16 features what are the features the position of the snake whether there is a danger in each direction what is the current direction of the snake the location of the food relative to the snake so collectively all these features represents the state of the snake and we'll pass the state of the game to the input layer so in our program we'll have 60 nodes these are also called artificial neurons now let's talk about the hidden air so hidden air represented by these blue nodes in the middle it's a simple asnake game so we'll have only one hidden layer so this layer processes the input features using weights and biases it applies nonlinear transformation activation functions learn complex patterns and relationship so for this s sn game we are going to use rectified linear unit as the activation function in short r e l u now the output layer so in our output layer we have total four nodes each of these nodes corresponds to a possible action that a snake can take so a snake can move left a snake can move right a snake can go up and a snake can go down so each node corresponds to the one direction so total four nodes so the network predicts a q value for each action which represents how good it is to take that action,358,8,8,M1TD52VxDsQ
10,in the current state the action with the the highest q value is selected by the agent and every node in the input layer is connected to the every node in the hidden layer and every hidden node is connected to the every output node these connections have weights which are adjusted during training to minimize the loss and improve the network performance now let's quickly read what vicki says about artificial neural network so ann consist of connected units or nodes called artificial neurons so these are artificial neurons which loosely model the neurons in the brain artificial neurons models that mimic biological neurons so these are connected by the edges which model the synapsis in the brain each artificial neuron receives the signal from the connected neuron then process them and sends a signal to the other connected neuron so for example this neuron takes the information from the neurons here process them and pass the information to the output layer the signal is a real number and the output of each neuron is computed by some nonlinear function of the sum of its input called activation function we'll create a new class here let's call it as agent.,261,8,8,M1TD52VxDsQ
11,pi now let's create our artificial neural network class this will inherit from nn do module nn stands for for neural network okay now before we proceed we need to install the py library so we'll do pip install torch okay pyot is successfully installed import torch.nn okay now let's create the constructor and we'll have state size and action size okay so remember the state size input and the action size output let's call the super constructor now let's have fully connected layers so we'll have two fully connected layers so let's c1 and nn do linear state size 64 and second fully connected layer 64 to action size so let's understand this we have two fully connected layer which takes the input as a size of state size and the output 64 that's the number of neurons in the hidden layer 64 i'm going with 64 4 neurons in the hidden layer you can try 128 256 general practice is to choose number of neurons in the hidden layer is 2 power n but again there is no fixed rule so you can try with 32 64 128 256 i think it's a simple s snake game so 64 should be good enough and the second fully connected layer the hidden layer 64 neurons to the action size so as i said earlier we'll have 16 as the state size and the action size is four so we'll have 64 neurons in the hidden layer 16 in the input and four in the output so first fully connected layer pass the data from the 16 neurons to the 64 neurons in the hidden layer,358,9,9,M1TD52VxDsQ
12,and the second fully connected layer pass the data from 64 neurons to output layer which is four neurons okay so this is important to understand so again the state size the size of input layer which corresponds to the state representation of the game action size the size of the output layer representing the number of possible actions that a snake can take okay fc1 this is a fully connected layer with a state size as input and 64 hidden units as output fc2 another fully connected layer with 64 inputs from the here hidden layer and action size the output now let's have forward pass and this takes state as an input so let's understand the forward pass the forward method defines how the input data flows through the network so this is the first transformation the input state is pass to the first fully connected layer fc1 which performs the linear transformation and produces the 64 features and then the activation function so output of fc1 the fully connected layer one it passed through reu function rectified linear unit activation function and then at the end output lay the activation features are passed through fc2 the fully connected layer two to produce the final value for for each action okay now with that we have successfully created the artificial neural network that is the brain for the snake and now we are done with the step three now let's move to step four replay memory leveraging past experiences to enhance learning so let's understand what that means so replay memory replay memory class is designed to manage agents memory of the game experiences,358,9,9,M1TD52VxDsQ
13,it stores the state action reward next state and whether the episode ended done for each step in the game so for each step we store all of these information right so this is one experience a tle which contains a state action reward next state and done okay so at the present state agent takes a step that means agent takes an action based upon this action the agent will be rewarded and then it will land on the the next state and this done is to check whether the episode has ended so again from the present state take an action based upon the action agent will be rewarded it could be positive reward or negative reward then agent lends to the next state and this flag is to check whether the current step has ended the episode so during the game we store all of these experience es an agent will learn from these experiences so for example let's say we have a snake here and there is an apple here and when a snake eats this apple we give a positive reward to the snake let's say plus 10 so this is one experience and let's say this is a wall and a snake hits the wall and dies we give the negative reward let's say minus 10 and the episode is done because the game is over so this will be true so this is another experience so this is one positive and one negative experience we store these experiences in memory and a snake will learn from this experience for example next time there is an apple next to a,358,9,9,M1TD52VxDsQ
14,snake a snake will know that it will get the positive reward by eating this apple and if there is a wall next to snake snake will try to avoid the wall because from the past experience the snake will know that it will get the negative reward if it hits the wall okay so we store all of such experiences in memory and agent will be trained based on these experiences so let's create a replay memory class let's have a class for replay memory let's create a constructor first let's have the device so let's understand this this determines whether to use gpu or cpu for computation okay why because leveraging a gpu significantly accelerates training when it's available so we'll check if it is available if not we'll default to cpu next self.,176,9,9,M1TD52VxDsQ
15,append event where the event is nothing but this tle also we need to check if memory exceeds its capacity we need to delete the old experience so if length of is greater than self do capacity delete self dot so if memory exceeds the capacity we delete the oldest experience now let's have the sampling experiences method let's call it as sample k first we need to randomly select the batch of k experiences okay so experiences experiences and we'll use random do sample we have to select it from the memory and the size is k now we have list of experiences that is list of tles which contains state action reward next states and dce now we have the list of tles and each tle has these five components now we need to separate these components and convert them into p torch tensor so that it will be compatible with neural network so import numpy okay we don't have numpy so we need to install that so pip install numpy okay so we have it so it is installed okay as andp so now let's have the states state torch do from napai so let's understand this so we have list of experiences experiences are the list of tles and the first component in the tle is the state so you of zero gives the state and this gives the list of all states now we vertically stack these states using v stack function so this creates a structured batch of states that can be fed into neural network now we have this stacked nump array and we'll convert this into p torch,358,13,13,M1TD52VxDsQ
16,tensor using torch do from numai now this is necessary because py torch neural networks operates only on tensors then we'll use this float method to convert integers to float because neural network typically expects floating points and at the end two device this moves the tenses to appropriate device if gpu is available then we'll use gpu else will default to cpu so this is slightly complex so let's understand this again with an example because this is very important okay so we have the list of experiences so this is a list so let's say these are the list of experiences so we have states reward next state and done okay so here we can see that states are 1 2 5 6 and 9 10 in different experiences first we'll extract these states e of z so that's the first component in the tupple so that will give us the list of states 1 2 5 6 and 9 10 then we'll use v stack function to stack them vertically so if you notice we stack them vertically in one np ar then we convert them into p torch tensors then cast to float then we'll move to device gpu if it is available else cpu by default okay so let's go back to our id we are going to repeat this process for all other components so we have five components so take actions rewards next states and dan so actions is e1 reward is e2 next state is e3 and ds are e4 if you notice the fifth component that is done is a boolean not integer so we need to,358,13,13,M1TD52VxDsQ
17,"convert it to integer before we call this float method so let's do that so we'll call s type so this method converts data to unsigned 8bit integer so this will convert true to integer value one and false to integer value zero so we have made a small mistake here so this is a list okay and now we'll return states actions rewards now we are done with the sample method so let's summarize so first we randomly select the k experiences from the memory then for each component we'll extract it from the sample experiences stack them into 2d numpy array then convert them into p torch tensors then ensures that the tensor uses the floating point precision then moves the tensor to the correct device gpu or cpu so at the end we'll return states actions rewards next states and d so current states take the actions get the rewards move it to the next stage and done now we are done with the fourth step replay memory okay now before we build the agent let's talk about hyper parameters okay so hyper parameters are tunable parameters set before the training process to optimize the performance of the model so let's see what those parameters are so let's have those parameters so we can have the parameters here okay so the first parameter is number of episodes so that defines how many episodes the agent will train on we'll start with 10,000 episodes so the second parameter is maximum number of steps per episode so we have 10,000 episodes and per episode let's say we'll have 200,000 uh steps okay so each time",358,13,13,M1TD52VxDsQ
18,"the snake moves that is one step okay so per episode a snake can move 200,000 steps sets the limit how long an episode can last next parameter is epsilon start value and we'll start with one so this is the initial value for exploration exploitation trade off so let's quickly brief about exploration exploitation tradeoff exploration exploitation trade-off is a fundamental concept in reinforcement learning that describes the balance between two competing objectives exploration and exploitation the agent tries new action to discover potentially better reward or strategies so that is exploration and exploitation agent uses the knowledge it already has to maximize rewards by choosing the best known action so before we look into the sample let's understand why trade-off is important so exploration is necessary to ensure agent doesn't miss out the better option or strategies that hasn't tried yet exploitation ensures the agent leverages its current knowledge to achieve high rewards efficiently so let's understand this from the snake game perspective imagine an agent is playing the snake game so exploration the agent might move in random directions so this is important random directions even if it means dying to learn how environment reacts and which moves are safe okay so for example there is a wall and the snake is here and the snake randomly chooses the direction and hits the wall right so with this step snake will learn that hitting the wall is a bad move the snake will receive the negative reward exploitation the agent deliberately takes a move it knows increases its score such as moving towards food based on the past experience so in exploration agent",358,13,13,M1TD52VxDsQ
19,randomly chooses the direction and store that knowledge the outcome right then in exploitation agent uses this knowledge to determine the best move that is should not hit the wall or move towards the food so trade why tradeoff is important okay these are the same points let's move to the next slide let's clear the screen now let's discuss about methods to handle the tradeoff okay first epsilon dd policy a common approach where agent chooses a random action with probability epsilon and the best known action with the probability 1 minus epsilon and over time epsilon is gradually decreased to favor the exploitation the second is decaying epsilon the value of epsilon starts with high let's say one for more exploration it decays gradually to lower value 01 favoring exploitation as the agent gains more experience okay so initially epsilon value is one let's say this is the snake game and this is the snake so initially when the epsilon value is one all the moves of the snake would be random okay so basically snake will randomly explore the game and store all the knowledge or outcomes from those random moves and over the period of time the epsilon value will gradually decrease what that means is a snake will exploit the knowledge which he has gained through the random moves during the exploration so again when game begins snake will take all the random moves gain some experience and then use those experience to make better decisions okay let's clear the screen so our epsilon starting value would be one and the epsilon ending value 0.001 okay so this one so here,358,13,13,M1TD52VxDsQ
20,"it is 01 but we'll go with 01 now next is epsilon dekay value so we are going with 0.9995 rate at which epsilon decreases over time so we'll start with 1 we'll end with 01 and we'll decrease with the rate 9995 as you can see it will take long time to decrease the value of epsilon from 1 to 001 okay so we want the snake to explore for a much longer time before the snake starts making the decisions next the learning rate we'll go with 01 the step size used by the optimizer to update weights in the neural network so this is 01 so we want the snake to learn very slowly next is mini bad size we'll go with 100 the number of samples used in each training step okay so we here we have created the uh method sample so the k would be 100 okay so we'll take in each training step we'll take 100 experiences to train the model so next parameter is gamma we'll go with 0.95 the discount factor for future rewards in q learning okay next is uh next parameter is replay buffer size we'll go with 100,000 that's the maximum capacity of replay memory so the over the capacity would be 100,000 next is interpolation parameter we'll go with 01 the parameter for soft updates in the target network so all of this might not be making much sense at this point of time but trust me when we move forward it will all start making sense so these these are our hyper parameters let's initialize few more important variables state size 16",358,13,13,M1TD52VxDsQ
21,and action size four four so state size represents the input size of the state vector as we have discussed here we'll go with 16 and the output size represents the number of possible actions the agent can take okay so we'll go with four because a snake can move in any of the four directions right so where action size is four so we have created a dq here of the max length 100 and this we will use to keep track of last 100 scores by the snake okay now we are done with hyper parameters and variables let's go back now let's talk about agent agent will be the decision maker that will power these sink okay let's go back to our ide and create a class for agent let's have a constructor it will take state size and extion size so self.,189,13,13,M1TD52VxDsQ
22,state size state size dot action size so we have the state size that is input size and the output size is the action size and now let's have the network so we'll create two networks one for local and one for target we'll discuss that in detail as we move forward that why would we need two networks so let's have the local network what's the name we have given here ann so ann and we'll pass the state size and action size we'll create another one for the target we forgot to create the device so let's copy it from here here so now we have the device so we'll move the neural network to the specified computational device okay so we say two and self.,166,14,14,M1TD52VxDsQ
23,device same thing for target network now let's have the optimizer so so this is the optimizer the purpose is it optimizes the parameter that is weights and biases of the local network using the adam optimizer now adam optimizer is a widely used for training neural network due to its adaptive learning rates and efficiency so we have already defined our learning rate that is 01 step size used by the optimizer to update weights now let's have the memory and that would be replay memory and the capacity is refl for size next is uh t step so we'll initialize to z so i will explain this t step when we will use it now let's have a variable for record we set to minus one at the beginning let's have a placeholder variable for epsilon let start with minus one so at the run time this epsilon value will start with 1 and ends with 01 okay now let's create the function to get the state of the game call it a get state and this will take game now let's understand how do we get the state of the game before that let's understand the state of the snake so imagine this is the head of the snake okay so just imagine these are the eyes of the snake so snake can look up down right left and then right up right down left down and left up so total eight coordinates so when it name looks into these directions snake has to determine whether it is safe to move to that direction so imagine head of the snake is here then,358,15,15,M1TD52VxDsQ
24,it's not safe to go up because the snake will hit the wall it's not safe to go right the snake can go left and snake can go down i know snake can move only in four directions but still we will provide but we will provide the diagonals information as well to the network because that will help the network in training so imagine a snake is here and the coordinate is 0a 0 and let's say snake moves 20 pixels in one step so we'll provide all the eight coordinates to the network as part of the state so we'll provide 0a 20 and uh 0a - 20 20a 0 - 20a 0 and then left up is - 20a 20 this is 20 comma 20 now we have calculated all the eight coordinates around the snake next we'll determine whether these coordinates are safe for the snake to move and provide that information as part of the state so let's understand this with the help of an example so imagine this is the game screen we have a snake here okay this is the head of the snake the snake current direction is up that means a snake is going up apple is here and these are the walls okay so a snake representation for the snake game as a binary vector each element in the vector corresponds to a specific feature of the environment or snake's current situation okay and these are 16 elements okay now let's break down these elements so first eight elements these eight elements represents these eight directions so right down left up right up right down left,358,15,15,M1TD52VxDsQ
25,down and left up and whether it is safe to move into that direction so that represents the first eight elements in the state so next four elements represents the direction features this indicates the snake current direction of movement so left right up and down so left right up and down and here the snake is going up since these are binaries so we have set the third element to one so these last four elements represents the food location feature these indicate where the food is located relative to the snake so left right up and down this is the head of the snake food is here so food is on the right side of the snake so we have set this bit to one and it's also in the down direction relative to the snake head so we have also set this bit to one so these 16 features represents the state of the game so again first eight features represents the safe coordinates for the snake to move next four features represents the current direction of the snake and the last four features represents the food location with respect to a snake now let's go back to the ide so first we'll get the head coordinates so game will have the snake let's check that so this is the game class it has the snake and in the snake we have x and y for the snake body okay so so coordinate at zero index would be head coordinates let's have the y coordinate as well okay so now we have head coordinates now let's calculate the coordinates for all of these,358,15,15,M1TD52VxDsQ
26,directions okay so let's do that so we'll call it as point left and uh this would be head x minus game dot game do block width comma head y so this would be the coordinates for the point in left now let's calculate for right so we'll add it okay so these are the eight coordinates around the snake head now let's have the state of the game so state so game dot is danger and we'll pass the point left okay so as of now if danger function does not exist in the game class we'll create that in a moment but for now let's complete the get state function so we'll repeat this for eight times and we'll pass each of these coordinates and this is danger method will return the boolean value true or false based upon whether this coordinate is safe for the snake to move or not so these eight features are the first state features in the state next four features for the direction of the snake so move direction and this is left right up and down so let's check that so game dot snake dot direction so snake has the direction as well left okay so these four are done last four for the food location so game dot apple do x if it is less than head of x that means food is in the left of a snake okay so now we have the food location as well with respect to the head of the snake and now let's return the state so return np do array state in so this array method from numai,358,15,15,M1TD52VxDsQ
27,converts this boolean array into integer array okay true values will be converted to one and false value will be converted to zero so now we are done with the get state method and uh this method will give the state of the game and we'll call this method every time a snake moves to get the current state next we will understand why do we need two networks so in agent class we had created one local network and one target network now let's understand why do we need them so local network versus target network local q network and target q network is a design choice in deep q learning to improve the stability and convergence of training so in local q network we update the network actively during the training and this network is used to predict the q values for the current state okay so local q network predicts the q value for the current state whereas in target q network the network updates less frequently than local q network to provide stable targets and this is used to compute the target q values for the next state during the training let's understand that within example but before that we'll talk about bman adaptation bellman adaptation for deep q learning so we compute the target q value using this balman equation so this is the q target s and a s stands for current state and a stands for action r stands for the reward this is gamma the discount factor this is the q network s prime is the new state and then a is the action in the new state,358,15,15,M1TD52VxDsQ
28,so on the current state agent takes an action a on that action agent receives the reward r and then agent lands on the new state s prime then again on the new state agent can take all the possible actions and we will pick the action that gives the maximum reward so this is in future in the next state multiplied by the discount factor which determines how much importance is to give the future reward the gamma value lies between 0 and 1 we generally take the gamma value as 0.9 so again on the current state s agent takes an action a for that agent receives the reward r and then agent lands on the new state s prime on that new state agent can take any of the possible actions we will pick the action that gives the maximum reward multiplied by the discount factor added to the reward that becomes the targeted q value for the current state so this is on the target network so let's take an example so imagine in the snake game this is the snake this is the head of the snake and this is the apple okay so the current state the snake is one step away from the food okay action is move up so snake will go up reward plus 10 for eating the food next state after moving up snake is in the new position so head will be at this position right now q values for s prime so s prime is this state snake can go left right up and down and these are the predicted values for each action,358,15,15,M1TD52VxDsQ
29,for going down there's a negative reward because the snake will hit its own body if it goes down so negative reward so we'll compute the target q for current state which is this and for action going up immediate reward is our 10 let's say the discount factor gamma is 0.9 so we'll pick the maximum q value for the s prime so we'll pick the maximum among these so that is 3.5 so we'll pick 3.5 discounted future reward this is multiplied by the discount factor so this will become 3.15 so targeted q value would be 10 3.15 is 13.15 so this is how the targeted q value is calculated now let's understand where the local network comes into the picture on target q network we'll compute the target q value and on the local network we'll compute the expected q value so for the same state s and for the same action we compute the targeted q and the expected q both on the different network and then we'll compute the loss and the objective is to minimize this loss so we back propagate this loss to the network and network will adjust its weight and biases accordingly so you remember this diagram so this is our network right so in the output we'll have the expected q values in the local network and target q values in target network so we'll compute the loss and we'll back propagate that loss to the network and network will adjust the weights and bies accordingly to minimize this loss so let's go back to the slide so in reinforcement learning the target q values,358,15,15,M1TD52VxDsQ
30,is drived from q network itself this is the bman adaptation which we just discussed so imagine if we use only one network then we'll have to use the same network to compute the current q values and the next state q values the targets then network updates itself using its own rapidly changing predictions which can lead to oscillations and divergence in q value estimation so oscillations using a single q network for both action selection and target estimation can lead to overestimation of q values so basically you are using the same network to predict the values and based upon the prediction you will update and predict and update so network will update rapidly and that can lead to overestimation of q values so solution is the target q network serves as a fixed reference for the target q values and it updates less frequently via soft update to ensure target values change slowly and stabilize training so how do these networks interact during training local q network predicts q values for the current state to determine the best action or calculate loss the target q network predicts q values for the next state to compute stable target q values for loss calculation the soft update ensures the target q network gradually incorporate updates from the local q network so now we have understood that why do we need two networks and the bman equation now let's go back to ide now let's create few more functions so first we'll create a step function so this step function controls the process of storing the experiences and trigger the learning so what do we need,358,15,15,M1TD52VxDsQ
31,in an experience is state action reward next state and done so this function will take these components as parameter first we'll store this experience in memory so self.,37,15,15,M1TD52VxDsQ
32,push and this function takes an event and it event is tole so here so now we have pushed the event into memory now we will trigger the learning but we will limit how often the agent updates the neural network so we do not want that neural network updates after every step rather we want it to get updated after few moves so for example the agent will only learn once every four step instead after every step so this balances the computation and performance so you remember we had a variable called tep here so we'll make use of that so we'll say self dot tstep so here we increment the counter tep to keep track of number of steps the agent has taken and the counter resets after every four steps now if self do tep is zero then we'll take sample experiences so so here we are randomly picking up the k experiences k is equal to mini bath size and the mini bath size is 100 the number of sample used in each training step so we get the experiences and then self dot learn and we'll pass these experiences so we have yet to implement this learn function next we'll create a function get action so get state function determines which action agent should take in a given state also it balances exploration and exploitation so remember we had discussed the exploration and exploitation trade-off this function will handle that so first this function will use the local q network to estimate the action value for a given state so this function needs the state for which it needs to,358,17,17,M1TD52VxDsQ
33,get an action so the get state function returns the numpy array so first we need to convert this numpy array into py tor tensor so let's do that so state is equal to torch dot from numai state then convert to float uns squeeze and then two self dot device and here zero so first we'll convert the numpy array into p torch tensor then we'll convert it to float needed for the neural network and uns squeeze zero so this adds a batch dimension because q network expects input as a batch of states move the tensors to the correct device gpu or cpu so now we have the state so now self.,149,17,17,M1TD52VxDsQ
34,nograd is to ensure that no gradients are calculated so this saves memory and computation while we compute the action values on the local network so now we have q values for all the possible actions so it would be something like list of q values for each action so state and action underscore 0 action one and so on so in our case the action size is four so we'll get four q values now before we do anything else let's put back the q network to the training mode so self dot local q network tray next we will select the action with the highest q value so we'll have the move torch dot max action values do item and with that we are done with the exploitation part that means agent using is the knowledge it already has to maximize rewards by choosing the best known action so we are choosing the best known action now the exploration part where agent might move in random directions to learn how environment reacts and which moves are safe so we'll add some random moves here so random dot int 0 to3 so y 0 to 3 because our action size is four so this will give values either 0 1 2 or 3 so now we have the random move and we also have a move computed from local network and we'll choose one of this move based on the epsilon griding policy which trads off between exploration and exploitation so what we will do if random dot random is greater than epsilon so this function will also take epsilon as a parameter so if,358,19,19,M1TD52VxDsQ
35,greater than epsilon then choose the move from local network else pick the random move and here return move now let's have the learn method so this learn function is the heart of training the agent where q network gets updated based on the agent experience so this function will take the experiences and these experiences will be used to train the network so let's get all the individual components from this experiences so states let me copy it from from here experiences now we will compute the target q values using bman equation so remember bman equation we'll use this equation to calculate the target q values so first let's calculate the predicted q values on the next state call it as next q targets self.,165,19,19,M1TD52VxDsQ
36,target network and we'll pass the next states do detach so here we have pass the next state through the target network to get the predicted q values for all the possible actions in the next state this detach ensures no gradients are computed for these values and this max function finds the maximum q value for each state across all actions so this represents the best future actions q value this part and this uns squeez method reshapes the sensor to ensure it's compatible with further computations next we will compute the target q values so that is reward plus discount factor times predicted q values on the next statee so q targets rewards plus gamma into so why this one minus dun so this is to ensure that no future reward is added for the terminal states so basically when the episode is over next we'll calculate the expected q values on the local network q expected so here we pass the current state through the local network to predict the q values for all the actions and this gather function extracts the q value corresponding to action taken in each step from predicted q values next we will calculate the loss so loss so we are using mean squar error loss function and we'll pass q expected and q targets now self. optimizer so we have the optimizer initialized here the self.,305,20,21,M1TD52VxDsQ
37,optimizer dot z gra so this clears the previous gradients to prepare for new update now we will back propagate the loss to the network so loss dot backward so this computes the gradients of the loss with respect to the network parameter and then we'll do self dot optimizer do step so this updates the network parameter based on the computed gradients here and now we'll call the soft update function so next we will implement the soft update method so remember the soft update method ensures the target network gradually incorporates update from the local q network now let's understand how soft update works so we have a local model the neural network actively learning then we have a target model a more stable network and we use something called interpolation parameter a factor between 0 and one that controls the update speed so we gradually update the target model and that update speed is controlled by this interpolation parameter so if the parameter is one the target model immediately copies the local model weights that is hard update and if you use a very small value let's say 01 then the target model updates very slowly blending in only 1 of the local model parameters at a time and we are going to use 01 as the interpolation parameter value because we want our target model to updates very slowly and this will be the equation we'll be using so parameters of local network multiplied by interpolation parameter plus 1 minus interpolation parameter into parameters of target network here we can see if we take the value 1 then 1 minus 1 so,358,22,22,M1TD52VxDsQ
38,this entire part will become zero and the target would be equal to local so that is the hard update and if we take interpolation parameter as 0 01 then this is 01 local 99 time target so 99 of target parameters 1 of local parameters so we are blending in with only 1 of local model parameters now let's implement the soft update method so function soft update and this will take local network target network for local rs so in this line we have combined the parameters of both the networks into pairs so that they can be updated together so for example local parameter is w1 w2 w3 and target parameters are v1 vs2 v3 then after combining the pairs would be w1 v1 w2 v2 and so on then target params dot data copy then we will implement this function so interpolation parameter into local params plus so this is the right part of the function and then we'll copy it to target params now let's have few more helper functions okay so we have created four helper methods now let's talk about these methods so first we'll talk about save model so this save model method saves the current models weight into a specified file so this is the file name first we'll check if the folder exist if it doesn't we create the folder then we'll get local network.,305,22,22,M1TD52VxDsQ
39,"state and then save it in the file using tor. save method so this ensures you can save the agent's progress and reuse the model later and what is this folder the folder name is model under the root directory now let's talk about save data method okay so this save data method saves the important metadata like best score record and current exploration rate epsilon into ajon file so this ensures the data persist between runs so agent doesn't lose its progress or fine-tune parameters so for example let's say we have uh trained the agent for 2,000 episodes and now we want to stop the training and we'll resume the training after some time so that is when this save data method will be useful because we'll be saving the metadata like record and epsilon value to a file so that later when we resume we'll resume from the same point where we have stopped the training now let's talk about retrieve data so this method retrieves the metadata from data. json file so save data saves the data in data. json file record and epsilon value this retrieve data will retrieve data from this file and load it to self. record and self.",267,23,27,M1TD52VxDsQ
40,"record and self. epsilon so this function will be called when agent resumes the training now the load method okay so this is data okay so save model method saves the current model into this file and load method loads the model from this file into the network so this function checks if pre-train model file exist in the specified directory so that is this folder if it exist this function loads the model weights into local network using torch. load method and further this method also calls the retrieve data to load the additional metadata like record and epsilon values so these four methods ensure that the agent can continue from where it left off next in step seven we'll train the agent so let's create the uh driver method so if first we'll get the game object game and here we'll import the game from two files with ui and without ui next we will create the agent object so agent let's look at the constructor okay so this takes the state size and the action size which we have defined here 16 and 4 so state size and action size okay so we have a game object and agent object next we'll start the training so we have number of episodes here 10,000 so for episode in range 0 to number of episodes so at the start of the episode we'll re reset the game so game.",312,27,29,M1TD52VxDsQ
41,"load method and further this method also calls the retrieve data to load the additional metadata like record and epsilon values so these four methods ensure that the agent can continue from where it left off next in step seven we'll train the agent so let's create the uh driver method so if first we'll get the game object game and here we'll import the game from two files with ui and without ui next we will create the agent object so agent let's look at the constructor okay so this takes the state size and the action size which we have defined here 16 and 4 so state size and action size okay so we have a game object and agent object next we'll start the training so we have number of episodes here 10,000 so for episode in range 0 to number of episodes so at the start of the episode we'll re reset the game so game. reset we'll set the current score to zero let's have a variable here for the maximum score now the steps so for t in range maximum number of steps per episode now at each step agent retrieves the current state of the game so let's call it as state old agent. get state and we'll pass the game so what is it complaining about the type game doesn't have expected attribute is danger okay so we have yet to create is danger method so we'll do it after this so we have the current state now we need to get the action so action agent.",344,29,31,M1TD52VxDsQ
42,get state and we'll pass the game so what is it complaining about the type game doesn't have expected attribute is danger okay so we have yet to create is danger method so we'll do it after this so we have the current state now we need to get the action so action agent. get action and we'll pass the state okay and this expects the epsilon value as well now let's look at the retrieve data method so here when agent resumes the training we upload the epsilon value from the file else it is uh initialized to minus one so let's do one thing here here we'll say epsilon is equal to epsilon starting value and uh so in case the training is stopped in between and resumes later then we need to make use of the epsilon value from the data. jent file so we'll check if agent. epsilon value if it is not minus one that means training was stopped in between so abs okay so this is epsilon epsilon agent.,227,31,34,M1TD52VxDsQ
43,epsilon value if it is not minus one that means training was stopped in between so abs okay so this is epsilon epsilon agent. epsilon and we will pass this epsilon value to get action function now we have the action now agent needs to perform this action so perform the action so let's say move and we'll do move action equal to 1 so remember action size is four so action value would be between 0 and 3 and we will set that particular index to one so for example if the action is two the move would be 0 0 01 2 it would be this now we will pass this move to game so game dot run and we'll pass this move so we have yet to complete this run method but this run method will return few informations like reward done and score so we'll pass the move to this run method let's call it as move so snake will make this move and based upon the move we will determine the reward we'll check if the game is over and what is the current score so we'll return this three information reward done and score so we'll get reward done and score so after this move we'll get the new state so state new agent.,286,34,35,M1TD52VxDsQ
44,epsilon and we will pass this epsilon value to get action function now we have the action now agent needs to perform this action so perform the action so let's say move and we'll do move action equal to 1 so remember action size is four so action value would be between 0 and 3 and we will set that particular index to one so for example if the action is two the move would be 0 0 01 2 it would be this now we will pass this move to game so game dot run and we'll pass this move so we have yet to complete this run method but this run method will return few informations like reward done and score so we'll pass the move to this run method let's call it as move so snake will make this move and based upon the move we will determine the reward we'll check if the game is over and what is the current score so we'll return this three information reward done and score so we'll get reward done and score so after this move we'll get the new state so state new agent.,254,35,35,M1TD52VxDsQ
45,get state and game and now let's train the agent so agent do step and this function takes state action reward next state and done state hold action reward new state and done next we will check if with this current move the game is over that means is if done is true if it is true we'll break we'll come out from this loop so once the episode is over we will check if we have the max score in this episode max max score and score then scores on 100 episode aent score and now let's update the epsilon value so epsilon max of epsilon ending value epsilon decay into epsilon and what is the epsilon ending value 001 so epsilon value will never go below that okay and after each episode we'll save the model so we'll call save model and save data so agent dot save data and we will pass the max score and epsilon value so we are saving the model okay we have not used the load method so once we create the agent object let's call agent. load so this will load the model model if it exist now let's do a print after every 50 episodes okay so we have a print statement the current episode score on the current episode maximum score so far and the average score in last 100 episodes and let's do one more thing here so here we can restore the max score uh from the file so max score max agent.,332,36,37,M1TD52VxDsQ
46,record maxis score so now we are done with the driver method it's exciting that we are almost ready to see the snake game in action so let's implement the remaining methods so first one is u is danger method in the game class so let's go to the game class and uh here we have a method is collision this method checks if head of the snake has a collision with rest of the body or with the screen is danger method would be quite similar to this method so let's copy this method danger and we'll pass a point so here uh in this method we pass the coordinates and we'll have to check if these coordinates are safe for the snake so basically if head of the snake moves to these coordinates will there be any collision or not so let's do that so here we get the point so let's change this r underscore to point and this would be point of zero one so we'll check these coordinates against the snake body and the screen size and if there is a collision we'll return true else we'll return false next let's complete the run method so this run method gets the move now we'll have to give this move to the snake so first let's find out the direction in this move so let's implement this method so this will take move let's have a variable called new direction and let's default it to the right direction now if so this method translates the given move into corresponding direction for the snake movement okay so this move parameter is one hot,358,38,38,M1TD52VxDsQ
47,encoded array of length four where if first element is set to one the snake will move in the right direction if the second element is set to one snake will go down if the third element is set to one the snake will go left and if the fourth element is set to one the snake will go up so this is right down left and up right down left and up now let's call this method now let's move the snake so if di equal to left self dot snake dot move left else if now we are done with all the methods so we have is danger method and we have completed the run method okay so what is it complaining about okay so we need to return the new direction now let's copy these methods to our new ui file as well so here and uh this part now let's see uh what else is remaining so this run method will be called by the agent okay agent expects reward done and score from this method so let's do that so we don't have rewards yet no we don't have rewards yet so let's have a variable for reward now we need to reward the snake for the different experiences so here um if a snake eats the apple so let's say when a snake eats the apple we'll give plus 10 points so self do reward 10 and in case of collision negative reward so self.,326,38,38,M1TD52VxDsQ
48,reward let's say minus 00 so much bigger negative reward in case of collision now let's add very small negative reward with every move so the negative reward ensures that the snake learns to prioritize finding the shortest path to the fool okay so we have a score we have reward and we have game over which is done so let's return that uh after every step so return the order is this now let's copy these changes to no ui file as well we return that now let's check if anything else is missing so here game over game over okay this would be self.,137,39,39,M1TD52VxDsQ
49,game over let's change it here as well we are pretty much done with all the seven steps now let's make few final changes before we run the agent and see ai in action okay so first uh uh we'll change the screen size so i believe 800 is too big let's change it to 600 let's have one more variable here uh for food location index so we'll say max food index and self dot screen size minus self dot now let's make changes in the uh game files so we'll go to apple class yes i don't like this hardcoded value here so let's do max food index and max food index same changes in this file so we'll go here self dot max food index and now let's run the agent class okay so this is weird um okay we have error sample larger than population here sample class okay we have missed a condition here so here mini bad size is 100 and we'll have to make sure that memory has at least 100 experiences before we start taking samples so if self. memory is greater than mini bad size now let's run it again okay ui is not looking good um let's check that so in the game class here um let's close this screen update this looks good timer is one and where is the update okay so here we have missed py game. time. clock dot tick 200 and break here we can see that model folder is created and we have data.,338,40,43,M1TD52VxDsQ
50,clock dot tick 200 and break here we can see that model folder is created and we have data. json epsilon record and the model let's do delete that okay let's run it again okay so another error nd array no attribute as type this line okay there's a typo here this should be s type np another typo this should be u n okay let's delete the model okay another uh issue q expected local network action actions dtype int 64 for index so actions so we are getting actions from from here okay action can't be float value so let's change it to log and uh anything else uh did we miss let's run it again okay another issue um looks like another type okay this is data.,169,43,44,M1TD52VxDsQ
51,"copy let's delete the model now let's run it again um so this seems to be working now and we are printing episode current score max score and average score after every 50 episodes and here uh epsilon value .91 so that is why snake is making all the random moves so basically snake is exploring the environment let's stop that because ai will take a long time to uh train the agent let's make use of uh no ui classes uh which we have created for this purpose game and okay so now we are using game no u and since we have saved the model and data uh we will not have to start from scratch um now let's run the agent so looks like we have another issue uh no attribute is danger so looks like this function is missing in no ui so let's copy it from here and uh yeah that function is not here okay okay now let's run it again and hopefully it will run without any issues okay so we can already see that this is much faster we have already completed close to 2,000 episodes we can see that average score is increasing so we have close to 3,800 episodes average score is 2 now the max score is 12 so that is increasing okay so at 5,000 episodes the max score is 30 average score is six okay so after 5,700 episodes max score is 49 average score is 11 6,200 episodes max score 49 average 12.78 with max score around 60 65 average score around 30 plus we can call it as a win okay",358,45,45,M1TD52VxDsQ
52,"average score is 14 plus now so at episode 7,300 max score 59 average is 23.4 6 let's see if we can achieve the target by 10,000 episodes as we can notice that episodes are taking longer time now because snake is trained and it's playing well so average score went up to 24 and max score is 61 okay we have a new record max score 73 average score 26.5 s so we have reached our target after 99,100 episodes max is score is 73 average is score 30 plus so let's stop this and uh let's do one thing let's see the ui in action so we go to agent and let's run it okay so this is a trained snake it's playing well music okay let's try one more thing um we'll reduce the epsilon value and we'll see if we can get any better results so where are the hyper parameters okay so we are using 999 for 5 let's change it to 99 now our training would be much faster so let's okay this is no ui let me delete this model delete i shouldn't have closed the previous tab we could have compared the results okay so after 350 episodes average score is four so we can see this is much faster tr training max score 39 after 450 episodes average score is 13 max score 44 okay so after 700 episodes max is score 63 average is score 31 so this is much better okay average score 34 so as i said earlier you can change these hyper parameters and play around to see if you can get better",358,45,45,M1TD52VxDsQ
53,results you can also change the network you can add more hidden layers you can add more nodes to the hidden layer so after 900 episodes max score 79 average score 33 so let's stop it now and uh we got maximum score of 79 and best average score 34 now let's see the ui again but this time we'll stop the trading so here in the driver method let's comment this uh let's comment this out so we have stopped the training and let's go to the top and enable only ui now let's run this okay so this is looking good the snake is trained well and with that we have completed the project so thanks guys watching this tutorial on building a snake game ai with deep you learning i hope you have enjoyed this video the full code is available on github and i have shared the link in the description feel free to experiment and share your results do not forget to like subscribe and leave a comment if you have any questions see you guys in the next video bye-bye music,243,45,45,M1TD52VxDsQ
0,all right everybody so what is going on phil here with another video i want to let you know that my new course has finally gone live that is what i've been working on for the last month and why i have not made any youtube content so you get five and a half hours of content that teaches you how to read interpret analyze and implement deep reinforcement learning papers we're gonna implement deep q-learning double deep cue learning and dueling deep cue learning all from scratch using the source papers nothing else but source papers and pi torch if you don't anything about reinforcement learning it's not a huge problem because i have an entire module about an hour of content set aside specifically for that where you will code a q learning agent from scratch just from first principles you really only need high school mathematics and a basic understanding of deep learning and a willingness to learn so i only have two favors to ask first of all if you are going to purchase if you decide you want to purchase please use the link provided otherwise udemy takes half the credit and two if you do purchase a course they will prod you for a rating right after you buy the course regardless of what rating you give it then once you've taken some portion of the material and have it an accurate assessment of the quality of the course please leave an honest review for other students to know what they are buying so i'm going to include an entire module here on youtube as a promotional video as thank,358,0,0,AR0Mjl4jwVk
1,you to all of my youtube subscribers and viewers and you can also click the link down below to head to the course landing page to check out other free modules from the course i'll also check also include a link down in here to the github where you can see the code for the course so you can see what type of stuff you're going to be coding you can get an idea of how it is structured and yeah so i'm quite pleased with the results here this is five and a half hours of content a fairly enormous undertaking for me i know you're gonna love it go ahead and check it out and i'll see you in the course in the previous lesson we learned that all agents face a fundamental dilemma to the expand their knowledge of the environment at the expense of a short-term loss and reward or take the best known action there are a number of strategies for dealing with this dilemma and we're going to rely on epsilon greedy this is a strategy that balances exploration and x before settling on a mostly exploitative strategy recall that the value function is the estimate of the expected future rewards given that the agent is in some state and follows its policy this process of exploration and exploitation helps the agent to refine its model of the environment and gradually approach the true estimate of the value function each time the agent samples the state and receives a reward it can update its estimate using a formula of the following form new estimate equals old estimate plus some step,358,0,0,AR0Mjl4jwVk
2,size multiplied by the quantity target minus old estimate now this target is aptly named as it is in the direction in which the agent is moving its estimates it can be noisy and have a high degree of uncertainty but it is generally assumed to be the right direction to be moving in the simplest case the target is precisely the reward the agent is given at some time step about to modify this later but the basic idea is that it is a term proportional to the future rewards the agent expects to receive the next parameter we want to look at is the step size it controls how quickly the estimate changes for each step in practice you can have many forms and this can have a big impact on whether or not the agent converges on some accurate estimate functional dependencies including 1 over n such that it approaches zero as the estimates get more accurate or possible we're going to use some constant but small value the frequency which is estimate is updated varies based on the class of algorithms for instance in monte carlo algorithms another type of model free reinforcement learning the estimates are updated at the end of each episode using the returns that follow each time step t remember that the returns are the sum of the discounted rewards these returns are the target in the prior equation rather than just the reward q learning on the other hand is in the class of what we call temporal difference learning algorithms here we are going to update the agents estimate of the value function at each time,358,0,0,AR0Mjl4jwVk
3,step rather than at the end of the episode this means that q learning is a sort of online algorithm that can learn in something approximating real time rather than sitting around and waiting to learn it's also well suited for non episodic tasks meaning tasks that never really end in an episode due to this online nature and temporal difference learning our update for the value function is of the form v of s at time t equals v of s at time t plus alpha times a quantity r sub t plus 1 plus gamma times v of s sub t plus 1 minus v of s sub t since equation has terms that are proportional to the estimate at time t and the time t plus 1 we are effectively using one estimate to update another estimate this makes it what is called a bootstrapping algorithm one may ask is that really a good idea or if we start out with bad estimates garbage in garbage out right well not quite as long as we have a fixed policy pi and the step size parameter is sufficiently small these estimates actually converge to the true values so long as we sample each state and up times the definition of enough is a bit fuzzy right now but it should be of order hundreds or more the more times the better but a dozen times isn't really going to cut it for most cases ok so then i've convinced you that the value function converges but how can we use this in q-learning way back in the video on value and action value functions,358,0,0,AR0Mjl4jwVk
4,i also talked about action value functions which tell us the value of state and action pairs it's this quantity we want to use rather than the value function don't worry the update equation is similar to what i just showed you but there are some differences the action value function is denoted by q and we update it according to the following equation q of sn a at time t is equal to q of s and a at time t plus alpha times a quantity r sub t plus 1 plus gamma times the max over actions of q of s at time t plus 1 for all actions minus q of sn a at time t there are a few things to note here first we've just swapped out q for v and included the fact that it's a function of both state and action second we are taking a max over actions for the q v for the next state as part of our update equation well talk more on that in a minute but the frozen lake environment there are only a small number of states and actions this means we can represent this action value function q as a table each row is a state and the columns are the actions with the entries being the value for that combination this is an example of what's called a tabular learning method because it uses a table to keep track of the states and actions the agent has visited so for each state the agent finds itself in we're using an epsilon greedy policy to choose actions look up at state,358,0,0,AR0Mjl4jwVk
5,in the table and check out the estimates for all the possible actions if you're taking a greedy action then take the action whose entry is highest for that state if you're taking a random action well pick one at random use a reward to plug into the above equation to update the q table and then just repeat the process after playing enough episodes and not resetting the table between episodes the agent will sample each state and action pair enough times to get an accurate estimate another thing to note here before we get to the full q learning algorithm we want to implement we're using an epsilon greedy strategy to pick actions but we are updating the estimate for q based on the purely greedy action when you're using one policy to update an estimate for a completely different policy this is what is called off policy learning there are also on policy learning methods where we use the same policy to update the action value function as well as to choose actions the unpoliced temporal difference learning method is called sarsa which is an acronym for state action reward state prime and action prime we won't be implementing it in this course that will see it mentioned in one of the papers later on if you'd like to learn more about it you can check out my youtube channel for some examples ok so now we have all the vocabulary out of the way and we're ready to take a look at the full algorithm to implement start by initializing q for all states and actions in the state and action spaces,358,0,0,AR0Mjl4jwVk
6,this is arbitrary except that as always the terminal state is by definition valued at 0 initializes a learning rate alpha discount factor gamma and random parameter epsilon repeat for each episode initialize the starting state repeat for each step of the episode choose an action a based on the current state s using an epsilon greedy strategy in queue take your action get the reward a new state plug the reward into the update equation to update the estimate for q set the old state to the new state that's really all there is to regular q learning let's implement this and finally conquer the frozen lake environment but before i cut you loose i'll give you some pointers on how to structure your solution the agent should be represented as a class which has a cue table learning rate discount factor and epsilon he'll need class functions to initialize the cue table choose actions based on the current state as well as learn given an input of a state action reward and new state it's easiest to represent the cue table as a dictionary with states and actions as keys you also want to decrease epsilon over time to some minimum value i also recommend putting the aging class in a separate file from the main loop as this is considered coding best practice the output for this should be a plot that shows the running average of the past rewards for the past 100 games this is known as a learning curve as an overall upward trend in the average reward over time is a clear demonstration of learning by the agent you,358,0,0,AR0Mjl4jwVk
7,"can also print the running average an episode number to the terminal every thousand games or so just so that you know it's running let the agent play a large number of games something like 500,000 pause the video here work on your solution and come back to hit play when you're ready to see my solution alright now that you've had a chance to code it on your own let's take a look at my solution our imports are gonna be relatively light with just numpy as instructed if you stick the majority of the functionality in the agent class our initializer is going to take the learning rate gamma number of actions number of states epsilon star epsilon end and epsilon decrement factor and of course we want to save all those parameters as member variables of our class now our cue table gets initialized as an empty dictionary and then we'll call a specific function to initialize the cue table as stated in the instructions the initialization for each state and action pair is arbitrary except for the terminal state which must identically be zero for all actions now since it's arbitrary we may as well set everything equal to zero for all state and action pairs now that we've finished initializing queue we are free to handle the functionality for choosing the action of course choosing the action will be a function of the current state of the environment and we're going to use epsilon greedy just as a refresher epsilon greedy means we're gonna calculate a random number if that random number is less than epsilon then we're gonna choose an",358,0,0,AR0Mjl4jwVk
8,action at random if that number is greater than or equal to epsilon then we will find the maximal action all we've done here is use a list comprehension to create a list of integers in the range of number of actions and use numpy random choice to choose one at random now what we've done here is use a list comprehension to create a list of elements corresponding to the action values for the given state by looking up the relevant quantities in our queue table then we're going to use the number arc max function to find the index of the maximal action from that list now there is a caveat here the numpy arg max function has the built in behavior that when two elements have a tie it will return the index of the lowest element so that means of action 0 and action 1 have the same q value and will always return an action 0 probably a better way to do this would be to write a custom function that breaks ties randomly such that half the time i chooses the lower index and half the time i choose as the higher index we'll see that it doesn't really matter but it's just a way to improve upon my solution and of course in either event you want to return the action you chose next we can turn our attention to the decrement epsilon function now i chose to decrement epsilon linearly you can use a 1 over square root dependence you can use a logarithmic or exponential dependence it's not really critical i'm just gonna use linear so if,358,0,0,AR0Mjl4jwVk
9,your solution is different doesn't matter so long as a decreases epsilon down to the minimum value over time all right i'm going to come to our learn function this will take the state action reward and new status input we're going to want to calculate the maximal action for the new state of the environment and then use that in our update equation for q and of course we want to decrement epsilon at the end as well now of course all the same caveats as above and the choose action function apply to the arc max here for the a max so this is just the update equation for q we're incrementing it by learning rate multiplied by reward plus gamma times the q value for the next state and maximal action and subtracting off the value for the current state and action next we have to handle the decrement epsilon now of course you could argue that you can handle the decrement epsilon in the main function i've chosen to stick it here just to adhere to the principles of object-oriented programming but which i mean that the main function really doesn't need to know how the agent handles decreasing its own epsilon either way as long as it works it's good but i've just done it this way to be a little bit more consistent with oo p alright let's head to the main function and write up the test loop for this now our imports are going to be a little bit more heavy-duty here we're gonna need jim pipe lot numpy and our agent so let's start with that and,358,0,0,AR0Mjl4jwVk
10,"of course we want to start with initializing our environment as well as our agent now i've chosen a set of defaults these aren't written in stone these are simply what i chose if you use different values that's okay so long as we end up getting a similar result so i'm gonna pick a learning rate of zero zero one the gamma 0.9 and epsilon ended 0.01 and a rather slow epsilon decrement factor and of course we need an empty list to keep track of our scores and win percentages and as instructed we'll want to play 500,000 games and at the top of each game we want to reset the done flag environment and set the score 2-0 in simply play each episode of course we start by selecting an action taking that action learning from the new observation action reward and old observation tuple increment our score and set the old state to the new state and of course at the end of each episode we're going to append the score to our list and every 100 games we want to calculate the win percentage for the previous 100 games and every thousand games we want to print our debug information and at the end of all the games we want to plot our win percent list and show it ok now that's done let's head to the terminal and see how it works all right here we are ok so i forgot the self dot in the q-learning file let's go ahead and change that so that was line 37 which is right here self dot in actions save and let's",358,0,0,AR0Mjl4jwVk
11,make sure i didn't do it anywhere else here here is good alright let's head back to the terminal let's try again and it is running so let this run for a minute and then i'll show you the results when it finishes you all right that is finished running you can see an overall upward trend in the agents win percentage over the course of the games and it tops out around 70 average with some win rates over 80 if you were able to achieve a comparable score congratulations let's head back to the lecture just to recap temporal difference learning is a type of online learning it bootstraps by using one estimate to update another hue learning is a tabular off policy learning method that uses an epsilon greedy policy to update the estimate for the purely greedy policy by implementing a curing agent in our frozen lake environment we were able to achieve a consistent 70 win rate which is a massive improvement over our paltry 20 rate from earlier in the course and the next modules we're going to talk about what to do in the case of continuous state spaces,254,0,0,AR0Mjl4jwVk
0,in this video series we're going to implement the deep q learning algorithm with pytorch and then train flappy bird with it i'll show you my whole process from beginning to end here are the tentative topics i may add some topics or change up the order in this video let's get our de environment set up and get flappy bird installed oh don't forget to subscribe and get notified when the next video becomes available let's get our environment set up i'm at d iium website for those of you who don't know gymnasium is a library of reinforcement learning environments that you can train on the left side i'm going over to the third party environments there are two flappy bird environments to choose from let me hop into the first one and i'm going to scroll down to the observation space so this environment gives us back an image of the game which requires us to have a convolutional known network in order to train this that's much more complicated than what we're going to get into in this video so let me go back and hop over to the second option let me scroll down to the state space there are two options for the state space one is liar sensory information which i'm not sure what they do with but the second option basically tells us the position of the last pipe the position of the next pipe and where our player is so using this information we can easily feed it into a regular neural network to train the environment so this is the environment that we're going to go,358,0,0,arR7KzlYs4w
1,with let me hop down to my start menu i'm going to start typing anaconda so i am going to use uh miniconda as my package manager you can use whatever you want let me create a new environment kinda create dasn and i will call this uh dqn environment okay let me activate my environment all right my dqn environment has been activated i'm going to install python 3.1 so you can match the version that i have okay while this is installing let me go back to my flappy bird page and go down to the installation instructions and this is what i need i'm going to copy it okay python is installed which means i should have pip in my environment so let me install flappy bird without the dollar sign now let me see uh how to run the game let me copy this okay it's installed let me paste this in keeps copying the dollar sign okay let me see if it runs all right it's complaining about not having tensor flow so there's some dependency on tensor flow i'll just install that okay tension flow is done let me try this again all right it went this actually says we can play the game so let me give this a shot i have actually never played flappy bird in my life so let's see how i do oh my god all right i can't even get past the first pipe let's hope our reinforcement learning agent is better let me try this one more time oh jesus okay let's get to our uh deq network i'm going to hop over,358,0,0,arR7KzlYs4w
2,to visual studio code and and i have a empty folder open here let me create a new file i'll call it agent.,29,0,0,arR7KzlYs4w
3,py h back to the flppy bird page and copy the uh usage i'll just paste it in here and let me try to run this make sure it runs and then we can go over the code and i'm just going to select the environment that we just created and it is called dqn environment i'm just going to hit f5 and it should run this file cool okay okay let's see what the code is doing we're importing the flappy bird environment any game that is compatible with gymnasium will follow this uh general pattern the first thing we do is to create an instance of the flappy bird environment we're passing in render mode human to render the game on the screen and they have a cust parameter to turn on liar or not and since uh i want to just use the position of the pipes and where the bird is i'm going to turn off liar we'll call the reset function to initialize the environment and inside this infinite loop we're calling the sample function on this action space to get a random action let's go back to the website look at what the action space is so the action space uh we're going to get either a number number of zero or one from the sample function zero is for the bird to do nothing one is for the bird to flap his wings and try to fly up so that's what sample is going to give back zero or one with the action we'll pass it into the step function to execute that action the step function will give,358,1,1,arR7KzlYs4w
4,us back the observation what the next state is how much reward we got from the last action if the bird hit the ground or hit one of the pipes will get terminated equal to true otherwise it's false the next parameter is not used info is just additional information you can use for debugging or something and then if terminated we'll exit the loop and close the environment let me put a break point here and run it again and then we can check what are actually being returned in these variables okay i'm going to hit f5 okay the bird is here i'm going to hit f10 to go to the next step let's see what action was chosen let me go over to the debug console action is one so the bird should flap its rings and then i will execute the next line let's see what we get back in the observations let's go back to the web page to see what those numbers mean okay so there are 12 parameters in option two and each one of the numbers here should correspond to one of these in the same order and it looks like the number numbers are normalized maybe between -1 and 1 let's see if we can see it d 12 here tells us that we have 12 numbers in this array the floating point numbers between -1 and 1 so it is common to normalize the values to either 0 1 or1 1 when training a neal network so this is expected now what reward did we get we got a reward of 0.1 let's go back here,358,1,1,arR7KzlYs4w
5,we got a reward of 0.1 means that we are still alive we are we were able to take that action and did not die so we got 0.1 when we pass through a pipe we'll get a one when we die we'll get negative 1 and if we touch the top of the screen so this is used to discourage the bird from keep going up so that's how the reward system is set up okay so i assume that we're not terminated though so the is going to keep going all right we don't need to go any further let me just stop it and then we can move on oh before i move on when coding up a uh reinforcement learning algorithm ourselves we should not try to train a complicated environment to begin with because we don't know if there are bugs in our code or is it just that this environment takes a really long time to train so during coding we want to test on a much simpler environment and let me hop back to the gy page go back to gynasium page go down to classic controls so card pole is one of the basic environments that uh you might see a lot in examples so let me copy the environment id i'll make a copy of this line i'll just comment out flappy birth for now and i will change this to card poole c poole doesn't have use liar let me take that out okay we moove the break point hit f5 let me put a break point here so that we can see it again okay,358,1,1,arR7KzlYs4w
6,here it is uh in this environment this black piece is supposed to be a cart that can move either left or right and there's a pole that the cart is trying to balance so the idea is for the cart to learn how to balance the pole okay now that our dep environment is ready join me in the next video where we build the deep q network if that video is available it should have pop up by now otherwise maybe check out one of my other reinforcement learning videos,119,1,1,arR7KzlYs4w
0,today i'd like to overview the exciting field of deep reinforcement learning introduced overview and provide you some of the basics i think it's one of the most exciting fields in artificial intelligence it's marrying the power and the ability of deep neural networks to represent and comprehend the world with the ability to act on that understanding on that representation taking as a whole that's really what the creation of intelligent beings is understand the world and act and the exciting breakthroughs that recently have happened captivate our imagination about what's possible and that's why this is my favorite area of deep learning and artificial intelligence in general and i hope you feel the same so what is deep reinforcement learning we've talked about deep learning which is taking samples of data being able to in a supervised way compress encode the representation that data in the way that you can reason about it i would take that power and apply it to the world where sequential decisions are to be made so it's looking at problems and formulations of tasks where an agent an intelligent system has to make a sequence of decisions and the decisions that are made have an effect on the world around the agent how how do all of us any intelligent being that it's tasked with operating in the world how did he learn anything especially when you know very little in the beginning it's trial and error is the fundamental process by which reinforcement learning agents learn and the deep part of deep reinforcement learning is neural networks as using the frameworks and reinforcement learning where the neural,358,0,0,zR11FLZ-O9M
1,network is doing the representation of the world based on which the actions are made and we have to take a step back when we look at the types of learning sometimes the terminology itself can confuse us to the fundamentals there are supervised learning there semi-supervised learning there's unsupervised learning there's reinforcement learning and there's this feeling that supervised learning is really the only one where you have to perform the manual annotation where you have to do the large-scale supervision that's not the case every type of machine learning is supervised learning it's supervised by a loss function or a function that tells you what's good and what's bad you know even looking at our own existence is how we humans figure out what's good and bad there's all kinds of sources direct and indirect by which our morals and ethics we figure out what's good and bad the difference we supervised and unsupervised and reinforcement learning is the source of that supervision what's implied when you say unsupervised is that the cost of human labor required to attain the supervision is low but it's never turtles all the way down it's turtles and then there's a human at the bottom there at some point there needs to be human intervention human input to provide what's good and what's bad and this will arise in reinforcement learning as well i have to remember that because the challenges and the exciting opportunities of reinforcement learning lie in the fact of how do we get that supervision in the most efficient way possible but supervision nevertheless is required for any system that has an input and,358,0,0,zR11FLZ-O9M
2,an output that's trying to learn like a neural network does to provide an output that's good he needs somebody to say what's good and what's bad for you curious about that there's been a few books a couple written throughout the last few centuries from socrates to nietzsche i recommend the latter especially so let's look at supervised learning and reinforcement learning let like to propose a way to think about the difference that is illustrative and useful when we start talking about the techniques so supervised learning is taking a bunch of examples of data and learning from those examples where a ground truth provides you the compressed semantic meaning of what's in that data and from those examples one by one whether it's sequences or single samples we learn what how to then few take future such samples and interpret them reinforcement learning is teaching what we teach an agent through experience not by showing a singular sample of a data set but by putting them out into the world the distinction there the essential element of reinforcement learning then for us now we'll talk about a bunch of algorithms but the essential design step is to provide the world in which to experience the agent learns from the world the from the world it gets the dynamics of that world the physics of the world from that world that gets the rewards what's good and bad and us as designers of that agent do not just have to do the algorithm we have to do design the the world in which that agent is trying to solve a task the design of,358,0,0,zR11FLZ-O9M
3,the world is the process of reinforcement learning the design of examples the annotation of examples is the world of supervised learning and the essential perhaps the most difficult element of reinforcement learning is the reward the good versus bad here a baby starts walking across the room we want to define success as a baby walking across the room and reaching the destination that's success and failure is the inability to reach that destination simple and reinforcement learning in humans the way we learn from these very few examples appear to learn from very few examples of trial and error is a mystery a beautiful mystery full of open questions it could be from the huge amount of data 230 million years worth of bipedal data there who've been walking what mammals walking ability to walk or 500 million years the ability to see having eyes so that's the the hardware side somehow genetically encoded in us is the ability to comprehend this world extremely efficiently it could be through not the hardware not the five hundred million years but the the few minutes hours days months maybe even years in the very beginning were born the ability to learn really quickly through observation to aggregate that information filter all the junk that you don't need and be able to learn really quickly through imitation learning through observation the way for walking that might mean observing others talk the idea there is if there was no other around we would never be able to learn this the fundamentals of this walking or as efficiently it's through observation and then it could be the algorithm totally,358,0,0,zR11FLZ-O9M
4,not understood is the algorithm that our brain uses to learn the backpropagation that's an artificial neural networks the same kind of processes not understood in the brain that could be the key so i want you to think about that as we talk about the very trivial by comparison accomplishments and reinforcement learning and how do we take the next steps but it nevertheless is exciting to have machines that learn how to act in the world the process of learning for those who have fallen in love with artificial intelligence the process of learning is thought of as intelligence it's the ability to know very little and through experience examples interaction with the world in whatever medium whether it's data or simulation so on be able to form much richer and interesting representations of that world be able to act in that world that's that's the dream so let's look at this stack of what an age what it means to be an agent in this world from top the input to the bottom the output is the there's an environment we have to sense that environment we have just a few tools as humans have several sensory systems on cars you can have lidar camera stereo vision audio microphone networking gps imu sensor so on whatever robot you can think about there's a way to sense that world and you have this raw sensory data and then once you have the raw sensory data you're tasked with representing that data in such a way that you can make sense of it as opposed to all the the raw sensors and the i the,358,0,0,zR11FLZ-O9M
5,cones and so on that taking just giant stream of high bandwidth information we have to be able to form higher abstractions of features based on which we can reason from edges to corners to faces and so on that's exactly what deep learning neural networks have stepped in to be able to in an automated fashion with as little human input as possible be able to form higher-order representations of that information then there is the the learning aspect building on top of the greater abstractions form through the representations be able to accomplish something useful well--there's discriminative tasks a generative task and so on based on the representation be able to make sense of the data be able to generate new data and so on from sequence the sequence to sequence the sample from sam of the sequence and so on and so forth to actions as we'll talk about and then there is the ability to aggregate all the information has been received in the past to the useful information that's pertinent to the task at hand it's the thing the old it looks like a duck quacks like a duck swims like a duck three different data sets i'm sure there's state-of-the-art algorithms for the three image class education audio recognition video classification - activity recognition so on aggregating those three together is still an open problem and that could be the last piece again i want you to think about as we think about reinforcement learning agents how do we play how do we transfer from the game of atari to the game of go to the game of dota to,358,0,0,zR11FLZ-O9M
6,the game of a robot navigating an uncertain environment in the real world and once you have that once you sense the raw world once you have a representation of that world then we need to act which is provide actions within the constraints of the world in such a way that we believe can get us towards success the promise excitement of deep learning is is the part of the stack that converts raw data into meaningful representations the promise the dream of deeper enforcement learning is going beyond and building an agent that uses that representation and acts achieve success in the world that's super exciting the framework and the formulation reinforcement learning at its simplest is that there's an environment and there's an agent that acts in that environment the agent senses the environment by a by some observation well there's partial or complete observation of the environment and it gives the environment and action it acts in that environment and through the action the environment changes in some way and then a new observation occurs and then also as you provide they actually make the observations you receive a reward in most formulations of this of this framework this entire system has no memory that the the only thing you two could be concerned about as a state you came from the state you arrived in and the reward received the open question here is what can't be modeled in this kind of way can we model all of it from from human life to the game of go can all this be model in this way and what are is this,358,0,0,zR11FLZ-O9M
7,a good way to formulate the learning problem of robotic systems in the real world in simulated world those are the open questions the environment could be fully observable or partially observable like in poker it could be single agent or multi agent atari versus driving like deep traffic deterministic or stochastic static versus dynamic static is in chess dynamic again and driving in most real-world applications the screen versus continuous like games chess or continuous and carpal balancing a polo on a cart the challenge for rl in real world applications is that as a reminder supervised learning is teaching by example learning by example teaching from our perspective reinforcement learning is teaching by experience and the way we provide experience the reinforcement learning agents currently for the most part is through simulation or through highly constrained real-world scenarios so the challenge is in the fact that most of the successes is with systems environments that are simulated so there's two ways to then close this gap to directions of research and work one is to improve the algorithms improve the ability of the algorithm student to form policies that are transferable across all kinds of domains including the real world including especially in the real world so train and simulation transfer to the real world or is we improve the simulation in such a way that the fidelity of the simulation increased increases to the point where the gap between reality and simulation is is minimal to a degree that things learn the simulation are directly trivially transferable to the to the real world okay the major components of an rl agent an agent,358,0,0,zR11FLZ-O9M
8,operates based on a strategy called the policy it sees the world it makes a decision that's a policy makes a decision how to act sees the reward sees a new state acts sees a reward she's new states and acts and this repeats forever until a terminal state the value function is the estimate of how good a state is or how good a state action pair is meaning taking an action in a particular state how good is that ability to evaluate that and then the model different from the environment from the perspective the agent so the environment has a model based on which it operates and then the agent has a representation best understanding of that model so the purpose for an rl agent in this simply formulated framework is to maximize reward the way that the reward mathematically and practically is talked about is with a discounted framework so we discount further and further future award so the reward that's farther into the future is means less to us in terms of maximization than reward that's in the near term and so why do we discount it so first a lot of it is a math trick to be able to prove certain aspects analyze certain aspects of convergence and in general on a more philosophical sense because environments either are or can be thought of a stochastic random it's very difficult to there's a degree of uncertainty which makes it difficult to really estimate the the the reward they'll be in the future because of the ripple effect of the uncertainty let's look at an example a simple one helps,358,0,0,zR11FLZ-O9M
9,us understand policy's rewards actions there's a robot in the room there's 12 cells in which you can step it starts in the bottom left it tries to get rewards on the on the top right there's a plus one it's a really good thing at the top right wants to get there by walking around there's a negative 1 which is really bad you wants to avoid that square and the choice of action is this up-down left-right for actions so you could think of there being a negative reward of point 0 4 for each step so there's a cost to each step and there's a stochastic nature to this world potentially we'll talk about both deterministic stochastic so in the in the stochastic case when you choose the action up with an 80 probability with an 80 chance you move up but with 10 chance to move left another 10 move right so that's the catholic nature even though you try to go up you might end up in a blocks to the left into the right so for a deterministic world the optimal policy here given that we always start in the bottom left is really shortest path is you know you can't ever because there's no stochasticity you're never gonna screw up and just fall into the hole negative 1 hole that you just compute the shortest path and walk along that shortest path why shortest path because every single step hurts there's a negative a reward to it point 0 4 so shortest path is the thing that minimizes the reward shortest path to the to the plus 1 block,358,0,0,zR11FLZ-O9M
10,ok let's look at it stochastic world like i mentioned the 80 up and then split to 20 10 to left and right how does the policy change well first of all we need to have we need to have a plan for every single block in the area because you might end up there due to this the castus 'ti of the world ok the the basic addition there is that we're trying to go avoid up the closer you get to the negative one hole so just try to avoid up because up the stochastic nature of up means that you might fall into the hole with a 10 chance and given the point zero for step reward you're willing to take the long way home in some cases in order to avoid that possibility the negative one possibility now let's look at a reward for each step if it decreases to negative two it really hurts to take every step then again we go to the shortest path despite the fact that there's a stochastic nature in fact you don't really care that you step into the negative one hole because every step really hurts you just want to get home and then you can play with this reward structure right yes instead of negative 2 or negative point 0 4 you can look at negative 0.1 and you can see immediately that the structure of the policy it changes so with a higher value the higher negative reward free step immediately the urgency of the agent increases versus the less urgency the lower the negative reward and when the reward flips so,358,0,0,zR11FLZ-O9M
11,it's positive the every step is a positive so the entire system which is actually quite common in reinforcement learning the entire system is full of positive rewards and so that then the optimal policy becomes the longest path is grad school taking as long as possible never reaching the destination so what lessons do we draw from robot in the room two things the environment model the dynamics is just there in the trivial example the stochastic nature the difference between 80 percent 100 percent and 50 percent the model of the world the environment has a big impact on what the optimal policy is and the reward structure most importantly the thing we can often control more in our constructs of the task we try to solve them enforcement is the what is good and what is bad and how bad is it and how good is it the reward structure is a big impact and that has a complete change like like robert frost say the complete change on the policy the choices the agent makes so at when you formulate a reinforcement learning framework as researchers as students what you often do is you design the environment you design the world in which the system learns even when your ultimate goal is the physical robot it does still there's a lot of work still done simulation so you design the world the parameters of that world and you also design the reward structure and it can have a transformative results slight variations in those parameters going to huge results on huge differences on the policy that's arrived and of course the example,358,0,0,zR11FLZ-O9M
12,i've shown before i really love is the impact of the the changing reward structure might have unintended consequences and those consequences for real-world system can have obviously highly detrimental costs that are more than just a failed game of atari so here is a human performing the task gate playing the game of coast runners racing around the track and so it's when you finish first and you finish fast you get a lot of points and so it's natural to then okay let's do an rl agent and then optimize this for those points and will you find out in the game is that you also get points by picking up the little green turbo things and with agent figures out is that you can actually get a lot more points even by simply focusing on the green turbos focusing on the green turbos just rotating over and over slamming into the wall fire and everything just picking it up especially because ability to pick up those turbos can avoid the terminal state at the end of finishing the race in fact finishing the race means you stop collecting positive reward so you never want to finish collected turbos and though that's a trivial example it's not actually easy to find such examples but they're out there of unintended consequences that can have highly negative detrimental effects when put in the real world we'll talk about a little bit of robotics when you put robots for wheeled ones like autonomous vehicles into the real world and you have objective functions that have to navigate difficult intersections full of pedestrians you have to form intent,358,0,0,zR11FLZ-O9M
13,models those pedestrians here you see cars asserting themselves through dense intersections taking risks and within those risks that are taking by us humans will drive vehicles we have to then encode that ability to take subtle risk into into ai based control algorithms perception then you have to think about at the end of the day there's an objective function and if that objective function does not anticipate the green turbos that are to be collected and then result in some understand the consequences could have very negative effects especially in situations that involve human life that's the field of ai safety and some of the folks will talk about deep mind and open ai that are doing incredible work in rl also have groups that are working on a ai safety for a very good reason this is a problem that i believe that artificial intelligent will define some of the most impactful positive things in the 21st century but i also believe we are nowhere close to solving some of the fundamental problems of ai safety that we also need to address as we those algorithms okay examples and reinforcement learning systems all of it has to do with formulation or rewards formulation of states and actions you have the traditional the often used benchmark of a cart balancing a poll continuous so the action is the horizontal force to the cart the goal is to balance the poll so stays top and the moving cart and the reward is one in each time step if the poll is upright in the state measured by the cart by the agent is the pole,358,0,0,zR11FLZ-O9M
14,angle angular speed and of course self sensing of the cart position and the horizontal velocity another example here didn't want to include the video because it's really disturbing but i do want to include the slide because it's really important to think about is by sensing the the raw pixels learning and teaching an agent to play a game of doom so the goal there is to eliminate all opponents the state is the raw game pixels the action is up down shoot reload and so on and the positive reward is when an opponent is eliminated and negative one the agent is eliminated simple i added it here because again on the topic of ai safety we have to think about objective functions and how that translate into the world of not just autonomous vehicles but things that even more directly have harm like autonomous weapon systems and we have a lecture on this in the agi series and on the robotics platform the manipulate object manipulation and grasping objects there's a few benchmarks there's a few interesting applications learning the problem of grabbing objects moving objects manipulating objects rotating and so on especially when those objects don't have have complicated shapes and so the goal is to pick up an object in the purely in the grasping objects allenge the state is the visual racial slurs visual visual base the raw pixels of the objects the actions is to move the arm grasp the object pick it up and obviously it's positive when the pickup is successful the reason i'm personally excited by this is because it'll finally allow us to solve the,358,0,0,zR11FLZ-O9M
15,problem of the claw which has been torturing me for many years i don't know that's not at all why i'm excited by it okay and then we have to think about as we get greater and greater degree of application in the real world with robotics like cars the the main focus of my passion in terms of robotics is how do we encode some of the things that us humans encode how do we you know we have to think about our own objective function our own reward structure our own model of the environment about which we perceive and reasonable in order to then encode machines that are doing the same and i believe autonomous driving is in that category but to ask questions of ethics we have to ask questions of of risk value of human life value of efficiency money and so on all these in front of ethical questions that an autonomous vehicle unfortunately has to solve before it becomes fully autonomous so here are the key takeaways of the real-world impact of reinforcement learning agents on the deep learning side okay these neural networks that form high representation the fun part is the algorithms all the different architectures the different encoder decoder structures all the attentions self attention recurrent sallust engr use all the fun architectures and the data so that and the ability to leverage different data sets in order to discriminate better than perform this crematory tasks better than you know mit does better than stand for that kind of thing that's the fun part the hard part is asking good questions and collecting huge amounts of,358,0,0,zR11FLZ-O9M
16,data that's representative over the task that's for real world impact not cvpr publication real-world impact a huge amount of data on a deeper enforcement learning side the key challenge the fun part again is the algorithms how do we learn from data some of the stuff i'll talk about today the hard part is defining the environment defining the acts of space and the reward structure as i mentioned this is the big challenge and the hardest part is how to crack the gap between simulation in the real world the leaping lizard that's the hardest part we don't even know how to solve that transfer learning problem yet for the real world in fact the three types of reinforcement learning there's countless algorithms and there's a lot of ways to economize them but at the highest level there's model-based and there's model free model based algorithms learn the model of the world so as you interact with the world you construct your estimate of how you believe the dynamics of that world operates the nice thing about doing that is once you have a model or an estimate of a model you're able to anticipate you're able to plan into the future you're able to use the model to in a branching way predict how your actions will change the world so you can plan far into the future this is the mechanism by which you can you can do chess in the simplest form because in chess you don't even need to learn the model the models learnt is given to you chess go and so on the most important way in which,358,0,0,zR11FLZ-O9M
17,they're different i think is the sample efficiency is how many examples of data are needed to be able to successfully operate in the world and so model based methods because they're constructing a model if they can are extremely simple efficient because once you have a model you can do all kinds of reasoning that doesn't require experiencing every possibility of that model you can unroll the model to see how the world changes based on your actions value based methods are ones that look to estimate the quality of states the quality of taking a certain action in the certain state so they're called off policy versus the last category that's on policy what does it mean to be off policy it means that they constantly value based agents constantly update how good is taken action in a state and they have this model of that goodness of taking action in a state and they use that to pick them optimal action they don't directly learn a policy a strategy of how to act they learn how good it is to be in a state and use that goodness information to then pick the best one and then every once in a while flip a coin in order to explore and then policy based methods our ones that directly learn a policy function so they take as input the the world representation of that world neural networks and this output a action where the action is stochastic so okay that's the range of model-based value based and policy based here's an image from open ai that i really like i encourage you to as,358,0,0,zR11FLZ-O9M
18,we further explore here to look up spinning up in deeper enforcement learning from open ai here's an image that texana mises in the way that i described some of the recent developments in rl so at the very top the distinction between model free rl and model-based rl in model free rl which is what we'll focus on today there is a distinction between policy optimization so on policy methods and q-learning which is all policy methods pause optimizations methods that directly optimize the policy they'll directly learn the policy in some way and then q-learning off policy methods learn like i mentioned the value of taking a certain action in the state and from that learned that learned q value be able to choose how to act in the world so let's look at a few sample representative approaches in this space let's start with the with the one that really was one of the first great breakthroughs from google deepmind on the deep irl side and solving atari games dqn deep queue learning networks deep queue networks and let's take a step back and think about what cue learning is q-learning looks at the state action value function queue that estimates based on a particular policy or based on an optimal policy how good is it to take an action in this state the estimated reward if i take an action in this state and continue operating under an optimal optimal policy it gives you directly a way to say amongst all the actions i have which action should that take to maximize the reward now in the beginning you know nothing you,358,0,0,zR11FLZ-O9M
19,know you don't have this value estimation you don't have this cue function so you have to learn it and you learn it with a bellman equation of updating it you take your current estimate and update it with the reward you seed received after you take an action here it's off policy and model free you don't have to have any estimate or knowledge of the world you don't have to have any policy whatsoever all you're doing is roaming about the world collecting data when you took a certain action here award you received and you're updating gradually this table where the table has state states on the y-axis and actions on the x-axis and the key part there is because you always have an estimate of what of to take an action of the value of taking that action so you can always take the optimal one but because you know very little in the beginning that optimal is going to you have no way of knowing that's good or not so there's some degree of expiration the fundamental aspect of value based methods or ami are all methods like i said it's trial and error is exploration so for value based methods that q-learning the way that's done is with the flip of a coin epsilon greedy with a flip of a coin you can choose to just take a random action and you slowly decrease epsilon to zero as your agent learns more and more and more so in the beginning you explore a lot with epsilon 1 and epsilon of zero in the end when you're just acting greedy based,358,0,0,zR11FLZ-O9M
20,on the your understanding of the world as represented by the q-value function for non neural network approaches this is simply a table the q this q function is a table like i said on the y state x actions and in each cell you have a reward that's at this counter reward that you estimated to be received there and as you walk around with this bellami equation you can update that table but it's a table nevertheless number of states times number of actions now if you look at any practical real-world problem and an arcade game with raw sensory input is a very crude first step towards the real world so raw sensor information this kind of value iteration and updating a table is impractical because here's for a game of break out if we look at four consecutive frames of a game of breakout size of the of the raw sensory input is 84 by 84 pixels grayscale every pixel has 256 values that's 256 to the power of whatever 84 times 84 times 4 is whatever it is it's significantly larger the number of atoms in the universe so the size of this cue table if we use the traditional approach is intractable you'll know it's to the rescue deep rl is rl neural networks where the neural networks is tasked with taking this in valley based methods taking this cue table and learning a compress representation of it learning an approximator for the function from state action to the value that's what previously talked about the ability the powerful ability of neural networks to form representations from extremely high dimensional,358,0,0,zR11FLZ-O9M
21,complex raw sensory information so it's simple the framework remains for the most part the same in reinforcement learning it's just that this cue function for value based methods becomes a neural network and becomes an approximator where the hope is as you navigate the world and you pick up new knowledge through the back propagating the gradient and the loss function that you're able to form a good representation of the optimal q function so using your networks with you'll know it's a good at which is function approximator x' and that's dq 1 deep q network was used to have the initial incredible nice results on our k games where the input is the raw sensory pixels with a few convolutional layers for the connected layers and the output is a set of actions you know probability of taking that action and then you sample that and you choose the best action and so this simple agent whether the neural network that estimates that q function very simple network is able to achieve superhuman performance on many of these arcade games that excited the world because it's taking raw sensory information with a pretty simple network that doesn't in the beginning understand any of the physics of the world any of the dynamics of the environment and through that intractable space the intractable state space is able to learn how to actually do pretty well the loss function for dq n has to q functions one is the expected the predicted q value of a taking an action in a particular state and the other is the target against which the loss function is,358,0,0,zR11FLZ-O9M
22,calculated which is what is the value that you got once you actually take in that action and once you've taken that action the way you calculate the value is by looking at the next step and choosing the max to singh if you take the best action in the next state what is going to be the q function so there's two estimators going on with in terms of neural networks those two forward passes here there's two q's in this equation so in traditional dq n that's just that's done by a single neural network with a few tricks and double dq n that's done by two neural networks and i mentioned tricks because with this and with most of rl tricks tell a lot of the story a lot of what makes systems work is the details in in games and robotic systems in these cases the two biggest tricks for dq n that will reappear and a lot of value based methods is experience replay so think of an agent that plays through these games as also collecting memories you collect this bank of memories that can then be replayed the power of that one of the central elements of what makes value based methods attractive is that because you're not directly estimating the policy but are learning the quality of taking an action in a particular state the you're able to then jump around through your memory and and play different aspects of that memory so learn train the network through the historical data and then the other trick simple is like i said that there is so the loss function,358,0,0,zR11FLZ-O9M
23,has two queues so you're it's it's a dragon chasing its own tail it's easy for the loss function to become unstable so the training does not converge so the trick of fixing a target network is taking one of the queues and only updating in every x steps every thousand steps and so on and taking the same kind of network it's just fixing it so for the target network that defines the loss function just keeping it fixed and only updating any regulator so you're chasing a fixed target with a loss function as opposed to a dynamic one so you can solve a lot of the atari games with minimal effort come up with some creative solutions here break out here after 10 minutes of training on the left after a to have 2 hours of training on the right is coming up with some creative solutions again it's pretty cool because this is raw pixels right we're now like there's been a few years since this breakthrough so kind of take it for granted but i still for the most part captivated by just how beautiful it is that from the raw sensory information neural networks are able to learn to act in a way that actually supersedes humans in terms of creativity in terms of in terms of actual raw performance it's really exciting and games of simple form is the cleanest way to demonstrate that and you the the same kind of dq and network is able to achieve superhuman performance and a bunch of different games there's improvements to this like dual dq one again the q function can,358,0,0,zR11FLZ-O9M
24,be decomposed which is useful in to the value estimate of being in that state and what's called and in future slides that we called advantage so the advantage of taking action in that state the nice thing of the advantage as a measure is that it's a measure of the action quality relative to the average action that could be taken there so if it's very useful advantage versus sort of raw reward is that if all the actions you have to take are pretty good you want to know well how much better it is in terms of optimism that's a better measure for choosing actions in a value-based sense so when you have these two estimates you have these two streams for neural networking the dueling dq n dg qm where one estimates the value the other the advantage and that's again that dueling nature is useful for also on the there are many states in which the action is decoupled the quality of the actions is decouple from the state so many states it doesn't matter which action you take so you don't need to learn all the different complexities all the topology of different actions when you in a particular state and another one is prioritize experience for play like i said experience replay is really key to these algorithms and the thing that sinks some of the policy optimization methods and experiments replay is collecting different memories but if you just sample randomly in those memories you're now affected the sampled experiences are really affected by the frequency of those experience occurred not their importance so prioritize experience replay assigns,358,0,0,zR11FLZ-O9M
25,a priority a value based on the magnitude of the temporal difference learned error so the the stuff you have learned the most from is given a higher priority and therefore you get to see through the experience replay process that that particular experience more often okay moving on to policy gradients this is on policy versus q-learning off policy policy gradient is directly optimizing the policy where the input is the raw pixels and the policy network represents the forms of representations of that environment space and as output produces a stochastic estimate a probability of the different actions here in the pong the pixels a single output that produces the probability of moving the paddle up so how do pause gradients vanilla policy grading the very basic works is you unroll the environment you play through the environment here pong moving the paddle up and down and so on collecting no rewards and only collecting reward at the very end based on whether you win or lose every single action you're taking along the way gets either punished or rewarded based on whether it led to victory or defeat this also is remarkable that this works at all because the credit assignment there's a is i mean every single thing you did along the way is averaged out it's like muddied it's the reason that policy gradient methods are more inefficient but it's still very surprising that it works at all so the pros versus dq one the value based methods is that if the world is so messy that you can't learn a q function the nice thing about policy gradient because it's,358,0,0,zR11FLZ-O9M
26,learning the policy directly that it will at least learn a pretty good policy usually in many cases faster convergence it's able to deal with stochastic policies so value based methods can out learners the gassing policies and it's much more naturally able to deal with continuous actions the cons is it's inefficient versus dqn it's it can become highly unstable as we'll talk about some solutions to this during the training process and the credit assignment so if we look at the chain of actions that lead to a positive reward some might be awesome action some may be good action some might be terrible actions but that doesn't matter as long as the death the nation was good and that's then every single action along the way gets a positive reinforcement that's the downside and there's now improvements to that advantage actor critic methods a to see combining the best of value based methods and policy base methods so having an actor two networks an actor which is policy based and that's the one that's takes the actions samples the actions from the policy network and the critic that measures how good those actions are and the critic is value based all right so as opposed to in the policy update the first equation there the reward coming from the destination the that our war being from whether you won the game or not every single step along the way you now learn a q value function q s a state and action using the critic network so you're able to now learn about the environment about evaluating your own actions at every step,358,0,0,zR11FLZ-O9M
27,so you're much more sample efficient there's a synchronous from deep mind and synchronous from open ai variants of this but of the actor advantage actor critic framework but both are highly parallelizable the difference with a three c the asynchronous one is that every single agency just throw these agents operating in the environment and they're learning they're rolling out the games and getting the reward they're updating the original network asynchronously the global network parameters asynchronously and as a result they're also operating constantly an outdated versions of that network the open ai approach that fixes this is that there's a coordinator that there's these rounds where everybody all the agents in parallel are rolling out the episode but then the coordinator waits for everybody to finish in order to make the update to the global network and then distributes all the same parameter to all the agents and so that means that every iteration starts with the same global parameters and that has really nice properties in terms of conversions and stability of the training process okay from google deepmind the deep deterministic policy gradient is combining the ideas of dqn but dealing with continuous action spaces so taking a policy network but instead of the actor actor critic framework but instead of picking a stochastic policy having the actor operator on the since the casting nature is picking the best picking a deterministic policy so it's always choosing the best action but ok with that the problem quite naturally is that when the policy is now deterministic it's able to do continuous action space but because it's termina stick it's never exploring,358,0,0,zR11FLZ-O9M
28,so the way we inject exploration into the system is by adding noise either adding noise into the action space on the output or adding noise into the parameters of the network that have then that create perturbations and the actions such that the final result is that you try different kinds of things and the the scale of the noise just like well the epsilon greedy in the exploration for dq on the scale of the noise decreases as you learn more and more so on the policy optimization side from open ai and others we'll do a lecture just on this there's been a lot of exciting work here the basic idea of optimization on policy optimization with ppo and trp au is first of all we want to formulate reinforcement learning as purely an optimization problem and second of all if policy optimization the actions you take influences the rest of your the optimization process you have to be very careful about the actions you take in particular you have to avoid taking really bad actions when you're convergence the the training performance in general collapses so how do we do that there's the line search methods which is where gradient descent or gradient descent falls under which which is the how we train deep neural networks is you first pick a direction of the gradient and then pick the step size the problem with that is that can get you into trouble here there's a nice visualization walking along a ridge is it can it can result in you stepping off that ridge again the collapsing of the training process the performance,358,0,0,zR11FLZ-O9M
29,the trust region is is the underlying idea here for the for the policy optimization methods that first pick the step size so that constrain in various kinds of ways the the magnitude of the difference to the weights that's applied and then the direction so it placing a much higher priority not choosing bad actions that can throw you off the optimization path should actually we should take to that path and finally the on the model-based methods and we'll also talk about them in the robotics side there's a lot of interesting approaches now where deep learning is starting to be used for a model-based methods when the model has to be learned but of course when the model doesn't have to be learned it's given inherent to the game you know the model like ingo and chess and so on out zero has really done incredible stuff so what's wise what is the model here so the way that a lot of these games are approached you know game of go it's turn-based one person goes and then another person goes and there's this game tree at every point as a set of actions that could be taken and quickly if you look at that game tree it's it becomes you know a girl's exponentially so it becomes huge a game of go is the hugest of all in terms of because the number of choices you have is the largest and there's chess and then you know it gets the checkers and then tic-tac-toe and it's just the the degree at every step increases decreased based on the game structure and so,358,0,0,zR11FLZ-O9M
30,the task for a neural network there is to learn the quality of the board it's that it's to learn which boards which game positions are most likely to result in a are most useful to explore and a result in a highly successful state so that choice of what's good to explore what's what branch is good to go down is where we can have neural network step in and without phago it was pre trained the first success that beat the world champion was pre trained on expert games then with alphago zero it was no pre training on expert systems so no imitation learning is just purely through self play through suggesting through playing itself new board positions many of these systems use monte carlo tree search and during the search balancing exploitation exploration so going deep on promising positions based on the estimation then you'll network or with a flip of a coin playing under play positions and so this kind of here you can think of as an intuition of looking at a board and estimating how good that board is and also estimating how good that board is likely to lead to victory down the end so as to mean just general quality and probability of leading to victory then the next step forward is alpha zero using the same similar architecture with mcts what do you call it research but applying it to different games and applying it and competing against other engines state-of-the-art engines and go and shogi in chess and outperforming them with very few very few steps so here's this model-based approaches which are really extremely,358,0,0,zR11FLZ-O9M
31,simple efficient if you can construct us such a model and in in the robotics if you can learn such a model i can be exceptionally powerful here beating the the engines which are far superior to humans already stockfish can destroy most humans on earth at the game of chess the ability through learning through through estimating the quality of a board to be able to defeat these engines is incredible and the the exciting aspect here versus engines that don't use neural networks is that the number its it really has to do with based on the neural network you explore certain positions you explore certain parts of the tree and if you look at grandmasters human players in chess they seem to explore very few moves they have a really good neural network at estimating which are the likely branches which would provide value to explore and on the other side stock fish and so on are much more brute force in their estimation for the mcts and then alpha zero is a step towards the grandmaster is the number of branches need to be explored as much much fewer a lot of the work is done in the representation form by the neural network it's just super exciting and then it's able to uh perform stockfish in chess it's able to outperform elmo and shogi and it's itself in go or the previous iterations of alphago zero and so on now the challenge here the sobering truth is that majority of real world application of agents that have to act in this world perceive the world and act in this world are,358,0,0,zR11FLZ-O9M
32,for the most part not based have no rl involved so the action is not learned use neural networks to perceive certain aspects of the world but ultimately the action is not is not learned from data that's true for all most of the autonomous vehicle companies are all of the autonomous vehicle companies operating today and it's true for robotic manipulation in the industrial robotics and any of the humanoid robots have to navigate in this world under uncertain conditions all the work from boston dynamics doesn't involve any machine learning as far as we know now that's beginning to change here with animal the the recent development where the certain aspects of the control a robotic could be learned you're trying to learn more efficient movement you're trying to learn more robust movement on top of the other controllers so it's quite exciting through rl to be able to learn some of the control dynamics here that's able to teach this particular robot to be able to get up from arbitrary positions so it's less hard coding in order to be able to deal with unexpected nishal conditions and unexpected perturbations so that's exciting there in terms of learning the control dynamics and some of the driving policy so maybe behavioral driving behavior decisions changing lanes turning and so on that if you if you were here last week heard from way moe they they're starting to use some rl in terms of the driving policy in order to especially predict the future they're trying to anticipate intent modeling predict what the pedestrians the cars are going to be based on environment that are,358,0,0,zR11FLZ-O9M
33,trying to unroll what's happened recently into the future and beginning to move beyond sort of pure end to end on nvidia and to end learning approach of the control decisions are actually moving to rl and making long-term planning decisions but again the challenge is the the gap the leap needed to go from simulation to real-world all most the work is done from the design of the environment and the design and the reward structure and because most of that work now is in simulation we need to either develop better algorithms for transfer learning or close the distance between simulation in the real world and also we could think outside the box a little bit at the conversation with peter bill recently one of the leading researchers in deep rl it kind of on the side quickly mentioned the the idea is that we don't need to make simulation more realistic what we could do is just create an infinite number of simulations or very large number of simulations and the naturally the regularization aspect of having all those simulations will make it so that our our reality is just another sample from those simulations and so maybe the solution isn't to create higher fidelity simulation or to create transfer learning algorithms maybe it's to build a arbitrary number of simulations so then that step towards creating a agent that work that works in the real world is a trivial one and maybe that's exactly whoever created the simulation we're living in and the multiverse that we're living in did next steps the lecture videos will have several in rl will be made,358,0,0,zR11FLZ-O9M
34,all available on deep learning that mit id you will have several tutorials in rl on github the link is there and i really like the essay from open ai on spinning up as a deep our researcher you know if you're interested in getting into research in rl what are the steps need to take from the background of developing the mathematical background prop stat and multivariate calculus to some of the basics like it's covered last week on deep learning some the basics ideas in rl just terminology and so on some basic concepts then picking a framework tends to flow our pi torch and learn by doing i implemented guram as i mentioned today those are the core rl algorithms so implement all isms from scratch it should only take about two hundred three hundred lines of code there actually when you put it down on paper quite simple intuitive algorithms and then read papers about those algorithms that follow after looking not for the big waving performance the hand waving performance but for the tricks that were used to change these algorithms the tricks tell a lot of the story and that's the useful parts that they need to learn and iterate fast on simple benchmark environments so open the i jim has provided a lot of easy to use environments that you can play with that you can train an agent in minutes hours as opposed to days and weeks and so iterating fast is the best way to learn these algorithms and then on the research side there's three ways to get a best paper award right two to publish,358,0,0,zR11FLZ-O9M
35,and to contribute and have an impact in the research community in in rl one is improving existing approach given us a particular benchmarks there's a few benchmark datasets environments that are emerging so you want to improve on the existing approach some aspect of the convergence in the performance you can focus on an unsolved task there's certain games that just haven't been solved through their rl formulation or you can come up with a totally new problem that hasn't been addressed by rl before so with that i'd like to thank you very much tomorrow i'll hope to see you here for deep traffic thanks you you,141,0,0,zR11FLZ-O9M
0,okay looks like the stream is good all right we're live all right so let's go thank you to everybody that showed up and everybody that is going to be at some point hopefully watching this in the future the purpose of this this is actually part one of i have no idea how many streams but the purpose here is to go through building a dqn example with pi torch and outside of using pi torch for the neural network representation and training as well as using opening a gym for the environment everything else i want to do from scratch and i also don't want this to be what somebody i was reading through comments on a sent text video and somebody was talking about how a lot of tutorials of this nature are sterile that there's kind of smoothed over you don't see a lot of the mistakes and stuff like that and so i want these streams to at least serve as a basis for a video that i might add it later that isn't sterile so if i make mistakes if i have to debug i want all of that to be part of the video part of the stream and yeah so that's sort of that's that's an important bit and the another bit that's important for this video and you know for the stream and whatever videos might follow i keep seeing these repeated posts on various machine learning subreddits where it's always the same thing somebody's coming here to ask a question like hey i set up a dq n it seemed to be working fine on a,358,0,0,WHRQUZrxxGw
1,simple environment and then i went to a more complicated environment usually it's pong or breakout and all of a sudden i i can't tell if it's working it's doesn't seem to be learning i don't know what to do and it's like it's kind of hard to help those people without understanding their situation more very often it ends up being that they're not training long enough but there's obviously a disconnect between people learning to do this stuff and learning to diagnose issues and go through you know the various metrics that they can see and also understand sort of what are what are the expectations how long should it take for you to train in pong so i also want this dream to have somewhat of a like that kind of faux that kind of information in it as well okay so let's hit going enough talking now the first thing that i'm going to do is actually going to be getting the simple opening eye environment and with one of the funniest things i've been doing reinforcement learning for close to two months now and i actually haven't used a stock open a gym environment i've uh i've either built my own and i or i've used vis doom which but none of the stock ones so i'm a little new here to this but usually the simple example that everybody goes with is carpol right and the simple cart poll at least you're not learning from pixels you have as i believe the angle and maybe the velocity so looking through this example so yeah we can we can go ahead,358,0,0,WHRQUZrxxGw
2,and take this code i have i have an environment setup with jim and pi torch and everything so let's just start messing with the cart poll make sure that make sure that we have sort of everything that we need before we actually launch into the coding max reiter dqn it's just it just stands for dq network it's just like the standard in term for deep q learning essentially okay so let's go ahead and paste this code that we just got from the gym website let's see make the environment where he said it run it take the observation it's not doing a render step and i kind of want to see or is it doing or something i was just doing step yeah so that's important time and oh no it is doing there under step but i wanted to sleep so time don't sleep zero point let's do one because i kinda want to see how this goes i work on rl run the test okay so this is the basic environment and it's just taking random steps and it looks like if the angle goes above a certain point the game stops so that's i guess that's the failure mode so let's put an ip db in there because i'm also not quite fully familiar with some of these like how to get the information about the action space and everything so i'm gonna put an ip to be here and put up the terminal sort of front and center okay let's get the environment here okay so mdot action space what are you know okay it just tells you,358,0,0,WHRQUZrxxGw
3,oh okay so this is how you figure out how many different actions are possible and then you can sample from it it just gives you a number cool so we take the action and then in that step that's fairly normal i wouldn't what does it put an info it's just an empty date i guess maybe it's filled when the game finishes we'll find out and then done it's just a bull in this case false and then reward alright so in this game i guess the reward is a plus one for every plus one for every step survived and then negative one for failing i think i guess i can look at more information about the environment the goal is to prevent it from falling over or order plus one is applied episode ends when the pole is more than 15 degrees from vertical or the cart moves more than two point four units away from the center ok so it also can't move too far okay cool all right what does the observation look like the observation the observation is four numbers what are these numbers i guess it doesn't actually matter from the perspective of that's the whole point of a model free meta but i still would like to know what the observation means this is gonna give me the same information again artful observation okay so issue on opening eyes github position of cart velocity of cart angle of poll and rotation rate of poll so position velocity angular position angular velocity cool not that this matters again because model free so oh the network is supposed to,358,0,0,WHRQUZrxxGw
4,figure this out okay so good oh no not good because i still i there's a way that you can access this information observation space box for okay so then i can just get shape okay yeah so now i have all the information that i would need working through the dqn so that was a little bit of an aside just going over open a gym and the environment that we're going to be dealing with cool okay i'm gonna leave this here i guess i can take some notes here so end action say stop n to get number factions and then and observation space dot shape to get shape observation a tautology is a tautology okay i just do okay so let's see how we want to do so this what this is done so put together model class i actually want to change this to an actor class because often times i've seen people lay this out as a class that can take an arbitrary model has all the information about the environment and then sort of represents the actor itself so we'll do the model class separately and we'll do the actor class separately so these can actually go under this step and then for the actor class yeah so so yes so i guess the model class should not it should not worry about the target model that should be part of the agent the network for pass train step all these should be handled by the model this abstraction makes sense and then for the actor class it should it should probably be responsible of sampling action during training,358,0,0,WHRQUZrxxGw
5,or just in general i should be responsible for target model update i should be responsible for the epsilon i think i think that's that's it okay so a little bit of additional context and kappa there's a question about i knew this was gonna come up i knew this was gonna come up there's a question about why i'm not using i3 or why i'm not using a xfc this is my this is my home desktop that up until this point has only been used for gaming on windows unfortunately and i recently set it up with the maunder oh and i don't use it directly very often so i remote into it and because i was working with reinforcement learning often times i needed to see the windows and i three is not very conducive to sort of visual remote desktop so that so i left the standard gnome environment so this is actually just gnome it's not even i3 i'm gonna set it up like if i keep using it for streams i'll probably set up by three on it but for now it's just no and that plays well with teamviewer which is what i use for promoting okay so let's see we are ya good 14 minutes in and i haven't done any coding so let's let's fix that so agent done fine yeah thanks for asking the concern about me using inferior windowing systems is touching oh right before before actual coding i'm going to drill myself again but before actual coding i should go over some of these some of these concepts so so basically a dq n,358,0,0,WHRQUZrxxGw
6,is an extension off-key learning right and you have the exact same update equation for like as normal cue learning but then you kind of sort of modify it for deep learning so that you can use a neural net so the agent class is obviously going to be responsible for either using the model which is a stand-in for the cute able to sample the next action given some observation and then it it may or may not make sense for it to also handle an epsilon value which is gonna be part of exploration during training because we're not just we don't just want to sample whatever the model knows best right now we also want to we want to have some randomness so epsilon controls that randomest during training not during testing and then the target model is a trick that actually makes deep q learning work without it deep learning actually becomes kind of infeasible and basically what's happening is it's a the if you just have one neural network and you're using that neural network to train some data and try to sort of try to fit these you know the self-consistency condition that the bellman equation provides you in q learning it's gonna it's it's essentially it almost metaphorically is just chasing its own tail right it's trying to fit to data that is going to change in the next step because of the update right because of the gradient descent and so the target model is a trick that was developed at deepmind as far as i know where you you fit to data that corresponds to a fixed,358,0,0,WHRQUZrxxGw
7,"model and you update that model every so often you don't update it every step you update it every 5,000 steps or something like that so that's one of that and then the replay buffer is another trick that makes deep learning work with reinforcement learning and the reasoning for that being you usually want a lot of data for deep learning so instead of saying like hey i ran the environment for a few steps use this for for training we keep a very large buffer it's gonna be something like a hundred thousand steps or something like that right and then we randomly sample for that from for every epoch so this is sort of the the context for each of these steps okay enough talking let's code so for our agent what do we want as an input if we said we wanted the agent or i guess the actor in this case whatever sorry if it's gonna be sampling action or everything it needs to it needs to know the size of the action space it needs to notice anything action based there's a lot of redundant information that's got to be flying because the model knows this as the action space and then the environment already like the environment creates the size the action space and then the agent knows the size of the so does the action space we'll figure it out i'm i'm over optimizing too early so let's just do new actions i don't think the agent really needs to know the observation space size it might not even need to know the number of actions actually now",358,0,0,WHRQUZrxxGw
8,that i think of it because if it has the model either way it definitely needs the model and i'm actually gonna say no it doesn't let's put some of these questionable things here so i don't know where epsilon and target model update should go we'll figure it out but i don't see it fitting well in the agent abstraction so let's start simple the agent only gets the model that's it self-taught model and then we can say yeah we can just say like act and you can pass it's an observation i should specify that this is a dq n agent okay so act we specify some observation and then the whole point is that we're gonna apply the model to this observation and get the q values did i die yeah it's past the observation so in this case let's take some notes so the input shape thank you for the question - it so from the agents perspective it should not matter if the observation is a single frame or a stack of frames that metals from the matters from the models perspective in the simple environment that we're gonna tackle and probably before i get really sleepy and crash tonight i'm probably only going to be able to do the card poll and for a cart poll a single frame is fine because car pool defines both the the position angular position and then also the velocity right and usually when you're stacking frames the velocity is what you're looking for so if you already have the velocity we don't need more than one frame once we get to breakout,358,0,0,WHRQUZrxxGw
9,we will need at least two frames and usually the standard just use four but i do want to do a follow-up to this tutorial later on where we restrict ourselves to a single frame and use a recurrent agent and then sort of have that state be implicitly part of analyst tm or ora gated recurrent unit so i'll tackle it later i've done that in the wisdom environment and it does work if you if you do it that way it's kind of cool okay so act as one take some notes here so the observation shape is going to be e this needs to work on batches if the model expects batches right so n would be the batch size which for let's say for a test scenario is just gonna be one and then so i'm just taking the notes for carpool right now so it's just before right as we know q valles is new actions so at the shape of q valves no that's fine so the shape of q wells is going to end up being just n and then the number of factions which in this case is two yeah and this is the i see i was tracking myself into a corner here let's no no so i'm trying to figure out if the agent should have a reference to the environment and if the agent should be responsible for stepping through the environment or not let's say for now no in in the future we might come back and change that so then the function should not be called act it should be called get actions,358,0,0,WHRQUZrxxGw
10,and should get observations okay so we get q values and then we want to return a list of actions so q valves in this case is going to be a tensor pi torch tensor so i think we can just call the max function on it and we're maxing over at the second so just the last the last axis so yeah just don't need to do that just return that cool what's the time here yeah we took a good 10 minutes to write this wonderful very efficient okay so what is the next class we want let's do the model and the model does explicitly need some of these information so it needs observation shape well we'll name it something else later but yeah once we get to break out we're gonna need to specify these separately okay so it needs observation shape it needs the number of factions let's not give any other arguments for now all right and now we actually need to define the models so usually given the problem is relatively straightforward people use like it just i think just a couple of a couple of dense layers so let's do standard imports for pi torch so tonight my autocomplete is not working so this is going to end up being very very entertaining sorry i dropped something heavy as you might have heard ok import torch denon as an n and n dot functional i believe as f doing a lot of this from memory i should probably start looking at the documentation soon here let me just check my yeah there's no i don't know why i,358,0,0,WHRQUZrxxGw
11,don't know why the autocomplete is not working whatever i'll just look at an example of network in pi torch let's see mm-hmm and then ah there we go this is a pretty good example i'm just gonna copy this okay oh this needs to this needs to subclass if yeah this needs to subclass and how to use up class i think the 60 minute blitz on the pi torch website is pretty good it goes over not not this part though that's training a classifier and end up module okay so we need to subclass from anand dot module and then do this so super model yourself in it okay like eighty percent of programming is a syntax at minutiae it's kind of annoying yeah okay and we'll come back to doing the forward pass and everything cool alright so self dot i'm just gonna say net and then i think just a couple linear layers are probably going to be fine the input here is observation shape it's not observation shape so i'm actually let's still leave it as observation shape and then i can say just to because this is meant to be for problems that have a flat observation so i'm just gonna say len hopes shape is one so if it's not one then this network only works for flat observations yeah number of viewers is steadily going up new viewers please introduce yourselves and chat and talk to me ok so now that we know that observation shape should have a length of 1 we can start designing our fully connected layers so this is going to be ops,358,0,0,WHRQUZrxxGw
12,cero - i don't know it's probably something small it's gonna be fine for this problem so 26 and then 256 is actually kind of big about whatever 256 and then on the other end we receive 256 and then the output is going to be knowing the actions and in q-learning you wouldn't have you would have an activation after this hey michael you wouldn't have an activation after this because the q value is it's it's a it's the reward i o stream is going blurry is stream going blurry for anybody else oh i just saw that 44 percent of the frames are being dropped it might be fine for - let me turn down my bitrate anyway just in case because i am dropping frames apparently let's see if that changes anything i'm also recording this for when i make actually the video okay alright so this should be sufficient from the perspective of a model i was talking about how in q-learning you don't have an activation after the final layer because you're trying to represent a real number right in terms of the rewards so most activation functions won't really make any sense like if you put a value on there then you can't represent negative rewards but you you need to be able to represent negative rewards so no activations after this cool so the fordpass for this is going to be really really straightforward forward is just just overriding the for function from pi torch is an end up module and in this case we're just the model basically is containing in sided another model which is this,358,0,0,WHRQUZrxxGw
13,sequential object and we just apply that so return self dot oops self dot net apply that tax okay cool so we got all of that let's start doing some tests because we've written a fair amount of code without doing any tests and that's bad so if name rules so let's go ahead and make our environment i'm just gonna copy off this for now okay so jim don't make carpol fine get an observation start a loop no i guess i don't actually want the loop i just want a single step i don't need to render it for now i'm going to sleep don't eat that don't do that so we already have the observation so i can actually comment all that out and then we want to make let's say so an instance of the model so yeah and then i'm gonna say end dot and then n dot actions phase okay and then let's do a forward pass on it so q val equals that's not going to work is it let's try it but i think it's gonna fail i'm gonna fly back to the conservation okay cool so let's run this make some mistakes here okay yeah so first things first needs to be a tensor there we go all right let's ringing you again okay should that have worked and what is the shape if i say yeah so just has a shape of four created the right output i'm just wondering if if this should have worked because the first dimension was not the batch dimension i guess we can do a concatenation let's do torch top,358,0,0,WHRQUZrxxGw
14,cat nope that's wrong that's stack yeah so now this is a batch of two please the model gonna work roughly here yeah it is okay yeah i guess i had it wrong okay so for passing the model works all right so networks put together forward pass works we need to add the train step but before we do the train step i think we need to tackle the replay buffer i pdb is definitely a must-have it's like it's a lifesaver it's a really really good tool actor class sampling action um yeah i mean we already did this but it's not necessarily very useful at the moment okay so let's talk about the train step but okay before we talk about the train step we need to talk about the replay buffer and actually before we talk about the replay buffer we need to talk about the observation itself so in q-learning well in reinforcement learning what you're dealing with is usually state action reward next state right which spells out sars which is a i suppose relevant to our current predicament sorry that feels like dark humor anyway it's the state action or end state so the replay buffer is just going to be a list of these things and so it might be nice to organize this better so let's make a data class how's the sheltering in place or a quarantine has been for people life going normal drastic changes everybody doing okay so let's just say i'm just gonna call it saris for now and this has the state just state action and let's say and for now not,358,0,0,WHRQUZrxxGw
15,that it matters reward which is that's let's say it's a float the food is getting monotonous what food are you eating and then the next date next date is important as well see in a second okay so this basic data class should suffice and so we can now create the replay buffer self and yeah we just want to give it the size right so buffer size we can go to default so i think a hundred thousand tends to be a good option especially for small problems so self dot buffer size equals what am i doing buffer size so the the point of the replay buffer is that as as you add more samples to it once it gets full the old samples start getting pushed back oh restaurants are closed where where are you at menace of what country yes so replay before you add more sample to the replay buffer and they get they get pushed out once the old samples get pushed out once the buffer is full so there's a few different ways of implementing this the sort of the laziest way and that's what i'm gonna do tonight because it is it is midnight where i am i don't want to do anything fancy oh i see i didn't realize restaurants and fast food chains were closed in india that i mean i guess it makes sense but it's also michael were gonna use the dq method i was just getting to that i'm being lazy i guess i don't have to we can do something worse than dq let's do something worse than dq let's just,358,0,0,WHRQUZrxxGw
16,use an array and let's add an insert method give it a bird flu and then we'll do self dot buffer dot append and then the real cringy stuff and what it'll be here to be negative self dot buffer size yeah the height of efficiency this is terrible never ever do this but i think for a simple problem it's fine typically people use the python dq which is a linked list so it's not um it's not inefficient in insertion so this there's like a dichotomy here right so if you're using a list or just an array right it's the dynamic array the insertion is order one but you are essentially destroying the old array and making a new array every time you do this insertion and sort of push elements out right so so the insert operation is not very efficient but if you use a dq which is a which is a doubly linked list the insert is always fast but when you want a sample that's slow so it's pick your poison i i'm sure there are i'm sure there are faster implementations one could probably do this really well with the with the database and have it be more efficient than these methods but i think for a simple problem for learning into a problems this is fine okay so we'll do insert and then we should do the sample here i think so so let's do just sample and then and here we probably should put an assert - the pop would still be at zero so that actually would be even less efficient because every single pop,358,0,0,WHRQUZrxxGw
17,would make a copy right so whoa yeah actually yeah in this case because we're only adding a single example it would be the same because this doing this is the same as doing the pop zero i think because the pop zero is just going to shift everything so it's essentially copying the pointers all right so we want to assert that the samples that were requesting are less than or i suppose less than or equal to the objects that we currently have in the buffer and then i think we can just use the the random module has a sample method so we can just use that so return sample buffer make sure this is correct i found sample around that sample function yeah that that looks to be correct cool okay so we got a replay buffer so let's do some tests with the replay buffer so we got the environment got the observation i'm gonna change this so that it's no longer a for loop but is a while loop so while true gonna render the environment but take a random action get the observation your ward and done the info and if done then reset the environment now we'll get to it let's make sure we can interrupt this so i accept keyboard interrupt i think okay so now in a while loop we're gonna take these random actions create these and okay so let's make a reflow buffer and let's start inserting data to it so our the dot insert self and how did we define sars okay so it needs to take the state which is the that's,358,0,0,WHRQUZrxxGw
18,"the last observation so we always need to hang on to the last observation yeah the last observation the action that was taken the reward that we got and then the current observation and then we update okay cool all right so let's run this and then if when our p dot buffer is greater than let's say 10,000 let's drop down into an ip to be okay and then we'll examine our objects and make sure things are as we expect them to be observation is not defined know why might i don't need to be running that right now all right scope 10,000 may have been too large also because i didn't say 10,000 i said 100,000 my bad yeah let's just do like 5,000 there we go so our b dot buffer 0 okay so we're now keeping these objects we're holding on to the rewards that was a terrible idea i just want to see one we're done was equal to true so zero the state was this guy yeah i just want to make sure next day pierre is the same as the previous day here for the perceptive in the audience you might be thinking hey isn't this isn't this a little inefficient because you're storing the same point or twice and yes it is but it makes the design of the reflow buffer a little bit simpler and we're only replicating the pointer so it's not too bad it's like eight bytes whatever especially when you're storing images i don't think really it matters at that point okay so i think this is fine cool all right so we",358,0,0,WHRQUZrxxGw
19,got that oh one thing we'll implement that later but we're gonna once we get to training we're gonna be we're gonna want to look at some metrics and one of the metrics that's very interesting is to look at the average score of some number of recent examples so that can be like average score for the last 100 samples that we have and that's useful to monitor see if the models making any progress well we'll get to that we'll need to have like a similar sample method that just gets to last 100 or we can just do it manually what we'll get to it okay cool so we got i would say we have the replay buffer i haven't checked it'll work i haven't checked that it's uh works correctly after it's filled but like this is simple so now that we have that we can actually maybe forego epsilon greedy for now and just have the model learn from oh no because then we can't monitor it necessarily it's okay we can still monitor the loss so let's let's have the model learned from completely random completely random actions and observations and see how it behaves we do want the target model and we do need a training step so so we need an optimizer i always forget umm so let's look at let's look at the optimizer example again from pi torch are the training example not adam oh there we go so criterion we're gonna build it build to loss ourselves we just need the optimizer so door stopped him and i like to just embed the optimizer into,358,0,0,WHRQUZrxxGw
20,the model we can use adam and the learning rate of one a negative four is usually fine for some of these okay cool yes - that's uh that's fine i i do want the reef live offer to be large a hundred thousand is a good sort of i guess good default number to start with you can can be changed later after testing okay so we got the optimizer and now we need to do the actual loss calculation and and backwards that's right so we can just define trained step okay so this is where i also kind of forget something so let's look at whenever i'm trying to refresh my memory i'd like to go to this the set of slides from david silver from deep mind and right now i just want to look at the the loss equation for dqn to jog my memory there we go okay so the bellman equation being that the q value of some state s and some action a is equal to the expectation of the reward that you got from taking that action in that state plus some discount factor we haven't discussed that yet i don't think we really need to pay any mind to that for the simple example and the max over the q value of b of the next state and whatever max actions that you'd have so this this constitutes the the loss now oh right so the train step really can't be well no it can be it can be it's fine this constitute the loss what we're gonna do is the this evaluation this needs to,358,0,0,WHRQUZrxxGw
21,come from a separate model a fixed model so we need to essentially create two instances of model so there's model and then i mean to create the same thing again and should it be a copy i think it does need to be a copy in the beginning and then in the future we also need to make copy so i actually need to look up how to copy parameters from one model to another in pi torch copying weights from one net to another michel i switched to pi torch because i found i found it a lot simpler to represent some of the reinforcement learning ideas in pi torch so i'm only used by towards these days for rl for everything else i'm still using cares one of the very nice things about pi torch is that there's no additional abstractions at least at least visible to you so in tensorflow - it's a really nice api and you can do things like put well that's a good idea - oh that's literally what this says okay to finish the thought on pi torch for assistance or flow in tensor flow like if i have a trained function and i put the decorator on it like at you know tf function or whatever then if i want to debug that all of a sudden i have to deal with this sort of compiled down form which is representing the the underlying graph which a a lot of it adds more pain without necessarily adding any real other benefits and i wanted to learn pi torch anyway so for for all intents and purposes,358,0,0,WHRQUZrxxGw
22,for reinforcement learning i'm just apparently sticking with pi torch it's also nice with some of the some of the policy gradient methods you it's it's a nice abstraction to have categorical or normal distributions and in pi torch those are implemented already in a way that they're differentiable so you can do a backwards pass through them in tensor flow they're not and so you essentially have to write your own which is it's a pain so i hope that gives a good context in that okay so tgt we want to copy the parameters from em um that's a good question okay let's actually take a tiny detour for that so first of all this tutorial is really really good i highly recommend if you're interested in reinforcement learning and deep reinforcement learning that you read through this it's pretty approachable and it's pretty detailed then you go to youtube there is a lecture series from david silver again and it's also pretty detailed and it goes into a lot of a lot of theoretical context for reinforcement learning this is also a good one and then there's a bunch of recommendations so it's not deep reinforcement learning but there's a book that you can get for free from sutton and bartel and this is the e sort of the definitive text i actually haven't read this i'm starting now but it it's people say this is like a good good book to learn general reinforcement learning as well as the historical context for it and then a final book that i spent a fair amount of time with is this book from max,358,0,0,WHRQUZrxxGw
23,lapan dp'd reinforcement learning hands-on there's like a 2020 edition that just recently came out this is the old one i think this is the new one this is focusing on pi torch and it is focusing on everything being hands on so it's actually a little bit light on theory but it's a really good book both has a quick reference as well as sort of just going through some of these algorithms and and breaking them down yeah so and then i'm trying to build up a definitive set of tutorials will we'll see how that ends up i hope that was helpful in terms of sort of where to go for resources all right target model copying weights let's do this so tgt to upload state dict and then mdot state dict okay cool so we're actually going to need to do this step a fair amount of time so maybe we should just make a function for it all right so training step we still don't have the training step i'm kind of starting to give up on the abstractions that i was setting up i know not entirely okay so to run the train step we're gonna have we're gonna have a list of sample state transitions with rewards oh you know what i forgot i've done it's not sorry's it's ours death done i believe is important to not 100 sure yeah done is important done is important because going back to well i don't i closed the slides but when we look at the the max possible reward in the future so look at the q value from the,358,0,0,WHRQUZrxxGw
24,future given the next observation if the if the episode had ended then that's zero so or yeah that's zero that that part is zero the reward is the only thing that remains so done is important i have totally forgot about that okay so train step we get state transitions let's just call them that for now i think that's it okay so here we need to create our yeah so first off we need to create our state vector and that's just going to be stacked so her states equals torch dot stack and i will say s dot state or sn state transitions right we need their rewards nope that was terrible idea it's gonna be bored 4sn state transitions that could be fancy with it here but whatever let's see so we need the current states we need the rewards we need to know we need like a mask for when they're done so so the the mask is just going to be one if you weren't done and then zero if you were done and you'll see in a second why it's what torch that stack it's going to be not as done but it's going to be 1 if s dot done else no zero if s done else 1 for s and state transitions ok and then we need the next states right so torch that stack s dot next state for i sense a transition as actions forgot about the actions do actions matter here music i forgot if actions matter i think they do i went too far yeah but the model is not it's only a,358,0,0,WHRQUZrxxGw
25,function of the state oh right because it's gonna yeah of course actions matter just just to give context here i wasn't sure if actions mattered for the loss function calculation but they do so the loss function is built by essentially trying to ensure that the bellman equation is consistent so this we want to minimize this difference right and this is the right-hand side of the bellman equation which is the reward which we have now enter a for and then the max over the q values for the next state so this here we're just taking the max over the action dimension but then we're subtracting from it the q value for our state given a particular action so this is actually picking the q value i should have remembered this because when i was trying to implement this in the kara stuff so this was actually a major major pain point so i don't know why i forgot that maybe a block the memory out okay cool so actions do matter and we need to make that really so action for us in six transitions okay so we got the actions so we got all of this now to compute the loss we need to take our rewards add to it i'm not going to bother with the discount factor just yet if if i fall flat on my face we'll figure it out then so so we need to be a little bit careful yeah so okay we need to compute the q values from the target model i see this guy needs to be a separate function - not for,358,0,0,WHRQUZrxxGw
26,each action basically you can only compute you can only ensure consistency for the bellman equation for the actions that you actually did take so that that's why the actions are necessary model state transition and target model okay so with torch no grad i believe because we're not we don't want to do any backwards passes on the target model we need to compute the q values for the next state so q bells next equals just target model and pass it next states and we need to take the max over the action dimension so that the output of this would be that would be a shape n and then the cube l which is no mack sure that's right so let's just take the max here cool so we got that piece we have the rewards so the loss would be computed by taking the rewards adding to them q fouls next and then subtracting from it the curse date so this is this is where we do actual forward pass so this is going to be the model and we pass through it the current states and then we subtract from here that q values no no no right so we got the q values right and this is of shape and and then new actions what we need to pick only the q values for the actions that were chosen and i think i need to do one hot here yeah which means that i need to another number fractions okay let's see so i might need to test this in an eye python session no tour start and end at,358,0,0,WHRQUZrxxGw
27,one hunt and end up functional dot one hunt yes okay and one hot is done with the tensor or just an array i think so going to three and then the number of factions just fine i guess nope needs to be denser one hot is only applicable to index tensor right thanks i can say long tensor there we go okay so the idea then is that i can compute this one hot tensor multiply it with the q values and then sum those along along the action dimension and that gives me just the q values for yeah i could have also done that in a loop but this is obviously more efficient okay so one hot actions torch dot tensor um i wonder if that's gonna work it's also already done so that the torch don't stack right yes let's for actions i can't stack i just have to get the list okay and then we can do here we can do q val's times one actions which should both have the same shape then we can do i think torch dots um let's try the senior support at some i'm going to do it right it's not correct was it three yeah it was three actions cool oops seriously i have to restart my editor here okay torch dots um i get it one direction okay cool and then we want to take a mean of all of this so the total loss then is that okay so once that is done actually before that isn't i think we need to do optimizer dot zero grad music yeah optimizers dot zero,358,0,0,WHRQUZrxxGw
28,grad before we do a forward pass here so modeled on optimizer dot zero grad and then we take a step to a model dot optimizer i know we do it backwards so law stock backward and then optimizer dot step and then let's return the loss from this so that we can track it okay so that's our train step let's actually try this so we have everything they're missing one required position argument params oh totally forgot the optimizer needs to know about your model where did i define that here so self do it like that it's a function there we go it can actually just be self doesn't add parameters since all of our complexity is inside this one object cool okay so we ran this through the ip db so we can now try calling the train step function and see how that works can't get help on this all right so it needs to take the model i needs to the state transitions which are just going to be a sample from the replay buffer let's say a sample of 5000 we the target model and then the number of factions which that's sick - for now people have our object has no attribute buffers wonderful i never tested the sample function there we go let's set the actual training up here model our v dot sample let's say let's say 1000 that's sake of argument and then actions face that m let's put the ip db inside here because i want to make sure that each one of these steps is actually going correct expect a tensor as element,358,0,0,WHRQUZrxxGw
29,zero in argument zero right because torch can only stack tensors i should have seen that coming oh it's good that we did this because i just realized i didn't use the masks okay so this equation is wrong because q valves next needs to be zero when the game had ended so it needs to be multiplied by the masks there that's better okay so let's try this again argument tensors position one must be tuple of tensors not tensor really okay you have that make sense really like i'm doing this wrong there's definitely better ways of doing this all right 1000 is a batch mentioned before is the size of the state great i knew did it must be a sequence got float i guess we can't really have to be an array all right fourths times ah because i didn't do it correctly for the reward because it is also just a float okay rewards appear to be correct one for the most part not seeing a single negative one interesting i should make sure that this is correct in the replay buffer in a second definitely see that so that also means that mask is always going to be one because we don't have any example of a finished episode here which is a little bit odd let's see you next state's actions oh down there yet cool cue valve next oh waitwhat do it does this need to be evaluated in some way oh max is a tuple okay that's wrong we need them the max values we do not need the index sam needs to be applied somewhere else,358,0,0,WHRQUZrxxGw
30,or using max somewhere else yes here here we need one okay cool glad i caught that shape is 1000 this is not this is gonna run into a problem pretty sure a model object has no attribute optimizer because it's called off 1002 that's fine you received an invalid combination of arguments got a list hint of it expected something else alright that was my mistake they're cool i think i need to move that ip to beat down a little alright let's continue debugging okay so one of the things that always gets me is that there's often like a dimensions mismatch in these kind of calculations and they create incorrect values so let's let's make sure that that's not the case here so reward shape 1001 mascot shape 1001 few val's next shape 1000 so what happens when you multiply mask by cube else we get 1000 and how okey well next knock your mouse we get 1000 1000 this gets me every time the i guess the few ways of fixing it as we just do mask this that's probably the correct way to do it here okay so that's an important change and then cue vowels has have a shift one thousand to one hot i just realized the terminal i pushed it out one hot has a shape of one thousand two so this this works fine we just multiplied by one actions but then we need to sum it and that's gonna be in the negative one direction right and the shape of that is one doesn't so that's fine that's consistent so i think we're good so we,358,0,0,WHRQUZrxxGw
31,"can just print print the loss here and quite just make sure oh i think it's just backward not towards cool we got a loss okay so before complicating things any further let's make sure so what we're gonna do is we are going to let's see so let's add some some constraints here so minimum men are be size let's say is five thousand right all right no let's say it's ten thousand and then for each epoch we're gonna do sample size and goals 5,000 right and then we should be tracking things as a function of number of steps played we don't necessarily want to do and update every single step so actually before this resumes i mean remember every size oh yeah so the only known leader training once the buffer is big enough right and then the other thing is that we don't want to train on every single step so we need to figure out how many steps we want to run 10,000 5,000 so let's do let's do 5,000 steps in the environment no that's maybe too much let's do 1,000 test in the environment before doing the next step of training so and steps before train equals 1 and we can we can tweak these later depending on how things are working ok so i want the heart beat up a furnace kevin besides we do this but there's a second test here yes let's do steps since train equals zero right and every time we're gonna add to that all right and then if and steps in strain it's greater then and steps before train then do",358,0,0,WHRQUZrxxGw
32,this and then just set steps since training to zero okay and we can and we can just print out the lost loss for now alongside oh yeah we should also be she also be tracking environment steps us actually do negative one times i'm in our be size because i don't know otherwise it would just be weird so step no plus equals one and then here we can print the step number as well so we're not really doing any excuse me sound okay i'm just getting sleepy and kicking things all right so we're not really doing any target model update yet we're just gonna make sure that the loss behaves in the way that we would expect it so right now we have a fixed model and a model that we're training and so over time the loss should start start decreasing right so let's just run this and see if that's actually the case okay yeah looks like it is loss of steadily going down now this means nothing it's kind of funny i feel like it's actually slowing down a little bit and that's very likely to be because of the insertion time being stupid okay i think we can fix that but we'll come back to it okay all of this is still happening on the cpu i'm not sure if i'm gonna try to tackle the gpu bit tonight or not okay so we can do actually before we do anything i do want to set up a little bit of visibility i've been using this service call weights and biases that's that's been very helpful in tracking,358,0,0,WHRQUZrxxGw
33,some of these agents so let's go initialize and so the project here is dqm tutorial and the name is car pool okay and for now the only thing that we would log is just loss i'm gonna decrease some of these numbers and because i feel like maybe things would go a little bit faster right so we want to lock the loss when deep in don't blog and then step is equal to step them do we want to log anything else no i think we're fine so we're training let's see everyone under steps were training on 2500 samples and then let's say every 5000 steps yeah so instead in addition to steps and strain we're gonna say steps since tgt let's do that here so then we can say target model update should happen after 50 epochs of training i think that works ok if steps since ddt is greater than target model update then we do the update pass it the model the target model and that's it right no it's update to gt model ok okay update tgg model pass the model past the target and then set steps and stick to two equals zero and actually instead of calling it steps we can say any epochs so that it's more clear okay i also like to get some visual feedback on on the terminal so let's get let's do this we'll get a get a ticket here instance and then i'll just update it yeah i could do the modulus that's true okay we got a progress bar let's go ahead and run this see if it actually works,358,0,0,WHRQUZrxxGw
34,oh i'm still printing a loss that's i don't want that all right okay let's start this and we'll be able to see how the total speed of the algorithm evolves in the mean time as its training let's go to weights and buys as an open our project so here's our current run i believe it's make sure yes this was the old one i can delete that yeah so these spikes in the loss are expected though i didn't expect them to be this sharp but the spikes and their loss are gonna happen because we we have been sort of fitting to a particular target model and then we just change it and then we fit to the this new target model then we change it and the fit to the new target model when we change it so this is running at about 500 iterations every second what we're looking for and i'd probably kill this pretty quickly afterwards but what we're looking for initially is just to see if this overall trend and the law starts to drop and sort of at what rate it starts to drop there's also this clothes iron icon for smoothing out your plots again more clearly see the trends so it is definitely going down but again that doesn't necessarily mean anything so that's that's a good sanity check but we now need to start tracking we need to start tracking the average reward and i guess oh and this is the whole thing starting to slow down i want to replace the array with a dq there's a reason for that we sample a,358,0,0,WHRQUZrxxGw
35,lot less often than we insert so having insertion be heavy the reason in slowing down - is that our insertion is getting slow in the replay buffer as the buffer expands in size adding a renew item which is which we do a lot slows it down so i'm going to replace the list with a dq and that should it'll still slow down because the samples are gonna be slower but we do the sample a lot less often so it's just slowed down less let's find out actually i don't remember python dq this is how you spell dq kids apparently collections there we go okay and i think it's max len is the argument so yeah max line equals buffer size yeah dq is definitely better than list at least in this particular example we'll find out i suppose i guess we know right we can benchmark this so this was starting to it started at about 600 iterations per second and then went down to about 250 so we'll see how dq does we no longer need to do the push we just do the we just do an append and the sample works the same because i think the interface for the dq is the same as a list so yeah let's let's run this again and come back down to earth here let's rename this to dq lost dynamics should be roughly the same i think i think i can add a plot for let's see you do a line plot between relative time and step maybe well that's sad thought i could yes this hasn't really slowed down,358,0,0,WHRQUZrxxGw
36,"well it's slowing down a little bit but that's because the sample but it hasn't slowed down as much so yeah dq wins hooray now let's do okay i lost an amex are the same things they're running a little bit faster i think if we went with gpu it might run a little bit faster as well but concerns for a future time we want to start tracking the reward by resetting the environment right um we can actually so at the moment everything that we're doing is completely random right we're just sampling from the action space so we could do one of two things one is we can do the epsilon decay right and long be long the average award let's just do that okay so how do you have salon let's start off with 1-0 epsom n we will say is 0.01 and then that's decay so needs to do a little bit of math here let's do some rough math so we can we can do the decay so if we raise this - yeah so if we can do that we can do the decay by number of steps of the environment so if we want the epsilon to decay let's say in 250,000 steps it should decatur its minimum value ya know so we need a lot of one nines here better this is so scientific - too much there's a formula you can come up with this that i've done before but i'm lazy right now i think this is fine yeah we don't actually need to say max epsilon so we can just use the decay as",358,0,0,WHRQUZrxxGw
37,"the stand-in so ninety nine nine eight okay and then we can just say for at every update the actual epsilon value yeah so the absent and the actual epsilon value is going to be equal to epsilon decay raise to step numb yeah let's let's add the first 5,000 that's fine yeah and then we'll decide if we're sampling from the model or for sampling from the environment so if so from random import samples an afford random as well so if random is less than epsilon right then we take the totally random action otherwise we use the model we need to pass it the last observation no wait this is why we made the agent class that we haven't used yet we spent 15 whole minutes writing this this is ridiculous i'm not gonna use that class just call the model on the last observation and get that item and that's gonna be your action and then as we're doing this we can also log our epsilon right ok is there anything else oh rewards so as we're doing this now it makes sense for us to be logging an average reward at the end of an episode so we can just we can just keep a buffer keep a rolling buffer here so sued rewards and if it's done that i mean we'll say every time we get a reward we add it to the rolling reward and when it's done we add that to the episode rewards and set the role in order to zero and then when we do our update we take the list of last rolling rewards and",358,0,0,WHRQUZrxxGw
38,so drew ward's and then said episode rewards too and emptier a summit state flying around isn't it wonderful okay so i think we're set with everything just for the sake of for the sake of testing potentially afterwards let's save the model so tgd don't save right no how do you save models i always forget saving and loading models save the state date okay give it a path so we got the state dict and then for a path let's just let me make sure there's a folder for this so i'm not just like cutting up stuff okay and we can just say save it in models slash and then whatever the step number was dot p th p th is just the by convention what's used in pi torch okay so we're saving on every target model update the models shouldn't be too big anyway let's go ahead and run this make sure it's not crashing it is crashing and irri has no object oh i need to make a tensor here only one element tensors can be converted to python scalars max i didn't do a max max negative 1 negative 1 there yeah looks like it's working okay nope wait okay that was just an update all right so we can go back to weights and biases we we have a number of new runs now we can go ahead and delete all the ones except the latest one okay cool so loss is behaving as we expect it epsilon is going down this might be too aggressive i guess we'll find out soon the average reward is going down,358,0,0,WHRQUZrxxGw
39,so that's wonderful let's see if that starts going up i think the epsilon decay might be a little too aggressive thank you for your kind words remain okay so this is not particularly encouraging at the moment and this tells me something's wrong oh my god i totally forgot so the loss should not should not go negative i he yeah the lost function is wrong okay we squared this before we mean this yeah to this this is done as an mse and i wasn't squaring it so it was uh yeah that's that's bad we don't we don't want to do that that's completely wrong let's go ahead and stop this run so that should help maybe improve things but i think i still think the the epsilon decay is more aggressive than i wanted it to be so let's make it a little bit slower and run this again yeah we're in the waiting and debugging part of reinforcement learning which is painful okay a little bit better epsilon decay let's go ahead and hide that last run i am currently ignoring the discount factor on us i kinda wanted to see how this would behave without it um this is this is a finite time game it's not it's not that many steps so i feel like it should be fine without it but if this continues failing will will add one well look the last is going up so there's also a number of other things that we could be considering here so let's talk through some of the hyper parameters here so i may be training too often or,358,0,0,WHRQUZrxxGw
40,"i may be training not as often as i need to right so that's that's one thing to consider another one is target model update i'm currently doing it after 5,000 steps so it's like after 50 batches but sometimes it's a better idea to to wait longer for the target model so let's mix this because this isn't actually doing anywhere that the loss just keeps increasing and that this this actually does make a little bit of sense i would wager that the loss probably gonna increase like this for a while and then start coming down but the average reward situation does not inspire confidence in that idea and this is the reason i oh yeah there's a lot of hyper parameters so one of the things to consider about the loss is that we're trying to train a neural net right now that's essentially trying to predict kind of large numbers and that's not necessarily always very good um so what what what is the maximum length of this episode episode length is greater than 200 so 200 is the max that can be right this means that at time step 0 we're trying to train a network to predict a to predict the number 200 right to basically say if you take this action then the max reward that you can expect is going to be 200 discount factor can actually be helpful here - right from from that perspective but even without that i think that i think that it might be beneficial for us to to scale the reward down right so divided by 100 let's say we can",358,0,0,WHRQUZrxxGw
41,do that after we add it to the rolling reward so that we can still see that value correctly but we divided by a hundred to maybe help normalize it consider salt when the average or is greater than or equal 295 over 100 consecutive draws okay okay cool so we got that and then what was the other thing that i was thinking of oh update rate so instead of doing every 50 batches we can do every 150 and see if that makes a change all right like good scientists we're going to change two variables and see what happens okay i kind of have to hide this now because that lost value is so different from the old one okay and now we wait so the law says not bouncing as much and i think the two reasons for that one of them is just that we normalized and then the second one is the slower update of the target model yeah i saw the spike i wouldn't put much stock in it oh i would put stock in this see that me that that little bit of a an increase one of the things we're gonna get philosophical here i guess but one of the things that is really it's not just a problem with the reinforcement learning it's a problem with neural nets as well but as soon as you start plotting your metrics sometimes you'll just watch your metrics for a long time and that gives you a feeling of being productive but it's actually just it's just under normal circumstances i would just close this window and come back,358,0,0,WHRQUZrxxGw
42,yeah not an hour i mean if this upward trend continues that implies some learning so let's see it's been about 180 so about three minutes right so this decay implies it's gonna get maybe in 10 to 15 minutes the epsilon should have decayed very close to zero by the time it starts reaching like 20 or even 40 this trend is going to become more pronounced right so we shouldn't have to wait too long and we can potentially just interrupt this and write a test as well and that that would be a good a good end to the stream running a test where it's gonna get more interesting is in something like break out or palm because anytime you're throwing convolutional nets into the mix things get a lot slower yeah it does seem like it's working but i want to give it a little bit more time get the epsilon to decay further all right it's getting close to the max score but it shouldn't be going above 250 though what i wonder if something's broken again because i don't think it should be unless this is what version of this read one right oh this is v1 500 it's not 200 is 500 next one yeah i'll let this go for another few minutes and then stop it and write the well we can start writing the test well this is training do this really lazily well i forgot about that okay that's no bueno anyways let's let this run let's write a very very quick test so we can visualize some of these before finishing this dream so i've,358,0,0,WHRQUZrxxGw
43,test equals true then we don't want a target model well actually it doesn't matter i guess to leave that we just don't want the update step right and we want the epsilon to be so if test then that's long it's just zero right also if a checkpoint is passed then you should load how do you load model download state dick torch salad okay so this is if not none then load it and pick the act so yeah epsilon should be zero if it's test i don't care if we keep inserting this i think that's fine too all the all the weights and biases stuff should be disabled and yeah and then no training so if not test and okay so yeah no training no anything i guess we can print the print the rolling reward so i have test right and then if this is a test then you should render so and i would sleep a little bit as well as a 0.01 or something it's 120 no not 20 i say 200 frames a second that's maybe too fast not what am i saying 20 frames a second okay going back to weights and biases this is a little discouraging it's like degrading and performance all of a sudden but it's also relying more and more on the model now so it should maybe continue to increase we can still run a test though so we've been saving the model hopefully here now we can go buy modified time god i hate gnome so much move this whatever so here's one that we can load that i think is,358,0,0,WHRQUZrxxGw
44,recent i realize it wasn't clearing this directory between runs so this might have some crud from past runs okay so hard oh oh i'm gonna be lazy again i'd say testicles true just hear and then give it the checkpoint so models there yeah 191 so it is kind of biased kind of pushing it to the to the side so yeah like it's keeping it balanced it's getting high score but it does keep kind of pushing it to the side it's not very stable progress started learn let's see i don't think the spike really meant a lot i mean i was still a combination of randomness in the model but we can try to find the weights around that time so this was this was about two hundred thousand so i think this file order this file is probably representative of it yeah it's not that much better it's actually much worse okay all right it's also not it's not actually improving anymore the average award is starting to suffer more and more over time so this is roughly so we got to a point where some stuff started to work right i'm gonna stop the stream here we can run a test with this as well there's like the latest weights yeah it's kind of biased towards going to the left there okay yeah so i'm gonna stop the stream now some of the things of course is one of the things that could potentially help this over time is the there's a few different things one is the epsilon value could be decayed a lot slower slower than what we,358,0,0,WHRQUZrxxGw
45,have now to the the scaling that we did we can we can change it a little bit because lost being this small i think this can potentially impact the gradients that we're seeing so it could it could have drastically still done learning so that's something to check those are the two things that i can think of to tinker with anyways yeah so thanks everybody for joining and especially thanks to the three people that stayed throughout i will try to schedule more follow-up streams i'm going to spend some time sort of off-camera looking through some of the hyper ammeter stuff here and then in the next stream we can sort of start with the discussion of that and talk through potentially more results and then move on to trying to trying to apply this to something like breakout and yeah look at look at sort of some of the functional differences between doing this with a very very simple model versus a relatively complex one so thanks again everybody for joining and until next time bye,234,0,0,WHRQUZrxxGw
0,so replay memory replay memory class is designed to manage agents memory of the game experiences it stores the state action reward next state and whether the episode ended done for each step in the game so for each step we store all of these information right so this is one experience a tle which contains a state action reward next state and done okay so at the present state agent takes the step that means agent takes an action based upon this action the agent will be rewarded and then it will land on the next state and this done is to check whether the episode has ended so again from the present state take an action based upon the action agent will be rewarded it could be positive reward or negative reward then agent lends to the next state and this flag is to check whether the current step has ended the episode so during the game we store all of these experiences an agent will learn from these experiences so for example let's say we have a snake here and there is a apple here and when a snake eats this apple we give a positive reward to the snake let's say plus 10 so this is one experience and let's say this is a wall and snake hits the wall and dies we give the negative reward let's say minus 10 and the episode is done because the game is over so this will be true so this is another experience so this is one positive and one negative experience we'll store these experiences in memory and snake will learn from this,358,0,0,rBWkcysP4NI
1,experience for example next time there is an apple next to a snake a snake will know that it will get the positive reward by eting this apple and if there is a wall next to snake a snake will try to avoid the wall because from the past experience the snake will know that it will get the negative reward if it hits the wall okay so we store all of such experiences in memory and agent will be trained based on these experiences so let's create a replay memory class let's have a class for replay memory let's create a constructor first let's have the device so let's understand this this determines whether to use gpu or cpu for computation okay why because leveraging a gpu significantly accelerates training when it's available so we'll check if it is available if not will default to cpu next self.,193,0,0,rBWkcysP4NI
2,so if memory exceeds the capacity we delete the oldest experience now let's have the sampling experiences method let's call it as sample okay first we need to randomly select the batch of k experiences okay so experiences experiences and we'll use random do sample we have to select it from the memory and the size is k now we have list of experiences that is list of tles which contains state action reward next states and dce now we have the list of tles and each tle has these five components now we need to separate these components and convert them into pi torch tensor so that it will be compatible with neural network so import numpy okay we don't have numpy so we need to install that so pip install numpy okay so we have it so it is installed okay s np so now let's have the states state torch dot from napai so let's understand this so we have list of experiences experiences are the list of tles and the first component in the dle is the state so e of zero gives the state and this gives the list of all states now we vertically stack these states using v stack function so this creates a structured batch of states that can be fed into neural network now we have this stacked nump array and we'll convert this into p torch tensor using torch dot from numai now this is necessary because p torch neural networks operates only on tensors then we'll use this float method to convert integers to float because neural network typically expects floating points and at the,358,6,6,rBWkcysP4NI
3,end two device this moves the tensors to appropriate device if gpu is available then we'll use gpu else will default to cpu so this is slightly complex so let's understand this again with an example because this is very important okay so we have the list of experiences so this is a list so let's say these are the list of experiences so we have states action reward next state and done okay so here we can see that states are 1 2 5 6 and 9 10 in different experiences first we'll extract these states e of zer so that's the first component in the tuple so that will give us the list of states 1 2 5 6 and 9 10 then we'll use v stack function to stack them vertically so if you notice we stack them vertically in one nump array then we convert them into p torch tensors then cast to float then we'll move to device gpu if it is available else cpu by default okay so let's go back to our id we are going to repeat this process for all other components so we have five components so take actions rewards next states and d so actions is e1 reward is e2 next state is e3 and ds are e4 if you notice the fifth component that is done is a boolean not integer so we need to convert to integer before we call this float method so let's do that so we'll call s type so this method converts data to unsigned 8bit integer so this will convert true to integer value one and false to,358,6,6,rBWkcysP4NI
4,integer value zero so we have made a small mistake here so this is a list okay and now we'll return states actions rewards now we are done with the sample method so let's summarize so first we randomly select the k experiences from the memory then for each component we'll extract it from the sample experiences stack them into 2d numpy array then convert them into p torch tensors then ensures that the tensor uses the floating point precision then moves the tensor to the correct device gpu or cpu so at the end we'll return states actions rewards next states and ds so current states take the actions get the rewards move it to the next states and dance okay let's run it again okay so another error nd array no attribute as type this line okay there's a typo here this should be s type np another typo this should be u okay let's delete the model okay and another uh issue q expected local network action actions dtype int 64 for index so actions so we are getting actions from from here okay action can't be float value so let's change it to log,257,6,6,rBWkcysP4NI
0,what is going on everybody and welcome to a reinforcement learning with stable baselines 3 tutorial series so stable baselines 3 is to reinforcement learning like ssit learn is to general machine learning the idea here is to abstract away some of the more complex and nuances of writing and applying these reinforcement learning algorithms and sort of homologize the entire experience such that once you've created your environment and you can you can essentially just try all these various algorithms really quickly really easily um and just kind of get a lot of the code written for you that you're going to have to write every single time and so at least for me um when i was embarking on a recent series for the uh simulating robots and doing reinforcement learning to try to teach the robot how to walk i did indeed find this very challenging to apply these existing reinforcement learning algorithms to some environment and then if you want to change it or anything like that it becomes just a massive uh massive headache so anyways i eventually stumbled into stable baselines and found it to be unbelievably easy to use so i highly recommend it so with that to use stable baselines 3 you're going to need pytorch that is the back end that they use so you can come here and install pytorch this is py torch.,304,0,0,XbWhJdQgi7E
1,org and you can get started and then also you will need stable baselines 3 which is just a pip install stable das baselines 3 and then the extra stuff i don't actually know if you will need the extra i don't know what's included but grab it anyways so uh once you have that uh let's go ahead and get started so we will be using uh open ai gym as well i believe gym is included when you install it but i don't actually like see anything specific here but you also might need to do a pip install gym uh and then also possibly some of the extras but i think we're just going to be using the 2d environment so a pip install gym might be successful but if not uh check the description so i'll throw something in there there's also a text base right up so if uh if we have any problems i think it'll be fine i think the only thing you'll need here i'll copy and paste this in here you'll need the box 2d so pip 3 installed gym box 2d that's all we're going to use from the gym um and obviously eventually we'll use a custom environment i'll show you guys exactly how to do that uh as well but for now to keep things simple we're going to use the open ai gym environments so uh before we get too deep into that i could spend a whole bunch of time on all the theoreticals of reinforcement learning i'm not going to do that i think the best way to learn reinforcement learning,358,1,1,XbWhJdQgi7E
2,is to jump in and if you find an algorithm that works really well um then you can dive in and learn more about that algorithm but i think you're going to learn by using much better so um so i won't get too deep into the weeds but what i will say is we are going to cover some terms that you must know before you get into this so first of all uh we have the environment this should be fairly obvious but it's what you're trying to solve so c poole lunar lander some other custom environment if you're trying to make some ai play a video game the game is the environment then you're going to have model so this is just what basically what algorithm are you using it's your specific trained algorithm is your model then you have agent this is the thing that's interacting with the environment so and using that algorithm or model to do so then you'll have observations or states what's the state of the environment this could be like imagery or visuals or it could be other vector information like sensors and stuff like that you have an action these are basically what is your agent going to do in that environment essentially per step and that's another term is just simply the step so when you step an environment you quite literally just take a step in general you pass your action to some step method the environment performs that step and returns a new observation and reward for you to continue in this kind of loop until you're done so if you think,358,1,1,XbWhJdQgi7E
3,"in in terms of like frames per second if if you're playing a game at 30 frames per second then you have 30 steps that you would take per second it can get far more complicated than this a step is just simply progressing in the environment and then finally when it comes to action spaces you have discret or continuous actions you can think of discrete like classifications cartpole for example has a discrete action space you go left or you go right it's nothing in between there's no uh scare uh there's no like continuous value use here it's left or right not somewhat left somewhat right and and so on discreet then you have continuous you can think of continuous like regression it's a range of nearly infinite possibilities the bipedal walker environment is a continuous action space as is much of robotics because you're going to set the servo torque anywhere in a range between let's say in the example of bipedal walker negative 1 and positive one or a sero position it's not always going to be torque typically a continuous environment is harder to learn but sometimes this is a requirement so again in robotics uh it this tends to be more of a continuous type problem even though servos are actually discret with something like 32,000 you know positions uh that's far too many discret positions to pass into your neural network so instead we we treat this as a continuous problem but those can be converted into often a continuous problem anyways can be converted in various ways to a discret problem and and more on that later",358,1,1,XbWhJdQgi7E
4,and and why we want want to do that more on that later too so assuming you all have gy and um stable bas lines 3 installed and pytorch obviously uh let's go ahead and run through some of the basics of gym and these environments and and how things are working so we'll import jy and then m will be jy.,80,1,1,XbWhJdQgi7E
5,make and in this case we're going to make uh lunar lander v2 as in aside i've been coding almost entirely with co-pilot and it's so painful writing things out completely i've been super spoiled um so every time you have an environment before you can actually step in that environment you have to do an m. reset so we'll go ahead and call that and then we can print um sample action and we will do m. action space. sample so we're just going to see an example of what is an example action for this environment and then uh we can also print uh obs observation space shape and we will say uh m. observation space. shape so painful typing things out print uh and we'll do a sample observation so sample observation and we'll make this m. obser observation space do um sample yeah so oh and that's a method cool all right so once we have those things let's go ahead and finally close this off with an m.,224,2,8,XbWhJdQgi7E
6,close and let's go ahead and run that get rid of this cool so sample action you can see here is a zero if we run it again possibly will get something else right so just an example there so your observation space shape is just a flat eight values and as you can see these are the eight values honestly i don't even know what they are but it's just as you get into reinforcement learning this is what i mean by i think the harder part of reinforcement learning is no is less about the algorithm yes you can tweak some of the hyperparameters in your algorithm but in general it's going to come down to uh your your environment your reward mechanism and your observation so like what you feed in to those algorithms those algorithms especially by now are pretty robust algorithms i mean they still take a very long time to train uh in that regard but if it's going to learn it will learn and changing those hyperparameters won't do too much at least you can screw it up but from the baseline values the default values i have found you know i can eek out maybe an extra 5 10 performance possibly by tweaking hyper parameters but for the most part the the actual gains that you're going to make are going to come from algorithm choice but also like reward mechanism and yeah your observation space so anyways keep that in mind so um so yeah so these are just some examples of kind of what we're working with so in this case our observation is just yes,358,9,9,XbWhJdQgi7E
7,this flat vector and then we have an action space of certain values that we can pass so um and this was hopefully the the fact that you know we don't even need to really know what these values are should kind of show you um how uh robust i suppose stable baselines 3 really is because you'll see as as we go to train a model on this environment it's not that stable baselines 3 just knows the lunar lander environment it's that that's what's so nice about stable baselines 3 is if you decide later you want to change something about your observation or your reward mechanism or anything like that you just make the change in your environment you don't have to touch your algorithm at all and that's what makes it it just it makes it it really really helps it saves so much time so let's go ahead and uh see an example of our environment in action so if we go ahead and do four step in range 200 we'll do m.,231,9,9,XbWhJdQgi7E
8,sample so we'll just do a random action here so we'll go ahead and run that and here we have our um agent so it just ran for 200 steps that was actually a really good really good example let's do another one it's not the worst random usually it's like flies around um not bad okay so uh yeah so so we have that also that's like really small let me uh okay that looks better so hopefully we'll be able to actually see what what's going on so i'll kill that hopefully that'll go away cool okay so uh so that's just a random lunar lander now um what we one thing we can do is every time you take a step you actually are getting back an observation a reward whether or not the environment is done and any extra information uh that you care you might care about so let's go ahead and print reward and then i'll change this by minute 5200 just so i could uh uh modify the uh recording so we'll run that and then we can kind of see here so these are just our uh rewards essentially and since it eventually crashed we got a negative 100 but here you can see the rewards or whatever they are and again it it's almost irrelevant as long as we can assume the environment's been been made uh reasonably well which we know the open ai environments have when you go to create your own environment it's completely a different story but we'll get there so uh so now what we want to do is uh train an,358,13,13,XbWhJdQgi7E
9,algorithm on this um this environment so what what are some of the things that we know about lunar lander well the only thing we really need to know to know which algorithms we can work with is is it discreet or continuous and in some respects that can actually get even more complicated especially with like there's discret and then there's multi- discreet so what does that mean so if you have discreet that would mean what we have here which is just discret your actions is you have one action choice per step to make whereas you could have multi discret like so in the case of like a robot for example you're actually trying to uh get servo positions for let's say in my example with doing the the bidd robot dog there's eight individual servos all that need uh predictions to be made so that's a multi- discreete uh problem for example uh if you're using a discrete algorithm anyways so uh so to know what we can use we can come back so to know what we can use we can go to essentially the homepage here go to rl algorithms and they have a lovely little table it almost reminds me of scikit learn initially uh which which algorithm to use it was like this like walkway path thing i have many memories of like revisiting that over and over uh for various projects but anyways you can see here uh what things can we work with so in this case so box you can think of box as uh this is your continuous discreet is uh like the classification,358,13,13,XbWhJdQgi7E
10,and then multi- discreet whether or not that's supported uh and then multi binary um essentially that would that's going to be the same as multi- discreet it's just binary um so uh so yeah so now what we're going to do is we can check out let's just do the first one on the list because we're just going to say okay we want to do discreet cool which ones of the which ones of these which of these algorithms uh can support discreet and we can see there's there's a huge list that we can choose from so we'll just start with a2c to start so so how do we do this so again imagine that this is like it doesn't have to be the lunar lander environment this could be your own custom environment which again we will get to soon um so you have this environment and what you want to do is test one of these algorithms on your environment so to do that uh it's it's just so simple from stable baselines 3 we're going to import and we will first bring in a2c all caps so from here what we want to decide is um essentially once you have your environment we'll go ahead and uh i'll reset there and then we'll come down here and then all we want to do is first define our model so we'll say our model is equal to in this in this case it's a2c uh this is a uh mlp policy so mlp police policy um and we pass env and then verbose we'll set to one for and we'll we'll,358,13,13,XbWhJdQgi7E
11,"cover like tensor board later and stuff but um for now this is pretty much i think i've changed a lot of algorithms and i'm pretty sure i haven't touched this obviously multi-layer perceptron i don't even know what the other policies are but obviously there are other ones i think that's just so it doesn't have to be fully connected i don't know maybe maybe that in a distant video but so far through my personal uses of stable baselines 3 i have not i don't think i've ever changed this line at all i think i'm just editing what algorithm we're using anyways now to train model do learn but boom done so here we specify how many total time steps how did you know that that's what i wanted already interesting double check co-pilot uh total time steps 10,000 will set for now uh so this will train for 10,000 time steps it should go very very quick uh and then we will instead do music um um i kind of want to change this code for 200 steps or do we want to just let it go i'll just let it go we'll say well the correct way to you would normally do this we'll just i'll just write it out episodes we'll set to 10 and then we're going to say so for ep in uh range episodes episodes what do we want to do well rather than fourth step and range we're just going to say um first we'll start with obs equals m.",338,13,13,XbWhJdQgi7E
12,"reset so other than step this is the reset method also return an observation but nothing else there's no reward because you just reset the algorithm there's no info to pass or anything like that so it just will return an observation um so we will grab that observation we're going to set done to be false and then we will just simply while not done we will do this um i'm no longer going to print the reward i don't want to see it also stable baselines 3 will give us a little output of information so no need so i'll go ahead and save that and um i think we can display it in here i think that'll be fine um so let me make sure i did everything somewhat decently i guess we'll find out go ahead and run it so first we'll start training we're already a thousand in th let's make this a little bigger so we can actually see it so this is the length of the episode so how long is the agent surviving sometimes that's a very useful metric to know uh and then episode reward bean this is just us what is our typical average reward per episode at this time step there's other information here like frames per second iteration all this other stuff sometimes this will be important to you sometimes not so after 10,000 steps we can see this is our agent and actually given what i showed you before of the random agent this doesn't look too far off of the random agent but i assure you the random agent is worse than",358,14,14,XbWhJdQgi7E
13,"what we initially saw but as you can see it's just it's not very good so so the next thing that we could do is increase the time steps so we can go for a 100,000 steps instead of just 10,000 steps and if you're wondering about uh saving and loading and all that that is the subject of the very next video but for now uh let's train this for a 100,000 steps okay moment of truth here we go honestly uh that's not looking oh not very good uh at least the the episode reward you know average episode reward did come down but but that's still not that great that's that's not the best that i've ever seen right so yeah coing up um wow it actually interestingly so in this case episode waring and then we come down here it almost it looks like it got even worse over time so about i don't have the full history here unfortunately we will get to tensor board logging uh soon the next tutorial uh but for now as you can see um it was decent at around 70ish th000 steps and then um eventually just kind of went bad so um this is why in the very next tutorial is going to be saving and loading models and saving as you train models because this sort of thing is not uncommon but for just to show you now you've got a2c which is a you know not as robust algorithm um we also have po so literally to try an an entirely different reinforcement learning algorithm you just import it and then we",358,14,14,XbWhJdQgi7E
14,"change it right here and that's it now we use po so again we'll do the exact same thing um i'll just go ahead and run this and we'll run this again for 100,000 steps and we'll see see what kind of difference did we get so we ended here at 48 but at some point we did actually get a reward of like 50 so anyway i'll be back when this is done okay so we have our result and in this case uh again it's almost it's very hard to compare these two algorithms because they both look uh quite bad after only 100,000 steps i mean it's it's better than random but it is comical how my random example worked pretty well and then the train examples in it um in reality we we are training these models for far too few steps so we need to train them longer but as you can see even like when we did the first 10,000 steps well when we decided okay let's try 100,000 steps well we had to retrain the model all over again so what really we want to do is be able to save load and then track performance and all that so that's what we're going to be doing in the next tutorial in this case uh the only standout difference uh one is it trained um maybe 10 faster than the a2c model did um and then also it's staying a lot alive a lot longer on average um but again like i said neither of these models even remotely close to their actual training time so to to put",358,14,14,XbWhJdQgi7E
15,that in perspective you know generally you're going to do millions of steps so i you know 10 to like 100 million steps i can imagine there are some environments where you might even do more than that um but anyways uh yeah so by this point hopefully you have a pretty general general understanding of stable baselines 3 reinforcement learning kind of what are the the main uh you know terminology like observation and actions and steps and all that kind of stuff and in the coming tutorials uh we'll hopefully show you how to actually start using this to train a model and kind of track its progress over time and then eventually how to apply this to a truly a custom environment a custom problem and really get into the the nuts and bolts of what i think makes reinforcement learning hard which is actually engineering your own environment entirely it has really nothing to do with these algorithms if you're interested in learning more about neural networks and how they work then you might want to check out neural networks from scratch a book by myself and daniel kuwa inside of which you'll learn how to code neurons activation functions how to calculate loss do optimization and back propagation and of course apply everything that you've just learned to a problem the book itself is in full color for graphs and code syntax highlighting everything's on high quality pages with full font and a comfortable line spacing to make reading easy finally every concept including the math and coding is broken down into the truly atomic parts and then put back together,358,14,14,XbWhJdQgi7E
16,step by step to give a full understanding of neural networks and how they work including all of the math behind them by the end of the book you will have created your own neural network framework from scratch this is the book that i wish existed when i was learning deep learning and i'm extremely happy to be able to offer it to you if this sounds interesting to you then you can head over to nn fs.,102,14,14,XbWhJdQgi7E
0,what is going on everybody and welcome to part three of the reinforcement learning with stable bass lines 3 tutorials in this video what we're going to be talking about is indeed using custom environments the main crux of using a custom environment involves basically just doing this converting your environment to a gym environment sort of structure but it actually gets far more complicated than that and arguably the hardest part is actually coming up with what will your observation space be and what will your reward be so with open ai gym environments um all that's been it's just been handed to you and um it's actually very simple because um you don't even have to think like what would be a good observation what would be good features to be input into the reinforcement learning algorithm it's just it's just done for you same thing with the reward so what would be an actual good reward so reward and observation are not necessarily as obvious as you might think they might be so uh first we're going to need some sort of environment so if you're wanting to follow along with your own environment that's totally fine you can do that or you can use the environment that we're going to use here and then take what you've learned to your own environment it's really up to you so yeah so the one that we're going to use is just the snake game i really just searched the internet for a snakey game i landed on this one made sure the game at least worked because they don't always work and uh it,358,0,0,uKnjGn8fF70
1,does and i think this would be a fairly simple environment to convert it's also a nice kind of short simple game so it won't take us hopefully too long to convert it to a gym environment so uh with that in mind let's go ahead and first we'll convert this just all to like be like one script so copy paste that and i will copy paste that and then finally one more copy and paste go ahead and save that and uh we don't need to be doing the writing to the d drive i don't even have one cool so we'll go ahead and save that and let's go ahead and also run that so snake game let me just make this nice and big python three snake game dot pi and here we have the game it's w a s and d for control also you could just in theory not press any key and i'll just do whatever the last direction was and the idea here is you're the green thing the snake and you want to get the apple let's see if we kill ourselves no we got it nice so anyways as you get an apple the snake gets longer it becomes harder and harder to actually get the apple so the question is can we create uh you know can we make this an environment and then get a reinforcement learning agent to solve this environment so the first thing that i'm going to do is actually speed up this environment just slightly because that is uh really slow so first of all it's slow because of this,358,0,0,uKnjGn8fF70
2,so i'll make this 0.05 and then also for the weight key i will just set that to be one so coming back over here just to test that make sure that worked and in indeed it does so i'm sorry so it's too fast for me to play but i think our ai will be fine so uh let's go ahead and close out of this so now that we've done that let me i'm just going to move this terminal out of my way i know you don't see it anymore but it's still in my way okay and now what we want to do is convert that to an environment so again we'll come back to here and i'm just going to copy and paste this there is a text based version of this tutorial so if for whatever reason if i'm copying something and you don't know where it's coming from or whatever uh you can copy and paste from there also if if anything you know fails you in the conversion uh i'll have a text-based version really what i'm trying to get you to grasp here is what are the steps required uh to really convert an environment and then the subsequent video will almost certainly be okay well now how do we tweak and think start thinking about observations and rewards so anyways that's really what i'm trying to get across here rather than you know you need to follow along exactly so i'm going to copy this and one thing to think about is i keep meaning to submit a pr someone can feel free to submit one,358,0,0,uKnjGn8fF70
3,i don't really care everything else on stable baselines three is a four space tab indention for some reason this little script here is two spaces uh i wish they didn't do that if anybody from sp3 is watching just fix that for me real quick it's been very tedious copying from there whoops i don't want to do that i want to come over here uh so snake env and we'll just paste that in so now what we want to do is we want to convert um basically everything from snake game over here to m to do that uh i kind of just want to like cut and paste so i'm going to just do this copy come over here paste and i think i really will just cut that way i know that i've left something or not um actually i guess it doesn't really matter i don't know i can't really decide what i want to do there um maybe i won't cut so we definitely need those helper functions and all that now coming down into and we'll just leave those outside of the actual object here for the environment we'll call it uh i don't know we'll call it snek env and when we go to super we will also super snekem for now we won't pass any parameters to the init method we there are some that we might wind up doing later for now i'll just leave it empty um discrete actions we already know this will be four and then for observation space we actually do not know at this time uh what will the observation,358,0,0,uKnjGn8fF70
4,be we haven't thought of that yet so we will get there uh we'll get through when we get there which will be pretty soon um so the next thing i want to do is just address the next logical step which is as soon as you initialize your environment really all we need to do in the initialization of the like gym environment object is set the action space in the observation space the next thing that gets called is actually the reset so before you can start stepping in an in an environment you need to call the reset method for this reason i would actually kind of think of the reset method as your initialization method for the actual environment itself so not the gym enviro not the gym object for your environment right that's this method but in this reset method that's probably where you initialize all your other stuff so and also i'm going to get rid of close and render close would be if you need your environment to be cleanly closable you might throw something in there and then render i don't really care about that i'm going to render either way so i'm just i'm just going to get rid of that so we're literally going to render everything in in step anyways so i just don't care so um so logically the next thing that happens is the reset method so we're going to tackle that one next in fact i kind of am noticing the screen is a little or the text is a little small so we'll zoom in a wee bit it's probably a,358,0,0,uKnjGn8fF70
5,little absurd but there you go you cell phone watchers uh so we'll come over to snake game and basically the initialization for sure is like this stuff right before the tr like while true that is in theory like i was saying before you can think of step as like your frames so while true the frame is resetting every single time so at least in this case basically it's this information here that will be our the initialization of our environment and in our case will be the reset method so we'll come down here and i just told you about the two-space indentation and uh i never fixed it shoot i can't remember in vs code if i can just can i get away with like that no no not quite there's some way to like do it all in one fell swoop someone will comment below i'm sure anyway this is good enough i think right or did we just oh my gosh am i am i that i don't know what i did oh my gosh okay we're good i think okay anyway so i'm going to paste this code in here we're going to tab this over and really at this point because the game or the environment that we're attempting to convert here was not already in like object oriented programming uh syntax uh it's going to be a lot of self dot editions um so you know yay so so here we go um so we'll start off with uh self.reset uh i think also even though this says done info can't be included it's really just about,358,0,0,uKnjGn8fF70
6,the return we could still in theory calculate a reward done and info if we really wanted to and one of the things that we're going to do is we are going to set self.done to false because every time right before like as you iterate over your over your episode when an environment is done you will do it a reset so when we do a reset what are the things that we want to reset well we definitely want done to be false so now what i'm going to do is copy self dot and uh yeah i guess we're just gonna do a lot of self dots here yay fun awesome i love it my favorite thing to do uh and then we'll make that self.observation um even though we haven't defined an observation yet i understand um okay i think we're good so now we need to talk about the observation so what ought the observation be so my proposal for an observation let me zoom in a little more i think we want to know where is the head of the snake right we want to know that so we would want like head x and head y then we would want to know where is and head x would be this and head y would be this and i might have those flipped because we're doing everything's in like a grid with what appears to be numpy and opencv i think y and x and y are like actually flipped so like this is probably the y and this is the x um so if you want to comment below,358,0,0,uKnjGn8fF70
7,furiously that i've got it wrong that's okay we could just call it head a and head b because really all that matters is we have even if it's flipped as long as everything was flipped we'll be fine so so i acknowledge that it actually might be flipped but anyways head x head y we wanted to then know the apple x apple y apple y um and uh then we also want to know the snake length snake length and then finally we probably want something about previous moves so um so the snake needs to be aware of its body and you might be thinking well why aren't we just passing the entire um uh you know this basically why aren't we just saying the observation is quite literally self.image right and you can do that feel free to do that if you really want to do that but my argument here is that's a lot of noise so in general with machine learning you want to whittle down the noise as much as possible and passing that entire image is just mostly noise right it's almost all like black space and to be honest i don't know that there's any other than being easier to code and like less things that you need to think about i don't see the benefit there so instead what we're going to do is just do some feature engineering where we just want to know where's the head of the snake where's the apple and then how long is the snake and then what were the snakes like previous moves and my expectation is that the,358,0,0,uKnjGn8fF70
8,ai could extrapolate the previous moves basically extrapolate you know where is the snake's head how long is the snake and what were the previous moves and then it would know like it could know where the rest of the tail is essentially um i think that that is suddenly the most complex part of this entire thing for the agent to learn and maybe it never learns that maybe instead it just simply learns to take certain actions that don't result in death and that's fine and if you want to come up with something else that's totally fine too just obviously the observation needs to be a fixed size so you couldn't you can't really make this like where is each set like what's the location of segment two in segment three instead i mean you sort of could like if it didn't exist you could say it's a negative one or something like that um i don't know like i said i'm just trying to show you guys how to make your own custom environment but yes i acknowledge that there's probably better ways to do this so now uh with that in mind let's go ahead and um start defining basically what what all these are so head x will be self.snakehead zero and we'll make head y this would be so much easier if with copilot on i kind of i turned co-pilot off um because i think it would be detrimental to uh tutorial videos but um comment below if you have an opinion either way because it would make things like so much faster if i could just leave,358,0,0,uKnjGn8fF70
9,copilot on like stuff like this is so tedious so we've got head x head y now we need to get the apple x and apple y um and actually i kind of want to make this apple delta delta x and y i'm trying to decide which would make more sense to make it a delta or like the actual position i don't know that it would really matter if it's a delta or if it's not a delta then i think the agent would just have to do the calculation right um and again this is just back to my original point that you know coming up with the observation or yeah the observation on the reward is kind of the challenging part but anyways we'll call it apple delta x and we'll say that's head x minus apple didn't we have apple something apple position that's actually self.apple position hello okay thank you make sure it's not running over yeah okay um something happen position zero and then again i wish i had copilot right now uh y we'll make this a one we make this a y and we make this a y fantastic so okay so we've got the deltas now we need snake length so uh snake length and that will be quite literally the length of snake position so snake position contains like all these uh like two you know just the coordinate positions for each of the little snake squares so as we get more of them this just gets longer and longer so really we just want to know what is the len of self.snake position len,358,0,0,uKnjGn8fF70
10,of self dot snake position okay so once we have that uh we do okay we reset the screen we have these so this is basically our observation um up to this point except for we really want to have the previous actions so self.prev actions will be equal to we need to import a deck subject deck and then max len will be equal to um i don't know snake len goal okay we have a lot of things that we need to hurry up and not forget about so we'll come to the tippity top from collections i'm going to import deck and then i'm just going to define this as 30 for now um so again because of how we need we need this to be not dynamic we need it to be affixed so we're just going to say hey we want to it we want to shoot for a snake to be up to 30 little units long um i'll call it a win if we make it that far um so coming back down here okay so now again this is our in it so we need this deck to actually be filled completely before we get going so what we're going to say is for underscore in range uh snake len goal we are just going to populate self self.prev actions with a negative one so it just populates with negative one which will hopefully be learned by the agent that that means nothing so our actual usable actions will be zero one two or three okay so now that we've done that we can actually create the observation,358,0,0,uKnjGn8fF70
11,so we're gonna say self.observation is going to be equal to and we'll make it a list for now head x head y apple delta x apple delta y um snake length right snake length and let's go ahead and fix this typo length like that and then finally we will just plus and add the list of prev actions awesome um and actually it's self.pre-reactions because we do need to continue to be able to reference that so let me come back over here self.preve actions cool okay so that's our observation let me zoom out a little bit just so just in case anybody missed anything okay i think it's good enough so now what we want to do is get to the stepping part so for the actual stepping we'll come back up here i'm going to get rid of this and make some space there make sure that is still self.observation i'm gonna go ahead because i i'm pretty confident i'm gonna forget about this so we'll add self dot self dot uh self.done yeah uh info because i'll probably forget about that as well we'll just make that a empty dictionary save that and now we're ready to step so every step what happens essentially all of this code this is all step code right here so so i am going to do i want that wait key 0 i know i don't want that break i'm not sure about that weight key 0.,323,0,0,uKnjGn8fF70
12,we're going to copy everything for now uh so copy that come over to snake enve and we will paste and then i need to tab everything over twice apparently yes okay all right no why did that why did that get tabbed over i don't think i did that i'm not sure what happened i swear uh vs code does stuff like that to me all the time anyway okay now um i can't really decide if i want to run through here first and do all the self dots or i don't really know what i want to do uh either way it's going to be painful we're going to definitely forget some self dots okay self.image so we show the image we wait keyone self.image again cool cool everything's good there rectangle self.image apple position gotta be a self.apple position every single time again if you don't want to do this uh feel free to just wait hit copy and paste it self.snake position cool we don't need to this is just a temporary variable but this does boom cool cool got it got it got it coming down here um all right in this case uh what do we want to say um i actually think that we don't need any of this code because this is just determining what is our button direction well we don't really need that and in fact we don't even need this it just should be action if action is one we do this thing if action is zero why do they go that way that's interesting anyways action action not my code uh coming back,358,1,1,uKnjGn8fF70
13,up here just to grab a self dot real quick copy copy come down here okay self.snakehead self self self cool uh increased snake length if it ate an apple great we're super happy if that happened awesome apple position and self.score equals this and again boom boom so many self dots i wish there was like some sort of ai like in github co-pilot that just knew and it just added all the self dots that would be awesome cool coming down here self increase so if it didn't okay self.,119,1,1,uKnjGn8fF70
14,self dot self dot oh so tiring okay collision snake head cool um i'm getting ahead of myself i'm back thinking back about this weight key i don't think i want that there um okay so if we collided what do we do we're going to say font yes fine uh we are going to reset self.image uh put text self.image format self.score show self.image and we are not going to do this and this is a collision so if this happens we do want to say self.done equals through um all right i'm sure i've messed something up along the way that so many self.dones or self dots rather i can't even think straight so okay now that we've done that we just need to calculate two more things one is the observation one is the reward for the observation i'm quite literally going to just copy everything we did right here boom very simple um so that's done so we have our observation i believe now we need to calculate the reward now in the case of the reward again this might seem like oh my gosh that's so simple the reward should just be what is the length of the snake or the score or whatever i don't even i can't where was he calculating score anyways um is it just at the very end uh where does he calculate score self.score so when is self.score defined okay sorry music okay so it's only updated at the apple point so we could say the reward is the self.score um and just try that is so is that the first definition of self.score okay,358,2,2,uKnjGn8fF70
15,so he starts at zero self.score is zero zero zero so you could just wait um okay so here's what we'll do we'll say where am i so reset sometimes so we'll just come i guess like right here so let's let's create the reward so if if self.done uh we're going to say the reward is and actually it should be i think self.reward i think we threw self in front of that self.reward equals negative 10.,101,2,2,uKnjGn8fF70
16,so if the snake dies for whatever reason we'll say it's a negative 10 and then else hello else self.reward c word reward equals um i guess we could just say self.score right so it should be like hm apple's eaten okay i think i think we are ready-ish for a moment of truth so uh the next step is coming over to using custom environments um stable baselines three has given us a sort of check m functionality so we'll come over to checkout.pi paste that in uh custom m is snekm snack m no parameters were passed and we will from snake m import snack m save that um oops i think we're ready to see how many times oh and also my snake my other snake game is apparently still up back already i'm not sure what the heck just happened uh check m check m dot pi um not positive uh do we remove self from the init method maybe i can't think of what else would have happened yeah okay i guess that's alrighty i'd love it if that was the only error please no oh right we never revisited um this i knew that was gonna happen okay so uh coming back over so uh we we knew our discrete spaces will just be four uh the observation space uh in theory the delta so head x head y could be anywhere from zero to 500 uh delta could be anywhere between music zero what that what could that be it could be up to negative 500.,341,3,3,uKnjGn8fF70
17,um so in that case we need this oh i got lost low was a low negative 500 high positive 500 i don't actually know uh if these values serve any purpose other than just for like sanity checks so you might even be totally fine and safe to just say low is negative np.if high positive np.m that might be good enough i don't know um and then shape we actually just have a vector here of values so it should be um it'll be what's uh it'll be essentially the snake length goal plus however many other things we have so one two three four five so five plus snek len goal so five five plus snake line goals that's our observation um also do not forget don't forget your little comma right here right this is for the shape so you can't just put like a single ink in there it's a tuple add that comma so next data type uint it's definitely not uint i think our data is acceptably an intate but i'm actually just going to say mp.float32 so i don't have to actually go through and convert all those because i think that's the default data type for all numpy arrays um feel free to change that if you want i don't care um i just want this to run i'm just trying to show the most basic i can so we'll run that please collections oh dot append so we didn't do a an append there on line 147 hey we made it a long way so far i'm okay self.previous actions don't append although we only made,358,4,4,uKnjGn8fF70
18,it to the reset but we made it pretty far in the reset so hey that's boating well the observation returned must be a numpy array we never converted it okie dokie self dot observation equals np dot numpy.array self.observation i was literally just talking about that and acknowledging that it's a numpy array and then i forgot to do that awesome so i'm just converting both in the reset and the step method um to make sure it is indeed a numpy array oh we made it so far actually what brave did i copy that what did i do there oh dear oh dear how did i make this mistake so okay first of all we did not append the action so we definitely need to say uh self.preview actions dot append action and then coming down here i've got no good explanation for why and how i left that in actions because at least in my head i guess this was all like observation stuff i didn't recognize that okay let me make sure this is what i want uh music okay yeah so we'll just delete that so we really only wanted that line to occur that's a good thing good thing i forgot the append i would that dude we would have left that in there that would have been a nasty bug i would never have caught that somebody would have told me i'm sure okay so quick check em uh it does appear that things are indeed working it's it's so quick when you run check em so the next thing is sometimes i've missed certain things by just,358,4,4,uKnjGn8fF70
19,simply running checkm like checkm can't actually check like literally everything in your environment some things are just need some logic so i would recommend you also i just made another script called literally double checkm and all it does is just 50 episodes it runs through and does a random action just so you can literally see the actions and rewards and make sure everything is kind of like what you're expecting it to be um and so i would also just recommend this again i would just go to the text based version and grab it there's nothing fancy here um python python three double check m yeah i mean that looks good uh the reward negative i mean he never actually gets the apple so um shouldn't it have a reward of of zero at some point though let me think here uh like i feel like the reward was always negative 10.,202,4,4,uKnjGn8fF70
20,so reset self.score gets reset we're going to also just reset reward equals zero how come um and this is what i mean this is why i think it's important to run it again uh y is self.done equals true self.reward is negative 10 otherwise i think this should be zero right i mean i think that should be zero every time i'm not sure what the wild true is while not done right shouldn't that be let's run that okay so it resets reward zero zero zero zero negative ten boom cool okay so high punishment for death um probably a pretty low punishment for the actual uh score i might even suggest you know before we even waste any time is um maybe self.score because score would is just going to be a plus one so i might even say self.reward is self.score uh times 10.,192,5,5,uKnjGn8fF70
21,so at zero it's nothing but if we did ever get an apple uh it would be rewarded relatively decently okay now what i want to do is come over to where we've been working let me make that maybe a little bigger oh i wish i could make it even bigger but okay and basically what i'm gonna do is take part the part two video i'm just gonna copy and paste that and we'll make this uh p4 snack learn whatever um we just mostly want that code so we're going to open up vds here we don't need that we aren't going to use a2c we will use ppo so i'll actually copy ppo that will be the model that we use our m will be double check em doo doo doo snackin so actually we'll do this uh come over to snake learn paste cut that paste that wonderful this will be ppo uh and in fact music we're gonna make the model during the time that time so that's fine um we don't need any of this uh this should be ppo probably i don't actually we're always gonna use ppo so i'm not gonna give it it'll just be time.time um we definitely want to do more than this so i'll just say i'm just going to say wow true and just let this go for a while so wow true we do that um actually no we kind of want that i huh um we could just do a counter or i can do this okay i think that's good so now what was this p force neck learn,358,6,6,uKnjGn8fF70
22,python 3 p4 snek learn make sure that actually begins something okay i think it's off okay i'll just leave this stuff up let it record for a while hopefully this will update i'll check on it in a little bit i don't really expect the greatest results but we will see um how it does anyways so uh i will check back with you guys in a bit all right so i've left this running for quite some time about 41 000 steps into training uh the agent has improved uh it's not an incredible agent but you know we haven't done that many steps yet but really we probably could tweak some things and make pretty drastic improvements here so checking the results the episode length has increased quite a bit and that's no surprise because there's no punishment for staying alive and there's a minus 10 for death so that makes sense that this has worked out um and then our episode reward has been slowly but steadily improving um again it's possible that with more time more episodes we could actually see even more improvement and this might even continue trending up at least right now it does look like an upward trend so that's pretty cool now coming over to the code i already forgot did we do a times 10 we did on the score so this is with the times 10 so we've already you know tried to reward it heavily for eating apples but it doesn't seem to matter so so so i think what we need to do is give it a little bit more help to,358,6,6,uKnjGn8fF70
23,actually get to the apple so that's what we're going to be focused on in the next video this video's idea and the whole point was to at least get us started with a custom environment and then yeah in the next video we'll talk about okay well how can we do better feature engineering maybe change the reward a little bit how can we get this agent to hopefully learn faster at least be better than what it is right now in 40 000 steps so that's all for now if you have questions comments concerns whatever you know the deal feel free to leave them below if you've got uh if you want any of this code like you didn't feel like copying and pasting all those self dots no problem there is the text based version of this tutorial it is linked in the description so anyways that's all for now and i will see you all in another video you,213,6,6,uKnjGn8fF70
0,stable baselines 3 comes with ready to use reinforcement learning algorithms of the box but as beginners how do we know which one to use that's what we're going to talk about today i'm on the staple baselines 3 homepage on the left here if i click on rl algorithms it'll give me a list of all the available algorithms to choose from this is quite a list let's see how we can narrow this down i took a screenshot of this table and brought it over to my whiteboard so i can draw on it now all these ones with a superscript of one on it it's referencing this footnote let me hop back to the website i'm going to click on this link here s sp3 contribute this is actually not part of the stable baselines 3 library it's more like an extension it implements more of the experimental code down here you can check out why the motivation for this repository but for now we can disregard it i'm going to cross these out okay that brings our list down to half now herer let me hop back i'm going to click on herer right here h stands for hindsight experience replay it's actually not a learning algorithm we're supposed to combine it with a another algorithm it's more of an enhancement to the other algorithm so by itself it doesn't do anything we can take it out and maybe check it out in the future okay now we're left with six to further narrow this down we need to look at what environment we want to train i'm on the genium website,358,0,0,2AFl-iWGQzc
1,i'm looking at the basic uh aquabot the goal of the agent is to swing around and uh figure out how to get itself up past this line once it gets up there this environment is considered solved now the action space is what we are concerned with let me put this side by side you see this note right here certain algorithms will only work with certain types of action space so over here we see that the acrobot has a discrete action space of three so i'm expecting uh values of 0 1 and two let's see what those numbers mean 0 means to apply torque to the left one means to do nothing two means to apply torque to the right so with the discrete action space we're looking at the second column here ac2 works with discrete so does dqn and po we have these three to choose from okay what do we do now i found this nice diagram i found this nice diagram that shows the category and lineage of all the reinforcement learning algorithms i'll put a link in the description to this diagram let me show the whole thing okay we're mainly concerned with the light blue box all the algorithms in sp3 has to do with the model free algorithms and just to note i have videos on q learning deep q network and reinforce if you check those out you'll get a good idea of what the uh difference between this path and this path reinforce is very basic so it's it's not even implemented in sp3 also remember h eer you can see that we,358,0,0,2AFl-iWGQzc
2,can combine h r with dqn and ddpg okay let me eras all this stuff okay let me put this side by side like this we're down to dq and ac2 po so out of those three how do we further narrow down let me zoom out and zoom in down here if you look at down here it's actually a uh timeline of when those algorithms were introduced okay we can see that in 2017 2017 is around when pop and ac2 was introduced we can reasonably assume that po and ac2 are probably better than dqn maybe not always but this is an assumption that we can go on so without really knowing the details of how po and ac2 work i would probably try both of these on the aquabot environment okay let's do another one really quickly i'm going to go down to pendulum somewhat similar to aquabot the idea is for the agent to swing itself up vertically and balance itself there the act space is of box type which means uh we get one number between -2 and 2 basically we get a floating point number between -2 and 2 which means that the action space is infinite versus the discrete space of acobot which was only three possibilities okay so for action space of box we look at the first column we can do acor critic ddpg ppo quite a lot to choose from everything except dqn now going over to our diagram we can see that newer algorithm is td3 and sa those are both newer than ppo and h2 and much newer than the dpg so we,358,0,0,2AFl-iWGQzc
3,should probably try these two algorithms first to see if it can solve the environment now in the next video we'll see how to use multiple algorithms to train the agent and if i'm done with that video it's going to pop up right about now be sure to check out my get started with stable bas lines 3 video i'll catch you next time,84,0,0,2AFl-iWGQzc
0,"hey welcome back so have you ever run reinforcement learning using for example stable baseline 3 on some this doom scenario and like here are your graphs right like they're different and some of them don't learn right here you come across this if you come across these graphs you want them to look more like this right these graphs are on the exact same scenario and the only thing that's changed between these two graphs is a single hyper parameter and there's no cherry picking here i just ran these a bunch of times um so what's going on here so let's take the save file from the end of training of this run here and we'll open it up and run it all right so we can see straight away what's happening the model is doing this local optimum thing where it like it always goes right and if the monster happens to be in front of it it'll shoot the monster like half the time the monster's on the right and it'll shoot it like this but as soon as the moner's on the left like this then it gets stuck and this is after like a 100,000 steps and it it's not improving so it's basically not exploring it's just like i'm just going to go right cuz that sort of works for me half the time it's like i always have to go right so we need to make it explore basically so how do we make it explore and so i'm using stable based 3 i'm using po and how do we make that po explore so po out of",358,0,0,1ppslywmIPs
1,the box doesn't necessarily explore very well but we can make it explore using entropy regularization which if you've watched my videos before you'll know i'm kind of a fan regularization and the only thing that's different between these graphs and these graphs is entropy regularization okay uh so how do we do entropy regularization in po so here's the po constructor right and we can see we've got n cfal 0.0 not very documented kind of mysterious uh if we scroll down it says entropy coefficient for the loss calculation it's pretty mysterious but we can look into the source code so let's look into the source code all right so this is the source code for the po algorithm so you just go to like stale baselines 3 ppo p.y yeah so we can see the loss is made of the policy loss which is basically the well the policy loss like the equivalent of the reinforced loss uh plus the value loss to encourage the value functions learn and then we've got the entropy coefficient times the entropy loss now where does the entropy loss come from well it comes from this term uh it's the mean of entropy and that comes where does that come from that comes from here self.,279,0,0,1ppslywmIPs
2,"pi and we go down to the actor critic here actor critic policy like this is what's used by ppo um and then if we scroll down to evaluate actions right and we can see that it's basically calculating the entropy from the distribution uh if we borrow it down through this little rabbit whole we're going to find that this is is the entropy just like what we were calculating before in my previous videos right so that's how we can get entropy regularization in po now is entropy regularization useful in po well actually it's not quite as useful as i found it was so i found it essential with reinforce uh with po it's sort of useful but it's less essential but i did a lot of experiments i ran a bunch of experiments so let's have a look so these experiments is on the basic viz doom viz doom basic scenario we can vary the entropy regularization here so basically this just control this just is a selector for the names right and the way i named these is so 0 underscore 0 uncore this is 0.0 this is there's no entty regularization so uh we're running over 100,000 ep uh steps and then this is the reward from -200 to 60 and we can can see that sometimes it learns really really well but like once or twice it's learning less well now this is like my best result when we go to uh defend the center the entropy regularization is less necessary it just gives a very slight boost but i'll show you my results right so in basic scenario",358,3,3,1ppslywmIPs
3,without any any entprise entropy regularization sometimes it works and sometimes it doesn't uh now i can tried um so this is 0 point if you make this underscore point right so this is 0.00001 so this is basically almost the same as having zero so entry reg regularization of 0.001 almost the same as no in regularization a lot of the time it works and then a couple of times it doesn't and then we can increase to 0.001 it's looking a little bit better but we still got like one where i didn't really learn like but compared to like one where it didn't really learn one where it it it was kind of late but it got there uh if we get up to 0.01 now they're all learning nicely there's a couple of like bits where they go down but generally li uh beyond this it's possible to make the enty regularization too high so reminder with the enty regularization what it does is it forces the agent to explore what it does is it encourages the distribution over the actions to be more equal so without entp regularization the probability might be like oh i'm always going to fire in this situation so it always fires and it never thinks about trying anything else it'll just always fire so it doesn't try things it doesn't explore it's just like oh i fire i get some points that's great right now the entp regulation forces it to add some probability to mass to the other action so like oh so i really want to fire but you know i'm just going to,358,3,3,1ppslywmIPs
4,write go right anyway or i really want to fire but you know this time i'm just going to go left right it forces it to try some of the other options but if make the ent reg regularization too high it'll just be random it's like the original random model where it's always random if it's always random of course it's just going to be rubbish right so we can't make it too high anyway so if we go to like 01 actually that's not too high yeah like it's still getting a decent reward of 60 like 0.1 is is okay it's it's actually pretty good actually if we go up to like 1.0 so this is like make the underscore one right sorry make the underscore dot so this is 1.0 so now the rewards gone down to zero right so 1.0 is too high like if we compare that to uh 0.1 right so this reward goes up to 60 or 70 but 1.0 this is too high and then we actually end up to 10 and then it's uh got like a negative reward so that's obviously far too high um yeah so basically on this basic scenario if we use an ent regular ation of about 0.01 maybe 0.1 that gives pretty decent results um 0.001 is a bit too small and 0.001 is just like no enty regularization so this is basic this is the best set of results i got uh i also went to defend the center i i ran a lot of experiments on defend the center we get slightly weaker results we don't get this,358,3,3,1ppslywmIPs
5,"random stuff we always learn so let's look at defend the center so this is defend the center uh let's start with um no enty regularization so i did two sets of experience and this one we're going up to 100,000 steps uh this is no ent regularization and we can see that they all succeed in learning and they get up to somewhere between about seven reward and 10 reward like 10 seven right now so what happens is in the defend the center it seems like the scenario is sufficiently random that the the agent naturally tries things anyway i didn't find that in reinforce in reinforce i needed entp regularization even for defend center in fact especially for defend center like it was just my agent in bo reinforc was just firing front of all the time because there was like a a monster spawning in front of it like oh if i just fire all the time i'm going to make some points uh it died really quickly so it never made many points but it got a couple of points each time uh with the po po more powerful algorithm um so it's actually getting between seven and and 10 points now let's throw in some so this is 0.001 basically in distinguishable from no entry regation we got to 0.001 so it basically it's it's learning just the same but it gets a bit higher right so instead of being from 7 to 10 then our reward is like between just below eight and nearly 14 right so nearly 14 compared to 10 it's quite a lot higher um 0.01",358,3,3,1ppslywmIPs
6,"so basically in for defend the center like 0.0 to1 seems to be like the optimum and the exact end to be regularization it's sort of like an empirical question all right if we got to 0.01 that's too high and 0.1 is way too high i think i actually went up to 1.0 i did yeah um so for defend the center 0.0 to1 is good but uh this is kind of my best result for defend the center cu i did some others experiments so in these i went up to 100,000 i was like well what if we went up to 200,000 like so let's try that so basically when we go up to 200,000 it doesn't make that much difference all right so let's start with no ent regularization so that's this so with no ent regularization it ends up between 8 and 9 it's actually worse than after 100,000 it sort of decays somehow uh let's go to 0.001 uh so this is like interesting between 8 and 10 go up to 0.001 so now it's like a little bit better like it sort of peaks very quickly here it's getting up to 30 but then it dropped down so by the time we gone for 200,000 it's less clear that we need the enty regularization but it helps a little uh 0.01 so i think the best result here was 0.001 which i think is a better result than like 0.00001 um but it's less strong than when we stopped at 100,000 so basically it looks like for the po the np regularization is less necessary than for the reinforce",358,3,3,1ppslywmIPs
7,it's less necessary if you're going to train for a really long time like it seems like mostly uh the p will eventually learn but sometimes it will take longer uh but it it could be something handy to add in as far as like what is ent top regular how is it calculated i've got another video where i go into like how to calculate it um and so we've already gone through how to do it in po it's really easy you just put in u yeah you just put in anf yeah cool all right so i hope uh this was interesting useful um please add into comments like things you want to see things you want me to try experiments you want to run and um yeah i hope if you got this far thanks and i hope to see you in another video for,193,3,3,1ppslywmIPs
0,in the last few videos we implemented the q learning algorithm on a few basic gym environments as the environments get more complex we need to apply more sophisticated reinforcement learning algorithms in order to solve them today we'll use the stable baseline 3 library to train this humanoid to walk for this tutorial we need two libraries if you haven't installed the gymnasium library you can go to the gymnasium website and copy this install command the other library that we need is stable baseline 3 what you'll need is the pip install stable baseline 3 but make sure you add the extra here the extra here will install tensorboard and we'll take a look at what that is in a bit let's take a look at our code in the first line i'm importing the gymnasium library then i'm importing three reinforcement learning algorithms from the stable baseline 3 library that includes the soft actor critic okay the second one is going to be a long one it's called twin delayed deep deterministic policy gradient wow then we have the advantage actor critic algorithm i'm also importing some out of the box python libraries here we need a models directory and a log directory the whole hour training models and training logs if those two directories do not exist then we'll create it i have two functions one for training the model and one for testing the model in the training function i'm passing in the gym environment as well as the algorithm that i want to run depending on the algorithm that i pass in i'm creating my stable baseline model using the,358,0,0,OqvXHi_QtT0
1,selected algorithm now there are more than three to choose from if we go to the stable baseline documentation on the left side click on rl algorithms we can see a list of all the implemented ones here for the tutorial i just selected three to train with there's a better way to create the model in a more dynamic way rather than just using this match case statement but i think listing them all out is a little bit easier to read for the tutorial when we declare the model the first parameter is the type of neural network that we're going to use the default one mlp multi-layer perceptron is the one that we're going to use the other one cnn convolutional neural network is more for image recognition type of stuff so we'll stick with mlp for the second parameter we're passing in the gym environment we're turning on for both so that we can see some printouts once in a while if you have a nvidia graphics card you can pass in cuda to do the training on your gpu if you don't have any video card just pass in cpu finally we're passing in the log directory and we'll see what tensorboard is in a moment now that we have our model we can start training we can start training by calling model dot learn we're basically going to train indefinitely until we're happy with the results the first parameter of the learn function is the number of steps to train one step is one action performed by the ai we're basically telling the model to train 25 000 steps and,358,0,0,OqvXHi_QtT0
2,then save a version of the model that way we can actually test the model while training is still going on the reset number of steps parameter has to do with the way the trending information is graphed by turning it off we want to graph a continuous line if we turn it on it's going to repeat from 0 to 25 000 over and over again when we try to graph it now why did i choose 25 000 this is more of a trial and error number you can choose ten thousand fifty thousand whatever you want in this tutorial we're now going to talk about how these algorithms work but if you are interested in digging in if i point to the model it's going to bring up the help and by scrolling down you'll find that there's a paper talking about what soft active critic is and also an introduction if you're interested you might also be wondering what happened to the learning rate or discount factors that we have to set in the q learning algorithm it is possible to set those parameters when declaring the model but the model came with defaults and those usually work so you don't necessarily need to change them okay let's start the training let me scroll down to the main function and then bring up a new command prompt i created a argument posture here so that i can start the training from command line so the way it works is i'm gonna do python the name of my file my first parameter is the gym environment i want to do the humanoid version,358,0,0,OqvXHi_QtT0
3,4.,1,0,0,OqvXHi_QtT0
4,my second parameter is the algorithm i'm going to start with a sac and then i have a choice between training or testing i'm going to do dash t for training so when i select training i come down to here declare the gym environment and then call my train function let's kick it off okay training should be going on now i want to copy the same command go to the plus sign here to create another command prompt paste in my command this time i'm changing from sac to t d3 start training this one i'm gonna create another command prompt and i'm going to start training ac2 so now i have all three models training at the same time let's check the logs we can see logs being created take a look at the models no models has been created yet we can see some information being printed in the command prompts i'm going to create another command prompt this time i'm going to bring up tensorboard tensorboard requires us to pass in the log directory we'll give it our logs directory enter that command just started a local web server i'm going to control click this link and here's our tensorboard let me change this color okay tensorboard finds the logs for the three models that are training at the moment let's go over to the gears here at the top right make sure we load data is checked so every 30 seconds our graph will get updated as you can see it just happened in the background 10 support offers a lot of insights into what's happening during the training for,358,1,1,OqvXHi_QtT0
5,us we can just look at the top two here the one on the left is the episode length the x-axis is the number of time steps the y-axis is the episode length over time we expect the episode length to be longer and longer because the ai should be able to walk for a longer period of time on the right side we have the rewards similarly we expect that over time the ai gets more and more rewards here we can see that the td3 the light blue algorithm is not really improving at all so after a little while i think we can terminate this one it might not be the white algorithm for this environment if we look at the pink one for ac2 this is the fastest one it's already done a hundred thousand time steps however it's not really improving it's going back and forth down here so the one that's most promising is the blue one sac which is consistently improving over time i think at this point it's safe to assume that the light blue and the pink one is not going to get any better so we can go back to visual studio code i think the third one is the ac2 c to stop the training and then we'll also stop the second one tt3 okay let's let the soft actor critic algorithm run for a while we take a look at what the test function looked like so in the test function similarly we're passing in the gym environment the algorithm and also the path to the model as you can see on the left,358,1,1,OqvXHi_QtT0
6,ac2 has already created a bunch of models but we know those are pretty much useless because the training wasn't really getting anywhere with this algorithm depending on which algorithm we chose we are loading the respective models here we're resetting the instance of the environment the first element returned is actually our observation or state we pass the state into the model's predict function the predict function selects the best action to take and then we take that action when we take that action we receive a new state and also a done flag if the ai has already fallen now in the animation the ai falls about halfway and then the scenario ends so what i did was i added another 500 steps so that we can see the ai falling all the way to the ground so that's the test function we still don't have the soft actor critic model yet but let's just try the ac2 one my parameter let's go down to the parameters python name of my script environment name ac2 algorithm this time it's dash s for tests and then the path to the model ac2 125 000 .,254,1,1,OqvXHi_QtT0
7,sip this command is going to trigger this path which declares the environment and calls the test function let's see what happens okay this is what we expected the ac2 training didn't get very far so the ai dropped to the ground almost immediately okay it's been two hours of training our soft actor critic algorithm has gained about 4 000 rewards which is looking pretty good after 400 000 time steps i've already stopped the training let's try to run the model okay change this to dash s path to the model let's see it walk okay cool he's walking it might not look pretty but he's he's walking okay guys thanks for following along in this tutorial if you want to see more stuff like this be sure to give me a like and subscribe i'll see you in the next one,187,2,2,OqvXHi_QtT0
0,what is going on everybody and welcome to part two of the reinforcement learning with stable bass lines three tutorial series in this video we're going to be talking about saving loading and tracking the performance of your models so this is the code that we left off on we definitely learned that something is happening when we train it for x number of steps the problem is how many steps do we train for and as we saw for sure with a2c i didn't even look through the ppo but we definitely saw where performance at one point was actually in the positives but it ended at 100 000 steps with a reward in the negatives so how do we like deal with this so so that's what we're going to be focusing on uh here so first off uh we will go ahead and we will import os and then we're going to specify a models directory and this will just be we'll make models models slash and in this case depending on which one we're using so in this case we're using ppo so first for now i'll just say ppo but if we change the algorithm change change that name don't be like me and forget i'm sure i will so that's modelsdirt and then we'll also have a loggeder for whoops i didn't mean to run that i have to what what have i done logder equals logs so uh so once we've specified those we want to make sure they exist so if not os dot path dot exists and models der we want to os dot make durs models,358,0,0,dLP-2Y6yu70
1,der and then we're going to do the exact same thing for the log directory so paste paste cool so that'll make them if they don't already exist these lines are totally fine and now what i'd like to do is set time steps so this could be in the form of a thousand time steps or ten thousand it really is going to depend on like how quickly does your environment step in my opinion so this just comes down to how free essentially how frequently do you actually want to save your model so for now i'm going to also comment this out because this was to actually run it and we'll get to that on the load here in a little bit but for now what we want to address is training and saving so so in this case let's say we're going to train every ten thousand steps you can change this to be five thousand ten thousand two thousand five hundred whatever whatever the heck you wanna use um put that number in i'll go with ten thousand since that's relatively quickly so let's say what we really want to do and we'll actually make a constant here time steps we'll say 10 000.,271,0,0,dLP-2Y6yu70
2,so model learn for those time steps and then we'll say for i in range and then how many time steps do you want to do so in reality we might actually make this a wow true loop but just so it'll be easy for me to kind of compare the two well again we'll be comparing a2c and ppo here so i'll just limit it in some way but for i in range 30 what we're going to do is model.learn for that many time steps now if we just leave it this way the model will reset every time the number of time steps that it's like already taken the model object i mean so every time you do model.learn it would reset the time steps that it's at but to stop that we can say reset num time steps and we'll set that to be false and then we'll also set a tb log name and again that will set that to be ppo um i feel like we need to have like a constant probably um i think we'll be okay with with two places to edit but if we add any more i think i should we should probably have a variable for that but anyways whatever model.learn uh so yeah so the so this reset time steps will help both with the output that we see in the console but also in the tensorboard itself so model.learn and then basically all we're going to add is model.save and we'll use an f string here models underscore oops we wanted to use those variables models dir slash and then in this,358,1,1,dLP-2Y6yu70
3,case it will be uh we're just gonna save uh time steps times i so we'll say um whoops not a capital i dude there we go so go ahead and save that so now essentially this will be time step zero uh the first time through and in fact maybe we'll do a range one to thirty to make this actually actually accurate otherwise we'll be ten thousand steps off so model that save so now it will save that model every 10 000 steps so i'll go ahead save that and in fact i think we probably want to run this in its own little console so we'll go ahead and run this python3 p2 uh save dot pi let's make this a wee bit larger and let's go ahead and run that make sure we don't hit any errors so far so good okay and that's off that is training uh our model for ppo now what i'm gonna go ahead and do is let's just shift uh i'll put this over here i guess so i'll move that over i wish it told you somewhere in that output what type of model it was but that's okay so we got ppo training now i'm going to go ahead and toss in uh a2c to kill two birds one stone at a time here so let's make sure that we make all the changes that we're supposed to make ooh find and replace probably would have made sense but that's okay uh a2c uh a2c wasn't there one more spot yes there was not that it'd be an interesting model uh okay so,358,1,1,dLP-2Y6yu70
4,i think i think we're all set we'll go ahead and save that let's confirm that uh we got models or logs so this will be tensorboard here models ppo so yeah we have our ten thousand twenty thousand and thirty thousand steps models already in there um whoops and i think we're good so let's just double check make sure i didn't screw anything up in fact let's go ppo found that one it's a good thing we didn't do a find and replace isn't it anyways okay cool so now whoops now we'll save and let's go ahead and i will run that one as well so open in terminal python 3 p2 save dot pi so again this should be running for a2c in our case so again we'll make it a little bigger and i will wait for that to at least start to output something for us let me check a2c cool and let's go ahead and check logs logs is empty why would logs be empty is it possible why would logs be empty maybe it's every hundred thousand steps is that what it's going to do to me i would think it would update the logs every 10 000 steps oh i know what i've done okay so i forgot to in model i told you we weren't going to be adding anything else but we are so so in here we're going to say tensorboard tensorboard underscore log equals log der and then this is the tb log name dang it okay so okay i'm gonna break these two that are training right now uh and come back,358,1,1,dLP-2Y6yu70
5,over here i just want to clean everything up so actually i'm just going to delete that okay coming back here a2c a2c cool i wish i would have checked that earlier anyways let's rerun that so this will be a2c going and we'll make sure that we're actually logging this time okay that's training we'll come over here well let's confirm before we waste our time shall we so a2c underscore zero cool models a2c very well okay so now what i want to do is switch this to ppo copy paste paste paste and just to be sure a2c very good we don't have a2c and will we run ppo now i'll slap this over here also a little curious um nvidia s am i curious where i'm at gpu-wise okay my gpu yeah we're comfy uh thanks puget so um okay so now we have our things training we should have tensorboard logging happening so now to actually view the tensorboard what we'll do is uh do actually we need to be in that directory so we'll come down here open a terminal and we'll go uh tensorboard logder logder equals logs very well so okay so here although the graphs are very very small admittedly let's make this a little bigger so the main graph that you're probably going to care about the most is the reward average okay um and in this case ppo is our blue and a2c is our orange so you probably care about the the average reward the most that is typically the thing that we're trying to create here but the length of the episode also,358,1,1,dLP-2Y6yu70
6,can carry quite a bit of significance um and maybe more on that later uh we'll see so ppo blue a2c is orange okay so what i'm going to do i think is i'll make i think i can do this no that's the wrong button i never remember which one which ones are the right buttons so i'm going to let these things train for the 300 000 steps hopefully i got that right it should take somewhere before that we should be able to solve these models these are very simple models okay obviously this is still training and i will continue to let it go for the 300 000 steps but eventually we'll get to 300 000 and either way while you're training like i said typically you're going to probably be using a wow true loop rather than you any known ending so now what we want to do is looking at our reward mean we might say okay we want to take ppo at you know maybe i don't know which one will we do 180 or maybe even 175 but we wouldn't have that saved so it'd either be 170 000 or 180 000 either one we can just grab one of those let's say we want to load that and run that also interestingly enough i'm kind of surprised that uh we've actually seen we actually see a2c overtaking ppo this is not that's not nothing is what i've expected so far in this series like i expected um our random agent in part one to be way more wild but actually our random agent part one was i,358,1,1,dLP-2Y6yu70
7,think better than the 100 000 age that's just like what the heck man trying to make a tutorial here um let's go look at some of these metrics though so one thing that should be pretty clear by looking at these um they are different algorithms so take these take this with a grain of salt but in general the reality or the thing that i was hoping to be able to show even with the reward here is but you can also see it with the reward um ppo is a smoother algorithm in general it does it doesn't have as much volatility and one thing you will grow to really hate as you train these reinforcement learning algorithms is how volatile they can they can become right um especially like when i was doing ddpg for example with the robot dog um it was very frustrating to have a model learn learn learn learn learn and then just tanks it just explodes and it's like no good anymore it's like it just it really guts you so um in general i like something that is a little smoother and we can see that basically in all of these other metrics like entropy loss all the all basically all of these metrics across the board um ppo is the smoother metric and so for that reason it is a little more less stressful to use something like ppo also in general the performance is much better in this case it is comical to see a2c is actually is it still winning it is still winning real hard okay live demos so the other thing,358,1,1,dLP-2Y6yu70
8,to think about too is a lot of models can be dictated by just randomness so where did you randomly initialize and where did you start taking steps from there so you can do two things you can either set your random seed specifically or i kind of just like to just run a few models randomly like to test truly test a model test your environment i would run like three to five something like that i've seen huge variation in exactly the same code and if and in fact maybe what i'll do before we leave it'll probably be an exact match but before the end of this this tutorial um i'll run one more ppo and i mean in fact we can just run it um i think we can get away ppo maybe make the models dirt could be a dash two and actually i don't want to overload the gpu so i'll wait until one of them is done but i'll show you at least another tensorboard before this is over um and you can hopefully see some of the variants my guess anyways these environments so basic that maybe there won't be much but anyways let's get to loading and kind of seeing one of our examples in action so for example let's say we want to take ppo and i forget already what was a good ppo one which one is ppo it's our blue so let's take ppo at 180 000.,323,1,1,dLP-2Y6yu70
9,so while things are training we probably want to see where we're at along the way because again you'll probably wind up doing it for just a while true loop and you might want to actually visually see it so sometimes you'll see the reward is what you want or maybe the episode length is what you want but nothing beats visually seeing it so again for example with the robot dog sometimes the reward looks really good but then you load it up and either one the uh the actual agent is somehow defying the laws of physics like it has found an actual like bug in the environment which is pretty common or two maybe it's doing something really dumb that will never be recovered from so one example that i've seen again from the robot dog scenario is it would like jump and do like this kind of roll but get stuck on its back and it just it was probably never going to get out of that that was a really short term kind of gain in reward but then actually recovering from that i've never seen it happen it's possible that it could like jump and roll and then jump and roll and then jump and roll or something like that but um in general it's good to see it visually so you have an idea of like what's actually going on in your environment uh to cause that reward even though it might look good on paper in practice it might not look good at all so so we want to see what's going on so to do that we,358,2,2,dLP-2Y6yu70
10,still need our environment we do need a model but we're going to use our one of our saved models so i'm going to copy the models dir paste it over here and the first thing that we want to do is we're going to specify our model path model path and that will be um that will be the uh we'll make that an f string and it will be models der and then slash whatever that file name is in this case we'll do 170 000.zip that one was looking good at least to start and now what we want to do is actually load that model so we're going to say model equals and this will be ppo dot load and we will load in the model path and then we specify the environment so to just be m vehicles m now we want to actually run with the model so episodes 10 that's totally fine we get the observation yeah yeah cool cool basically what we want to do is swap out this action here so we're going to cut this out and we're going to say we want to take an action now what is that action going to be so to make that action we're going to say action equals model dot predict and actually uh it's gonna be action and then we'll just say like underscore it in this case it also the model that predict is returning um some states so if you are using we're not i'm not touching this right now but if you're using a recurrent algorithm this will this will be of use to,358,2,2,dLP-2Y6yu70
11,you um but for now we're not doing that so we really only care about what is the action so we're going to say models.predict or model.predict rather and that observation so then we'll do the step based on our actual model's action we will get all of this other information um and then basically continue the loop so let's go ahead and save that and we can run that maybe yes okay so here we have i guess because i've changed the name it's all itty bitty let me fix that beautiful so here we can see our actual model in action this is our ppo model um so the episode length is starting to make a little bit of sense now it's not terrible right it's not crashing but the goal is to land in between these two flags and it's definitely not doing that right right now but this is better than random and it's better than the 10 000 model but clearly we need to possibly continue training so let's go ahead and check let's see here if anybody got even better no our best model in this case was actually around maybe 190 but also interestingly ppo and e2c performed identically it's this is so frustrating yeah you do like definitely ppo is supposed to win here i can't believe this and ppo also just turned to to crud by the end this is hilarious um so what i think i'm gonna do now is i'm gonna train i'm gonna train like i guess we'll do we'll do another run for each of them to be fair so i'll run another,358,2,2,dLP-2Y6yu70
12,ppo and another a2c but also while we're at it let's go ahead and check an a2c at i guess 190k so we'll come over here we will oh it already died it died before i could do it so we'll change this to 190 and then we'll change this to a a2c we'll change this to a2c a2c cool uh and let's just go ahead and import ppo and a2c we don't need to be switching that around every single time cool cool run that wait for it so here's our a2c it looks pretty good too it's very comparable to our i think that i don't think we ever saw ppo actually succeed in any way this is interesting when i was like making the text based version of this tutorial this is this is like nothing like i was seeing it's funny um classic so okay so now what i want to do is i'm going to train both of these again and maybe maybe i'm going to walk away while i train these so i'm going to do maybe i'll do like a million steps just because i am surprised at our results thus far so cool so what i'm going to do is we'll just kill this and p2 save let's do models do ppo so the only thing if i want to run a bunch of ppos would be here and maybe what i'll do is import time let's get real fancy about this super fancy i will do an int time dot time cool and music 30 let's do do i really want to do a million times kind,358,2,2,dLP-2Y6yu70
13,of i kind of want to see the a better result here what was it so uh ten thousand so we'll do a hundred time steps and i think maybe i'll do like two and two so let's save that p2 save so what i'll do is come over here i'll run p2 save once okay it was trying to rewrite to the uh the other tensorboard logs so i guess we'll also say logs and then we'll do this logs slash ppo time.time what did i screw up ah boom okay try that again what i really want to see like it was trying to overwrite each other so here i'll show you what i what tipped me off to that one okay well maybe it was this one i might have made the logs a little messy but yeah i saw logging to logs and then it was just ppo underscore zero for both of them obviously we don't want that um because then we'd have crazy logs so hopefully we didn't totally murder well we did we murdered our other log but that's okay um that's all right okay that one is running so i will run one more uh ppo and just to clean up the logs i will come into here and i will delete maybe we'll leave the original a2c but this one's been ruined so i'll just delete it and now what i'll do is i'll also add an a2c take that pasta pasta a2c a2c i bet we could have fixed it with that we could that would have been the more proper place to fix it actually,358,2,2,dLP-2Y6yu70
14,now that i think about it we should have added the time to that uh i don't think it will be relevant because it just doesn't it doesn't really matter but this we should have added the time stamp right there um whatever i think we're fine so let me check for ppo again cool okay so now what i will do we'll just run two more of these python 3 p2 save and i'm just going to open a tab python 3 p2 save let's check models make sure we're not screwing ourselves there look very good and then we'll check logs that's looking good so far and then finally let's check tensorboard looking good looking good i wish i didn't delete the original ppo oof whatever okay so see if i can fit this on the screen while it trains very good very good okay about an hour in some change hour and 20 30 minutes or so has gone by we are finally done training everything here and um interesting charts for sure so checking out kind of the results it does look like our best performer is this ppo model here but also curiously i believe that's this guy right here yeah so also curiously where's the talk there it is um who's our second best here see like our second best was like sort of even this like a2c for a second that for the original one that i said was really good for an a2c model by the way um that was actually our second or our uh second best for a little bit and then it appears that it,358,2,2,dLP-2Y6yu70
15,kind of went bad oh well that was the well that was the one that we trained for only 300k steps um but then it's another a2c model that was actually pretty good for a little bit and then yes it throws it all away just just like the other one was clearly going to do in it and it never recovered but by the end uh the the two victors here are ppo and ppo so if you can use ppo use ppo it seems to be the best one that i've you know at least in my experience of using in in all fairness a2c i don't know anybody that's still using a2c for anything specific but anyways um yeah so we have our results we've compared models um you've seen now real examples of how that random initialization can make all the all the difference um so again you might want to actually explicitly set your random seed possibly or just simply train a handful of models in my opinion i would just again i would just kind of randomly train a few models and then whichever one is going well continue training that model i don't i don't really see any statistical reason why you want to set like random seed 0 for example for all the algorithms that you test because in theory you might be you know maybe you were that that bad ppo from before uh when really you could have been you could have been good ppo if you would have allowed that seed to you know come across your desk so anyways uh let's check out one,358,2,2,dLP-2Y6yu70
16,of these ppos so it looks like um maybe around here is one of the better ones so we are where we step so let's do the 550k ppo so let's check that one out so p2c p2 load uh i already know i've i gotta figure out what was the uh 550 000 and which before i forget so let's see pb let me change this to be ppo um and which one was that ppo uh is there a way i can just yes i'll do that maybe cool come over here paste i think we can get away with that with the models there we'll save that let's run that and see if that works beautiful look at it look at it go yeah so yeah that's uh probably solved we'll look at a few more but i would say at this point yes that is indeed um that is indeed solved so the other thing that i wouldn't mind checking real quickly while that continues to play is what was our best a2c it looks like a2c like our original a2c is is the best one that went for 300 000 steps so the a2c at about not quite 200k steps but maybe 190.,270,2,2,dLP-2Y6yu70
17,is that going to be right for the modelster let me check that that'd be right for the logs but for models yeah it's just a2c i already forgot what i said was it 190k okay so this is our a2c um yeah i mean it's not it's not quite as good as the ppo it did learn at least but at least for sure on this environment i see no reason to use it ppo is faster and ppo is better so hopefully now you've got an even better understanding of reinforcement learning stable baselines three and you're coming to grips with how do you change your algorithms and all that now up to this point we've been using these pre-made environments and like i've been saying the really true hard thing is to figure out which environment do you want to use ppo on so so i'm just kidding there are other algorithms and obviously ppo is no good for there is continuous ppo but again we will we'll talk more i promise after the custom environments we will talk more about uh continuous versus uh discrete and all that kind of stuff um but i think the next best thing to do is i think anybody is getting into reinforcement learning if you're not just doing it just for the kicks is you want to apply this to your own environment so in the coming videos what i'm going to do is we'll just take some environment i can find online um i'm thinking some sort of pie game game or something like that convert that to a a gym environment so show,358,4,4,dLP-2Y6yu70
18,you kind of the steps that are involved to do that and then actually train a model in that environment again because this is an environment that we're going to have to make up the observations so you'll see that it's not you know in this case we've just been handed observations so how do you come up with your own quality observations how do you start thinking in that way and then also how to come up with your rewards because again these two things are actually deceptively complicated sometimes so i think i think you'll see a few good examples as we as we go through that so anyways that's all for now thanks for watching and i will see you guys in another video you,166,4,4,dLP-2Y6yu70
0,let's build a custom gym compatible reinforcement learning environment this exercise is broken out into three modules the problem module strictly models the rules of the problem how one interacts with the problem and how the interaction affects the problem it doesn't necessarily have an understanding of reinforcement learning this could be a business problem or the snake game that you just coded the wrapper module makes the problem gym compatible con in it into a reinforcement learning environment in the solution module we'll use two ways to train the agent one using basic h learning and the other using the stable baseline 3 reinforcement learning library okay so here's the super simple scenario we got a warehouse and we got a robot working in the warehouse the warehouse is divided into a grid and the robot's going to start at position row zero and column zero there's going to be random targets generated throughout the grid let's say right here let's say there's an item on the shelf at that spot and the robot's got to go there when we perform the reinforcement learning we want the robot to be able to find its way using the best path to any target on the grid again this is super simple doesn't need reinforcement learning at all maybe in the future we can add obstacles and other random stuff that happens in the the warehouse this is what we got right now so let's jump to the code okay i'm in this uh version zero warehouse robot python file as you can imagine with this simple problem you can accomplish this in basic python pretty easily,358,0,0,AoGRjPt-vms
1,so i'm not going to spend too much time on the code here so i'm going to go over it pretty quickly over here we define the actions that the robot can take which is basically left down right and up right here on grid tile this is for printing out the layout of the warehouse where the robot is and where the target is i overr the string function this returns the first letter of these names so when we do a print if we try to print a floor it's going to print an underscore r for the warbot t for the target so we're not rendering anything on the screen other than using basic print statements in the warehouse orbot class we can define the size of the grid we call the reset function which always starts robot add position 0 0 row 0 column 0 we'll optionally pass in a seed so that we can do reproducible executions this is where we randomly generate the target position next is the perform action function which takes in a wot action this basically just moves the robot around on the grid making sure the robot stays on the grid and will return true if the robot is at the target in the render function all we're doing is looping through the grid w by column and just printing out where the robot is where the target is and the floors okay that's it down here we got a couple of lines of code to test the code above so let me run it just to show you what it looks like okay it r,358,0,0,AoGRjPt-vms
2,let me scroll back up the robot starts on the top left the target is at row 012 and column 1 or randomly picking an action action is down so we can see that the robot did go down the next action is right the robot did go right okay rob back goes left goes back to here it tries to go left again we won't let it go off the grid it looks like the code works just fine let's move on i'm in the warehouse robot _ env file this is where we convert the problem into a gy compatible environment up here i've linked to a guide on how to create your custom environment this is more involved it's little bit more complicated but you can check it out if you want i'm importing a couple of gymnasium modules i assume that you have gymnasium installed if not check out my playlist i have a guide on how to install it i'm importing my warehouse robot module right here also numpy here i'm calling the register function to register this module as a gym environment once registered the id is usable in this gym.,256,0,0,AoGRjPt-vms
3,mic function the id here we can call it whatever we want this is the usual convention the entry point points to this module which is the name of the python file without thep colon and the class name the class name being this one so this is our implementation of the gym environment our environment class needs to inherit the gy. env parent class here's a link to the api if you need it metadata is a required attribute within metadata there is render mode and render fps in our environment we have two choices either none it's implicitly defined so we don't need to specify it here if it's none then we're not going to render anything onto the screen if it's human then we are going to render stuff to the screen with the built-in gym environments when you put human here they usually use p game to render their environment and fps frames per second is used to control how fast they render the um graphics but for us uh since we're just printing stuff to the screen we're not really rendering anything so this frames per second is just a dummy value here in the init function we'll take in the size of the grid and use that to initialize our warehouse robot we'll also note down the render mode gy requires that we define the action space the action space is the robot set of possible actions in our training code we're going to call action space. sample to randomly select an action so that's how the action space will be used we use the gym spaces.,354,1,3,AoGRjPt-vms
4,"discrete class to define the action space let me jump back to the robot actions so the robot actions are basically from 0 to three so when we pass in the length here the length of four our discrete is going to go from 0 1 to 3 so this just happens to match up with the robot actions if the robot actions are not this uh straightforward if it's like 1,000 2,000 3,000 4,000 then we're going to have to divide by 1,000 to convert the actions to work with the discrete space so let me jump to the discrete class i'm holding down control and then clicking on the class name in the discrete class it it takes in n the number of elements in this space we passed in four here we didn't pass in the start so by default it starts at zero and with a length of four it goes from z to three so keep this in mind that we need to map we need to map the problems possible set of actions to this discrete space here here gy also requires us to define the observation space when an action is taken what is the state of the problem what is the impact of that action that is the observation for us we want the robot to be able to navigate to a target regardless of where the target is so for that we need to observe the robot's position and the target's position and we can do that using a one-dimensional vector that looks like this the first two elements are the robot's position and the second two",358,4,4,AoGRjPt-vms
5,elements are the targets position we use gy spaces.,11,4,4,AoGRjPt-vms
6,boox class to define the observation space how this works is first we define the data type since we're in a grid we're using uh integer but usually boxes is used with floats so if we use floats we can put the robot and the target anywhere inside a box but since we're keeping things simple we're just using a nice and neat grid now for the four element array we define the shape here the high and the low is used to check the boundary of these values since all of their positions are going to have a minimum of zero we set the low to zero for max what we do is since it's a four element vector we pass in a four element array here this first element here is the maximum of the robot's roll position the second element checks the robot column max position and same thing for the targets row position and the targets column position the observation space is used to validate the observations returned by the reset and step function that we'll take a look at down below gy requires us to implement the reset function in our training code usually at the beginning of each episode we'll call the reset function to reset the state of the environment gy requires us to call the parent classes reset function and pass in a seed this is used to control randomness if you want to be able to re reproduce the scenarios we'll call the warehouse robot reset function let me go there all it does is initialize the warbots position and generate a random target position we'll use,358,5,5,AoGRjPt-vms
7,num's concatenate function on the robot's position list and the target it's position list and all it does is convert the format that we defined up at our observation space we can also return uh some debugging information just included in this uh dictionary here if random mode was set to human will call the render function let me scroll down in the render function all we're doing is calling the robots render function okay now we return the observation along with the info gym require ires us to implement the step function the step function takes in an action and applies it to the environment here all we're doing is folding the action into the warehouse robots perform action function let me go there okay we move the robot to the next cell and then return true if the robot reaches the target we'll keep track of that right here the step function needs to return a couple of things first the observation which is after taking that action what's the state of the environment so here we construct the observation just like the reset function after taking that action where's the robot where's the target we need to return the reward the reward influences how the agent is going to learn how the robot is going to learn so in this case when the robot's moving around on the grid we're not giving it any reward but if it reaches the target we'll give it a reward of one we'll also signal that the scenario is done so we'll set terminated equal to true so those two things gets returned here reward is something,358,5,5,AoGRjPt-vms
8,that you need to play around with to see how it influences learning the next thing that needs to be returned is truncated this is if you want to set a a limit on how many actions how many steps the robot can take let's say after 200 steps you want to stop this episode this is how you can do it using the trun parameter for us we'll just let the robot wander as long as it takes to find the target so we'll just not use this parameter and info you saw from before this is where you can set debugging info gym also requires us to define the render function here all we're doing is calling the robot render function which just prints the grid and where the robot and target is the main function is where we can check that our environment was created correctly remember up at the register function we register this id so that makes it possible for us to use it in the gym.,223,5,5,AoGRjPt-vms
9,make we're creating an instance of the environment we'll call the check environment function this is a gym provided function that checks the environment to make sure it's correct now jy automatically wraps the environment in a bunch of different wrappers by calling unwrap we can make sure that check env is checking our module this module here so assuming the check environment is correct we'll test the environment ourself by calling reset or'll grab the observation from the first index and we'll just do 10 actions here we use the action space called sample to select a random action and pass that random action into the step function and then we'll just do that 10 times so let me run that okay let me scroll up if you run this a couple of times you might see this warning here that we already registered our environment and it's overwriting it again so it's okay just leave that when we call the check environment function it automatically calls reset and step runs through a bunch of checks and we don't see any a so looks like we're good now inside our own check our own loop here our robot is on the top left the target is on the bottom right and we're just doing a bunch of random actions to make sure that nothing is broken we're not trying to learn or solve this environment right now we're just taking a bunch of random actions making sure that let's say it goes down it actually goes down so it looks fine now let's try to break the environment and see what the check env function,358,7,7,AoGRjPt-vms
10,reports let's say i i don't create a step function let me just throw something here run this code and see what check env reports it's not very specific it's throwing a not implemented error which is correct we did not implement the step function uh i wish the description is better but let me put this back now let's see uh um render frames per second let me put a zero here run it again so this is a much better message it's expecting render fps to be greater than zero and the value that i set it to is zero the check env function does work let's move on let me uh make sure i change this back okay now that we have our custom gym environment let's try to train it i'm in the warehouse robot train module as mentioned before i'm going to use two methods to train the environment first one using q learning and the second one using the stable baseline 3 library first q learning now i already did several videos on q learning if you're interested check my playlist you'll get more detail on the q learning code so i'm going to go through this quickly we'll create the uh environment here our q table it's going going to be a five-dimensional vector these are the five dimensions first two is where the robot is the next two is where the target is and then the last dimension is the actions if we're training we'll create the q table if we already did the training and we're doing a test then we'll load the q table from a,358,7,7,AoGRjPt-vms
11,"file we set the hyper parameters here i'm using an array to keep track of the number of steps that a robot takes per episode before it finds the target at the beginning the wot is going to wander around many many steps before it finds the target near the end of the training we expect the number of steps to be pretty much minimized we'll keep track of the steps and then we'll graph it as well i'm going to loop through the number of episodes this is a parameter that we pass in we set the uh environment and we're using epsilon greedy here so either we're selecting a random action or we're looking up the q table to find the best action at the moment we'll execute the action and update the q table okay so this keeps going until the terminated flag is true which means the robot has found the target and then we decrease epsilon and then and then we'll go to the next episode and repeat again that's q learning let me go down to my main function here first i'm going to train for 1,000 episodes after that we'll run a test okay hitting f5 this should be done pretty quickly okay the training is done before i review the steps let me go to the graph of the training this is a graph of the episodes down here on the xaxis and the number of steps taken on the y ais as expected at the beginning of training the robot fluctuates around around 50 to 60 steps before it can find the target over time as training",358,7,7,AoGRjPt-vms
12,"progresses the number of steps to reach the target decreases near the end it pretty much falls below 10 steps the pickle file this is a binary file we sav the q table here since it's a binary file we can't really look at it okay after the training we went one test with rendering on let me scroll back up okay so this is the test we'll start here the random target is here the best action is to go down and then go right go right again go down okay so our q learning works the next thing that we're going to do is to train with the stable baseline library let me comment out the q learning let me do the training first i've also done a stable baseline 3 video make sure you check that one out i'm going to go through this quickly as you can see it's no more than 10 lines of code we'll create an instance of the environment we'll use the a2c algorithm to train the environment this is just something i randomly picked down here we're doing in infinite loop we're basically performing training indefinitely and then every 1,000 time steps will save the model let me start the training and then i'll show you when i would stop the training and in f5 here you'll see in a second on the left you're going to see a models folder and a lock folder okay after 1,000 steps you can see that the model is being saved here and also the library is logging events here now as training is happening i'm going to create a",358,7,7,AoGRjPt-vms
13,"new terminal i'm going to bring up tensor board i'm going to pass it the locks folder tensor boort is started i will go to this y tensor board is looking at the logs folder it's graphing some information that we can use to determine if it's time to stop the training on the top left this graph is graphing the episode length that means how many steps is the robot taking before the episode ends and this looks pretty close to the q learning graph you can see that the model is already solving the environment so any more training we're not going to get any more efficiency so at this point we can stop the training we stop the training just by stopping it right here so we got way more than necessary i think the 1,000 step model is already good enough so let's test it comment out the train function and comment the test function the test function all it does is load or loading the 2,000 times depth which is this one right here predict is going to get the best action based on the environment after picking the action it's just going to execute it let's run this okay let's check we start up here targets here first action is go down go right go right go down go right and go right looks like the training works all right we made a custom environment and we trained it if this was helpful to you be sure to give me a like and subscribe and leave me a comment on what environment you're planning on making",353,7,7,AoGRjPt-vms
0,what's up clar decoders today i'm very excited to share a new reinforcement learning video we're going to go start to finish on how to train a reinforcement learning algorithm not necessarily coding the algorithm itself we're going to use stable baselines once you get this down you can really expand this to train any sort of reinforcement learning you want now i am going to assum just a bit of knowledge here in terms of how to install python and making sure it's on the path if you don't have that done already you can follow a tutorial online they're all over i'm also going to use python 3.8 so find that in the description below if you want to follow along exactly with what i'm using the newer versions of python might not work or might work differently depending on when you watch this video i'm also going to use visual studio code as you can see here if you want to use the exact same environment as me you can utilize that now the only thing i have set up right now is an empty folder so i'm going to go to open folder desktop mario p tutorial and that's where i'm going to do all my coding i also have get bash installed so you'll notice that's my terminal below so all we're going to do now is we're going to create a virtual environment to install our libraries and everything like that now in general you want to utilize a virtual environment so you don't have any overlapping dependencies that cause issues with other projects so what i'm going to do,358,0,0,PxoG0A2QoFs
1,is i'm going to do python dm vv env space env and that will create our virtual environment and it will name it env once that's complete you can see all we have is a virtual environment in folder up here now in get bash or linux you're going to do source env you can hit tab and then scripts tab and then activate to do this on uh command prompt i think it's a little different i don't think it's source you can google that as well or check out our discord if you have any need any help on these videos i will help personally on any videos that i created especially these tutorial videos that i'm going line by line all right so now we have our environment installed as well so now we're going to look to install some of the libraries that we're going to use we're going to use a library called jy now jim gives us a training environment that's uniform that way we can train all kinds of different reinforcement algorithms without changing our environment the environment is the world that our agent is going to interact in in this case it's mario so the environment itself is mario playing the game so we're going to go ahead and install gym this one's unique so we're going to install a certain version to make sure it works correctly so i'm going to go pip install equals equals 021.0 that's what i want you to install this is to avoid an air down the road we'll let that install and then we'll come back back all right that's complete,358,0,0,PxoG0A2QoFs
2,so now we're going to install gym retro so let's do pip install gy- retro this is going to give us an environment and some games that we can try to program reinforcement learning with it's pretty quick because we already have gym installed and then one other thing out of the gate i'm going to install we might have to do some installs later but i'm going to install stable baselines so what stable basel signes is it's a set of algorithms that are already set up and working correctly so if you code a po from scratch you might make a mistake in the mathematics or something like that we're not worried about that today we're just getting set up and utilizing the algorithm itself so we're going to install stable baselines to make sure we all have a working algorithm at least to start out with here so i'm going to do pip install and then stable das baseline s 3 and if you want to use the exact same version as me you can do double equals 1.6.0 and we'll let that install as well now stable baselines uses p torch they made a change from stable baselines 2 over from tensor flor flow to p torch so you'll see you'll notice it's going to install that and that may take some time it's a large library so we'll let this run and then i will come back and we'll jump back in all right now that that's installed we're going to have to go out and get the mario rom in order to actually utilize the mario game so gym retro,358,0,0,PxoG0A2QoFs
3,comes with a few games already pre-installed so you can substitute out anytime i'm referencing mario you can substitute out one of these games that i'll show here on the screen and that will work out of the box if you want to follow exactly and you want to utilize mario you're going to have to go out and get the rom and illegal away hopefully um if you need help with that you can jump on discord as well but once you have that rom downloaded you're going to look for mario ness so super mario bros ness and you should end up with a zip like this and we're going to go ahead and extract that zip so i'm going to extract here and then you will get a folder and if you look in that folder this has your rom inside of it now i'm going to take this path here and i'm going to copy it i'm going to go back to our install area here and i am going to do python dasm retro.,232,0,0,PxoG0A2QoFs
4,import and then i'm going to paste in that path so i'm going to open a double curly bracket nope i'm going to open a double quote and then shift insert on windows and then another double quote so i have the path in here and i'm doing python dm retro. import and then the full path so let's do that if you got the correct rom it should say imported one game and now you're ready to utilize super mario bros das ness all right so let's jump right in and see if we can get a random agent working so what we're going to do first is we're going to just take a random agent and let him do random things and then we're going to see if we can create a better trained agent after that so i'm going to create a new file here and i'm going to call it random agent.,202,1,2,PxoG0A2QoFs
5,import and then the full path so let's do that if you got the correct rom it should say imported one game and now you're ready to utilize super mario bros das ness all right so let's jump right in and see if we can get a random agent working so what we're going to do first is we're going to just take a random agent and let him do random things and then we're going to see if we can create a better trained agent after that so i'm going to create a new file here and i'm going to call it random agent.,136,2,2,PxoG0A2QoFs
6,piy and we're going to do some imports at the top so let's import retro let's import gy and then from stable baselines 3 h we don't even have to do that yet let's not do that yet so let's just leave stable baselines out of it see if we can get a working random agent so the first thing we want to do is create our environment so in this case we're going to do env which is just a variable name and we're going to do retr doake game equals and then we're going to type in our game name here so you're going to want to use exactly what it's called down here you can just copy that and paste it in now remember if you didn't go out and get the rom you can't utilize super mario bros ness you can utilize this game i'll show on the screen and it'll work just the same so this is our environment and now we're going to get a first observation from this environment so what's an observation the observation is what our algorithm is going to use to learn in our case we're actually going to send our algorithm a picture of the current environment not multiple pictures not anything like that it's going to have no sense of time or speed or anything like that we're just going to give it a screenshot and see if it can utilize that to create an intelligent agent so we're going to say obs that's going to be our observation we're going to do env.,345,3,3,PxoG0A2QoFs
7,reset so every environment in in gym library has a reset function which basically starts to starts the environment at the beginning so we're going to go ahead and do that we can get some information about this observ too so let's print out observation. shape this is going to show us what shape are we're working with for our observation hopefully it's an image we're going to set done equal to false so our environment also has a done flag to say when the environment is done right so in the case of mario it's losing all three lives and we'll say while not done we want to utilize our agent so we're going to grab every time we run our agent we're going to grab another observation we're going to grab the reward we're going to grab our done flag and we're going to grab some info so every time our environment takes a step it's going to pass back these values to us so now we're going to say env. step and we're going to do env doaction uncore space.,237,4,6,PxoG0A2QoFs
8,step and we're going to do env doaction uncore space. sample and what this is going to do it's going to give us a random action that our agent is able to do now these environments have already been set up a little bit so the action space is already there in mario's case you can think of going left right jump whatever it might be so it's going to give us it's going to take a random action here and then step it into our environment and pass back some information to us you'll notice the done flag for example it's going to pass back whether or not our program's done from there we can view it by using the render function and then after we'll just do environment.,169,6,7,PxoG0A2QoFs
9,close let's see what happens it's fun part about programming right so in order to run this we can do python uh space random agent now a couple things if you're in visual studio you can go contrl shift p and we can select interpreter and you want to make make sure you're inside the env folder if you've created a virtual environment which we are and then you can use a little play button up here as well so let's go ahead and do the play and look at that you'll see that we already got a random agent now it's not very skilled you can see that he's not making any progress and you'll probably notice one of the issues that i'm having here with this environment if we let him run good thing it's extremely fast if we let him run you can see it takes a lot of time and he doesn't make any progress and he's probably not going to die because naturally because there's no enemies yet cuz he's not going to the right at all now let's get into training our agent so we're going to try and train an agent now we're going to use some built-in things like tensor board so we can utilize to see how our training is going and try different things which i'll talk about in a bit but right now we're just going to set it up to train the agent and see how far we can get so let's go ahead and create a new file and we can call it train.py and again you can you can call it whatever,358,8,8,PxoG0A2QoFs
10,you want there so let's go ahead and import numpy as mp i think i'm going to use that let's go ahead and do from stable baselines 3 import poo and then we're going to create a vectorized environment so i'm doing this on just my laptop so you can do this on pretty much any computer but we're going to run four environments at once so we're going to vectorize that environment and you should be fine as long as your cpu has four cores you shouldn't have an issue here so i'm going to do from stable baselines 3 again stable baselines 3.,136,8,8,PxoG0A2QoFs
11,common do vcore environment import and then we're going to do sub process vc environment let's try and on this and see if it finds those it's being weird about my it's trying to finish it with different library names okay so those both look good we're going to do some callbacks so we can keep track of our results so i want to add in callbacks we also want to load results and do some other things there so i'm going to do a couple more imports and we'll get to what exactly we're doing with these in a minute but from stable baselines 3 do common do results plotter we're going to import load uncore results and we're going to import ts2 xy it's a weird one we'll talk about it and then from stable bas lines 3. common do utils we're going to to import set random seed this is going to allow us to get even with random environments it's going to allow us to roduce results so we can really tell if changing things like the learning rate makes a difference or something along those lines couple more here from stable baselines 3 do common. callbacks we're going to import base callback so this is important these trainings take a lot of time so we want it to be able to check back in with us so we know how well it's doing we don't want to train something for 2 days and then find out that within an hour we would have known it was failing and should have stopped it from stable baselines 3. common.,353,9,12,PxoG0A2QoFs
12,atari wrappers we're going to import max and skip env that's that's another common thing with learning environments it's actually making too many decisions too fast in our mario environment so we're going to want to skip some frames to help it learn a little better and i'll show you some results that i got with that as well so we're going to import os and import retro now this should be all the libraries that we need you can actually break these down and see them as well so for example if you wanted to see the mac skip env we could actually go into our env folder we could go into lib we can go down to stable baselines 3 we can open up common you can see i'm following along here common then our atari rappers do common atari wrappers we can open that bad boy up and actually see how it works so what did we imported max env skip if we search for that oops you can see here it is and you can actually see how it works how it was coded and this can help you write your own down the road as well right so you can investigate those on your own time and that's our basic setup so now let's start coding so we're going to create a log directory so we're going to say our log dur and you can call this again whatever you want but we're going to call it temp in our case and we're going to do os.,340,14,14,PxoG0A2QoFs
13,make dur we're going to make that log dur if it doesn't exist already we'll say exist okay equals true so if it doesn't exist we're going to create that directory which in our case let's go ahead and minimize this envir or collapse that environment folder in our case it doesn't exist right now um but it will after we run this program now we're going to create a function to make our environment so this is going to allow our program to call and make an environment when whenever we want it to so it's just a neat way to kind of encapsulate this so let's say def make underscore env so this is a callable function this is the things we're going to pass into to create the function so we're going to do env uncore id we're going to do rank and we're going to do seed and we can make a default seed of zero so you don't have to pass in a seed but you can i was it going to see maybe i should crank my font size up just a little this might help you guys out okay that's pretty big that should help okay so we're back inside of our function here so we got our function defcor make env and inside of here we're going to create an env and we're going to do it just a little more intricate than our last one and this is actually we're actually doing this because we want it to be able to create vectorized environment so i'm going to do def uncore ait and then inside of here,358,15,15,PxoG0A2QoFs
14,we're going to create our env so we're going to say env equals retr doake and the game equals whatever env id the individual passed in here so this would work uh with any game down the road that you wouldd want to use and our env we want to go ahead and do max and skip env and we're going to say we only want it to make a decision every four frames now you can test this without using this at all but i already know from running it and from games in the past that they make decisions way too quick so it helps the learning and makes it more a quicker training process if you skip some frames so we're going to skip four frames in this case but you can certainly play around with that and make your own kind of decision there as well and we're going to do env.,202,15,15,PxoG0A2QoFs
15,seed and this equals seed plus the rank that will give our random environment and then we can return that env okay so this is going to be the consistent way that it creates an env every time and now we want to be lined up with the d andore anit so make sure your par or your tabs match up with mine that isn't important we're going to do set random seed and we're going to pass in our seed and then we're going to return underscore a knit so we're actually returning that function we can actually i think we can actually pull this down too let's pull this down here kind of with the start of our program that emv will leave up there and now let's go ahead and start creating our program now this is actually pretty quick not a lot of code here so hang with me let's do en vore id and we'll set it equal to just what we did in our random agent right our super mario brothers nest and then on the next we can say how many vectorized environments we want so we can set our numb of our cpus how many cpus we want to use we can set that to four that should be a good safe number and it actually trains pretty well and then we want to wrap our env so we're going to do env equals vc monitor and then inside that we're going to do sub proc vc env and then another set of parentheses and inside here we're going to use some list comprehension to create as many,358,16,16,PxoG0A2QoFs
16,environments as we have in our numor cpus so now we're going to do square brackets for list comprehension again this is all stuff you can kind of research on your own time if you're not really great at python so follow along and just kind of do as i say um but then make sure to do your homework afterwards this is the fun thing about python is you can get something spun up really quick and do something really cool and then dig in and figure out how everything work to make it your own so we're going to run our function from above so we're going to do make underscore env env uncore id and we're going to pass in i and then we're going to say for i in range and then however many we had in num cpu and then we're going to end our square bracket that looks pretty good so all we're doing is we're mushing all these different environments into a single list and then we're passing that to sub proc vc environment and then we're wrapping that in our vector monitor so we can keep track of it while it's training so we're doing a lot of stuff here i know this is kind of a big line in our program don't let it intimidate you look up list comprehension and that will help on to the next line so now that we got our environment set up let's create our model so our model is going to be a poo model model we're passing in images now how did i know that well you can look,358,16,16,PxoG0A2QoFs
17,at the observation but remember when we printed out our observation down in the command prompt here you can see that it is a color image it's a 224x 240 image with three color channels so we have to tell our po network that we want it to be a cnn policy again we're making something cool out of the gate but there's a lot of research to do here so you should look up see nns and see how they are trained as a neural network it uses images to find features inside of the image itself utilizing multiple layers so again continue to dig deep here we're doing something cool but there's also a lot of homework you can do to really start to understand this so we're going to pass in our env from above that we created above uh verbose we'll set to one so this is going to be how much it uh tells us as it's training the network and now this is really cool and important we want to create a tensor board log so we need to tell it where we want to put this log as well so let's say we want it to be in the tensor board folder which we don't have yet uh but we'll create and then we can also set a learning rate now i'm going to give you a little cheater here you can leave it at the default learning rate but i already know what works best and it does take some time to train so i'm going to go ahead and change the learning rate and i did this,358,16,16,PxoG0A2QoFs
18,from experimenting and looking at my tensor board which i'll show you so we're going to go ahead and set our learning rate to 000000 z four zeros three awesome now i'm going to comment out a line here so if you already have a model trained you can actually use that uh to train to start your training so you don't have to start over every time so if you didn't want to train from scratch you could do model equals p.,107,16,16,PxoG0A2QoFs
19,load and then the path to your model and then do your env just like you would do before so i'll leave this commented out so don't make sure there's a hash symbol in front of this we can utilize that in the future great let's do some checks here so let's just print out to the screen that we're starting our learning this is just for a sanity check so i can kind of see how it's flowing through here and then we're actually going to create a call back a custom call back so what we want to do is we're going to train for a long time and let this run but we want it to be saving the best model every time so we can kind of decide when to stop and either do something else or that our agent is good enough so in the real world this might be the agent starts completing the task and although it's getting a little better at it you're like diminishing returns here let's go ahead and stop it so those those might be the cases so we're actually going to create our own we're going to call it save on best training reward call back now we're going to define how often we want it to check that we have the best model so far so we're going to define it to check every thousand steps see if it's got a better model and then we're going to save that to the logor dur logor dur okay so if you go over to the nice documentation which i'll link down below you'll see,358,17,17,PxoG0A2QoFs
20,all kinds of examples of different things and you'll see where i got a lot of this information from again this is your next step you're following a line by line tutorial then you can go to the documentation and see exactly how i figured this all out and start creating your own programs from scratch but they did have this good example inside this documentation of how to save your best call back so you can see they named it just like we did here they say what you can pass in and different things like that and then it returns it back essentially it's just checking to see if we have the best model we can and and then save it so all we're going to do is copy this so i'm actually going to just copy paste this in to our program and then we can utilize it now obviously the cool part of this is from that example you could see how you could do something totally on your own as well right you could create your own call back whatever you wanted it to do um but we're going to use this default one uh i'm i'm copying and pasting the code in just so you can see how it works so you can edit it in the future so that's our custom callback you don't have to do that again it's just something that we're utilizing so we can keep track of our training so now that we have our call back we can do model.,340,17,17,PxoG0A2QoFs
21,"learn this is where we're actually going to train we can do total time steps now i know here that total time steps a million would be pretty good so we're going to do 1 million that's 1,000 and 1 million there we're going to set our call back equal to callback now you might notice that i'm using these attributes and my variables as the same those don't have to be so you could call this call back a and then it could be call back a down here right they don't have to match uh mine just do for simplicity oh and then for our tensor board i i like to name these so there's a table a tensor board log name so you can name this if you're going to try different algorithms and you're going to try different learning rates on the tensor board which you'll kind of see here i'll show you a little preview uh you want to name it so you can keep track of what you were doing so i'm going to do po and i'll do dash the only thing really unique that we did about this po is we set the learning rate now what else can we do with it you can go to the documentation right so so we can go to our documentation and that i now showed you and we can go to po and here if you slide down you can see all the parameters you can change so you can see learning rate right here right so the default learning rate if you don't set it is going to be",358,18,18,PxoG0A2QoFs
22,00003 now i know that this trains a little better with 03 uh so we're going to edit this but you can see you can change all these other values as well experiment it's programming you're not going to end the world or anything crazy try it out why not so now back to our program after we're done learning let's do model.,81,18,18,PxoG0A2QoFs
23,"saave and we can pass in the env id so this will save our model now remember we're going to keep creating our best model over and over again every thousand steps so we're going to know what our best environment is this one won't necessarily be our best right at the end of training our training curve might go down a little it might get a little worse towards the end of it so we might we're probably going to want to use our best model but just in case let's save this as well and then let's do a sanity check at the end and just say done den done learning so we'll print that out cool we should be good so this should train now it's probably not going to work we're probably going to get some erors but that's okay that's part of programming we'll dig in and fix those so let's go ahead and try and run this and i'm going to fast forward in time a bit now and i will come back and then we'll show you how to look at how your results are going on tensor board let's pause for just a second movie magic jump ahead all right so we don't got much here but we have a few time steps 16,000 right now i'm gonna hit this little plus to create a new terminal and here i'm going to do tensor board d- log underscore dur and i'm going to say board so i'm passing in where oops no underscore so i'm passing in where where our tensor board stuff is found now i can",358,19,19,PxoG0A2QoFs
24,control click it started up a tensor board and here we're going to start getting our training information back so we're going to see our average episode scores and things like that as that updates so we're going to let this run you can refresh this page and it will keep telling you updated statistics and information along the way now we got one more thing to code right so we want to see our agent after we finish it we'll call this run.py we'll do a few imports not very many compared to the last one we'll import retro from stable baselines three we'll import po and then we want to do the exact same environment that we did before so we still want to do our skip so we'll do max and skip env now we're going to load our models so our best models are going to show up in this best model folder so that's where i want to load it from now we haven't we don't have one yet you might already if your training is complete but we'll do p.,241,19,19,PxoG0A2QoFs
25,load and we're going to load in from tmpb model. zip that's where our best model will go and let's do def main in inside here we can create our env so we're going to do env retro. make it's very similar to our random agent game equals and here we can pass in super mario bros i'm going to copy paste that then remember we got to make it the same environment as what we trained on so we're going to do max and skip so if you don't skip frames in the training don't skip frames here observation equal env. reset done equals false we'll say while not done then each time we're going to get back an action and a state from model. predict this time we're going to predict based on the observation not mode model now we'll do obs reward done info that equals env. step we're going to step based on the action that our model predicted then we'll do env.,217,20,25,PxoG0A2QoFs
26,step we're going to step based on the action that our model predicted then we'll do env. render so we can see it that should be good and then we'll do the same thing we did before if uncore uncore maincore uncore oops if uncore uncore name uncore _ equals uncore uncore maincore uncore then we're going to run our main function so this should work and then you should be able to view your best model all right let's pause here and i will come back when we have a trained model to show you all right now i have another computer that i've been training on for a while so i'm going to show you that result so so if we go ahead and run this program i have the same exact setup here so i'm going to do run.,184,25,26,PxoG0A2QoFs
27,pi do python run it's actually still training run.py this is after two million time steps maybe and you'll see that this is sped up but he progresses through the level beats the first level but you'll notice he does much better than our random agent so we've clearly made progress here i also actually before we quit here let's do loger dash dash log d this is on my other pc that i've been training training on and we can log to board this one you'll notice we have way more training so i did a po with the training skips see if we can look at this so this is the po that we're training now that inevitably had the best results so we'll take that off the board for right now you'll see this is the network that i originally trained with the skips and the default learning rate now you'll notice it reacted really extreme so when it would mess up it would basically crush the reward performance so i knew the learning rate rate was maybe a little bit too high that's why i lowered the learning rate in our successful one this po was without frame skips and it really didn't get any better after this so after a million i thought we were doing good so i decided to train that one again so this train again you'll see i continued training that network for another million and it didn't really progress so i knew something was up there so that's kind of how i utilize tensor board to kind of come to these results i hope you guys,358,27,27,PxoG0A2QoFs
28,like this video this line by line tutorial style it's what i love to teach we can have more of these on stable baselines ai whatever interest you let me know in the comments if you haven't joined the discord join we can answer your questions there and until next time keep coding,68,27,27,PxoG0A2QoFs
0,today we will be playing video games a game called lunar lander actually we will be training an ai using reinforcement learning to play lunar lander gymnasium makes it very easy to do reinforcement training for an environment using an api by the way this is my first time learning reinforcement training so i don't really know what i'm talking about they have a lot of video games from the atari that you can train agents for we will be training an agent for the lunar lander game if you go to the usage page they have a really nice diagram of the process the agent that we will be training uses a thing called a policy to determine what action it will take in the game environment upon taking that action the game environment will respond back with the observation and the reward for the action like if the lunar lander crashes the reward will be -1 100 points but if the lander lands successfully between the two flags the reward will be positive 100 points the goal of the agent is to take the actions that will result in the highest points with this back and forth between the agent and the environment it updates the policy and tries to take actions that gain points instead of lose points this back and forth is the training process enough rambling on to the install you will need miniconda i will leave a link to this page where you can install it launch miniconda and let's get started create a new environment using python 310 i will leave a link to all the commands and source,358,0,0,TXpBqgAqqek
1,code used in this video by the way who am i where is the regular host why am i here ah who knows anyways once the environment is created remember to activate it using this command then from the pytorch website install p torch using the command shown on the p torch page p torch p guys i'm getting hungry once that finishes we could try installing gymnasium using pip install gymnasium followed by the word all in brackets this will install all the different environment dependencies however if we do this now we will get an error but i am going to go ahead anyways so we can see what error we get and how we can resolve it you will be shown a small to mediumsized wall of text with the error message in the middle it mentions the error with sg.,187,0,0,TXpBqgAqqek
2,exe pies and now swigs now i am hungry and thirsty anyways this error can be resolved by installing swig separately with pip install swig with that installed if you try to install gymnasium now you will get to move on to the next error this one says microsoft visual c 14.0 or greater is required and that we can get it with the build tools from that url so let's go to that url and download the build tools this is basically the visual studio installer once you download lo and run the installer it will take you to the workload selections page you want to check mark the desktop development with c this will require 6.77 gigabyte space click the install once that finishes we can go back to the cond prompt and now we should finally be able to successfully install gymnasium with that done we can now install stable baseline three which provides p torch implementations of reinforcement learning algorithms we will install it with the extra in brackets that way we get optional dependencies like tensor board open cv and al pi to train on atari games if we wanted to with that done we are now ready to start training first let's try out the example code provided in the gymnasium usage page this code will basically launch the game and have the ai agent make random moves each time i am going to create a folder called ai and create a python script in that folder with that code remember to show file extensions in the folder options if you don't have that enabled like me something i learned,358,1,1,TXpBqgAqqek
3,to day was if you name your python script random.,13,1,1,TXpBqgAqqek
4,"py i'm going to delete this pach folder it created and rename the python file random agent let's run random agent this will show the agent doing random things the goal is to have the lunar lander land safely within the two flags as we can see the current agent playing this game is not trained in doing random moves resulting in the lunar lander crashing every time let's take a look at what the code is currently doing first we have the import to be able to use gymnasium the next line creates an instance of the lunar lander v2 environment from the gymnasium library the render mode equal sign human parameter specifies that the environment should be rendered visually for human viewing this resets the environment to its initial state and returns the initial observation and info the observation is the current state of the environment and info contains additional information this starts a loop that will run for 1,000 iterations this line randomly samples an action from the environment's action space this applies the above chosen action to the environment and returns five variables and finally if the episode as they call it has ended due to termination or truncation it resets the environment to the next episode what are all of these words like episode and termination and truncation you might ask well i have no idea really i hope that was helpful i will leave a link to the documentation and maybe you can tell me what all of this really means here is the stable baselines three website if you scroll down on the left you will find different available",358,3,3,TXpBqgAqqek
5,"algorithms we are going to try out dqn dairy queen network or delicious queso nachos did someone say nachos yes nachos indeed anyways if we click on that and scroll down on the right we will find sample code i will copypaste this code to a new python script i will call the script dqn dopy i hope dqn is not also a reserved keyword or something this code shows us how to train an agent save the model and then later load that trained model to play the game it has everything we need first we import gymnasium then import the dairy queen this creates the environment with human viewable rendering remember to change the cart pole to our lunar landing environment this initializes a dairy queen nachos model using the mlp policy and connects to the lunar lander environment the verbose setting enables logging this trains the model for 10,000 time steps logging progress every four intervals this saves the trained model to a file remember to change the cart pole to lunar lander the next line of code deletes the model we just saved this is here to demo the save and load operations so we can delete that line if we want to load a saved model we do it like this by specifying the model file name that we saved previously on line nine remember to change cart pole to lunar lander line 13 resets the the environment before going into an infinite loop line 15 in the infinite loop uses the trained model to predict the best action based on the current observation the deterministic equals true parameter means it",358,3,3,TXpBqgAqqek
6,"always chooses the action with the highest estimated value then it applies that chosen action to the environment and gets the new observation and reward back from the environment for that action the terminated and truncated means the episode or cycle has completed by either the lander crashing or landing safely so we reset the environment for this demo i am going to reduce the time steps online 8 to just 1,000 since this is just to show the process for training the model saving it and then loading it we don't need to train it for very long i'm sorry but i think it is time we speed this along first let's do the training so begin by commenting outlines 11 through 18 to speed up training you can remove the render mode human but since we are doing a short demo i will leave that in save the file and let's go back to the condo prompt and run it python dairy queen nachos dopy and there it is the training has started this will show a visual of the agent controlling the luna lander and over time as the training progresses it should slowly start to improve initially every attempt will result in a crash once the training is completed it will save the model in a zip file this is the name we specified when saving the model in our code within the zip there are multiple files representing this pytorch model there is also a system info text file which has some details about the environment we are running this showing the installed packages in this environment so where is the",358,3,3,TXpBqgAqqek
7,code for the environment where it has this rewards logic it is on the gymnasium github page go into the gymnasium folder and then into the environments the box 2d will have the lunarlander environment code let's take a look at that python script we can do a search in this file for the word reward and see how it is handling the reward logic for when the agent interacts with the environment it says after every step step a reward is granted the total reward of an episode is the sum of the rewards for all the steps within that episode for each step the reward is increased or decreased the closer or further the lander is to the landing pad slower or faster the lander is moving and it is decreased the more the lander is tilted increased by 10 points for each leg that is in contact with the ground decreased by 0.03 points each frame a side engine is firing and and decreased by 0.3 points each frame the main engine is firing and then the minus 100 or plus 100 depending on if it crashes or landed successfully and at the top it mentions the space actions do nothing fire one of the left or right engines or fire the main engine further down on the page is the actual code where it assigns the reward based on the criteria that was just mentioned so this logic is executed when the agent interacts with the environment based on this logic the environment returns back the reward value and the observation back to the agent so that the agent can update the,358,3,3,TXpBqgAqqek
8,"policy and perform its next action this is done over and over again as part of the training process anyways run the rest of the code that we had commented out which will run the model that we trained this is simply a matter of uncommenting lines 11 through 18 and commenting out lines 7 through n keep in mind we only train for 1,000 steps so the training is almost like no training this is just to show the process you would take in real training that 1,000 would be a much higher number so that you can train for longer and get better results save and run the dairy queen nacho script and there it is this is our trained agent playing the game it doesn't look much different than when we ran when it was moving randomly this is the result of only 1,000 steps of training the reason i didn't train this for longer here is because i will be doing that using the stable baseline zoo which offers a simpler and streamlined way to do the training without needing to write python code we will look at that next on this page there is a link to the reinforcement learning zoo repository let's go to this repository we can install it as a python package using this command copy paste and run it in our cond environment once that is done we can go back to the github page to get the command to start the training i will use the second command as it has a couple of additional flags that we can use let's copy paste it to a",358,3,3,TXpBqgAqqek
9,text editor so we can edit the values the first thing to notice is that it is calling a train.,26,3,3,TXpBqgAqqek
10,python script well we don't have that script so we will need to do a gate clone of this repository first before we can use this command did you mean to say get clone because it sounded like you said gate clone get g gate goat oh go away al if i try to do a goat clone right now it is not going to work because i don't have gate installed you can install gide from within cond by running the command cond install grits i don't even know what to say with it installed i will run the git clone again once it is cloned we can cd into the directory it created so that we can execute that train python script before we run the script we'll want to change some values the algorithm we are using is dairy queen nachos so change that to say dqn for the algo flag and then change the environment to lunar lander i'm going to leave the rest as it is if you want to train for longer you can increase the episodes from 10 to something higher let's run this command to get started with the training and that is it we didn't need to write any code this will automatically run the training using the algorithm and environment we specified we can modify parameters and flags to configure the training further we will look at that documentation briefly in a bit as it's doing the training it will output certain metrics let's see what they mean the ep length mean is the mean episode length average over whatever the stats window size parameter value,358,4,4,TXpBqgAqqek
11,is which represents the episodes and it defaults to 100 the stats window size parameter is the window size for the rollout logging specifying the number of episodes to average the reported success rate mean episode length and mean reward over what i asked ai to explain all of that to me like i am five let's imagine you're playing a fun game and we want to know how well you're doing for the ep len mean imagine you're playing hide and seek each time you play is called an episode we want to know how long each game usually lasts so we look at the last 100 times you played that's called the stats window size add up how long each game took and then divide by 100 that gives us the average or mean length of your hide and seek games for the stats window size this is like a big window where we can see your last 100 games we only look at these 100 games to figure out how well you're doing it's like saying let's only look at your last 100 hideand-seek games to see how you're improving not all the games you've ever played so when we say mean episode length we're really saying on average how long do your hideand-seek games last when we look at your last 100 games the stats window size of 100 is like saying we'll always look at your last 100 games to figure out how you're doing not just your very last game or all the games you've ever played next is the ep reward mean this is the mean episodic training reward,358,4,4,TXpBqgAqqek
12,averaged over 100 episodes this one is simpler to understand this is the mean reward score the higher this is the closer the agent is to being able to consistently land a value of 200 is considered really good then is the exploration rate the current value of the exploration rate when using dairy queen nachos it corresponds to the fraction of action taken randomly imagine you're playing a game where you have to find treasure in a big playground you have two ways to look for treasure you can look in places you already already know might have treasure you can explore new places you've never looked before the exploration rate is like deciding how often you want to try new places if the exploration rate is high like 100 it's like saying i'm going to look in new places all the time if it's low like 10 it's like saying i'm mostly going to look in places i already know but sometimes i'll try a new spot if it's 0 you're saying i'm only going to look in places already know about in the game sometimes the computer will choose randomly where to look that's the exploring new places part the exploration rate tells us how often it does this for example if the exploration rate is 20 it means 20 times out of 100 the computer will look in a new random place 80 times out of 100 it will look in places it thinks are good based on what it has learned this helps the computer learn about new good spots while also using what it already knows about where treasure might,358,4,4,TXpBqgAqqek
13,be the total time steps is simply the total number of steps in the environment that will increase over time showing the counter the learning rate determines how quickly an agent like a computer program or robot updates its knowledge based on new information it receives think of it like this imagine you're learning to play a new game every time you play you learn something new the learning rate is like how much you change your strategy based on each new game you play if the learning rate is high it's like saying i'm going to change my strategy a lot based on this new game the agent makes big changes to its knowledge quickly this can be good for learning fast but it might also make the agent forget old useful information too quickly if the learning rate is low it's like saying i'll only change my strategy a tiny bit based on this new game the agent makes small gradual changes to its knowledge this can be good for stable learning but it might take longer to adapt to new situations finding the right learning rate is important too high and the agent might keep changing its mind too much and never settle on a good strategy too low and the agent might learn too slowly and take too long to find a good strategy in reinforcement learning the learning rate is usually a number between zero and one it's often represent presented by the greek letter alpha for the loss imagine you're playing a game where you're trying to guess a secret number every time you make a guess someone tells you,358,4,4,TXpBqgAqqek
14,if you're close or far from the right answer the loss is like a score that tells you how wrong your guess was the bigger the loss the further away your guess was from the correct answer in reinforcement learning the computer is trying to learn the best way to do something like playing a game or controlling a robot the loss tells the computer how bad its current way of doing things is it's like a measure of how many mistakes it's making when it says current total loss value it means this is how bad the computer's current strategy is overall it's adding up all the little mistakes to get one big number the goal is to make this loss number as small as possible as the computer learns and gets better the loss should get smaller if the loss is big it means the computer still has a lot to learn if the loss is small it means the computer is doing pretty well so when you see current total loss value it's telling you how well or badly the computer is doing right now in its learning process the computer uses this information to know if it's improving and to figure out how to get better and finally the ncore updates first imagine the computer is learning to play a game like chess the computer has a strategy for playing this strategy is like a big set of rules it follows to decide what moves to make as the computer plays more games it tries to improve its strategy each time it makes a change to its strategy we call that an,358,4,4,TXpBqgAqqek
15,update these updates are called grent updates because they use something called a gradient to figure out how to change the strategy you can think of a gradient as a signpost telling the computer which direction to change its strategy to get better the n underscore updates is just account of how many times the computer has changed its strategy so far for example if n underscore updates is zero the computer hasn't made any changes yet if an underscore updates is 100 the computer has made 100 changes to its strategy if an underscore updates is 1 million the computer has made a million changes this number helps us know how much learning the computer has done a higher number usually means the computer has had more chan to improve its strategy however more updates don't always mean better performance sometimes too many updates can make the computer overthink and actually get worse so when you see an underscore updates number of gradient updates applied so far it's telling you how many times the computer has tweaked its strategy in an attempt to get better at its task i will leave a link to the documentation page which has these details this training is still ono so i will skip ahead to when it has finished when the training finishes you can go to the logs folder and then the dairy queen nacho folder to see the model files there are two zip files that it created one is the latest model and the other is what it thinks might be the best model based on the metric data during training you can try each,358,4,4,TXpBqgAqqek
16,of these out using that random python script we created or you can use the enjoy python script that is part of the rl zoo i'm going to use the script from the zoo let's copy paste it to a text editor and modify some values we will need to modify the algorithm the environment and folder by the way dqn stands for deep q network but i like the way dairy queen nachos sounds even though i don't think you can get nachos at dairy queen or can you anyways once you made the changes to this command we will need to move some files for some reason the files it creates during training is one folder too deep than where the enjoy script expects maybe i am doing something wrong with the command parameters but in any case i have to move the four files out from that extra folder back up one folder for this enjoy script to work by the way if you add the load best flag it will load this best model and without that flag it will load the latest model i will copy these four files then go up one folder and paste them and then delete the folder where i got them from and now we can run that command and now we see the agent from our trained model is able to land the luna lander between the two flags i will leave a link to where the documentation is for the different parameters you can use for the training script and for the enjoy script this was a fun adventure learning how to use this,358,4,4,TXpBqgAqqek
17,gymnasium with stable baselines 3 and the zoo to see how the whole reinforcement training process works there are a lot of other game environments available to play around around with you can even make your own environment i think i will need to try that out at some point create our own game and then train an agent to perfectly play that game sounds like fun anyways that's all for this video thanks for watching nachos,100,4,4,TXpBqgAqqek
0,"each of these cars is controlled by an artificial intelligence ai in the racing game trackmania. and this ai is designed to improve over time through trial and error. the longer it trains, the better it gets. with enough training, it should be able to find the best lines, to drift perfectly, it might even become unbeatable. at least that's the theory. actually, i've already tried to build such an ai, several times, and it could drive but i've played this game for years and it wasn't enough to beat me. still i think it has some potential. so about six months ago, i decided to give this project one last chance, and to offer the ai an opportunity for revenge. this video is the conclusion of a three-year journey to make an ai that could beat me in trackmania. and this time it got much better than i had expected. but first let's start with this very simple track, to better visualize what this ai does. the ai uses something called an artificial neural network. basically, it's some kind of mathematical tool which roughly models how a brain works. every tenth of a second, this neural network receives a few numbers which describe what's happening in the game. in response, it outputs new numbers specifying the action to perform. hopefully, the network will select actions that ensure the ai finishes this track as quickly as possible. but this will only happen if the network is configured correctly, that's the tricky part. for this, i'm using a method called reinforcement learning. here's how it works. the ai starts from scratch with zero prior knowledge about anything. so at first its decisions are quite random.",369,0,20,Dw3BZ6O_8LY
1,"so at first its decisions are quite random. but for every action it takes it's rewarded, depending on how good that action was. the faster the ai progresses along the track the higher the reward. so with each new attempt, the ai explores the game and gathers data from it. now the idea of reinforcement learning is to use this data to progressively tweak the neural network, in a way that reinforces actions leading to more reward. actually, all the cars i'm showing are controlled by slightly different versions of the neural network, and they represent successive states of the ai as it learns. in each new attempt, the ai is trying actions based on some of its current knowledge. sometimes it goes well, sometimes not. but whatever happens, the ai can use this fresh knowledge to update its decision process, and it's through this trial and error loop that the ai gradually learns the game, all by itself, until it has mastered it. at least, that's the theory ! in practice, it's been a real nightmare to get this thing working properly... let's go back a few months in this project. this where the ai's first decent attempts on this track, compared with my personal best. it wasn't my first time with reinforcement learning, and the ai seemed to pretty much understand what it had to do. but it would often get stuck in a sub-optimal strategy, unable to come up with anything better. in particular, the ai loved to hit the walls. and in a way it makes sense.",339,20,36,Dw3BZ6O_8LY
2,"and in a way it makes sense. when the ai decides to smash into a wall at full speed, it actually collects more rewards initially, and it's only later that it turns out to be a bad decision. such conflict between short and long-term reward is one of the many things that makes reinforcement learning difficult to get right. and when you observe the ai doing weird things, it's quite tricky to figure out where the problem lies. maybe there is a bug in my code ? or maybe it's the method itself, is the reward signal clear enough ? i could let it train a little longer to see, but i don't know.. it could be that the ai doesn't see enough to locate the walls. and did i configure the learning algorithm properly ? should i try a different algorithm ? this seems interesting... let's check out the comments on my last video, i might find some help there. oh maybe it's because the ai can't break ? no. that i'm sure is not the issue. but for the rest, well it's hard to know. so like my ai, i entered a trial and error loop of guessing what to fix, re-running the training, and waiting to see if it got better. usually it didn't. this was a painful process. especially because the training part needs to run for a long time before producing any results. it's hard to learn on your own from scratch. an ai like this usually needs to gather a lot of data to even begin to understand what's happening.",345,36,57,Dw3BZ6O_8LY
3,"an ai like this usually needs to gather a lot of data to even begin to understand what's happening. so each time i want to test an idea, i need to let these training sessions run during hours on my laptop, before getting any useful feedback. that's why i went back to this project with such a simple track and without enabling the brake. every simplification of the decision making space tends to make the problem easier and quicker to solve, and i found it easier to progress this way. and eventually, as the days passed, all these efforts started to pay off. after many small adjustments in my code, the ai finally stopped hitting the walls and it was getting closer and closer to my time. music until finally, it happened. and this was only the beginning. music music three years after starting this project, i had finally trained an ai that i would probably never be able to beat. and i have to admit: after playing this game for so many years, it was a strange feeling to be outmatched like that by a computer program ! but this track: it's quite simplistic. it was time to challenge the ai one step further. last year, i spent a few hours trying to get a good time on this map. i then trained an ai on the same map, without much success. now it's time to try again and see if the ai can get its revenge. music music overall, the training method remains the same as on the first track. the main difference lies in the observation input the ai gets when it drives.",358,57,73,Dw3BZ6O_8LY
4,"the main difference lies in the observation input the ai gets when it drives. just like on the first track, the ai gets a few car metrics, such as its current speed. it also gets its position relative to the road centerline. but on this track, the road layout is no longer repetitive, the ai must anticipate the upcoming turns. so i added new inputs to encode the map path for the next three corners. to make the video easier to follow, i'm only showing the attempts from the starting line. but in reality, the ai regularly spawns anywhere on the map during its training. this prevents it from focusing too much on the first turns. here's an overview of the first 5 hours of training. some attempts are already ahead of my personal best in the first few turns. this looks much more promising than last year. but the ai is not sufficiently robust, and it never maintains its lead for long. at least for now. one thing that surprised me is that the ai takes most turns very inside. it appears that the game's physics are slightly more complex around these road edges. for this reason, i had to include a few more inputs to make sure the ai understood everything that was going on. adding these two inputs gives the full orientation of the car, and these inputs indicate which wheels are in contact with the road, and whether they are sliding. music after nearly 9 hours of training, the ai managed to complete the map, with a pretty good pace. it completely crushed the ai from last year, finishing only 13 seconds behind my personal best.",364,73,91,Dw3BZ6O_8LY
5,"it completely crushed the ai from last year, finishing only 13 seconds behind my personal best. and it continued to complete the map on subsequent attempts, it was becoming very consistent. but could it also drive faster ? music music music that's it. after playing this game for 35 hours, the ai was already faster than me on this map. but i wasn't ready to give up yet. now it was my turn to train. this time, i will try with the brake. braking doesn't add a huge advantage on this map. still, it makes it possible to drift, which can save a few tenths of a second here and there. this time, i was able to finish the map in 4min36. of course, this gives me an unfair advantage over the ai, since i don't allow it to break yet. but i was still curious to see how close it would get to this new time. well, not only did the ai beat this new time, but it wasn't even close. more than 5 seconds ahead. now, it seemed it was time to admit the superiority of the ai on this track. but only on this track, right ? well, i tested the ai on another map where it had never trained before. and it was quite good ! i also tried to make it drive the training track in reverse, and it adapted pretty well to this new context. but overall, on these unseen tracks, the ai is less precise, it makes more mistakes, and sometimes it just gets completely confused. especially when it's approaching a long straight line for some reason. all this stuff relates to the question of generalization.",367,91,113,Dw3BZ6O_8LY
6,"all this stuff relates to the question of generalization. there would be much more to say on this subject, i've tested a whole bunch of other things like modifying the road surface, adding obstacles, even changing the car's physics. but let's forget all that for now, i'd prefer to return to the training map. because even there, there is something about this ai that doesn't fully convince me. of course i agree, the ai completely crushed me on this map. but does it really drive faster than me ? i mean, my personal best run contains quite a few mistakes. trust me, it's super hard to get through these 230 corners at a good pace without failing. the ai on the other hand is extremely consistent. and i think that's what makes it so strong, in this kind of endurance scenario. however, if i focus on just one part of the map, until i drive a run with no errors that i'm fully satisfied with, will the ai still be faster than me ? this is the third and final map where i will face the ai. and if it beats me here, then i will be fully convinced. music this time, the ai didn't beat me once. it's becoming clear that without brakes, the ai is badly disadvantaged. so far, i've always disabled the brake on the ai. braking makes the game much more complicated to understand and master. i was afraid it would make ai training too complex. maybe i was wrong. anyway, i think it's now time to make the ai drift. here's the plan: i'm going to retrain the ai on the long map with the brake available.",366,113,133,Dw3BZ6O_8LY
7,"here's the plan: i'm going to retrain the ai on the long map with the brake available. then, i will compare this new ai with my best run on the shorter map. this will be our final dual, and it will determine, once and for all, which of us is driving faster. music thanks to the brake, the ai is now slightly faster than before. but for some reason, it doesn't drift. that's a surprising choice. just out of curiosity, i also tried to train it with the brake on the first track, and the result is the same: it's faster, but it doesn't drift. though i'm not sure drifting is useful on the first track, i'm pretty confident it saves time on the other maps. and yet, the ai chose not to drift. actually i found a few rare cases where the ai does some kind of drift. it appears to deliberately clip the road corners to unbalance the car, and initiate a drift. definitely not the most straightforward approach ! it's not so surprising to see the ai struggling. the road is very narrow, and the many curves prevent high speeds. and in trackmania, there is usually only one way out in such cases: by using a trick known as the neo-drift. by applying this precise sequence of actions correctly, it's possible to trigger a drift, even at relatively low speeds. it's the kind of trick that's pretty hard to discover on your own. i think the ai must have stumbled across it at least once during its many hours of trial and error, but it probably didn't have enough driving experience to explore further in that direction.",364,133,150,Dw3BZ6O_8LY
8,"i think the ai must have stumbled across it at least once during its many hours of trial and error, but it probably didn't have enough driving experience to explore further in that direction. such trick won't bring any advantage if it's not properly mastered. so, can an ai learn to neo-drift from scratch ? probably, but maybe we could also help it a little. from now on, the ai will get a big reward bonus whenever it's drifting. to detect if the car is drifting, i'm looking at this input. when it's high enough, it basically means that the car is pointing in a different direction from where it's actually going, which usually means that it's drifting. ok, let's restart the training. music well, i guess the ai outsmarted me ! apparently, it found a way to constantly trigger the reward bonus, just by spamming these weird action patterns at low speed. ok new rule: now the reward bonus only applies when the car speed is high enough. let's restart the training again. music now this looks good. the ai has clearly mastered the neo-drift. so well in fact, that it even chains multiple drifts in straight lines, to get more rewards. obviously, in terms of speed alone, its current strategy isn't effective. so let's continue the training without the bonus. now that it discovered how to drift, the ai shouldn't forget it. music music over the next hours, the ai learned to drift more wisely, only where it saves time. its driving looks cleaner than ever, and it destroyed its previous record on the endurance map. it's now 16 seconds ahead of me.",357,150,170,Dw3BZ6O_8LY
9,"it's now 16 seconds ahead of me. i think it's time to answer our question: aside from its endurance skills, is the ai still faster on a shorter map ? to answer that, i made the ai drive this final level many times and i selected its best run. this one. this run is the culmination of a three-year journey to create an ai that can beat me in trackmania. but i also did my best to drive a challenging run myself, putting into practice my years of experience in this game. now it's time to find out who is faster. music music music after showing that it was more precise, more consistent, the ai again outpaced me in this final test. and it seems time to accept that i can't beat it anymore. but is this ai truly unbeatable ? on this last level, i'm sure not. some of its lines are still quite far from optimal, and i'm certain that many better players than me could easily drive a faster time. but on the first two levels, i'd be quite surprised if anyone can beat the ai. if you want to try it for yourself, the game is free and these tracks can be downloaded online. of course, all these maps are quite simple, and we've only just scratched the surface of this complex and beautiful game. now that the ai is working pretty well, it deserves to be challenged a little more. but that's for another time. this video already took way too long to make. if you'd like to see more, and if you want to support this youtube channel, i just opened a patreon page to which you can subscribe.",370,170,188,Dw3BZ6O_8LY
0,hey nick what you've been working on oh man i've been working on some awesome stuff i'm actually using reinforcement learning to train a race car to race around the track oh really how's that going yeah it's going great yup great at doing burnouts music i promise guys it does get better than this let's get to it what's happening guys my name is nicholas chernotte and welcome to the reinforcement learning course in this video we're going to be covering a bunch of stuff but basically the core goal is to be able to allow you to go from absolute beginner to being able to go and leverage reinforcement learning we're going to cover a ton of stuff specifically how to set up your environment how to work with different algorithms we'll also test out on some pre-built environments using open ai gym so you'll be able to balance a cart pole you'll be able to build your own self-driving car and then last but not least we're also going to take a look at how to build custom environments something which is so so important when it comes to being able to leverage reinforcement learning for a use case which is relevant for you but all in all by the end of this video you should be able to take away those skill sets and be able to leverage reinforcement learning in a practical manner ready to do it let's get to it alrighty guys welcome to the full-blown reinforcement learning course now this course is intended to be a practical guide in terms of getting up and running with reinforcement learning so,358,0,0,Mut_u40Sqz4
1,ideally it aims to bridge the gap between a lot of the theory that you see out there and practical implementation now we're going to be covering a ton of stuff in this course so let's take a look at our game plan so first up what we're going to be doing is we're going to be taking a look at rl in a nutshell and this really talks about and specifically in this section we're going to be talking about how reinforcement learning works and learns some of the applications around rl as well as some of the limitations then we're going to take a look at how you can set up your environment to work with reinforcement learning and then we're going to be using a library called stable baselines then under step 2 we're going to be taking a look at environments so environments are one half of the equation when it comes to working with reinforcement learning so we need to be able to set up an environment and specifically open ai gym environments to be able to work with reinforcement learning then we're going to kick off our training so there's a whole bunch of different types of algorithms available inside of stable baselines so we're going to take a look at how we can set up some algorithms to be able to train a reinforcement learning agent then under step 4 we're then so once we've trained our model we're then going to test it out and evaluate it so this is easier than it sounds so you can set up an environment and test it out and see what,358,0,0,Mut_u40Sqz4
2,your agent actually looks like then we're also going to take a look at evaluation as well as how you can take a look at different metrics how to understand those metrics and we'll also take a look at how we can open them up inside a tensorboard something which i really really like then we'll take it one step further so step five we'll take a look at how we can leverage callbacks to stop our model trading once we hit a certain threshold we'll see how we can use different algorithms so there's a whole bunch of algorithms available in reinforcement learning you don't need to write them yourself there's a whole bunch already written for you that you can use and we'll take a look how we can use those and then we'll also take a look at different architectures so say for example you wanted to change the neural network that sits behind a particular agent you can do that as well but this wouldn't be a full-blown course unless we had some projects as well so we're going to be taking a look at three different projects so we're going to take a look at how we can solve the breakout environment which is an atari game so it's it's sort of like pong a little bit but not really we'll also take a look at how we can solve a self-driving environment so that's a car racing environment and how we can train our model to only have a picture as an input and train our car to drive along a racetrack which i think is pretty awesome and then,358,0,0,Mut_u40Sqz4
3,we'll also take a look at custom environments something which i think is so so often overlooked so this will allow you to get a better understanding of how to build an environment to work with reinforcement learning now the framework that we're going to be using when we build our customer environment is going to be open ai gym so i'm going to show you all the different types of spaces don't worry if you don't understand that yet or if you're not too sure what i'm talking about we'll go through it in great detail okay that's a game plan in a nutshell now it's time to take a look at irl in a nutshell so i wanted to include this section to give you a little bit of context about what reinforcement learning is how it's meant to be used and some of its applications as well as some of its limitations this is not going to be a full deep dive into the theory and the maths behind it it's just a high level overview so you get an idea as to where rl fits in in the big world of machine learning and data science so first up what is reinforcement learning well reinforcement learning focuses on teaching agents through trial and error that's a really really high level statement now i know there's probably a lot of hardcore deep learning engineers that will probably go nick that's not quite right but it sort of gives you an idea as to how reinforcement learning learns ideally you've got an agent and it learns based on the reward that it gets so,358,0,0,Mut_u40Sqz4
4,try something out if it doesn't get a reward then it tries something else if it doesn't get a reward or it gets a bigger reward it might try doing that multiple times we've also got this thing called the exploration exploitation trade-off so again i'll talk about that a little bit later but you sort of get the idea reinforcement learning is learning and based on actively engaging with an environment now that brings us to how the framework actually fits together well there's four key things or well five key things that you need to consider whenever you're working within reinforcement learning or there's four fundamental concepts so they are the agent the environment the action and then reward plus observations so think of your agent as something which is operating within an environment so this might be a machine learning model might also be a person or a player if you're working in a game environment your environment is where that particular agent is actually operating in so in this case say for example if we take a game so your player is operating within the game environment so it's getting reward based on what it actually does there now your agent will see what's happening within that environment so say for example we're taking a look at a game your player will be able to see what's around them so in terms of the observation so it'll see what the game environment actually looks like and then it'll also see what reward it accrues based on the actions it takes so ideally your agent might walk around the environment it might do,358,0,0,Mut_u40Sqz4
5,something and might accumulate a point might do something else it might not accumulate a point it might even lose a life that might be a negative reward a really really good way to sort of get your head around this is to think of how you might go about training a dog say for example you wanted to teach your dog how to sit or how to lay down well your agent in this case is going to be your dog because you're trying to train your agent to be able to take the right action now the reward in this case is you giving your dog a treat every time they do the right thing so what your dog might try to do is take an action so initially you might say sit and the dog might not actually do anything so in this case it hasn't actually taken it or it's taken an action of doing nothing and in this particular case the environment that it's working with is the environment with yourself in it so in it's trying to get a reward or trying to get a treat from doing a particular thing now your dog will eventually see that it gets no reward because it didn't sit down so it might try something else so in this case you might say sit again it might then sit and then it'll say that it'll get a reward so ideally it will then start to learn what action to take in response to the environment in order to maximize the reward so it's observing the command that you're giving to be able to,358,0,0,Mut_u40Sqz4
6,take the right action so this in a nutshell is how reinforcement learning works your agent tries to take an action in order to maximize its rewards in response to the observations within the environment now again i just wanted to give you a little bit of theory we're not going to delve into this too much but you sort of get the idea as to how reinforcement learning works it's a little bit different in terms of how you might work with tabular deep learning and machine learning because your agent is actively engaging with a simulated or a real environment now in this case we're going to be dealing with simulated environments but i'll talk a little bit about that later so what are some applications or practical applications of reinforcement learning well there's a whole heap out there and there's only becoming more reinforcement learning is really really popular right now because there's a whole heap of open world environments that people are trying to solve using machine learning and deep learning one of which is autonomous driving so you can see this picture up here this is actually from an environment called kala so kala is a really really popular driving simulation which allows you to actually train autonomous agents or perform reinforcement learning on it now you can actually train a car to be able to navigate through an open world using reinforcement learning it's pretty pretty cool right now another great application of reinforcement learning is securities trading so again think of this so your agent in this case will be like an autonomous trader your environment is going to,358,0,0,Mut_u40Sqz4
7,be the securities trading environment so ideally what you're going to do is you're going to try to train your agent to make trades that are going to make you profit so ideally it wants to buy low and sell high and sell high and buy low so if it's short selling again this is really really popular at the moment there's a heap of stuff happening in that space another one which i'm personally fascinated by is a neural network architecture search so what you can actually do is use reinforcement learning to build up a neural network for you and find an optimal neural network which i think is absolutely crazy so say for example you're trying to build a deep neural network to solve a particular use case you might not know what the best type of architecture in terms of layers in terms of number of units or in times of activations is you could actually use reinforcement learning to try to solve this problem for you now this is obviously super advanced but it sort of gives you an idea as to what's possible with the tech another place that reinforcement learning is super popular right now is in robotics so training agents or training robots in real life can often be quite expensive because say for example you've only got one robot can be hard to train on a lot of tasks so what you can actually do is build up simulated environments of that particular robot and train that robot to do a particular thing now in this case the agent is going to be the autonomous model which,358,0,0,Mut_u40Sqz4
8,is training the robot the environment that it's operating in in this case this agent i believe is trying to move a ball to the correct position this is actually based on a simulation environment called mujocho so again i'll show you that a little bit later but we're not going to be solving that one today but you sort of get the idea so we can actually train the robot the environment is going to be moving the ball to the right place and the reward is going to be how close or how far that ball is from its optimal position so again there are a whole heap of applications i've only sort of shown four there but there are a ton out there another place where it's really really popular is in gaming so again gaming is an open world environment so the reward function can be really really different each and every time you can sort of see how it can start to apply into different environments okay so what about some limitations and considerations for reinforcement learning so again reinforcement learning is absolutely amazing and i'm fascinated by it but there are some limitations specifically for simple problems reinforcement learning can sometimes be overkill so say for example we're taking a look at hyper parameter optimization there's already really really powerful models for that particularly when you're dealing with simple models but if you're getting to super advanced problems reinforcement learning could help you out in that space another thing that it assumes is that the environment is markovian that means your future states for your environment are based on your,358,0,0,Mut_u40Sqz4
9,current observations and there's no random acts but we know in real life that there's going to be random events that happen that influence our particular model so say for example you were training your mujoko robot right in this particular case your environment might not cater to people walking past the robot or knocking the robot so you never really know what's going to happen in real life you can only train in your best case scenario so again we can sort of deal with this because in our reinforcement learning model we're going to sort of isolate our environment but again it's just something to take in mind when somebody asks you that question another thing to note is that training can take a long time and is not always stable so we've got this concept called the exploration and exploitation trade-off so ideally what your model tries to do is explore the environment when it's starting out and then it tries to exploit it to be able to get the best possible rewards but sometimes what might happen is your model might not have enough time to explore and might start exploiting too early so sometimes we need to tune hyper parameters to be able to get our model to truly explore the environment and truly understand it sometimes because we don't get this quite right our model might not all be that stable so we might get to a certain point we might reach a cap in terms of our maximum reward but now another thing to note as well is that training can take a long time so if you've got,358,0,0,Mut_u40Sqz4
10,a really really open environment so say for example you're trying to train a reinforcement learning model for grand theft auto because it's such a huge environment training a model to sort of work out what to do in that particular case is going to take a long long time all right now not to be down i just sort of wanted to bring up some of those limitations and considerations now on that note let's start getting onto our setup so step number one is going to be setup what we're first i'm going to do is install our required dependencies now in this case it's really really simple to get up and running with this just a single pip install all you need to do is run exclamation mark pip install stable dash baselines three and then insider square brackets pass through extra so stable baselines is a reinforcement learning library that allows you to work with model free algorithms but again we'll talk about that later so we can work with stable baselines to build up a reinforcement learning agent to be able to train against a specific environment now the cool thing about stable baselines is that it's actually based off an original library called baselines which was built by open ai the great thing about stable bass lines is that there's a whole heap of really really useful helpers now i've got the documentation on the screen so this is the full this is the migration link but if you wanted to go into stable baselines i'm going to include these links in the description below as well as all the,358,0,0,Mut_u40Sqz4
11,code that you're seeing in here but you can see here that stable baselines actually is or this is the documentation there's a whole heap of guides and it's a really really well supported environment or really well supported library as well so again really really really useful um there's a whole bunch of getting started information if you want to be able to leverage that sort of shows you how to get started really really quickly so this here is one single reinforcement learning environment and training in a single what is that like 40 20 lines of code so again you can get started really really quickly with this but we're going to be going through all of this in great detail as we're walking through it so let's kick things off and start by installing stable baselines so i'm going to be working inside of a jupiter notebook environment for this and i'm going to give you the baseline code or the starter code as well as the completed code as well inside of the github repo in the description below so you'll be able to pick up all of this and work with it at your own pace so first things first we're going to have 10 different steps that we're going to be going through for our main tutorial and then we're going to have our projects as well so the first thing that we need to do well let's actually take a look at these 10 steps so first up what we're going to do is import our dependencies then we're going to load up our environment so in this,358,0,0,Mut_u40Sqz4
12,case we're going to be solving a reasonably simple environment called cartpole and i'll show you that in a sec we're going to take a look at how to understand an environment because that is so so important then we'll train a reinforcement learning model i'll show you how to save it down to disk and reload it so if you wanted to go and move it elsewhere or go and deploy it you could do that we'll take a look at how to evaluate it how to test it how to view our logs inside a tensorboard how to add a call back to the training stage so this allows you to stop your training at a certain point once you're happy with it how to change policies as well as how to use an alternate algorithm so we're going to be covering quite a fair bit but again take it at your own pace and if you get stuck or if you have any questions at all hit me up in the comments below or join the discord server again link will be in the description below always happy to chat there as well all right enough on that let's actually kick this thing off and write some code so the first thing that we're going to do is install our dependencies and import them so in this case we're going to be installing stable baselines three so remember we had exclamation mark pip install stable dash baseline street and then in square brackets extra so let's go ahead and write that alrighty that looks like it's all installed successfully so you can see,358,0,0,Mut_u40Sqz4
13,we don't we've got a warning there that says to upgrade pip that's fine don't worry about it but it looks like we're all good to go now in this case that is now done so that again really simple to get started with stable bass lines it's a single pip install but again there's so much you can do with it which makes it pretty cool so the next thing that we want to do oh let's actually take a look at that line so we've written exclamation mark pip install and then stable dash which i'm just going to screw that up stable dash baselines and then three and then extra now the reason that we're passing through three is that stable baselines has gone through a number of iterations so there was a stable bass lines one and then a stable bass lines two we're now up to stable bass lines three so this is the latest package again which runs on tensorflow and pytorch we're going to be using pytorch for this but just uh something to keep in mind so that's the reason that we're passing through the three all right now that's our installation done we're all good to go now the next thing that we want to do is actually import some stuff so let's go ahead and import some dependencies and then i'll talk you through each one of those okay so those are our main dependencies now imported so we've gotten written five lines of code there so first up what we've written is import os so os is just an operating system library that's going to,358,0,0,Mut_u40Sqz4
14,make it a little bit easier later on when we go to define our paths to save our model as well as where to log out then we've imported jim so jim is for open ai gym but i'm going to talk about this a little more once we get into our environment section of our slides so jim allows us to build environments and work with pre-existing environments really really easily then we've imported our first algorithm so we've actually imported ppo so to do that we've written from stable underscore baselines three import ppo and again there's a whole heap of different types of algorithms so if we actually take a look we're actually taking a look at the stable baselines package that should be stable baselines three so if you actually take a look there's a whole heap of different algorithms there so there's a2c ddpg dqn her ppo sac and td3 now again there's a whole bunch of stuff here so i'm actually going to talk about when to use which algorithm um and under which circumstances so again don't fret if you've seen this and you're like oh my god there's so much we're actually going to go through this and i'll actually give you a little bit of a guide or at least some guide rails as to when to use which particular type of algorithm but in this case we're going to be using this one here so ppo so again if you want to see the documentation it's all there and you can take a look at the performance of that particular algorithm cool right okay so that's,358,0,0,Mut_u40Sqz4
15,this line here so from stable underscore baselines import ppo and then the next one that we've written is from stable underscore baselines three dot common dot vec underscore env import dummy vec env now i'll talk about this a little more once we get to the breakout tutorial but basically stable baselines allows you to vectorize environments this means that it allows you to train your machine learning model or train your reinforcement learning agent on multiple agents at the same or multiple environments at the same time this means that you can get a huge boost in your training speed by doing that now in this case we're not going to be vectorizing our environment so we're going to be able to use this dummy vec env wrapper instead you'll see what it actually looks like when we do vectorize in the breakout project but again for now just think of this as a wrapper around your environment makes it easier to work with stable bass lines then the next thing that we've written is from stable underscore baselines three dot common dot evaluation import evaluate underscore policy so evaluate underscore policy makes it easier to test out how model is actually performing so what you'll actually get when we run this is the average reward over a certain number of episodes again i'll talk about it more later and you'll also get the standard deviation for that particular agent that you're training so again five lines of code so import os import gym import our algorithm which is ppo import our dummy vec wrapper and import our evaluate policy helper which will be,358,0,0,Mut_u40Sqz4
16,used somewhere around here cool that is pretty much it in terms of our dependency so we've written five lines of code now on to step two environments so i think a key thing to call that is the difference between simulated and real environments now this is why we're using open ai gym so open ai gym allows you to build simulated environments really really easily so there's a whole heap of helpers it's a really well supported library and that's particularly why it's really really popular when it comes to working with reinforcement learning now a key thing to call out is that when we're working with reinforcement learning often one of the benefits is that by using simulated environments we're able to reduce cost and we're able to produce better models a whole heap faster now say for example you're working for an engineering company and your engineering company wants to build an autonomous agent to go and train this robot over here to be able to move a particular ball to a certain position now this robot is actually called a fetch row but it's actually a real robot so you can actually take a look at what it looks like now they might only be able to afford a single robot so this sort of limits how fast they may be able to train that particular robot and obviously there's costs involved with actually training that robot so you're going to be wearing down the joints you're going to be using electricity it's going to take a lot more time and cost to be able to train that robot if you're,358,0,0,Mut_u40Sqz4
17,doing it in a real environment now one of the amazing things about reinforcement learning is that you can try to simulate that environment to be able to train the agent in the same way over here you can see that this is actually a replica of this robot and it's actually built inside of a simulation tool called mojoko so again i talked about this a little bit earlier but this makes it a whole heap easier to train and a whole heap more cost effective to be able to go ahead and train your agent which is actually pretty cool because i mean this technology hasn't been around for a whole heap of time but it obviously improves the ability for people to leverage reinforcement learning so rather than having to go and do it in real or in real time on that particular agent they can do it in a simulated environment and run it on there but again ultimately what you may find is that whilst we may train on a simulated environment the end goal is to take that agent and go and deploy it on a production-like environment which would be a real robot likewise if you're doing it on a game you might train on a testing version of the game and you might deploy on a real version of the game so you sort of get the idea between a simulated and a real environment this is simulated this is going to be real now this is where open ai gym comes in so open ai gym gives you a really lightweight environment but really feature packed to,358,0,0,Mut_u40Sqz4
18,be able to go and build out a reinforcement learning environment now in this case you can actually take a look at the docs so it's at colon forward slash forward slash jim.openai forward slash docs so if we actually go to that link which is over here we can go to docs you can see that there's a whole heap of documentation around how to actually use open ai gym and the nice thing about this and particularly why i've used this particular environment or framework is that there's a whole heap of support so it's really really well supported a lot of people are using this when it comes to open air gym so know when you're looking at cutting edge stuff or you're looking at what skills to learn if you wanted to go and do this for your career open ai gym tends to be the standard in this particular space now there's a whole heap of pre-built environments that you can actually use inside of openai gym so remember i was talking about mujoko for that particular robot so you can actually oh it's might not be mujo it might be under robotics so you can see that we've actually got our fetch robot over here there's also this shadow hand robot now these are actually based on real robots so if you actually google fetch robot that's not what i meant to type a fetch robot you can actually see so it's actually a real robot that i'm actually showing you here so this robot is exactly mimicking this this robot this hand one is actually called a shadow hand,358,0,0,Mut_u40Sqz4
19,robot shadow and robot i believe there you go so you can actually see these are actually based on real robots that are out there in the real world so people are trying to train them using open ai gym now there's also a bunch of environments around algorithms around atari which we'll do a little bit later around box 2d so we're actually going to be testing out this one classic control so we're going to be testing out cartpol mujoko robotics toy text so on and so forth there's a whole heap there's also a whole heap of third-party environments so if you wanted to do something really really hardcore you could definitely take a look at those as well so i remember i was talking about carla i believe there's one over here so there's actually a wrapper to be able to leverage color as part of open ai gym now in this case we are going to be dealing with classic control to begin with so we're going to keep this relatively simple and try to solve the carpol environment so if we actually take a look at this the goal in this particular case is to get this little robot down here to be able to balance this beam now you can see right now it's sort of bumping around side to side and the beam is sort of falling over now there's two actions that we can really take we can move it to the left or we can move the cut to the right but again i'll delve into this a little bit more so what we're going to,358,0,0,Mut_u40Sqz4
20,be able to do is train a reinforcement learning agent to be able to solve that particular problem now what we're going to do next up is we're actually going to be taking a look at what that environment actually looks like now key thing to note is that when you're actually taking a look at open ai gym environments is that these environments are represented by something called spaces there are a number of different types of spaces that open ai gym supports now the names might be a little bit tricky when it comes to actually leveraging them but let me sort of walk you through them so the first one is box now this is a range of values so think of say for example you wanted a continuous value you're going to want to use a box space so the way to instantiate a box space is by using box and then passing through the low value the high value and the shape of the space again i'm going to delve into this a whole heap more when we actually take a look at our environment and we're actually going to use some of these spaces to actually build up our own custom environment in project 3.,274,0,0,Mut_u40Sqz4
21,so it's actually going to give you discrete numbers that represent specific mappings to something so typically you'll see discrete actions used for or typically you'll see discrete spaces used for actions so action zero will be something action one will be something and action two will be something else uh you've also got tuples so tuple allows you to combine spaces together so you can see we can use tuple and then pass through discrete and box so this just allows you to join them key thing to note is that stable baselines doesn't support tupor so again good to know but you're not going to use it all that much you've also got dict spaces so this is just a dictionary of spaces so really similar to two balls but in this case we're just declaring dict and then we're passing through a dictionary of spaces the other two types of spaces these are ones that i haven't dealt with too much but it's important to note that they're there so you've got a multi-binary space so this is a one hot encoded set of binary values so if you pass through multi binary and pass through the value four what you're going to get is a list of values and you're going to have four positions so you'll have zero one two three so ideally four values and you're just effectively going to have binary flags so ones or zeros represented in those positions so it's a one hot encoded vector of different actions or different spaces you've also got multi-discrete so this is very similar to our discrete space but in this,358,2,2,Mut_u40Sqz4
22,case you can have uh multiple sets of values so you'll have 0 1 so if i pass through 5 2 2 what i'll get back is a range of values between 0 and 4 for the first position 0 and 1 for the second position and 0 and one for the third position so again you can start to see how these spaces sort of play but enough on that let's actually take a look and start building our environment so back to our notebook what we're going to do now is start loading up our environment so first up what we're going to do is we're going to use open ai gym to instantiate our environment and then we'll actually test it out and take a look at it so let's first upload our environment okay that is two lines of code to be able to go and create our environment now i've gone and separated it out into two lines of code but you can make it one and i'll explain this so the first line of code that we've written is environment underscore name equals cart pole dash v0 now this is case sensitive so make sure you get the case correct so we've got a capital c-a-r-t p-o-l-e dash v-0 so this environment name is just a mapping to the open or pre-installed open ai gym environments then what we're doing is we're actually making our environments we've written emv equals gym dot make and then to that will pass through our environment name variable so again if we actually just printed out the environment name variable it's just going to,358,2,2,Mut_u40Sqz4
23,be a string right environment name just a string right nothing crazy there now what we can actually do is we can actually test out this environment so remember what we're going to do initially is just take random actions in that environment but eventually what we're going to do is we're going to take our agent and specifically our reinforcement learning agent and try to get it to take the right actions in that particular environment to maximize our reward that's what reinforcement learning is all about so what we want to do first up is sort of get an understanding of the environment it's really really important to understand what's actually happening in that environment before you try to do anything because you might be trying the wrong algorithms you might be doing a whole bunch of random stuff but understanding the environment is going to make your life so much easier trust me on this so let's go on ahead and write a bit of a loop to test out our environment so let's go do this alrighty so i've written a lot of code there but i'm going to take it step by step with you so again we're going to take this step by step whenever we're going through this stuff so what is that so one two three four five six seven eight 9 10 11 12 12 lines of code now again all of this code including the beginner as well as the completed tutorials are going to be available in the github description below so if you want to take this and walk through it and compare your,358,2,2,Mut_u40Sqz4
24,code you can do that first up what we're going to do is walk through each step or each line of this code so what we're actually going to be doing is we're going to be trying to test out the carpol environment five times now what we've actually gone and done is created a variable called episodes and we've set that to five so this means that we're going to try to loop through our environment five times to see how we can operate within it so we've written episodes equals five and then we're looping through each one of those episodes so for episode in range and then we're starting off at one and then we're going episode plus one so this is effectively just looping through each one of the episodes if we actually write this out to episodes equals fives for episode in range one comma episodes plus one let's print out our episode so you can see that it's just going to be looping through one to five right so that's all these two lines are doing here then what we're doing is we're resetting our environment so by running env.reset we're going to get an initial set of observations remember there was those five key components to any environment or four key components there was the agent the action the environment and then the observations plus the rewards so by running env.reset we're going to get our initial set of observations so if i type in emb.reset you can see that these are the observations for a particular environment now i'll talk about what these values mean in a second,358,2,2,Mut_u40Sqz4
25,once we actually get to understanding the environment but for now just understand that these are the observations that we get for our particular pole right so we're getting these four values now what we're effectively going to be doing is passing these observations or later on we'll pass these observations to our reinforcement learning agent to determine what the best type of action is to be able to maximize our reward so our agents going to see these values and it's going to go hey i've got these values what should i do or what action should i take to be able to maximize my reward and get that bar in the straightest possible position then over here we're just setting up some temporary variables so we're setting whether or not the episode is done so you've got a maximum number of steps within this particular environment and we're also setting up a running score counter across the episodes then we've got a while loop so while not done we're then going to render our environment so the render function allows us to view the environment or view the graphical representation of that environment key thing to call out is if you're running this inside a collab the render function is not going to work like this you've got to do a little bit of extra work so hit me up in the comments below if you want a little bit of help with that then what we're doing is we're generating a random action so rather than taking in our observations and actually generating an action which is actually useful we're just going to,358,2,2,Mut_u40Sqz4
26,take a random one so this is akin to doing this so i can just type in let's actually move this down so what would be doing is emv dot action space dot sample so we're just generating a random action this is actually really good to note as well so if i actually take off sample remember how i was talking about the different types of action spaces in this case here we've got discrete is two so this will mean that we get two different types of action so we've got zero or one so if we actually type in dot sample you can see we've got one this time got one got one got zero so you can see our action space is just going to have those two different actions zero or one this is what discrete two which is what our action space looks like represents now we can actually take a look at our observation space as well this is a key thing to call that so there's going to be two different spaces within any environment your action space so these are the actions you can take on that environment and your observation space so this is what your observations actually look like for that particular environment so it's a partial view so if we type in observation space you can see that we've actually got a box environment so i've got these values here so that's going to be a lower bound and this is going to be our upper bound and then we've got four comma so this means that we're going to have four values so,358,2,2,Mut_u40Sqz4
27,zero oh zero one two and one two three four and then they're gonna be in the d type float32 so again you can start to see how our environment is actually built up now again we can sample this if we wanted to and this is going to look almost identical to what we get from enb dot reset up here then what we can actually do is we can actually pass through our random action so this is the next line that we've written to our environment so we can do this using a and v dot step so if we actually did that so amv step and pass through the values 1 you can see we're going to get our observation back so again we can keep doing this and this is really just us passing through our action now what we actually get back from this is actually really really interesting so we're going to get back our next set of observations which are what you can see there and we're also going to get a reward so this is whether or not we're getting a positive value or a negative value so one is obviously increment zero is going to be a decrement or negative one is going to be a decrement and then true is basically specifying whether or not our episode is done so remember we've got this done statement here and this done statement up here so once our episode is done we're going to stop it so that full line of code is n underscore state comma reward comma done comma info so this is just unpacking,358,2,2,Mut_u40Sqz4
28,the values that we get from env.step and then the next line of code is just accumulating a reward so score plus equals reward and then we're just printing out the results that we actually get from taking that step so we've written print and then open quotes episode colon squiggly brackets score colon squiggly brackets i call them squiggly brackets dot format and then we're passing through our episode and our score and then last but not least we're closing our environment so when we use env.render you'll get this python pop-up to close it down you just need to run env.close cool so that is all well and good let's actually test this out so if we run this now you should see down the bottom that's our environment testing itself out now if we didn't want to close it we can just comment this line down here so we can actually see it and there you sort of go so again it's running really really quickly and it's just sort of moving the bar around when we actually go to test it out we'll see it run a little bit more slowly but you can sort of see so if we keep running it looks like we've screwed it up let's actually close it say amv dot close and then it's closed down so let's run it again then we can close it you can start to see what's sort of happening there right our actions are moving this black box to the left and to the right and our bar is effectively swaying based on response to that so ideally the goal,358,2,2,Mut_u40Sqz4
29,is to hold that bar as straight as possible for as long as possible cool all right so that is a whole bunch of stuff now done now we've taken a look at how we can sample our environment so let's actually take a look at that in a little bit more detail so remember there's two parts to our environment there's our action space and our observation space so if we type in env dot action space that's going to be our actions and then we can type in emv dot observation space and those are going to be our observations now you're probably thinking well nick what are these values that we get from these observation spaces so and action spaces so let's actually type this so if we write dot sample and dot sample to the end of these you can see that we've got these values so let's actually duplicate this so we've got both so amv dot action space and emb dot observation space right so this is describing the type of action space this is actually an example type of observation space and then this is an actual example now we can actually take a look at what these represent so i've actually got this link here which actually gives you a little bit more detail so in terms of that observation so this is actually from the open ai gym documentation so you can actually zoom into this so in terms of our observation space remember we've got a box 4 which is this down here so box and then four the first position represents the cuts position and,358,2,2,Mut_u40Sqz4
30,it's got a minimum value of minus 4.8 and a maximum value of 4.8 we've also got the cut velocity which is going to be this value here we've then got the pole angle which is going to be this value here and then we've got the pole angular velocities i'm guessing this is sort of the speed at which the pole is moving up or down so again these observation spaces just map to this now not every environment is going to be well documented like this but i sort of wanted to give you an idea so cut position cut velocity pole angle pole velocity cut position cut velocity pole angle pole velocity then in terms of our action spaces remember we've got two possible actions zero or one these are the descriptions for our actions so action zero is going to push our cart to the left action one is going to push our cart to the right so you can sort of see how these actions and observations sort of play together now that is our environment in a nutshell so you can see that we've gone and done a few things there so we went and defined our environment we then went and tested it out and then we went and actually took a look in granular detail to be able to actually understand how this environment actually fits together and i think this is really really important because it gives you an idea as to what the hell you're trying to solve but keep in mind that whenever you're solving one of these environments you're typically going to have an,358,2,2,Mut_u40Sqz4
31,action space and an observation space and it's a good idea to try to understand what each one of those means but on that note that is our environment now set up so we can actually close this so let's go and take a look at what's next so this brings us to step three training so a key thing to call out is that there are a heap of different types of algorithms when it comes to reinforcement learning now typically these are grouped into model based rl and model free based reinforcement learning algorithms now we're mainly going to be focusing on model freebase reinforcement learning algorithms because that's where a lot of development is happening but that's not to say that model based reinforcement learning isn't useful as well so a core thing to note in terms of model-free rl so the whole idea between model-free rl is that it only uses the current state values to try to make a prediction with model-based reinforcement learning what's actually happening is it's trying to make a prediction about the future state of the model to try to generate a best possible action so there are a whole heap of advantages for and again so i'm not going to go through it in great detail there is a great document on this down at the open ai website so under colon forward slash forward slash spinningup.openai.com there's a really really good explanation of model 3 versus model based rl now if you so i guess a key thing to call that is that stable baselines really only deals with model 3 based rl there are,358,2,2,Mut_u40Sqz4
32,a number of other libraries that look at model base rl as well i believe rl lib is one of them so we're going to be focusing on a model free rl and specifically we're going to be taking a look at the a2c algorithm ppo and we'll also probably use i think we'll probably only use those two to begin with but again we'll also use dqn over here as well so again we'll use a few different types of algorithms so you can sort of see what they look like but that sort of gives you an idea as to what's sort of out there there's a bunch of algorithms broadly grouped into model 3 versus model based reinforcement learning now a core thing to note is choosing the best possible algorithm for your use case so we've talked a little bit about different types of action and observation spaces so far now the algorithm that you're going to try to use or the algorithm that i'd suggest you use should ideally be one that maps appropriately to your particular type of action space so you can see here that in terms of stable bass lines there's a whole bunch of different types of algorithms so we've got a2c ddpg dqn her ppo sac and td3 and i'll show you how to actually use these different algorithms a little bit later in the main tutorial but a key thing to note is that certain algorithms can only work on certain types of action spaces so you can see here the a2c works on boxes discrete multi-discrete and multi-binary ddpg works on box spaces only,358,2,2,Mut_u40Sqz4
33,dqn works on discrete spaces only and this is in reference to the action space so a key thing to call that is that it's based on the action space not so much the observation space so remember if we go back to our main tutorial so it's this over here so if you type in emb dot action space and you say that it's discrete then you know let's scroll back then you know that you can use any one of the models down here that has a green tick under discrete so we could use a2c to solve this dqn her and ppo now if you had a box environment so remember if we take a look at our observation space assume our action space had a shape of box well then you'd be looking at using one of these ones a2c ddpg her ppo sac or td3 i've got a little bit of a guide down here so discrete single process use a dqn discrete multi-process use ppo or a2c using a continuous single process sac or td3 continuous multi-process ppo or a2c a key thing to call out guys is that treat these algorithms as commodities so you can choose to use whichever one you want for your particular use case some are going to perform better than others it's good to know how they work it is better to know in detail how they're put together but again you don't really need to know that or that level of detail to be able to try this out or try your hand at it it's just important to know which algorithm you,358,2,2,Mut_u40Sqz4
34,should use for which type of action space and again all of them are available inside of the stable baselines documentation so you can see that we've got all of those here so remember to get to this you can go to stable dash baselines3 dot read the docs dot io forward en forward slash master forward slash models and then this is if you want to look at the ppo algorithm modules forward ppo dot html but again all the links are going to be in the description below so you can grab that and pick it up cool so we've talked a little bit about different types of algorithms and when to use which one now another thing to note is that you need to be able to understand your training metric now which type of algorithm you use is going to determine what type of metrics you get during training but broadly you should get something that looks a little bit like this and you'll see this once we kick off our training so we can break this up into evaluation metrics time metrics loss metrics and then we've got other metrics so our evaluation metrics are all to do with our episode length and our episode reward mean well so these are our averages so our length is how long our episode actually went for so if you're playing a game think of it as one single game when we're trying to balance our cart poll at one episode is the number of steps the maximum number of steps we're allowed to take a time metrics so in this case we've got,358,2,2,Mut_u40Sqz4
35,frames per second so this is how fast you're processing iterations so that means how many times you've actually gone through time elapsed how long it's been running and total time steps so that's how many steps you've actually taken in an episode you've also got some loss metrics so you've got entropy loss policy loss value loss again if you want a greater detail or if you want a greater explanation on that hit me up in the comments below we've also got some other metrics as well so we've got the explained variance so this is how much of the variance in the environment your agent is able to explain you've also got your learning rate so how fast our policy is actually updating and you've also got n updates which is how many updates we've actually made to our agent now a core thing to call out is that by default when we actually go and install stable baselines using the pip install command we're only going to be installing that without gpu acceleration now if you wanted to use gpu acceleration you could all you need to do is just go and install the appropriate pi torch version so say for example i wanted to leverage gpu acceleration on my particular machine which i'll show you in a sec all i would need to do is go to pytorch.org so if we go to our baseline install page hit install and then if we scroll on down you can see that down here it sort of gives you the steps to go and install this so i could choose the stable install,358,2,2,Mut_u40Sqz4
36,i'm working on a windows machine but if i could if i was on a mac i could hit math linux i could hit linux so i'm going to choose windows and then in this case i'd say for example i wanted to install using pip i could just hit pip and then i could choose the language that i wanted to install for so if i wanted java i could choose that if i wanted python in this case we're working in python so choose python and then i need to choose the compute platform this is really really important here so cool thing to call that is that cuda and cu dnn are only supported on nvidia gpu so if you want to leverage gpu acceleration you have to have an nvidia gpu to use cuda now over here you've also got rock m now rock m is the beta package which is available for amd gpus now i believe this is only available on linux at the moment so if you wanted to use an amd gpu to be able to do this you'd need to be able or you'd need to be on the linux os in this case i'm on windows so you can see it's not available on windows so i'd be using cuda or in this case cuda 10.2 or cuda 11.1 now this is really only needed if you want to use gpu acceleration to be honest with reinforcement learning you're not going to see as much of a performance boost in training as you would with traditional deep learning by using a gpu so if you don't,358,2,2,Mut_u40Sqz4
37,have a gpu don't stress don't worry about it you don't need to do this step i just wanted to call it out for people that do have a gpu that wanted to do that but in this case we'll probably do that in one of our other projects and take a look at how to install that so for now you can sort of skip this it's just a good thing to note alrighty so on that note though let's go on ahead and let's go and train our agent so i'm going to skip back into our notebook and what we're going to do now is start training our reinforcement learning model so first up what i'm going to do is i'm going to define a log path and this is going to be where we save our tensorboard log so if we wanted to go and monitor our training we'd be able to take a look inside of this log directory and view how our model is actually performing so i'll show you how to do that down here so let's go on ahead and first up to find our log path so i'm going to type in log path or log underscore path and then we'll specify that now a key thing to call that is that this path needs to exist so we can also create it as part of our code but i've just gone and done it manually because it's reasonably straightforward so what i'm going to do is inside of the folder that we're working with i'm going to create a folder called training and then inside of,358,2,2,Mut_u40Sqz4
38,that i'm going to create two additional folders one called logs and one chord saved models let's zoom in on this so you can see that we've got a training folder and then we've got one called logs one called saved models so inside of our logs folder we're going to save our logs so in this case you can see i've got a bunch let's delete them because we don't need those and inside of saved models we've got a bunch of models as well so i'm just going to delete these ones because we don't need those for now so our logs is going to be where we save our logs for our model and our saved models are going to be where we save our saved model so our trained model so again we'll take a look at those a little bit later once we actually go and do it okay so that is that now done so again when you're doing this i'm going to add a comment so make your directories first all right so we're going to define our log path so again this is going to give us a path to our logs training backwards backwards logs and because i'm on a windows machine the path is represented by a double backward slash if you're on a mac or a linux machine i believe it's a forward slash cool so that is our log path now defined now the next thing that we need to do is instantiate our algorithm and specifically our agent now remember when we went and imported our dependencies we went and imported ppo so,358,2,2,Mut_u40Sqz4
39,in this case ppo is going to be the algorithm that we're going to be using for this particular environment so let's go ahead and define that and then we'll take a look okay that is our algorithm now set up now you can see here that it's printed out using cuda device this is because i do currently have gpu acceleration set up for this particular environment if you were not using the pytorch cuda version or the pytorch gpu accelerated version what you would see here is using cpu device so again no need to stress if you're not using gpu acceleration i'll show you how to set it up later if it says using cpu device that's perfectly fine as well you're still good to go all right so in order to do that we've written three lines of code here so written so we've gone and recreated our environment here this is just to keep it all encapsulated so i've written emv equals jim dot make and then to that we'll pass you our environment name so this line over here is no different to this line over here so again exact same thing then what we've done is we've wrapped our environment inside of that dummy vec env wrapper so remember up here we imported this this is where we're actually wrapping it so to do that we've written env equals dummy vec env and then we've created a lambda function so this is going to be an environment creation function so inside of square brackets i've written lambda colon and then env so this is going to allow us to,358,2,2,Mut_u40Sqz4
40,work with our environment that's wrapped inside of that dummy vectorized environment so again just think of it as a wrapper for a non-vectorized environment i'll show you a real vectorized environment when we get to our project one and then we've actually gone and defined our model so think of this as defining our agent so we've written model equals ppo so again this is the algorithm that we've gone and imported over here and then to that we've passed through what is that uh two arguments and two keyword arguments so the first one is defining the policy that we're going to use so in this case it's a mlp policy this stands for multi-layer perceptron policy now in this case this means that we're going to be using a neural network which is just using standard sort of neural network units we're not using lstm layers and we're not using cnn layers what we will do inside of project 1 and project 2 is we'll actually use a cnn policy core thing to call out as well is stable baselines two actually had one advantage over stable baselines three in that it actually had an mlp lstm policies so if you wanted to use windowed data sets which are particularly useful for trading or finance as well as certain gaming applications that particular policy isn't unfortunately available in stable baselines 3 as far as i know so again if that changes i'll mention it in the pinned comment below for now it supports mlp policy and i believe cnn policy you'll see cnn policy inside of project one the next argument that we've,358,2,2,Mut_u40Sqz4
41,passed through is our environment so this is going to be this vec dummy vectorized environment here we specified verbose equals one because we want it well we want to log out the results for that particular model and then we'll specified our tensorboard log path so tensorboard underscore log and we've specified it as this log path here so if we actually go and take a look at this algorithm ppo there are a whole heap of arguments that we can actually pass through here so you can see that we can pass through the policy the environment the learning rate the number of steps the batch size the number of epochs gamma gae lambdas so there's a whole bunch of different types of piper parameters that you can actually train on here as well and a whole bunch of different things that you can actually train so again if there's a whole bunch of documentation on this particular environment and you can see all of that there so again we're keeping this pretty simple and we're using the standard hyper parameters in this particular case so now that our agent is now set up the next thing that we need to do is just go on ahead and train it so again this is pretty simple from here on out so we just need to use model.learn to be able to go and train it so let's do it so if we type in model.learn and then we just need to pass through the number of time steps that we want to train it for so again i'm just going to pass that through,358,2,2,Mut_u40Sqz4
42,and initially i'm just going to set that to 20 000.,14,2,2,Mut_u40Sqz4
43,now you can play around with this number and in terms of how long you want to train so for a simple environment you're probably going to be able to get away with a lower number of total time steps for a sophisticated environment say for example breakout or the self-driving environment you're probably gonna need a heap more so for example for cardpole i've managed to solve it more often than not in under 20 000 steps for the breakout and the self-driving tutorial well breakout doesn't actually have a end goal per se but that actually took around about 300 400 000 time steps as did the self-driving tutorial so again the complexity in the environment is going to define how many time steps you need to train for so in this case we're pretty happy with 20 000 so we can go on ahead and kick that off and what you'll see eventually is once this model starts training it looks like we've got a bit of an error there okay it looks like it might have just been a warning okay so you can see our model is now starting to train so we're getting our time metrics and we're also getting a whole bunch of additional training metrics so let's let this go on ahead and run and then as soon as it's done we'll be able to test it out okay so we can see that our model has finished training and if we take a look so it looks like we've got an explained variance of 0.231 we've got an entropy loss of minus 0.599 a learning rate of,358,4,4,Mut_u40Sqz4
44,0.000 loss of 57.6 looks like it wasn't all that stable to the end but that's fine let's test it out and see what this actually looks like so that's our model now trained or at least trained for 20 000 steps now if we wanted to we could go and train this for longer so all we need to do is go and run it again it's going to start training again so you can see it's kicking off training and it's going on so again if you wanted to train it for longer all you need to do is go and run that again now now that we've kicked it off let's let that finish and then we'll actually test it out okay so that is our next round of training now done looks like our explained variance is a little bit higher learning rate's still the same we've gone through a total of 20 480 time steps so again this is just for this latest run now more often than not what you're going to want to do is you're going to want to save down this model and move it around if you wanted to go and deploy it you'd want to be able to save it so let's take a look at how we might save and reload our model first up and then we'll go and evaluate it so we're going to define a path and we're just going to call it a ppo path similar to what we did for our log path cool so that's our path defined so i've just written ppo underscore path so that's going,358,4,4,Mut_u40Sqz4
45,to be our path variable equals os dot path dot join and then we're going to be saving it inside of our effectively our saved models folder so training and then saved models and then our file name is actually going to be ppo underscore model underscore cart poll so this is going to save our model inside of this folder so reinforcement learning course well this is my current folder training and then saved models so it's going to be saved in here so if we go and save it now so you can see that our model is now saved there so ppo underscore model underscore carpol so again to save it all i've written is model dot save and then i've passed through this ppo path now if we wanted to we could actually go and delete this model and reload it so let's go ahead and do that because this sort of simulates deployment right you're going to be reloading from your saved model each time so let's do it so i'll just write del model to delete our model and then what we can do is we can reload this model back into memory so if i type in model so we're just going to define a variable called model and then we can actually reload it so to do that we're just writing ppo dot load and then we just pass through our path or the path to your actual model so if you save it somewhere else you're going to make sure or make sure that you pass through the full path to the models and then we're going,358,4,4,Mut_u40Sqz4
46,to pass through our environment as well so the full line is model equals ppo dot load and then to that we pass through the path where our model is actually saved so remember ppo underscore path is just going to be where our model is so in this case it's in training saved models ppo model carpol it's exactly this so training saved models ppo underscore model underscore carpal so that's the same file that we're working with so let's load it so right now so before i run this cell so you can see if i type in model dot learn for example total time steps equals a thousand so this would be our training step so you can see that we've got name model is not defined because remember we deleted our model over here now if we actually went and loaded it you can see that we've now loaded our model and if we go and run this you can see that we're now training again right so you sort of get the idea so you can go and train your model you can save it using model.save and then you can reload it using ppo.load so remember model.save and then you actually use the algorithm.load to be able to load it back up cool that is our training now done so in a nutshell we've done quite a fair bit there so we've ridden our so we've actually created our algorithm or our agent so ppo and then we'll pass through our parameters we've used model.learn to trade our model then use model.save to save our model and then ppo or,358,4,4,Mut_u40Sqz4
47,whatever algorithm you're using dot load to be able to go and reload it into memory so again those four key components are really really important so use the algorithm to find the hyper parameters model.learn to train it model.save to save it and then whatever the algorithm is dot load to reload it on to step four testing and evaluation so so far what we've gone and done is we've set up our environment we've gone and trained it but we haven't actually done anything with our trained model as of yet well what we're going to want to do is we're actually going to want to train our model to see how it's actually performing now you would have noticed that when we actually went and trained that model using the ppo algorithm so let's actually go back and take a look we didn't actually get those training metrics now the training metrics that i was sort of showing up here or the rollout metrics that you can see there are very much dependent on the algorithm that you're using so with the a2c algorithm i believe you get these rollout metrics but with ppo you don't so what you're going to want to do is you're going to want to evaluate the model itself to see what the performance is actually like now we can actually use the evaluate policy method that we imported right up at the start to be able to see what that actually looks like but a key thing to call that is if you do get these metrics it's a great thing so the two key ones that,358,4,4,Mut_u40Sqz4
48,you need to pay attention to are the episode mean length or episode or ep underscore len underscore mean so this is how long each episode actually lasted on average so say for example you're playing breakout it's how many times your model was able to play or hit the ball or how many frames it was able to go through before the model eventually died so with gaming this is particularly important the reward mean is effectively the your average reward so remember think back to our dog environment so it's how many times or it's on average how many times your dog got a trade or your average reward in this particular case now we can actually get metrics similar to these by using that evaluate policy method and we can also monitor those training metrics inside of tensorboard so remember when we actually set up our model we actually pass through tensorboard underscore log and we specified our log path so if we go back to that so you can see over here when we defined ppo we actually specified this tensorboard log path so we can then go and actually take a look at those metrics and those are going to be our training metrics cool so let's go on ahead and do that and this is how you start tensorboard but again i'm going to show you how to do this in a second so let's do it so we are now up to evaluation so let's go ahead and do this so we're going to be using the evaluate underscore policy method from up here so remember this is going,358,4,4,Mut_u40Sqz4
49,to be a method that allows us to test how well a model is actually performing now the ppo model in this particular case is considered solved if you get on average a score of 200 and higher so ideally we want to see that our model is scoring 200 on average to determine whether or not the environment is actually being solved now certain environments are going to sort of have a cap as to where it's considered solved others are just going to be continuous whatever highest score you get is the best so breakout and the self-driving tutorial i don't believe have caps but in this case the carpol environment does so let's go ahead and test this out okay so we've gone and written our line to go and test out our policy or evaluate it and the line that we've written is evaluate underscore policy and then to that we've gone and passed through two arguments and two keyword arguments so we've gone and passed through our model our environment how many episodes we want to test it for so in this case we've passed through n underscore eval underscore episodes equals 10 and then we've gone and specified render equals true so passing through render equals true determines whether or not we actually visualize it in real time so if you're evaluating this policy on colab then you want to specify render equals false because you don't want to visualize it it's not going to work at least with the default evaluation so let's go ahead and test this out and see what our model actually looks like so you,358,4,4,Mut_u40Sqz4
50,can see it's way more stable this time so remember when testing it out at the pa at the start it was sort of falling down and going sort of all over the place now it's perfectly stable right so you can see that it's balancing it almost exactly and so it's going to do this 10 times so it's going to go through 10 different episodes and take the average reward and we'll actually see that in a second pretty cool right so in just a couple of lines of code you've been able to build a reinforcement learning agent now again the training speed is going to be pretty much very similar when you're training on a gpu or not on a gpu it's going to be very very similar so don't fret if you don't have a gp on your machine test this out regardless cool so that's now done and you can see on average our reward is 200 so this environment is now considered solved so we're good so these two values that you get out of evaluate policy are the average reward over the number of episodes and the standard deviation in that reward so in this case we're getting a average score of 200 with a standard deviation of zero so we're perfect we're absolutely bang on in this particular case now the next thing that we want to do is actually close down that environment so again we have it over here so if we wanted to close it we can just type in emv.close and that is going to close it down now right now we've gone,358,4,4,Mut_u40Sqz4
51,and evaluated it but if we actually wanted to go and deploy this how would we actually go about doing it so this is sort of just testing out our model in an encapsulated function what would happen if we actually wanted to go and rather than doing it like this we actually wanted to do it sort of similar to what we did up here well we can actually do that the core difference is that rather than using env.actionspace.sample we're actually going to pass through our environment observations to our agent now to try to predict the best type of action because that's what reinforcement learning is all about remember we're going to take our observations pass it to our agent our agent is going to try to determine the best type of action to take to our environment to maximize our reward so the flow is going to be very much similar so we'll take a look at our environment so we'll use env.reset to reset our environment and get our observations we're then going to use model.predict on those observations to try to get the best possible type of action and then we're actually going to take that step so we can actually copy this entire block of code here and right down here let's go ahead and test out our model now to this we're going to make a few key changes so rather than using env.actionspace.sample we're going to change this to model dot predict and then to our model.predict function we need to pass through our observations now right now we've got our observations named two different things so,358,4,4,Mut_u40Sqz4
52,we want to change this so env.reset we're going to change that variable to be equal to obs and then down here in env.step.action rather than having n underscore state we're going to change this to obs as well so ideally what we're going to change is this line over here so before it was state equals e and b dot reset we're going to change our action line so rather than having env.actionspace.sample we're going to change it to model.predict and to that we're going to pass through our observations and then in our env.step line which is where our action is taken we're going to change that first parameter to obs as well so now if we run this rather than taking random steps we're actually going to be using a model to take steps so remember we've now subbed out our model so we're now now using model here so if we go and run this looks like we've got a bit of an error let's take a look and we might need to oh we're actually going to get two parts from our model so if we actually uh let's actually take a look at this we use model.predict obs we actually get back two values so we get this array and we get this none value so our action is actually the first bit and the second component are our states we actually don't need that second component so we just make that underscore so if we actually do that now and we take a look at our action that's looking better all right cool so we're just going to make,358,4,4,Mut_u40Sqz4
53,this one change so we're going to unpack this value and our environment is still open so let's go ahead and close it it's closed all right let's try this again there you go so you can see performing way better than before it's now balancing that that pole way better than what we had initially when we were just taking random steps and you can see the score being printed down below so we're getting 200 pretty much every single time which means we're smack bang on solving that model and there you go so you can see that we've now gone and done that and again we can close this now if you wanted to run this continuously you could as well but in this case we're doing it in a nice sort of loop and then and again we can go and do this again so let's try running it pretty cool right so we're now actively using our reinforcement learning agent to be able to go and interact with our open ai gym environment so it's now balancing the poll a whole heap better now let's actually take a look at what we did there so we went and let's actually delete this so if you cast your mind right back up to section two where we're loading our environment and we're playing around with how we can actually play with it now what we actually did is we had one really important function which was emv dot reset now remember when we type in env dot reset we're going to get the observations for our observation space what we can actually,358,4,4,Mut_u40Sqz4
54,do is we can take these observations or what we're actually doing is we're taking these observations here and we're passing them to our model so model.predict obs now what you're actually getting back here is two values so let's actually take a look at what we're getting back to do so we are going to get back the model action and the next state so that's used in recurrent policy so again because we're not using our current policy we're not getting that next state so what we actually get back in this particular case that is relevant to us is this first value here which is our array now remember in terms of our action space space remember there were two different types of action so zero let's see if we get one zero and one now what we're basically getting here is rather than just getting a random action we're using our model dot predict function on our observations from our environment to generate this action here so you can see that rather than getting env.actionspace.sample our model is actually predicting that based on the observations of our current environment right now you should take action one in order to get the best possible reward so this is effectively what reinforcement learning is all about so if you cast your mind back to that diagram so we've got our agent we've got our environment we've got our action and we've got our reward let's actually go back to that slide right so we've got our agent so in this case our agent is this model we've got our action oh that's actually so,358,4,4,Mut_u40Sqz4
55,we've got our agent we've got our environment so remember our environment is emv this variable here we've got our action in this particular case which is what we're just generating here so this one and we've also got our observations which is this value here so remember our observation so we can print that out is these four values now if you cast your mind back we actually took a look at what each one of these observations meant over here so our observations are our car position our cart velocity our pole angle and our pole angular velocity so again you can start to see how this is all sort of fitting together you've got those four key components you've got your you've got your agent you've got your environment you've got your action and you've also got your observations now a core thing that i haven't called out yet is the reward right so we saw that we got our model.predict now how do we actually determine what our reward is well we get our reward when we run env dot step so if we actually do that now in b dot step and remember our model just predicted take this action so if we go and take that extract that out and if we pass our action to this what we're actually getting back is those values that are relevant to us so we're going to get our state so this is the state after taking our action on it then this value over here is actually our reward so you can see our reward in this particular case is a value,358,4,4,Mut_u40Sqz4
56,of one now let's actually take a look does it talk about reward uh reward there you go so reward is one for every step taken including the termination steps so this basically means that we haven't let up hole four down completely which means that we get a reward of one if you pass a certain threshold and your pole starts to fall down then you don't get that reward so by basically keeping our pole in the upright position and not falling down we're getting accumulating a value of one every single time which is how we've got this value of 200 here so that in a nutshell sort of shows you the theory all the way through to the practical so these five steps are getting or what is what are we up to step six these six steps sort of show you how to define an environment how to train a model how to evaluate it as well as how to test it out as well so we've done quite a fair bit there now while you're training so this obviously trained really really quickly right so we're able to spin it up train it really quickly and get it up and running now if you are training a way larger and way more sophisticated environment what you might want to do is view the training logs inside of tensorboard so what we can actually do is do exactly that now i'm going to start it up from within jupiter notebooks but ideally you would want to run this from a command prompt so that you're not locking up your notebook because,358,4,4,Mut_u40Sqz4
57,once you run this it's going to run continuously it's not going to unlock your notebook you're not going to be able to run anything else so i'll sort of show you how to do this and then we'll continue onwards so what we first need to do is we need to get the log directory that we want to view so if we go back to our folders so if i go into so this is our root folder so if i go into training logs you can see that we've got three different training sets now remember we kicked off our training three times so this is where we've got three different sets of logs so our one was our i believe were they all the same no our second one our first and our second one were the longest our third one was only that 1000 training steps so let's actually take a look at ppo2 so what we're going to do is we're going to go into that folder and we're going to specify tensorboard to run from that folder so first up what we need to do is give it a path to that ppo2 folder so let's specify that first up okay so we've gone and specified our training log part so if we go and take a look at that so you can see this is giving us a path to our ppo2 folder so training logs and then ppo2 so this is effectively where we gone so training and then logs and then ppo2 so this file over here is our tensorboard log file that we're going to,358,4,4,Mut_u40Sqz4
58,be able to use now all we need to do to kick off tensorboard from within that folder and you're going to need tensorboard lot installed so i believe it's just a pip install tensorboard to go and do that to go and run this we just need to run exclamation mark tensorboard dash dash log dear and then we need to specify our training log path yeah that looks about right so written exclamation mark tensorboard dash dash log dear equals and then inside of squiggly brackets training underscore log path we've written that wrong so let me quickly explain what this line is doing so i think i've had some comments on this before so using an exclamation mark inside of a jupiter notebook is known as using a magic command so this allows you to run command line commands from within your notebook so by me putting through exclamation mark this is akin to me going to a command prompt or to a terminal and writing tensorboard dash dash log blah blah blah whatever so in this particular case what i've actually written is exclamation mark tensorboard dash dash logged let me actually show you it's probably going to make more sense so if i went to d drive cd youtube cd reinforcement learning uh so let's actually go into let's actually specify the exact same thing so written training log pass that's going to go into training logs okay so this is akin to me doing this so tensorboard dash dash log dear equals training slash logs slash ppo2 so right you can sort of see how this is actually running inside,358,4,4,Mut_u40Sqz4
59,of a command prompt and eventually you should get a line that says it's running at http localhost 6006 this line over here that i've written inside of a command prompt is exactly the same as what we would be running over here so what i can do is i can go to this link over here which is being created by tensorboard and you'll get all of your training and doesn't look like we've got any training metrics what's happened there okay let's just go directly into the folder so we'll go into training then we're going to go into ppo2 and then we'll run tensorboard dash dash log dear equals dot that times the charm let's see if this works okay so it should be running at http dash or colon dash dash local host and then six and then six thousand and six let's refresh now okay way better so what we went and did is i went and just seeded into the folder i'm guessing i'm getting this path that i was specifying incorrect but that's fine you can sort of see how to run it there all right so from here you are going to get a whole heap of different metrics now specifically you're going to get train metrics so this sort of shows you the frames per second and you're also going to get a number of train metrics so you're going to get clip fraction approx underscore k and if you want to deeper dive into what these metrics mean how to evaluate them by all means do hit me up in the comments below i'll probably have,358,4,4,Mut_u40Sqz4
60,a little blue box somewhere in the corner up here that sort of explains them as well but you can start to see that you're getting all of your different training metrics so you're getting your entropy loss your explained variance which should go higher your learning rate which looks like it stayed pretty stable your loss which looks like it's going down your policy gradient loss as well as your value loss so again you're getting a whole bunch of different types of training metrics that you can view in tensorboard now this is obviously run from the command prompt which we had to do a little bit of reducing to get to work now rather than doing that you can just run it from the notebook as well so if i stop this now all right and close down this command prompt what we can actually do is run this command and it'll effectively do the same for us right so this is currently running you can see the little asterisks there over here now if i go to localhost 6006 that gives us tensorboard all up and running now so you can start to see how to view those logs as well now in this particular case that is our set of metrics now done uh if anyone has any feedback on any of this or has um a better way to launch tensorboard by all means do hit me up in the comments below i'm always welcome to feedback but that sort of brings us to the end of our testing and evaluation step so what we went and did is we,358,4,4,Mut_u40Sqz4
61,went and evaluated our model using evaluate underscore policy we went and tested it out and we also went and viewed our login tensorboard so it looks like it still runs even though you end the cell so that's something i might need to dig into if you do have any problems with that do hit me up in the comments below though now a quick word on performance tuning and performance in general when you are training your model the core metric that you should be looking at is your average reward so this gives you an indication as to how well your model is going to perform in that particular environment using that particular reward function now the other metric that you want to be taking a look at is your average episode length so this ideally aims to identify how long your agent is actually lasting in that particular environment now this is particularly important when you have environments that don't have a fixed environment length so when we take a look at the breakout environment and when we take a look at the self-driving environment those are really really good indicators to be taking a look at now what you can actually do is if your model is not performing that well there's three key strategies that i'd suggest you start taking a look at so these strategies are one train for longer so if you've only trained for say for example ten thousand twenty thousand or a hundred thousand steps try training your model for longer and see if that improves performance the other thing that you can also take a,358,4,4,Mut_u40Sqz4
62,look at is hyper parameter tuning so again when you're dealing with deep learning models or even traditional statistical machine learning models hyper parameter tuning can yield significant results now stable baselines supports hyper parameter tuning using a package called optuna so we're not going to show it in this course but if you'd like to see a little bit more on that do let me know the last thing that i'd suggest you take a look at is take a look at the different algorithms that people are using to perform state of the art training as well so this can be another thing that helps you out when it comes to improving your performance okay on to our next topic so what we're going to do are we skipped all the way through so let's go back to where were we we are now up to step five so call backs alternate algorithms and architectures so what we're going to be doing in this particular step is we're going to be recreating our model but this time what we're going to do is we're going to specify a reward threshold so this means that our training is going to stop once it hits a certain benchmark now this is really really useful when you've got really really large models that you're trying to train and you want to stop them before your model starts getting unstable now what we can actually do is we can use some of the helpers from stable bass lines to do this so we can use the eval callback and the stop training on reward threshold callback to do,358,4,4,Mut_u40Sqz4
63,that now the nice thing about this is that you can actually save your model as part of this as well so it will automatically save your best model we're also going to take a look at how we can define a different neural network architecture so remember we specified the mlp policy but we can actually pass through a different neural network architecture as well and then last but not least we're going to take a look at how we can use a different algorithm so that's the last thing that we should be doing in that particular section so let's kick this off and do it so the first thing that we're going to be taking a look at is how we can add a callback to our training stage now the cool thing about this is that if you need to stop your training after a certain reward threshold this gives you the automated capability to do that now this is really really important particularly when you're training huge models or models that are going to take a long time to train so say for example you're training the breakout tutorial the self-driving tutorial you might want to use this but again not mandatory just gives you the ability to extend out your training step so in order to do this we first up need to install a couple of additional dependencies namely some helpers from stable baselines so let's go ahead and import these okay so we've gone and written one additional line of code there so the line that i've written is from stable underscore baselines three dot common dot callbacks,358,4,4,Mut_u40Sqz4
64,import eval call back comma and then stop training on reward threshold so our eval callback is going to be the callback that runs during our training stage and our stop training on reward threshold is going to be think of it like a checker so basically once our model passes a certain reward threshold so remember our reward for our carpool environment was our the reward which indicates that solved for the carpool environment is 200 so we'd basically be stopping our training once it receives or once it achieves that 200 score on average so that's now set up now what we need to do is actually set up these callbacks so let's go ahead and set them up and then we'll kick off another training run using it okay so those are our two callbacks sort of set up so there's one additional thing that we need to do and we need to specify uh where our best model is going to be but i'll come back to that in a second let's actually take a look at what we wrote so first up what we're doing is we're setting up our stop training on reward threshold callback this is the callback that's basically going to stop our training once we pass a certain reward threshold so in order to do that written stop underscore callback equals stop training on reward threshold which is this that we imported up here and then we're passing through our reward threshold so this basically specifies after which average reward we want to stop our training on so in this case i've set it to 200.,357,4,4,Mut_u40Sqz4
65,and then i've also specified verbose equals one so we get some additional logging then the next callback that i've actually written is the eval callback so this is the callback that's going to be triggered after each training run now in this particular case i've written eval underscore callback equals eval callback and then two that will pass through a number of arguments so first up we're passing through our environment then we're passing through the callback that we want to run on the new best model so in this case i've written callback on new best and then we've specified stop callbacks so this basically means that every time there's a new best model it's going to run that stop callback and if the stop callback realizes that the reward threshold is above 200 then it's going to stop training overall now we can also specify how frequently we want to run our evaluation callback and in this case i've set it to 10 000 time steps and then i've also specified the best underscore model we actually need to do this so what we can actually do is have this eval callback save the model every time there's a new best model what we do need to do however is specify what we want that model to be called so let's specify that so in this case all i've gone and defined is the save path so this is where we want to save that best model so i've written save underscore path equals os dot path dot join and then i've just specified the same saved model folder so training and then,358,5,5,Mut_u40Sqz4
66,comma saved space models and then what we're going to do is we're going to specify our best model as our save path so this means that after every 10 000 runs it's going to double check whether or not it's past the 200 reward threshold if it has it'll stop the training and it'll also save out the best model to this save pass so so you'll actually see this when we actually trigger it so if we run this cell now it looks like we've written this this should be best model save path my bad there we go cool so we just needed to update that parameter there so it should be best underscore model underscore save underscore path and then we've specified our save path that's all good to go now so those are our two training callbacks now what we now need to do is associate this to our model so we're going to create a new ppo model and assign these callbacks so that's our new model created again this line is exactly the same as what we did when creating an initial model under step three so again exactly the same as this line what's different now is that when we run our training command so model.learn we're going to pass through our callback so let's do that okay so what we've then gone and written is model.learn and then rather than just passing through the total time steps we're also passing through the callback that we want to run and in this case we're passing through our eval callback so this is going to be the callback that,358,5,5,Mut_u40Sqz4
67,checks every 10 000 steps and again on every 10 000 steps it's going to save the best new model if it's got it into that save path and it's also going to check whether or not it's past that average reward threshold so if we go on and kick this off now we should see our training kick off so let's do that and so after 10 000 steps you should see the fact that it's evaluating whether or not it's past our reward threshold so you can see it's about 8 000 right now so it should check on the next one so you can see there so it's gone and evaluated it it's checked the episode length so it looks on average it's 198.8 so if we keep on going after another 10 000 we'll see that eval callback run again and there you go so because it's now hit the 200 it stopped our training and again we only had 20 000 but if we train for longer it would stop it regardless because it's now hit that 200 score threshold it's gonna stop the training pretty cool right so this gives you a lot of flexibility when you're actually going out and training really large models and you want to cap it off before it just sort of runs wild now another thing to note is that this will have also have saved our model so if we go into reinforcement learning go into our training folder in our saved models folder you can see this best model folder or this best model is now saved as well so this is,358,5,5,Mut_u40Sqz4
68,a as a result of actually having our callback saved it actually goes about and saves the best model as well okay so that is our callback now the next thing that we can do is also change our policy so say for example we wanted to use a different neural network what we can do is do that as well so let's take a look at how you might do that so in order to change our policies we can actually specify a new neural network architecture now in order to do this we need to specify a network architecture for our custom actor as well as our value function so i'll show you how to do this so this is akin to just changing the number of units and the number of layers inside of your neural network so again pretty simple to do and then we can pass it on to our model so let's do it okay so that is our new neural network architecture defined now what we need to do is actually associate it to our algorithm so what i've actually written here is new underscore arch so new arch equals and then inside of square brackets i've defined a dictionary now the first neural network architecture that i've defined is for our custom actor so in order to do that we need to pass through pi and then we're specifying that we're going to have a new neural network with 128 units in each one of those layers four layers 128 units 128 128 128 and 128 and then we need to specify the same for our value functions,358,5,5,Mut_u40Sqz4
69,again four layers with 128 units in each neural network layer so you can see that there so vf equals and then inside of square brackets 128 128 128 and 128 now you get again you might only do this if you had a really specific reason what i've actually found is the neural networks inside of the baseline algorithms work pretty well but again this sort of shows you how you might go about doing that so let's go ahead and associate this to our model and then kick off our training again oh this should actually be net arch my bad there you go alrighty cool so that is our new neural network now associated to our ppo model and specifically we've gone and updated the policy so what i've gone and written is model equals ppo and then mlp policy because again we're using the multi-layer perceptron policy then our pass through our environment verbose equals one tensorboard log log path so again no no real change from here onwards but then in order to specify the new neural network and specifically the new policy i've written policy underscored kw args equals and then i've passed through a dictionary and then to that dictionary we've specified the net underscore arch value and then we've set that equal to this over here so netarch so again this is just defining a new neural network or a new neural network policy attached to our model now what we can do is again type in model.learn and again we can apply our eval callback to this as well so again if we run that this is,358,5,5,Mut_u40Sqz4
70,now using our new neural network architecture and specifically our new policy another thing to call that is that inside of stable baselines you've actually got a few different policies so if you go to the documentation and go to custom policy network there's a whole heap of information on how to actually do this as well so you can define custom feature extractors so on and so forth so again pretty pretty cool what you can actually do with this and it can actually get really really sophisticated now in this case we've got our model looks like it's all training sufficiently and again our eval callback has kicked in so it looks like our episode length has hit 200.,157,5,5,Mut_u40Sqz4
71,looks like we're all good there and we've stopped all righty now the next thing that we want to now do is actually take a look at how we might go about using an alternate algorithm so rather than using our ppo model which we've been using so far we might want to use a dqn for example so how might we go about doing that well first up we need to import the dqn algorithm so let's do that okay so that's our dqn now imported so i've written from stable underscore baselines import dqn and then what we can do is just go and use this in a really similar manner to how we used our ppo models we could actually copy this over paste it down here and all we really need to do is sub out ppo for dqn we're going to get rid of our policy keyword arguments and so this has instantiated our dqm model and again model.learn and we'll pass through total time steps set it to 20 000.,228,6,6,Mut_u40Sqz4
72,and there you go so this is now training a dqn model rather than a ppo model so really really quickly that shows how to apply a different algorithm remember also in stable baselines you've got a bunch of different types of algorithms so you've got a2c ddpg dqn her ppo sac and td3 in stable baselines 2 there's even more algorithms in that as well but because that is now in maintenance mode i figured i'd show stable baselines three cool so that is our dqm model and now done now again to save and export these it's a very similar manner so we just type in model.save the only difference when you're loading from a dqn is that rather than typing in ppo.load you'd now type in dqn.load so that in a nutshell really covers how to add a callback to our trading stage how to change our policy as well as how to use an alternate algorithm now on that note that brings us to our projects so step six we're now going to go through our different projects so we've got three specific projects that we're going to take a look at so project one we're going to take a look at reinforcement learning for atari games and we're specifically going to be trying to solve the breakout problem then project two we're going to try to leverage reinforcement learning to build a racing car and this is sort of like almost going down the path of autonomous driving and then project three we're going to take a look at how we can build our own custom environments using the open ai,358,7,7,Mut_u40Sqz4
73,gym spaces that we talked about a little bit earlier on so first things first let's take a look at reinforcement learning for atari games okay so inside of the github repository you're also going to have a couple of additional projects so you can have project one project two and project three so project one is gonna be breakout so if i actually go to the github repo so you can see you can have project one which is breakout project two which is self-driving and project three which is custom environment now in this case what i've gone and done is i've started off with project one which is breakout now again even though we're working on different environments how we actually go about training them is going to be very very similar so whilst we've spent a lot of time going through the basics in the main course how we actually go about applying this to our different environments is going to be pretty much the same so let's start off by importing our dependencies first up okay so i've gone and written six different lines of code there so these are all going to be pretty familiar to you from the previous tutorial so the first one that i've written or the first line that i've written is importing jim again no difference there then from stable baselines three i'm importing a2c this is just a different algorithm so again remember how we import a ppo and then we import a dqn now this time we're going to be importing a2c just a different algorithm then we're importing from stable underscore baselines,358,7,7,Mut_u40Sqz4
74,three dot common.vec underscore emv import vec frame stack so remember how in the main tutorial we didn't vectorize our environment so we only trained on one environment at a time what we're going to do for breakout is we're actually going to train on four environments at the same time so this should allow us to speed up our training then what i've written is from stable underscore baselines three dot dot evaluation import evaluate underscore policy no change there again so we use that previously and then from stable underscore baselines three dot common dot env underscore util import make underscore atari underscore env so this line is a little bit different and just helps us work with the atari environment so atari environments are the environments that allow us to play atari game so if we actually go to the gym documentation and take a look at our environments you can see under atari you've got the ability to try out a lot of these games now we're specifically going to be training on breakouts so let's take a look at that one so it's going to look like this yeah it's actually this one so basically the goal is just to maximize the score that you can see up here and you've got a maximum number of lives as well actually this is your number of lives this is your number of scores so again the goal is to just maximize that score so there's no real cap that you can get to to completely solve the environment it's just about getting the highest possible score so let's go on ahead so,358,7,7,Mut_u40Sqz4
75,what else would we write there and then import os so again this is going to allow us to work with our operating system now another thing that i wanted to call that so say for example we wanted to use gpu acceleration so i said i'd show how to do that well all we need to do is go back to our pi torch link and in this case i'm going to choose the build that i want so stable windows pip python and then i've already got cuda 11 installed on this machine so if you don't you're gonna need to do that in order for this to work so what i'll do is i'll just copy this link and then bring it into my notebook so i'll add in an exclamation mark paste that in and i just need to get rid of this three here so if i run that this is now going to install the cuda accelerated version of pi torch so then what typically what you need to do is just restart your kernel so just hit kernel and then restart hit restart and you should be good to go and then what we need to do because we restarted our kernel is just re-import those dependencies so once that's done we should be okay now as of late there has been a change to the entire environment so previously you used to just be able to import them and they used to sort of just work but now you got to do something a slight bit different so what you actually need to do is download the raw,358,7,7,Mut_u40Sqz4
76,files from let me just grab this link you need to download the raw files from this particular link here so it's atarimania.com forward slash roms forward slash roms.ra so you can see that that's now downloading and i'll paste that in i'll make that available in the notebook as well so it'll be this so you can see http colon forward slash forward slash forward slash roms forward slash roms dot ra without the one you don't need that so that's going to download all of the files that you need to be able to work with the atari environment so you don't need to do that or you didn't need to do this previously but i think as of late this is a change to the entire environments that you need to do in order to use them so once that's downloaded you'll have a file called roms.ra so let's wait for that to download and then i'll show you what to do with it okay it looks like that has finished downloading let's go and take a look at it so you can see that we've got roms.ra so i'm just going to copy this and paste it into the same directory that i'm currently working with so you can see i've already got roms and i've got hc rom so these files so i can actually delete these and what you need to do is just paste in that roms.ra file and then unzip it so i'm just going to extract it and you can see i've now got hc roms and rom let me zoom in on this so i've now,358,7,7,Mut_u40Sqz4
77,got hc roms and roms so what we'll then do is extract these into the same folder and so that's hc roms let's do roms as well and so once those have extracted there's just a command that you need to run in order to install these so it's pretty straightforward but once that's done you should be able to leverage the atari environment so let's let that finish and then we should be good to go and if you get a warning you can just skip those cool that's good so you should now so we can actually delete these now so we can delete hc roms roms.ra and roms so we don't need those we just need the extracted folders which you can see there okay so that's all well and good now we need to go and ahead and install those so if we go back we just need to run a simple command to go and install those into our environment so let's go ahead and do that and there you go so we're all good so the command to run it is exclamation mark python dash m and then atari underscore pi dot import underscore roms and then you just need to pass through the path to those roms so again if i actually show you so those files are inside of a folder called roms and then roms again so you need to point to this particular file path here so in this case what i've written is exclamation mark python dash m atari underscore pi dot import underscore roms dot backwards roms backwards rom so again if you're on,358,7,7,Mut_u40Sqz4
78,a mac the file path might be a little bit different i believe it's forward slash rather than backward slash but you sort of get the idea so once that's done you should be all good to go ahead and test this environment so let's go ahead set up our environment and we'll actually take a look at it okay so that's our environment now made now if we type in emv.reset we should get our observation oh my bad so there you go so we've got our observations and if we type in uh what's the other one so emv dot action space i'll leave that env dot action underscore space so you can see our action space is discrete and we've got four different actions that we can take we can take a look at our observation space in v dot observation says you can see that our observation space is going to be a box with the values ranging from 0 to 255 and the dimensions are going to be 210 in terms of height 160 in terms of width and 3.,240,7,7,Mut_u40Sqz4
79,so this means it looks like it's going to be an image which in this case it's an image based model now what we can do is we can actually go on ahead and test out this model so remember if we cast our minds back to step 6 where we actually tested out our model we can actually copy this block of code and run the same thing here so remember this is just going to go on ahead and test out our particular model actually this is the wrong code let's actually write it from scratch so what we can do is go through a number of episodes and actually play breakout so let's give this a crack okay so let's take a look at what we wrote so this code is really really similar to what we used in step two where we loaded and tested out our environment so again we're setting up the number of episodes that we want to play we're looping through each one of those episodes and then we're basically going on ahead and taking random actions on that environment to see what it looks like so if we run this now should get a little pop-up and you can see we're effectively playing breakouts again it went really really quickly if we didn't want to close our environment we can just comment at that last line and you're going to see it play now you can see it's sort of just playing randomly and it's not exactly getting the highest score so it's what capping added around two four it looks like the highest that it got,358,8,8,Mut_u40Sqz4
80,was four now it gets a point for each block it breaks so you can see they've got one two three in that case we want to try to train a model that's able to play a little bit better now again training this model can take a long time so we'll give it a crack and if you want to take this further train it for longer let me know how you go in the comments below now what we're going to do here is a little bit different to what we did in the main tutorial because what we're going to do is we're actually going to vectorize our environment and train four different environments at the same time so let's go on ahead and test this out okay there you go so that's our environment at the moment so now if we type in emv.reset and env.render you can see that we're actually playing with four different environments at once so this means that when we actually go and train we're going to be training four environments at the same time so hopefully this should give us a little bit of a speed up now we can type in emb.close to close that down you can see it doesn't look like it's closed down in this case let's try that again sometimes it's not going to want to close down and you're just going to have this for shut it but i don't want to foreshadow it because sometimes it'll crash the kernel that's fine for now just leave it open so that is our environment now set up so all well and,358,8,8,Mut_u40Sqz4
81,good now what we can do is actually set up our model to actually go ahead and train this so let's do that and kick off our training oh let's actually take a look at what we wrote to vectorize our environment i completely skipped over that so what i wrote was emv equals make underscore atari underscore emv and then to that we pass through the type of environment that we want to run so in this case the environment that we're actually running is breakout dash v0 so this is the breakout game that we're actually working with now a key thing to call that is if you actually take a look at the gym environments there's actually a breakout ram version and a breakout dash v0 version so this one is going to train using images this one is actually going to train using ram we want the image based model because we're going to be using a cnn policy so then to that we've also passed n underscore e and v's equals to 4 because we're going to use four environments at the same time and we're going to specify seed as zero to get some reproducible results then what we've actually gone is we've gone and stacked those environments together so to do that written emv equals vec frame stack so this is that wrapper that we imported up here and then we've passed through our environment and we've specified n underscore stack equals four so this basically stacks our environments together then we're going to go and specify models so let's go and do it okay so that is our,358,8,8,Mut_u40Sqz4
82,model now set up so we've gone and written two lines of code there so first up we've written log underscore path equals os dot path dot join and then we'll specify training and log so again it's similar to how we set up our log path in the main tutorial then we've gone and specified our model so model equals a2c so remember we're just using a different model in this particular case so this is the different algorithm that we're using so rather than ppo or dqm then we'll specify the type of policy that we want to use this is a key differentiator so previously we used the mlp policy which is great for tabular data or tabular observations but because our image and specifically our observations are an image in this case our cnn policy is actually going to be a lot faster to train so we've specified cnn policy so this basically uses a convolutional neural network as part of the policy rather than just a multi-layer perceptron then we've gone and specified our environment which is coming from here specified verbose equals one because we want logging and we've also specified a tensorboard log path now what we can go and do is go on ahead and train this now we're going to train this for a little bit longer so what i might do is might stop the training if it runs for too long and then we'll actually load up one that i pre-trained and see how that performs but for now let's train this on about let's give it 200 000 steps so again if you want,358,8,8,Mut_u40Sqz4
83,to get a really really high performing model you might need to train up to a million or even two million steps but let's give it a hundred thousand and see how long that takes ideally we should be able to get a score higher than four from what you can see up here which is just random actions okay so no real difference there so what i've written is model dot learn and then pass through total underscore time steps and we'll specify that as a hundred thousand so again no different to what we did for our previous tutorial where we're training a model so we're gonna run this and we'll be right back okay so you can see that our training is kicked off so again we'll let this train and then we'll be right back as soon as it's done so it's gonna train for a hundred thousand steps so we'll give it a little bit of time okay so that is our breakout model now finished training so you can see that after a hundred thousand steps we've got an episode reward mean or average episode reward of 5.84 and an episode length of about 479 frames so not too bad overall now what we want to do is save this model and reload it before we do anything else so let's go ahead and do that now this is going to be really similar to how we've saved models before so again nothing too crazy there so that is our model now saved so if we go and take a look so you can see that we've gone and saved,358,8,8,Mut_u40Sqz4
84,a2c breakout model now i've also got an a2c model that's been trained for 300 000 steps for breakouts so if you wanted to take a look at that one i'll include that in the github repo as well so you can take a look and try that one out but in this case we are going to test out our own model so let's go on ahead and delete our model and then reload it just to make sure it all works okay so that's our model reloaded now again what we can do is use the evaluate policy method which we had over here to test it out so remember that the max score that we got after testing out our five episodes up here was four so ideally this model should ideally try to get a little bit better than that so let's try that out so what we're going to do is we're going to use evaluate policy and we're going to pass through our model environment and then the number of eval steps so we're going to do let's do 10 and let's do render equals true and let's take a look you must only pass okay so this is actually pretty common so when we actually go and evaluate we can only go and evaluate on a single environment now remember correctly when we went and created our environment right here we had four environments so we vectorized them and trained them a whole heap faster now what we can do is we can actually single this down and leverage our vectorized model on a non-vectorized environment or an environment,358,8,8,Mut_u40Sqz4
85,that only has a single particular environment inside of it so let's go ahead and create one of those first up okay so we've now gone and recreated our environment now this time rather than passing through four environments in our make atari environment function we've only passed through one but we're still stacking it up as though that there's four environments so this is going to allow us to leverage it so now if we go and run our evaluate policy method you can see it should all be running well so you can see it's playing way better now it's looking like got a five six seven six they're still playing way better than the random agent was so you can see there that on average we're getting a value of 6.1 with a standard deviation of 1.9 now what we could also do is we could also test out that bigger model that i had in there so again i can't remember how well that was performing but let's go ahead and test that out so the model path to that model is going to be a2c 300k models we can copy that name and try loading that up so i'm just going to update the a2c path and then load that one and then recreate our environment you don't need to recreate it but then let's try this one out so if you get your environment sort of freezing like this sometimes what you might need to do is restart your notebook so you can see there that it doesn't look like it's opening up so ideally what you should do is,358,8,8,Mut_u40Sqz4
86,just hit restart on your kernel for this but make sure you save down your model before you do this i'm just going to hit restart hit restart again this should ideally close oh we want to stop that yep so that's good that's from another kernel now what we're going to do is re-import our dependencies so just import that and then scroll on down and what we're going to do is define our a2c path load up that model we need to recreate our environment from down here then load up that model and then try it again so there you go so that looks like it's performing way better already and you can see that this model obviously it's been trained a lot longer but it's getting into the tens and possibly the 20s when it's actually playing so again the longer that you train the better that this model is actually going to get now you could also try using some of the recurrent policies but at the moment they're not implemented in stable baselines 3.,234,8,8,Mut_u40Sqz4
87,i will let you know once they are in the pinned comment but that sort of gives you an idea as to how you can go about training a reinforcement learning agent for breakout now what we'll do is we'll just clean this up so if we type in enb dot close that'll close our atari environment it's just this one left so that is all well and good so we went and did a ton of stuff here so what we did is we went and imported our dependencies and we imported a couple of new ones to work with atari we also went and installed the atari roms so remember you've got to download them from atari mania and again i'll include this link in the description below we vectorize our environment so rather than using a single environment we trained on four so this gives us a bit of a speed boost and then we also went and trained it up and then we went and saved it and evaluated it at the bottom and i also showed you the 300k model which you can see here it was getting an average score of 12.7 with a standard deviation of five so overall it was better but there was a higher standard deviation so that sort of gives you an idea of what's possible by just training a little bit longer hey guys editing nick here before we jump over to the next project i wanted to let you know that i ended up training the breakout model for an additional two million steps just to see what it would actually take a,358,9,9,Mut_u40Sqz4
88,look like now after training for around about two million steps what we ended up with is an average reward around about the 20 point range now this obviously is a markedly improved result over what we had in our original model so ideally you can see the impact of training for longer this is what it looked like so as i was saying i ended up training the model for a whole heap of additional time steps so all up i ended up training the breakout model specifically with the a2c algorithm for around about two million time steps now when i evaluated this model it looked like we were getting around about an average score of 21 which is again still way better than our random model still better than our 100 000 time step model so you can start to see the impact of trading for longer now i'll also make this model available inside of the github repository so if you want to test it out for yourself you can start to see what that actually looks like so the model name is a2c underscore 2m model so a2c trained for 2 million steps so what we can do is as per usual load this up into our environment and what we'll do is we'll load it into our a2c algorithm we'll create our environment that has a single frame at a particular time and then rather than evaluating for 10 time steps what we'll go on ahead and do is evaluate for 50.,335,9,9,Mut_u40Sqz4
89,so what i'll do is i'll run this and then i'll leave you to it so you can start to see the boost in performance and then we'll come back at the end and take a look at what our end score was so again we're going to run it for 50 evaluation episodes so let's go on ahead and do this and you can take a look music so already you can start to see that this is performing way better so we're clearing the tens we're clearing the 20s and every now and then the ultimate score is hitting 30.,132,10,10,Mut_u40Sqz4
90,so again way better than what we had in our previous models but again i'll leave you to it so you can enjoy the performance and take a look at how it's actually running uh music up uh music okay so that is 50 episodes now done so it looked like we actually cleared 50 around about halfway through there so again significantly better performance than our other two models now if we actually take a look at our scores you can see that our mean reward over 50 episodes is 22.22 so again way better than the other models that we were taking a look at and overall our standard deviation was 9.1 so again way better than what we were taking a look at before and ideally this begins to show you what is possible when you go and train your model for a little bit longer on to our next project now that is project one now done now we're on to project two so reinforcement learning for autonomous driving so for this particular environment we're going to be using the racing car environment so this is trying to get a car to drive around a randomly generated race track so let's go on ahead and take a look at this one so again still the same five steps that we're going to be going through but in this case slight bit different in terms of how we're going to set it up now the first thing to note is that in order to leverage the racing car environment you do need to install swig so in order to do that just take,358,11,11,Mut_u40Sqz4
91,a look at installing squig and it's going to vary depending on whether or not you're installing on a windows machine or on a linux machine so for windows i believe all you need to do is download the swig file and then extract it and add it to your path for a mac i believe all you need to do is use homebrew to install it let's take a look yep so you can actually use homebrew so brew install swig so way easier if you're doing it on a mac cool so once you've got swig installed so again for windows you just download it extract it and then add it to your path and then you should be good to go for mac you just got to use brew install but again if you need a hand with that hit me up in the comments below then what we're going to do is install two new dependencies so we're going to need the box 2d environment and we're also going to need piglet so let's go ahead and install these up we've typed in jim wrong that should be jim okay all good so what i've gone and written there is exclamation mark pip install gym and then inside of square brackets box 2d so when using the racing car environment you need to have box 2d installed that's what the racing car environment is built on top of so once you've got that installed you should be good to go and then we're also installing piglet so again this is the dependency of that particular environment once that's done all you need,358,11,11,Mut_u40Sqz4
92,to do is again import your dependencies so let's go ahead and do that alrighty so we've gone and written five lines of code there so the first line is importing open ai gym as per usual so import gym then the second one is from stable underscore baselines three import ppo next one from stable underscore baselines three dot common dot vec underscore env import dummy vec env so again this is really similar to what we did over here in our main tutorial so again we're going to be wrapping up our environment exactly as we had down there then the next line is importing our evaluate policy function and then last but not least we're importing os alrighty now the next thing that we're going to do is test out our environment as per usual so let's go ahead and do that okay that is our environment created so this is just a warning so you can sort of ignore that now what we can do is take a look at our environment again so emv dot reset so you can see that this is generating our track and we'll talk about that a little bit more we can take a look at emv.actionspace as per usual and you can see it's going to be a box and we've got a three different values between minus one and one if we take a look at our observation space you can see again it's going to be a box and it's going to be values between 0 and 255 and it looks like it's going to be an image so 96 by 96 by,358,11,11,Mut_u40Sqz4
93,3.,1,11,11,Mut_u40Sqz4
94,so this means that we're going to have an image to be able to go ahead and train our racing environment now if we type in emv.render we can take a look at the environment itself you can see it's not popping up let's bring it up that's sort of what our racetrack looks like so you can see that we've got the entire racetrack there now we're actually going to test this out so we can type in emb.close so env.render should probably talk about this a little bit more so env.render allows you to render the actual environment that you're working in so this is an optional thing you don't need to render it does slow down training if you're rendering while you're training but it gives you the ability to see your agent in action so we can type emv.close to close that down so that should close down that environment so that's the old one cool now what we're going to do is we're going to go ahead and test out our environment again so again we can just copy this from our breakout tutorial so what we can do is just copy this testing code and bring it in and again this is almost identical right so we can uncomment our emv.close and go ahead and run this so this is going to test out our environment so you can see that we're actually trying to drive this car along this racetrack now ideally you're going to get more points the longer it stays on inside of the track and the more turns it does now because this is just taking,358,12,12,Mut_u40Sqz4
95,random actions it doesn't actually know where the track is at the moment so it's just going to go straight you can see it's making a lot of movement it's kind of performing okay but it's not able to take the turn so the first time it gets up to that turn it's failing right all right we don't need to watch so you sort of get the idea so the goal is to get this car to go around the racetrack now we can actually stop this and hit in vr.close to close it this just sort of cleans it up that's fine we can leave that one open and now what we're going to do is go ahead and train our model again so again same sort of process this time we're going to be using the ppo algorithm so rather than using the other algorithm we're going to use a slightly different one so let's go ahead oh so rather than using a2c like we did for breakout we're going to use ppo here so let's go ahead and train up our model we should also take a look at what the different actions are so if we take a look uh what can we do mv doesn't look like we've got it let's take a look at how what we can pass a car racing environment open ai gym so let's take a look at what we've actually got here if there's any documentation on the different actions it doesn't look like it so again sometimes you're going to get better better explanations of what's actually happening in different environments it looks,358,12,12,Mut_u40Sqz4
96,like we've got something here so the raw actually this is useful so the reward is minus let's make this a bit bigger so the reward is minus zero or negative point one for every frame and plus one thousand divided by n for every tractile visited so this means that for every track tile visited you're going to get plus 1000 divided by n where n is the number total number of tiles it says slightly confusing or complicated a reward function but you sort of get the idea so you get more rewards or you get more points for being able to go down each and every frame as long as you're on the tiles so the game is solved when the agent consistently gets 900 plus points so again this is going to take some time to be able to get to that point um so remember it's a powerful rear wheel drive car don't press the accelerator some indicators are shown at the bottom so we've got the true speed for abs sensors the steering wheel position and the gyroscope so again pretty cool so we've got a whole bunch of information here now you can see there that this one does have a finite set of reward statements which dictate whether or not it's solved so ideally you want to get over 900 points that's going to take a long time so i trained for 438 000 steps and i think i was getting in the realm of 50 40 sort of points so again can take a while to solve now what we're going to do in this case is,358,12,12,Mut_u40Sqz4
97,again we're going to try to solve it and see how we go so let's do that so we're going to go and train our model so we're going to instantiate our environment and then we're going to go on ahead and train it okay so that is our environment now set up so we've gotten written e env equals jim dot make and then environment name and then again we're wrapping it inside of our dummy vectorize environment wrapper because we're not actually going to vectorize this one it's again pretty similar to what we did in the main tutorial then what we can go and do is set up our agent and our model so let's do it okay so that is our model set up so again we've gone and specified our logging path and this is where we're going to log out our tensorboard logs so we've gone written os.path.join and then specified training and then specified logs so that's going to be our directory and then we've actually gone and specified our agent so model equals ppo so again we're going to be using the ppo algorithm here and then we'll pass through cnn policy pass through our environment pass through verbose equals one and specified the tensorboard log path now again we're going to train but we're only going to train for 100 000 steps you might want to train for a whole heap longer if you want to try to hit that 900 score and if you do hit that 900 score do let me know in the comments below and share it out on twitter and linkedin i'd,358,12,12,Mut_u40Sqz4
98,love to see it so in this case we are going to go ahead and train our model for a hundred thousand steps so let's go on ahead and do that so again to train our model we're just gonna write model model.learn so you're going to start to see there's a repeatable process to this so you instantiate your environment you create the environment vectorize it if you need to and then set up your model and then go ahead and train it so in this case we're going to go ahead and train it so let's go ahead kick this off and we will be right back so let's just wait for the training to kick off successfully and there you go so you can see that we're starting to get our output from our algorithm so we'll let that train for a hundred thousand steps and we'll be right back okay so that is our self-driving racing car now train so again we've gone and trained it for a hundred thousand steps 100 352 to be exact now again as per usual what we're going to go ahead and do is save our model and then test it out so let's do it okay so that's our model now saved so what we've gone and written is ppo underscore path to set up our path variable so in order to do that written os dot path dot join and then specified that we want it in our training folder and then our saved underscore mod or saved models folder and then we're gonna name it ppo driving model and then again we've used,358,12,12,Mut_u40Sqz4
99,model.save to be able to go and save that down so if we now take a look we've now got this model here ppo underscore driving underscore model now i've also got another model that i trained for 428 000 steps so we'll take a look at that one as well but for now let's go ahead and delete our model as per usual just to double check this all works and then let's load it back up so model equals ppo dot load and then we're going to pass through our ppo path and our environment cool so that's all good now what we can do is go ahead and evaluate as per usual so let's go ahead and do that so we're going to pass through evaluate underscore policy and then we're going to pass through our model our environment and then the number of steps so we're going to do 10 steps and then we are going to pass through render equals true so it looks like our car's ripping it around the track doing a bunch of donuts so a key thing to note is that the car is high powered so it doesn't always so you can see it's going around the corner but it's having a bit of trouble sort of getting there this is great so you can see that because the car is so high powered it starts to lack traction so in this case it can get stuck in this loop whereas instead of actually driving forward it just goes and does donuts okay so it sort of gave up there got around the first corner up,358,12,12,Mut_u40Sqz4
100,spinned out okay i think it's just gonna keep doing this so all right so that's sort of what a hundred thousand steps gets to you so not exactly the best uh racing car driver just kind of all right cool so let's stop this because it's clearly uh it's driving me and saying that it's just going all over the place and we're just gonna type in env dot close all right so that's now closed we've only got that one open and rather than using that one let's go ahead and load up the one that i trained for i think it was 438 000 steps 428 so let's go ahead and load up this model and again i'll make this model available in the github repo so i'm just going to change the name of the ppo path and then load this one up and let's go ahead and test with this model so you can see it is a lot slower now but it is at least sort of sticking to the track it's just cut the corner it's fine it's back on so you can see it's getting the score that it's getting is much higher so it's we're up to 190 200.,270,12,12,Mut_u40Sqz4
101,so ideally you want to be able to get up to 900 so that means that it's going to have to accelerate off the turn so this is obviously trained for 438 000 steps so if you actually train for a lot longer you'd get a car that's actually rips it down the straights takes the corners appropriately in this case it's starting to sort of it's a little bit hesitant on the throttle which you can see there so it's it's going but it's maybe not going as fast as it could there you go it's just accelerated it's back on track there you go but you can see that this is obviously way better than the one that we trained for 100 000 steps so that sort of shows you the difference the training for a lot longer and i did nothing different apart from just trained for a longer period of time so again when you're training these reinforcement learning agents training for a longer period of time can obviously help you out a lot more and ideally produce a much better model so ideally for this i'd be looking at something in the realm of like a million to two million steps to be able to get something great so if someone does have the time and if you do run it for that long by all means do let me know if you'd like to see me do it do hit me up in the comments below i'd love to take a look at this again but for now that is our self-driving car sort of done it is a little,358,13,13,Mut_u40Sqz4
102,bit glitchy on the throttle but you sort of get the idea now we can go ahead and close this so stop that environment and then run emb.close to close it now again remember in our main tutorial we also went through the ability to test it like this so rather than going through and using the evaluate policy we could also do this as well so if i copy step 6 from the main tutorial we could actually plug this in so in this case we've got our environment that's all good our model that's all good we could actually just test this out so let's run it there you go so you can see rather than using evaluate policy we're now using the i know what do you call it flow to be able to go and train this and you can see the car is getting around turn so we're up to what 250 now god i don't know how you can watch this after too many times it does get a little bit glitchy but it's going around its corners it's moving around and keep in mind we've only trained this on the image right so like we don't have any additional information but the image that's actually coming out of this which is actually pretty cool right let's get oh it actually took the corner that's pretty cool come on 270.,306,13,13,Mut_u40Sqz4
103,but this is obviously way better than what we had in that random agent which was getting like negative values also i noticed that if it v is too far off the track and it's not able to see the track anymore it sort of gets stuck and just stops there this is on the 438k model but again you could train it longer and you'd get better results so that sort of gives you an idea as to how you can leverage reinforcement learning for autonomous driving and in this case racing hey guys editing nick here again so i also ended up training the self-driving tutorial for a whole heap more steps now again i trained this model for about two million steps and this significantly improved the performance of this particular model so in the actual tutorial we got around in the range of about 200 to 300 in terms of our reward estimate now when i went and trained it for a whole heap longer we were heading towards the range of around about 700 not quite completely solved based on the environment metrics but ideally you can see again it's performing a whole heap better this is what that looked like so as i was saying i ended up training the self-driving model for a whole heap longer all up i ended up training it for two million additional steps now the reason that i wanted to show you this is just so you can see the impact of training for longer so this is obviously one technique that you can go about leveraging in order to improve the performance of,358,15,15,Mut_u40Sqz4
104,your models so again all up two million steps and i'll make this model available in the github repository so you can pick this up so if you class your minds back we had three different models all up now so we trained the first model which was just doing burnouts it's not really making it past the first corner we had the second model which was super jittery and then we've got this model now as well so i'll make this one available so in order to load this one up all i needed to do is again similar to what we did for our previous models i can just load it using the ppo path and model.load or ppo.load and then we can go and run this model now what you should see is that this model performs a whole heap better than the previous models it'll still spin out on a corner every now and then but again it's getting a lot further and scoring a lot higher than those other models that we train so let's go ahead and take a look at this one music so you can see there it got up to about 800 so not too bad so every now and then it's gonna lose focus and sort of veer off the track but you can see it's performing a whole heap better than the other models that we had trained so it'll spin out but then it works its way back to the track and it eventually starts taking the corners pretty well again so every now and then you'll see one that performs not so well,358,15,15,Mut_u40Sqz4
105,but you can see that this is performing significantly better than what we had before so again getting into the 700 range there we go that was another 770 score so you can see down the bottom again when we're evaluating our model what our performance is looking like so if we bring that a little bit further up music and open it up it doesn't look like we're printing out so let's let these 10 episodes run and then you'll eventually see the total score or the automated sum of the results so i'll be quiet now and i'll let you enjoy this music music music and that is all 10 episodes complete so you can see that we had a significant boost in terms of our performance simply by training for longer now if we actually take a look at the results of our evaluate policy you can see that our final score down here our average score over 10 episodes was 741 so again not quite hitting that golden 900 mark but again still way better than what we had in our previous models this also had a standard deviation of about 123 so again a reasonably high standard deviation in this particular case but this sort of shows you what's possible when you ideally go and tune your model and train for a little bit longer on to our next project now on that note that is project two now done now the last project that we're going to be taking a look at is reinforcement learning for custom environments now if you've watched my shower environment or shower custom environment tutorial,358,15,15,Mut_u40Sqz4
106,this is going to be that same environment but we're going to be using stable bass lines as the algorithms to be able to solve this so without further ado let's kick off project three so again all of these notebooks are going to be available inside of the github repository so if you want to pick these up by all means do grab them let me know how you go with them and if you get stuck please do reach out to me i'm more than happy to help so let's go on ahead and do this so there's a bunch of dependencies that we're going to be importing here namely because we're going to be defining our own environment in this case so let's go ahead and import these dependencies and then we'll take a step back and take a look at those okay so we've gone and written nine lines of code there so there's quite a fair bit now in this case what i've gone and written is i've broken it up into three specific sections so these are our gym dependencies or open ai gym dependencies these are some of our helpers that we're going to need and then down here we've got our stable baseline stuff so first up from jim we're importing more than just the gym package this time so written import gym which is going to give us a pretty standard import then what we're doing is we're importing the gym environment class so to do that we've written from gym import env so this is going to be the super class that we're going to be,358,15,15,Mut_u40Sqz4
107,able to use to build our own environment then what we've written is from gym dot spaces import discrete box dict tuple multi binary and multi-discrete so each one of these represents all of the different types of spaces that are available inside of openai gym so i wanted to sort of show you what each one of these different types of spaces looks like and how to actually use them we'll probably only use the two common ones discrete and box in our environment but i wanted to give you an idea as to how these all fit together then we've gone and brought in some helpers so we've imported numpy so import numpy as np we've imported random so import random we've imported operating system so import os and then we've gone and imported our standard stable baseline stuff so from stable underscore baselines three import ppo from stable underscore baselines three import common dot vec underscore emv import dummy vec nv again pretty standard and then we've imported our evaluate policy function so again this is really really common most of this is pretty common and jim is pretty common the new stuff is these couple of lines here so let's go and have a look at our different types of spaces so we've got a bunch that we've imported over here let's take a look at each one of these so first up is discrete so we can dive in discrete and then say we wanted three different actions we can do that so that's going to give us our discrete space now we can actually sample it and take a look,358,15,15,Mut_u40Sqz4
108,at all the different types of values so again 0 1 and we should get up to 2.,23,15,15,Mut_u40Sqz4
109,so if you had an action and an action mapped or each one of those actions mapped to a specific value so 0 1 or 2 that's how you'd use that then we've got a box so that's our box space so to do that we're in box and then we'll pass through zero and then comma one so this is our low value this is our upper value and then this this is the shape of the output that we're going to get so we're going to get an array that's three by three so ideally you'll have a list of lists so if we take a look at that by sampling it so again you've got an array and inside of that array you've got three individual rays and those arrays have three values so again exactly the same formatting so we've got those values between zero and one so again you might use this if you were trying to look at different types of sensors or if you had continuous variables you'd use a box now again if you just wanted three values you could do that and that's just going to give you three values as well so all i've done is i've reduced it into an array of three values then what we've got is a tuple now at the moment stable baselines doesn't support a tuple but if you wanted to use it you could still take a look so to that we can pass through a discrete environment or a discrete space and really your tuple space just allows you to combine different spaces so if we do that,358,17,17,Mut_u40Sqz4
110,you can see our tuple is now combined of our discrete environment and our discrete box space so if we sample it you can see we're getting a discrete value first up and then we're getting our box second okay on to our next one so again so far we've done discrete box and tuple next one that we want to take a look at is dict so this is really similar to a tuple the only difference is that rather than passing through a 2-port or tuple you pass through a dictionary so let's do it okay so that is our dict space so what i've written is dict and then open braces and then to that we've actually passed through this dictionary here now this dictionary has two keys so height and then height is equal to discrete two so really it's no different to typing discrete two up here and then we've created another key which is speed and we've set that equal to box and then two box remember you're going to pass through three key arguments so you're going to pass through your lower value your upper value and then the shape that you want so in this case i've specified a shape of 1 comma which means i'm only going to get a single value back between the values of 0 and 100.,297,17,17,Mut_u40Sqz4
111,so if we actually go and sample this you can see we've got our height key which is represented as 0 because remember it's going to be between 0 1 and 2 and then we've also got our speed which in this case is a value between 0 and 100 pretty cool right so that gives us a dict space now the next space that we want to take a look at is multi-binary so in order to create that space we've just written multi-binary and then passed through the number of positions that we want in our multi-binary space so multi-binary 4 is going to give us 4 positions so we can go and sample it and in this case you can see that we've got 0 1 2 3 and 4 positions and in this case it's going to be a binary set of values so either zeros or one so if we go and sample it multiple times you can see it's just different combinations of zeros and ones in those four positions cool now the last type of space that we want to take a look at is multi-discrete so again going to be pretty similar to multi-binary except rather than being binary values they're going to be discrete values between any value that we want now so let's go ahead and do it okay so that is our multi-discrete space so to do that written multi-discrete and then two that i've passed through a list and the values are passed are five two and two so if we go and hit sample so again you're going to get three different values,358,18,18,Mut_u40Sqz4
112,and these values are going to vary depending on what parameters you've passed through to the list so because i've passed through 5 the first value is going to vary between 0 and 4 because i pass through 2 it's going to vary between 0 and two actually is this going let's actually take a look i believe max it's going to get up to yeah yeah so it's going to be zero to four and then zero to one and then zero to one again so again this is the upper cap so it starts at zero cool so we can keep going through that and you can sort of see what happens so if we go and pass through another value now we're just going to get another discrete value appended onto the end of that so that's really a summary of all the different types of spaces that you've got available inside of open ai gym so you've got a discrete space a box space tuple dict multi-binary and multi-discrete so discrete is when you'd have a discrete number of actions and those mapped through to a single integer box gives you continuous variables remember with your box you just pass through your lower value your upper value and then the shape of the box that you actually want your tuple allows you to combine different types of spaces together as a tuple but is not currently supported by a stable baseline so something to keep in mind so remember to your tuple you just pass through a set of braces and then the two different types or whatever types of spaces you,358,18,18,Mut_u40Sqz4
113,want so we've passed you discrete here and then box here as well so again this is inside of braces there then we've got our dick space so again to our dick we just pass through a dictionary of different types of spaces so we've got our discrete and we've also got our box space there we've got our multi-binary space so again we've got that now keep in mind you could actually grab that multi-binary space and add it to your tuple as well so if we do that we are just in there so again now we've gone and added another type of space to our tuple so again we could do this with that dict so say for example i could call this um color i don't know so again you can add multiple versions to the two-point dick they're sort of like grouping spaces right so we've got multi-binary and then to that you pass through the number of positions that you want in your binary space and we've also got multi-discrete and this gives you a bunch of different discrete types of values so again there's not a lot of documentation out there on these so i figured i'd do a little bit of a crash course on them if you'd like to see more on that by all means do hit me up in the comments below but now we're going to be building our own environment now the goals of this environment are to basically build an agent to give us the best shower possible now what's going to happen is randomly the temperature is going to fluctuate because,358,18,18,Mut_u40Sqz4
114,there's other people in the building so it's going to randomly go up and down now we know that our optimal temperature is between 37 and 39 degrees so we want to be able to train an agent to automatically respond to changes in temperature and get it within that 37 and 39 degrees range now keep in mind that our agent doesn't actually know that we prefer our temperature to be within 37 and 39 degrees so it's going to need to learn what types of adjustments it can make to get it to within that range just something to keep in mind so remember this is a simulated environment so our agent doesn't actually know how it accumulates its reward it just knows that by doing certain actions it's going to get a reward now we know it we want to get it between 37 and 39 but our agent doesn't so let's go ahead and build this environment so there's a few different functions that we need to implement in this environment to get it valid so let's go ahead let's build a shell and then we'll fill it up okay so at a high level that is our shower environment now we obviously haven't gone and implemented the different components into it but these are the four key functions that you need to have inside of your shower environment class so let's take a look at what we've got so far so what i've gone and read in this class and then inside a capitals or camel case i've got shower env and then to that we're passing through our env class,358,18,18,Mut_u40Sqz4
115,which is from our gm environment up there and then colon then we've got four different functions that we've gone and implemented so we've got the init function so which triggers when we create our class the step function the render function and the reset function so to do that we're in def underscore underscore init and then two that were passing through self inside a pair of brackets and then a colon and then right now we've just written pass this allows us to define it without having any errors for now then we've defined a step function so def step and then to that we'll pass through self and then we're passing through the action that we're actually going to pass through to our environment so remember when we use env.step we pass through our action and it actually does something then we've gone and defined a render function so def render and then two that will pass through self and then colon and then pass so we're not going to do anything in our render function for now i actually as part of building this course i actually built out a giant or started building out a giant pie game environment but it was sort of blowing out of proportion so if you'd like to see a video on reinforcement learning for gaming which involves building a custom environment using pi game please do let me know in the comments below i'd love to hear your thoughts and then our last function is reset so def reset and then to that again we're going to be passing through self colon and then right,358,18,18,Mut_u40Sqz4
116,now we've got past so let's go ahead and initially set up our init function and then we'll keep going okay that is our initialization function and now done so we went and wrote four lines of code there so first up we defined our action space so to do that we wrote self dot action space and we set that equal to discrete three so remember this is no different to saying discrete three and the three actions that we're going to have are whether or not we turn the tap up whether or not we turn the tap down or whether or not we leave the tap unchanged so this basically gives us three discrete actions now you could change this and have it actually as a box type action space where you actually turn the tap by a certain amount or by a certain number of degrees but in this case we're going to keep it pretty simple and say up down or hold then we've gone and defined our observation space so to do that written self dot observation underscore space equals box and then we've set it to equal to two numpy arrays so in this case we've got a low value so low equals mp dot array and then pass through zero and then we've gone and specified a high value so high equals mp.array and then to that we've passed through the number 100 so this means that our observation space let's actually extract that so we can take a look so this means that our observation space is going to be a value between 0 and 100 and,358,18,18,Mut_u40Sqz4
117,have the value of 1.,6,18,18,Mut_u40Sqz4
118,so if we type in dot sample we can do that so you can see that that's going to be the value that we get back now we can actually change this so we can just make it 0 and 100 and pass through shape equals 1 comma that should ideally give us the same type of output so delete that so there you go so same sort of output and if we type in dot sample again we're going to get the same type of output so again two different ways of defining it in this case i've just swapped it out but you can sort of see that you've got multiple ways of defining that box space then we've gone and defined as initial state so this is going to set our initial state so we'll set that equal to 38 plus a random integer between -3 and 3. so this means that our shower is going to start out at 38 degrees plus or minus 3. and the goal of our agent is going to be to get it within that magic range of 37 and 39 degrees now we've also set another variable so this is going to effectively represent our episode length in this case it's our shower link so we're only going to shower for 60 seconds it's a fast shot i know so what we've gone and written is self dot shower underscore length equals 60.,315,19,21,Mut_u40Sqz4
119,so what we're going to do inside of our step function is decrease that by one second every time we go through and take an action so let's go on ahead and now define our step function okay so we've now gone and filled out our step function so in this particular case what we've got is let's say one two three four five six six different code blocks so the first one is applying the impact of our action on our state so remember we had three different actions so zero one or two so zero is going to represent decreasing the temperature of our shower by one degree one is going to represent no change and then two is going to represent increasing the temperature of our shower by one degree so in order to do that reasonably simply we've written self dot state plus equals the action minus one so remember our action is going to be discrete what was it three so if we go and sample that so in this case we've got one so by minusing one we're going to get the value so actually let's actually print it out so in this case if we take the action of one that is going to be akin to leaving the temperature the same so if we minus one again it's going to apply zero change to our temperature so self.state is going to stay the same if we get a different value say for example we get 2 by minusing 1 which is what we're doing here we're going to increase the temperature of the shower by 1 degree and,358,22,22,Mut_u40Sqz4
120,if we get 0 we're effectively going to be subtracting one cool so now the next thing that we've gone and done is then decrease the shower time so every time we take a step or take an action we're going to decrease the length of our shower by one so remember we defined it up here initially to 60 seconds you could change this to something different if you wanted to so we've gone and defined self dot shower underscore length minus equals one so that's going to decrease it and then this is really really important so this is where we actually define our reward so again if you had a really complicated reward schema this is where you'd be doing it so what we've gone and done is we've written if self.state so remember state is our temperature is greater than or equal to 37.,192,22,22,Mut_u40Sqz4
121,so remember the magic ratio it's got to be between 37 and 39 degrees and self.state is less than 39 degrees then the reward is one in all other cases so say for example if our shower is completely out of that range our reward is going to be negative one now you could also make the reward zero as well then what we're also doing is we're checking whether or not a shower is done because if our episode is done then we want to stop that particular episode so if self.shower underscore length is less than equal to zero then done is set to true let's fix that we've gone screwed that up then else done equals false so again if it's not if we haven't fully consumed the 60 seconds then the ash hour is not done then we're creating a blank info dictionary so if we wanted to pass additional stuff out of here we could do that in there and then out of this we're returning our self.state which is going to be our temperature our reward for that particular episode whether or not we're done and our info so again out of this we're returning all of these bits of information that we've gone and calculated in our step function now in this case our render function we're not actually going to do anything in here so we could implement biz if we wanted to we're not going to do anything there but if you wanted to you definitely could then the last function that we need to implement is our reset function key thing to know is that if,358,23,23,Mut_u40Sqz4
122,you wanted a more detailed tutorial on how to implement render and again pygame i'd love to do something on that and if you've got a specific use case hit me up in the comments below because i'd love to hear some ideas as well in this case let's go ahead and wrap up this environment so for our reset function we just need to reset our initial temperature and we also need to reset our shower time to 60 seconds so let's go ahead and do it okay i think that is our environment now done so for our reset function what we're effectively doing we could actually potentially drop this self.state up here because we're going and re-initializing it inside of our reset but that's fine for now so what we've gone and written in self.state equals np dot array and then to that we're passing through our same random initialization function so inside of a set of square brackets we've passed through 38 degrees plus random dot rand int between -3 and 3.,228,23,23,Mut_u40Sqz4
123,so again you could choose a broader random initialization if you wanted to in this case i've just chosen three and then we've specified as type float because remember our box is going to be we haven't specified that it's going to be an integer we could do that as well so you could specify d types in this case we're going to leave it as a float now what we're also doing is we're resetting our shower length to 60 so self dot shower underscore length equals 60 and then we're returning our self dot state so that should be our environment all well and good now now what we can do is we can actually test out this environment so emv equals shower env and then we can run inv dot observation space as per usual and you can see we're getting our box back and if we type in dot sample this is our initial temperature and if we keep doing that you can sort of see that there and we can type in emb dot action space and again we've got our discrete space dot sample and there you go so you can see that we've gone through the breakout tutorial the self-driving tutorial and you're probably thinking how these space is built well this is exactly how they're done when you're dealing with gaming implementations there's a lot more work done around the render around the observation space as well as around the reward space because it's a little bit more sophisticated and again if you'd like to see that done let me know i've started i've already got the template,358,24,24,Mut_u40Sqz4
124,code sort of built love to do your tutorial if you guys are interested now in this case what we're actually going to go ahead and do is test our environment and train it as per usual so again what we can go ahead and do is let's just copy the exact same testing code that we used for our driving tutorial which is this here and this is under step two test environment we can actually paste this here now the cool thing is that we've actually gone and defined an environment to a state that we can actually use it as part of a template code so if we go and run this you can see that it's automatically gone and smashed through all of those episodes so it's got our score printed out now remember our score is going to increment by one if we've got the shower between 37 and 39 and it's gonna decrease by one if it's outside that range so you can see that just by running those five episodes we've got a high score of 26 and the lowest of minus 60.,247,24,24,Mut_u40Sqz4
125,so again we can keep running this and you can see it's very quick and this is because we don't have a sophisticated render function and it's just it's all text based so again it's going to go very very quickly now in this case what we're going to go ahead and do is train our model and then save it so you can sort of see how to do this on a custom environment so let's go ahead and do that okay so we've gone and initialized our model and it's automatically wrapped inside of the dummy vec nv so even though we've gone and imported it over here looks like it's automatically wrapped so we're good to go so we've written log underscore path equals os dot path dot join and then true that will pass through training and then log so remember it's going to follow that same sort of logging directory that we set up and then we specified model equals ppo pass through the policy that we want to use in this case mlp policy this is different because in our breakout tutorial in our self-driving tutorial we had an image returned now we've got sort of tabular data or tensorbase data or actually well is sort of the same but we've actually got a array of values rather than an image so we're going to use the mlp policy then we're passing through our environment specifying verbose equals one and then specifying a tensorboard a log path cool now the next thing that we need to do is just go on ahead and train so again this is going to,358,25,25,Mut_u40Sqz4
126,train really really quickly so you don't need to do a super long training run so let's just try it out so model dot learn to train and then total time steps i don't know let's set this to 4000 for example so let's go ahead kick this off and let it train so this should train reasonably quickly because again it's using an mlp policy and it's all tabular data i mean you can see the frames per second is five and that that's actually done so it literally went that quick so let's actually run it for longer so rather than 4000 let's give it 40 000.,141,25,25,Mut_u40Sqz4
127,all right so you can see that that's now running it's doing about 600 frames per second so really really quickly and that's one thing to call that so with the game environments they're going to take longer to train versus like a simple environment like this so again the more sophisticated the environment the longer it's going to take to train so just sort of keep that in mind when you're planning your projects and when you're sort of committing to clients when you're building this type of stuff and again if you need help by all means do call me out i'm more than happy to help you can start to see that we're getting our episode reward mean so in this case minus 28.2 minus 25.9 so it looks like it's dropping so it should ideally get into the positives minus 21.7 minus 14.9 it's getting close minus 16.9 and again the episode length mean is going to be the same pretty much all the time it's going to be 60 seconds because that's the maximum remember i might need to train this for a little longer looks like it's getting close but it's not into the positives yet all right it's minus 15.5 let's actually give it another i don't know 20 000 steps let's run that so minus 4.12 so again you can see it's starting to get close into the positives minus 5.18 9.8 let's let that run and we'll be right back okay so it got pretty close it's episode reward mean over here got to about minus 4.22 so i guess this depends on the starting point and,358,26,26,Mut_u40Sqz4
128,how the model actually develops from there so let's actually go and test it out and see how we actually go so again we can use evaluate policy here or we should save our model so model dot save let's define our path uh what are we going to call this shower model let's just double check our directory name again so it's training saved models so let's specify that and we're going to call this shower model ppo cool so we can type in model.save shower path and again if we go and take a look that should be music shallow model underscore ppo so again that is now saved so we're good again we can delete our model and if we wanted to reload it we can just type in model equals ppo dot load pass through our path pass through our environment and then if we wanted to test it out we can run evaluate policy pass through our model pass through our environment pass through the number of eval episodes and we don't need a render this time because we don't have a render function so if we type in render equals true should throw an error it might actually not throw an error because we've got the music what did we have because we've got past so we're all good now in this case we've got a mean episode reward of 12 with a standard deviation of 58.78 so again it's getting there but it's very it's got wide variance so again you could train this for a whole lot longer tighten up the environment make it a little bit more,358,26,26,Mut_u40Sqz4
129,realistic than only being able to adjust up down and sort of steady state but that sort of gives you an idea as to how to bring this all together so in this tutorial we went through a bunch of stuff as well so we took uh so we imported all of our dependencies we took a look at all of our different types of spaces remember there's a discrete space box tuple dict multi binary and multi-discrete and just keep in mind that stable baselines doesn't support tuples yet we also took a look at how to build our environment so remember we had to define our init function our step function our render function and last but not least our reset function and then again in terms of testing and training and saving the model it's all very much the same but again if you've got a really sophisticated model that you'd like to build by all means hit me up in the comments below i'd love to help you out and if you do build some really cool environments do let me know i'd love to see them as well on that note we have now finished project number three so it comes to our wrap-up so hopefully you've enjoyed this course so in it we've gone and covered a whole heap of stuff and remember the core purpose of this course is to bridge the gap between a lot of the theoretical stuff that you see floating around there in terms of reinforcement learning and show you a practical set of implementation steps so we went through rl in a nutshell and,358,26,26,Mut_u40Sqz4
130,talked about what reinforcement learning is and how it works we took a look at how to set up our environment with stable baselines we then went and built and took a look at some different types of environments using open ai gym in step number two we then went and trained a model we then went and tested and evaluated it so we took a look at how we can view it inside a tensorboard we then extended out some of our algorithms and specifically we went and implemented callbacks we went and used different algorithms so remember we all up we use ppo a2c and we used a dqn algorithm as well we even went and changed our policy architecture so again some pretty cool stuff happening there and then we went through our three different projects so remember we went and trained a model to play breakout we went and trained a model to race a car around a racing track and we also went and built our own custom shower environment so all up we did quite a fair bit now i want to leave you with some additional resources so if you haven't gone through david silva's reinforcement learning course i'd highly recommend you do his team are the team behind deepmind and the guys that actually built the alpha go model so obviously super smart dude got some awesome theories for floating around there and by all means i do recommend you check it out there's also a great book called reinforcement learning and introduction by richard sutton and andrew bartos some of the pioneers in the reinforcement learning field,358,26,26,Mut_u40Sqz4
131,i highly recommend you go and check those that book out it's got some awesome stuff in it as well now in terms of what to learn next i love it when people give me ideas as to where to go from here and i want to give you the same so one of the things that we didn't cover in this course is hyper parameter tuning so one of the ways that you can improve how you train your models is to tune the hyper parameters that you start and you progress your algorithms with so again that might be something that you take a deeper look into and if you'd like to see a tutorial on that or a course on that by all means do hit me up building detailed custom environments for example we talked about this a little bit in terms of implementing a render function with pi game as well as integration with other simulation systems like mojoko and unity and then last but not least i think one of the coolest things that you could potentially take a look at learning is how to do an end-to-end implementation so say for example you actually went and built a cart pole robot and actually got went and trained in a simulated environment and then implemented it on a real environment perhaps using a raspberry pi based robot i think that would be an awesome thing to go and take a look at next but on that note that about wraps it up hopefully you've enjoyed this and thanks again for tuning in thanks so much for tuning in guys if,358,26,26,Mut_u40Sqz4
132,you enjoyed this video be sure to give it a thumbs up hit subscribe and tick that bell and let me know any feedback or anything that you'd like to see going on from this and if you get stuck at all by all means do hit me up in the comments below i'm happy to help you out thanks again for tuning in peace,84,26,26,Mut_u40Sqz4
0,music so welcome everyone to my webinar and today we're going to talk about the stable baselines three integration with ways and biases uh and then by the end of the webinar you'll learn how to use stable baseline 3 to conduct reinforcement learning experiments and how to use wasted biases to keep track of the metrics and the videos of the agents playing the game so here is the agenda for the talk today so uh we're going to talk about what waste and biases uh what waste and biases is and we're going to talk about uh what stable based science is and i'll talk about the recent integration we made then we'll go into a live demo with a collab notebook and lastly i'll also showcase how you can use recent biases ui to do a lot of experiment analysis in a really easy fashion so um before talking about what ways and biases is what simply science three is i want to give you a little context of doing research in reinforcement learning uh so overall as a phd student uh at the start of my phd program i was really trying to find ways of how i can do reinforcement learning research and was very quick before i realized managing rl experiments can be a pain there's a lot of problems on the reproducibility side if you don't record your hyper parameters uh your your commands that was used to produce an experiment if you don't record the software dependency uh then it's very possible that after just a few months you will be able to reproduce your experiment anymore there's,358,0,0,ed1bqaZGOQw
1,also a lot of issues with analysis and visualization how you set up how you save the plots and data can be problematic sometimes and how you view them aggregately those are all little problems but in aggregate they become somewhat of paying for research and there's also the reliability concern with uh doing reinforcement learning research there's there are thousands of uh reinforcement learning implementations out there but we know that rl algorithms can be quite sensitive to implementation details so all also today i'll talk about how ways and biases will help address the reproducibility challenge the challenge with analysis and visualization and how stable baselines will address the reliability so let's talk about waste and biases in a nutshell so i will actually share a video from my colleague uh charles uh because he's had he has done a really great video just to showcase all of the features so let me go ahead and share the video weights biases is the developer stack for machine learning practitioners use these lightweight interoperable tools for the entire life cycle of your machine learning projects experiment tracking hyper parameter optimization data set and model versioning even sharing results wb is trusted by over 70 000 machine learning practitioners delivering better medicine safer self-driving cars more sustainable farming technologies that build a better collective future you can easily capture all the information you need to make your models reproducible your future self will thank you start with tracking metrics for a single experiment then compare hundreds of experiments visualize thousands of predictions and share the insights you've gained take your existing ml code regardless of what,358,0,0,ed1bqaZGOQw
2,framework you're using add a lightweight wnb integration and quickly get live metrics terminal logs and system stats streamed to the centralized dashboard wnb also organizes the results of entire projects even if you and your team tried thousands of experiments you can still painlessly access the results in a single place from any device at any time and query and filter results to spot issues and trends track your progress towards the optimal model take notes on your results across experiments and share findings with collaborators capture the valuable research efforts and insights that are often lost over time or when a team member leaves everything is built to empower collaboration just imagine what your team could do with a shared tool for data set versioning model management experiment tracking and hyper parameter sweeps that's a quick summary of weights and biases the flexible lightweight developers stack for machine learning practitioners get started now and see live results in just five minutes um cool so um that's basically a whistle in a nutshell let me go ahead and share my screen again and then you know you can uh you can always go to the waste and biases website i think we do our marketed team does a great job in listing all of the features and you can see how you can integrate ways and biases into your code there's some code samples um so i highly encourage you to check out our website and documentation and the website is wmb.ai slash science uh with that said we'll talk about what stable baselines is uh and uh uh uh antonion and ansi will give,358,0,0,ed1bqaZGOQw
3,you an introduction on what stable baseline is hand it over to you so thank you so well as mentioned the the idea between stable baseline is you have many many different rl libraries out there and we focus on mainly providing reliable implementation of rl algorithm and while still being user-friendly and we focus on the very specific case which is model-free single agent reinforcement learning and um the last thing we focus on and that makes us i would say different from other libraries is we favor readability and simplicity over modularity so we rather have some we don't have many um modular components we mostly have uh implementation uh that are not self-contained but almost self-contained and then as uh so for the user-friendly part uh this is how easy is it to use in very few lines of code uh this is for creating a software actor critic agent on the pendulum task um and uh so you you mostly need to define it's it's just follow a psychic learn syntax you you just define your um your model class and and then called learn on it so to train the agent and afterward after saving and loading with this simple api you can query your your agent with the predict method and this is most of what you need to know about the api for um most of the of the problem uh and for the rest i will leave it to ansi right thanks antonin so just to continue on what the antonio was telling uh the other highlights of the stable baseline street library is that we have almost,358,0,0,ed1bqaZGOQw
4,full test coverage almost there's some neat bits that are not tested but those are like small things uh we have a super active community so we have issues almost daily if we can handle and back reports people reporting bugs and so on and also doing pull requests so that's very handy we also like to advertise our documentation which covers almost everything and we always update it when there is some bigger questions and finally we also have this comprehensive leica we have callbacks you can modify but we also have tensorboard logging but that's going to be touched on by costa soon and finally we have this training framework included you can use to run your experiments or your runs etc but also to replicate the results so we have gone the great lengths to ensure that our implementations are correct and that end we have this zoo library you can use to kind of replicate the runs and start the hyper parameters and so on but all of this more on that from costa so now it's i think it's goes back to coaster now thank you for the excellent introduction on the stable base 93 i'm always a big fan of the library i think all of the features are really well tested and well developed and with that um i am ready to uh sort of introduce you uh to tell you uh to tell you more about how ways and biases could address the three talent to address the reproducibility challenge and analysis challenge and how we how simple-based science will address the reliability challenge um so going for,358,0,0,ed1bqaZGOQw
5,uh going uh continuing on the reproducibility challenge uh so there's a lot of open source reinforcement learning papers or libraries that when i see their instructions for building the library or reproducing a paper i see instructions like this it's like pseudo pip installed gym and basically they don't really pin dependencies and sometimes that's uh that's pretty problematic because if tensorflow if they modify an api or something then your code will break uh and it it also sometimes is confusing as to how people are recording their hyper parameters i've seen some repository where they put hyperparameters as the name of the folders um all of these and you know saving the data in csv files all of these um methods are fairly homebrew that definitely works but what we at waste biases is trying to do is to make this whole process much more streamlined so that you can log everything you can log the software dependencies hybrid parameters commands to help you reproduce your experiments a little easier so that's what we're trying to do to say goodbye to this instead we have is for each run that you run with your scripts we record your experiments in a centralized dashboard that you can see all of the information that are related so for example we record the hyperparameters and you can see all of the configuration for a particular algorithm we also log the reproduce commands in particular you can see this was a command that in the past i have used to run a reinforcement learning experiment so usually how i would reproduce this experiment is simply say i,358,0,0,ed1bqaZGOQw
6,want to clone this repository check out this and just copy and paste this command and i would be able to reproduce this experiment and we also automatically ping the software dependencies as well so over here as you can see we log the requirements.text and another challenge is analysis and visualization this is something that i personally have felt a lot of pain in uh so whenever i go see a reinforcement learning paper i always see some episodic return curves that look like this they're really pretty uh all the curves are smooth but when i try to do when i try to replicate similar applause i realize it's actually quite a bit of trouble i usually end up writing 50 to 100 lines of matpot lip code and spend hours to debug it uh it was overall just not a very smooth experience and you have to spend a lot of time and effort into it um whereas and also there's to help debug your experiments uh in a lot of situations you also really want to see how well your agent is actually learning so this episodic return is a great way to understand how good your agent is performing but it's just a metric it's not the it's not the ultimate measure to me the ultimate measure is to visualize what the agent is doing to actually then let the agent play the game or perform the task over here you see a video of this agent playing hachita and this agent is actually trained with ppo and this kind of explains why ppo in this episodic return graph is having,358,0,0,ed1bqaZGOQw
7,a less than 2000 return it's just because it was stuck in this sub optimal behavior uh so if by chance that ppo was um exploring the other ways to walk maybe we can see ppl match the performance of pd3 so recording everything as much as we can is very important and we also would really like a way to analyze things very easily without writing a lot of code worrying about how my data is going to be viewed and also um when we share our results or write experiments note in the past i tried to use google docs to write experiments notes and that wasn't exactly sometimes that was a little bit of trouble just because if i see a number over at this uh google doc i don't know where if it's after three months i wouldn't know where this number comes from um and those are all the things that in waste and biases we try to address so instead of this in waste by st in waste and biases you can basically take the data and just generate those flaws on the fly and that's the advantage of having a ui in a sense uh we have these panels that basically work as an interactive lib uh utility that you can just plot these algorithms and over here i also log the videos as well so overall it's just really helpful and the reason the reason why we're able to create these charts on the fly is because um i can simply do i conduct a lot of experiments and i can use the filtering and the group tools,358,0,0,ed1bqaZGOQw
8,to filter to get to to obtain the experiments that i really care about and group is a very powerful feature because if you group the experiments by the algorithm then it's going to create these um shaded area that are known as aero bars so that is a very common practice in in a reinforcement learning paper so with width and biases is really easy to do with matplotlib you usually end up writing a lot of code for it and we also have a feature called the weights and biases report that is you can basically write the things that you can in the google doc you can write in waste and biases report except all of the numbers in a report you can directly trace through the original experiment so that really helps with the reproducibility because if i see a number i can trace back to a particular run and if i see the run if you recall i have all those reproducibility information and then i can reproduce the results so overall waste biases just provide a lot of tool to address the analysis and visualization and reproducibility and as far as reliability is concerned our reinforcement learning algorithms is really sometimes very sensitive to implementation details in fact there is a number of paper on this topic talking about how implementation details could have an impact on the algorithm's performance and that's that's where stable baseline 3 really shines is because all of the code goes through a rigorous review process and all of the code follows um really high quality standards um so that you kind of have a,358,0,0,ed1bqaZGOQw
9,peace of mind that this is in a sense the the most correct implementation and now we're ready to talk about the recent integration we made with waste biases and stable baseline three in particular we in our wmb package there is this wnb callback that can be used with the stable baselines code and the way you would set it up is to you initialize the ways and biases run you create a model from stable baseline three and then you have this little callback that allows you to say i want to save the gradients of the model every 100 steps and i can save my model to a particular folder so that after the run has finished voice and biases will automatically upload the model to the cloud um so those are just some highlights uh oh right also waze and devices callback is also is going to record all of the hyper parameters that is associated with the model so uh the the general theme again is to try to record everything uh as much things as we can and uh in in the dashboard so that we can later query those results very easily and we also support recording videos of the agents playing the game as well and the code actually becomes a little a little bit more verbose but basically you need to use you can use this vectorize video recorder to report videos and our integration will automatically upload the videos that is produced by the gm environment to the cloud so now is a demo time so uh we're gonna open a collab notebook just to see,358,0,0,ed1bqaZGOQw
10,this from end to end uh how you can create a how you can do a carpool experiments with stable baselines and having all of the metrics and videos being logged to the cloud so if you go to wnb dot me slash sb3 that's where you can play with the notebook so i'm gonna go ahead and do that and let me zoom in a little bit um so i am going to connect to the notebook feel free to open a link and try to follow along i think this will be a really fun practice uh so over here there's some preliminary introductions eventually we want to put this notebook as part of the stable baseline's documentation as well so that it's just a little easier for the adaptation and then we're going to set up the environments by installing some dependencies so i'm just going to click run anyway and something you'll notice is that i'm installing the master branch of wmb this is because i have since made some improvement over the stable baseline integration in our wmb package but um the the changes is not live yet but we're pushing a release either today or tomorrow so by tomorrow you should be able to replace this with just wnb uh so what this part of the code is setting up is installing wmb stable baseline 3 and set up a virtual display so that we can record the videos and upload to the cloud the next step is to basically run everything we have i try to make the code as simple as possible so after this there's not much,358,0,0,ed1bqaZGOQw
11,left so if you just run this cell that will basically create a ways and biases around with all of the metrics so this is warming up and it's going to tell you to log in to waste and biases so you can click on this url to copy your api keys so if you would like to follow along uh with this uh co-lab notebook please register accounts um and then you can go to wmb.ai authorized and you'll be able to copy this api key and then you can paste it here and it will um log log your account in so that your all of your experiments will be uh be tracked into your account um so i'll just hit enter um but after this this this is uh this experiment is finished oh actually while this experiment is running feel free to continue the process if you want to follow along uh just register an account paste the api key and you'll be able to create uh produces experiments and over here this cell creates this run page that i can just click on and it's gonna have some basic stuff and then as this experiment finished we're gonna see more stuff and we're only training the agents uh for uh roughly one to two minutes for 25 000 steps so this should finish uh pretty soon and a couple details on the staple base size integration uh as far as how it was implemented under the hood oh for some display um so basically what's happening under the hood is a stable baseline three have tensorboard integration so if you toggle,358,0,0,ed1bqaZGOQw
12,this tensorboard log equals runs it will automatically save the metrics with your experiments to uh to the to the runs folder and in ways and biases we have this argument called sync test board equal to true so it's a really straightforward integration that as soon as you turn on this uh argument then all of your metrics is synced to the cloud so now if we go back this should finish yep the program has ended successfully it's just uploading the final metrics to the cloud and now if we go back here we refresh the page we should see everything ready so the first time you see this dashboard it's going to have quite a few components but i encourage you to play around with it and in particular the way i like to do it is to move this oh also sorry let me zoom it in a little bit the usual way i like to do it is to click on this dashboard this section over here and sort of move at top and then i usually move the roll outs related stuff to the top as well so there is a little bit of a customization so to speak music and then this global times a global step is not really that helpful so i'll also remove it and then i also move the video over here so that's oh there's also the fps information that sometimes can be quite handy to help you measure the performance as well but this is basically it uh over here you can see the mean episodic return and over here you can see,358,0,0,ed1bqaZGOQw
13,the mean episode that collects with stable base lines and over here is videos of is the video of the asians playing the game and the reason buys this dashboard is really handy because you can click on this button and see what the agent was doing at different stages of the training we see that in before almost before any training at all the the card poll will fail will fail almost immediately whereas if we move it to the middle it will stay alive a little bit longer but then it probably will fail uh will fall over and whereas in the end it would almost balance the carpool almost perfectly so for me uh having this video panel is just it's just a game changer because uh it really allows me to view all of my experiments uh in a very easy fashion and i can see exactly what the agent is doing which is really helpful for debugging purposes and of course over here you can see all of the training losses all of the metrics you can also toggle this optional gradient logging that unfortunately it's probably not very uh helpful to uh to reinforce my learning models but it's just so cool to see um and over here you can click play with a couple buttons over here you can sort of tune with this global smoothing and that you can also tune the smoothing individually another thing is that over by default ways and biases has this x-axis being the stack but using the tester board integration the test for integration by default uses the global step uh as,358,0,0,ed1bqaZGOQw
14,the x-axis so here is when you actually see the 25k total number of time steps for the training so this is just a training panel and over here you can see the system panels as well so this is also really handy it will show you all of the system metrics such as cpu utilization memory utilization and so on it's handy because sometimes you can see exactly how your memory blows up so if you're playing if you're training your agents using dqm with the one million samples replay buffer you'll see your system memory just gradually go on and then if your memory is limited it will it will sort of blow up and throw an error to you but this chart over here will tell you exactly how it was blowing up another really helpful debugging utility and also this gpu utilization is very helpful as well because it tells you exactly how well your algorithm is using gpu and unfortunately sort of the modern most of the reinforcement learning algorithms today uh have a lot of components that's running on the cpu part and you need to do constant transfer between the gpu transferring data between the cpu and gpu devices and as a result the gpu is not really utilized that much so for example over here you can see the gpu utilization is around 8.,301,0,0,ed1bqaZGOQw
15,gpu at least in my experience usually really comes handy when you have experiments that uses convolutional neural networks that's when you see the gpu utilization really goes up you can see the gpu temperature and the power usage all of those things are quite interesting if you want to check them out we also keep the original tensorboard files uh and we keep the logs the center output and center error of your algorithm so if you want to log something you're using a print statement you can perfectly do that as well and over here we have the files tab that contains a lot of useful information as well the first is this requirements.pax by default disable baseline the waste and biases integration will automatically log all of your dependencies secondly we also log this model.zip if you um if you uh i will also log that if you uh specify this part of the code if you say model say path so if you specify this to a particular folder at the end of your training this wnb callback will automatically up your upload your model to the cloud so again uh really helpful um and you can also see the videos over here you know if you want to download them as well so i think this might be a good point for a pause to see if there's any questions we did have one uh which was already answered by until now uh music but i guess it might be relevant to other people too so antana if you could just read the answered question so the the question was about,358,1,1,ed1bqaZGOQw
16,um does it work for recording with video with different simulators during training such as gazebo and pipe build it so the answer to that is all of the recording rely on your implementation of the gym environment and especially on the render method so if you random method return an image when called with the rgb array argument then it will work otherwise you will need to define a camera object in pi bullet and probably the same for gazebo perfect thank you and then we had another one from chong um it came in a few minutes ago and it says i'm curious if the parameters sweep support object function and class instead of string and number uh so uh i'm not sure if i understand the question if the if the parameters sweep support are you talking about the if we can log like function and classes in the hype in the config section of the run so the short answer is no i think you can only record strings so oh by the way thanks for mentioning that uh over here we log every metrics related to every parameters that are related to the model class so over here you can see the action space you can see the device it was on the hyperparameters of the algorithm such as number of epochs a number of steps but i don't believe you can record functions and classes because you would have to pickle them right um which you can perfectly do uh if you um use if you pickle them and then use wmb.save is going to upload those pickle files into,358,1,1,ed1bqaZGOQw
17,the uh into this run and then you can maybe have a logic to load them later if you want uh yeah so i'll circle back to this if you have follow-up questions um so sort of following up on uh following up on the video recording for the most part it's the answer is yes if you have the arcade environments such as the atari environments then you can uh it's automatically going to record the videos for you versus if you have pi bullets again it's also going to record the videos for you so uh before diving into this so i think we're good with the question and answering and then the next step for me um i think is to show you some of the ways you can manage experiments uh and and really do a lot of analysis with the experiments and and in a sense that's really weight and biases shine because a lot of i've seen a lot of software they really record the experiments while they record all of the hyper parameters the metrics really well but when you analyze them it becomes a little bit inflexible uh so i'm going to try to do that i'll show you that so let me clear everything so i'm gonna set this i'm gonna clear my workspace so this is if you go to your project this is essentially what you're going to see you're going to see a lot of runs but maybe this is not immediately enlightening to you that's where these filters and groups features really come in handy so in particular if i say i want,358,1,1,ed1bqaZGOQw
18,to group things by the algorithm oh actually sorry i want to group things by the environment name so immediately i see there are three environments the carpool which is basically what we demoed and there's this ant bulletin and there's breakouts and there was these runs that we don't not really gonna be helpful for us but we don't want to see it so we can use a filter to say i want my ant name to not equal to no right so we just kind of eradicate that and then if we see things on the right it's going to show me a bunch of videos and roll outs but these are going to you know it's going to combine every games of the data from every all three games so what i can do is to toggle this i button so that i can see all of the runs that are that are specific to breakouts all right so over here i see four runs uh sorry i also need to add a filter to say i'll go no i could run this is just to purge the experiments that i'm not really interested in so if you start a new run like this you shouldn't have to do it um yeah so this really showed me all of the interesting pieces i would like to see and then i can say using this slider i can see that oh in the beginning of the game this is where i zoom in a little bit uh you know the agent isn't performing well you see eight uh sort of atari games playing on,358,1,1,ed1bqaZGOQw
19,parallel that's because uh there are eight parallel environments that was used training agent and as you can see in the beginning they don't perform very well but then you can use the slider again to see how they perform in the end and i think our agent kind of performs very well and it might have in this instance kind of broke the atari simulator i have never seen this before but i think this is kind of like a waiting spring after talking to onsie but super interesting stuff and similarly i can see the experiments with the uh with the pi bullet environment and over here i see a bunch of experiments but this is actually so let me remove a few things real quick just to make seeing things a little easier and you can definitely make this whole process uh so in in a dash we'll re-record your settings so once you do this again all of the settings will be saved in in my case my personal workspace so that every i don't need to rearrange this dashboard every time uh it's just the first time i need to do it um so there are actually two algorithms there are td3 there are four algorithms that we that i run under the ants bullet and environments two of them is from ppo two of them is from pd3 and the way i can see it is to say i want to also group things by algorithm so if i click on this again i can see there are two runs from pd3 and two runs from ppo and this is,358,1,1,ed1bqaZGOQw
20,where things get very interesting so if i again visualize things using the global step i can see that both pd3 algorithm and ppo algorithm are trained uh trained agents for one million times steps but you see for a td3 which is this green curve the arrow bar is much larger and why is that so if i further group things again if i say i want to group things by name then it really gives me a lot of insight that is because the first td3 run which is the green run performs really well it achieves about 3 000 episode uh uh episodic return whereas the second uh whereas the second pd3 run which is this dandy cherry performs much worse that's why if i group things by name i see the algorithm's performance in aggregates i see the arrow bars and then i know that uh td3's performance at least in this limited settings its performance is a little bit unstable and i can you know smooth things however i like it um and then you can find the source code for sort of doing this so uh eventually i'll make these code into the stable baselines integration stable based science documentation um or in the ways and biases documentation but it's really just simple code right i have uh this code produce these atari experiments where i basically import ppl set up the hyperparameters that was specific to the atari environments and then call them out of dollar using the wnb callback all of a sudden it gives me the video recording and then the episodic return curves and similar thing,358,1,1,ed1bqaZGOQw
21,with the pi bulletins um it's really straightforward so let me post this into the chat real quick in case you want to check it out um so that is basically for the experiment analysis part um just trying to showcase you some basic uh tools such as the filtering and the group tools that's really helpful for doing experiment analysis and of course this report uh allows you to add a lot of things to it so if you want to write some notes about your experiments your you perfectly can so if i want to add something over here i'll just say this is uh pi bullet experiments and then i can add a rich set of medias over here so it's really flexible to help you edit a lot of stuff and in the past for my personal research i use waste and bias extensively i almost filled my entire i built my research infrastructure using websites and biases and so with my research with real-time strategy games you can see this report that's almost that almost looks like a paper that you can find all of the videos playing the uh all of the videos of the agents playing the game um overall just offers a really rich visualization and i can you know play around with it zoom in a little bit so this sort of goes back to what i was saying if i see a number over here i know how to reproduce it because if i see that uh this this these numbers over here for example if i see this purple run is performing really well then,358,1,1,ed1bqaZGOQw
22,all i need to do is to find the corresponding experiments right and then simply open it and i can see oh the command that was used to produce it the hyper parameters and then the requirements.txt as well so all of these are readily available to me um so uh i'm gonna pause another second uh so does anyone have any questions let me see there was another one that uh antona already took um in the chat uh and the question was is it possible to have two or more cameras per pie bullet and and for visualizations for example agents first person view and other and another camera hovering on top of the scene yeah that's a good question i think uh antonion has already provided a great answer so uh if anything i'm going to uh try to support that is if you implement your render function in a way that you sort of concatenate the images of both angles in pipeline then essentially what's going to happen is you're going to have videos that look like this where this image is going to be the first angle of your populative and this is going to be the second angle so as as long as you concatenate those images you can perfectly do so any further questions cool um so for the most part this is uh everything i have um i i hope it's uh it's uh it's useful to you i hope it will improve your workflow so i think at this point it's uh it's just really chatting oh yeah forgot i have this one more thing so i,358,1,1,ed1bqaZGOQw
23,was going to try to demo um sort of how you can visualize the parameters that's sort of another use case for the grouping feature so if i run my experiment with different number of time steps a number of steps how do i visualize their uh their impact on the performance of the agents and the answer is really just i group things by the environment name and i group things by the end steps then i see under and step number of times uh n steps 512 then we're going to see two runs so uh you probably want to run uh at minimum two runs so that you can see some arrow bars so that you know your experiments works in general not just that you got a lucky seed and we can go into these to check out the source code and again this is just pretty straightforward i just modify this steps to be 512 and a separate rounds i just modified them to 256 etc um so all of these will be in my in my opinion that you know really helps you to analyze the experiments and just a really great front end tool to help you extract insides of your experiments very easily um and of course another thing that i uh kind of forgot to mention is uh we log all of the models to the run so you can either download those models and manually load it to your code if you like but we also provide apis for you to basically pull every data you want so you can use our api to download the,358,1,1,ed1bqaZGOQw
24,every data is under the charts but you can also use the api to automatically locate this model dot zip and just load it to your code so uh it's really flexible and all of your data is always yours we just kind of store them um so yeah that's basically it uh feel free to join our channel uh follow us on twitter or follow our youtube channel if you like um and this is basically any questions i'll keep the slides so that you can see the links thank you so much for uh for coming to the webinar and thank you so much for the kind words um yeah so let me see how much time do we have left um so i think we have a few minutes left uh and it's sort of open for uh questions of course but probably we could go for a chat um you know if uh if anyone has any questions for our guests uh you also feel free to uh post in a chat um uh antonianes are very busy so this might be a great chance if you have any questions about uh stable baselines um i personally have one i was wondering you know um that's sort of about real map i know uh i'm tanya you're working on uh using dqm with vectorized environments um uh what what are your recent thoughts on it and progress and progress yes so the question is about using multiple environments to train uh of policy algorithm like the qn or soft tactical critic so i've got a minimal working version i would say that,358,1,1,ed1bqaZGOQw
25,is already online in in a separate branch and i'm in fact currently using it for a european project um so it works quite well and although to to collect to collect more data much faster uh but it needs some tuning compared to classic hyper parameters um that's mostly the the main issue and the other issue is that you will also lose some sample efficiency you will explore more you will collect a sample faster but you will also lose some sample efficiency and currently the version is only tested for the use case i'm doing so mostly soft tactile critique and tqc which is truncated quantity critics in the contrib folder a country repo so and it i know also france it doesn't work with our recent feature which is supporting any type of observation especially as a dictionary observation this doesn't work yet but i plan to to work on making that feature proper um in september because before i would be on holidays and i won't have any time for for that so that's that's mostly it so it's a nice feature uh it works well but it needs some polishing and it needs some tuning yeah i love to see eventually uh dkn works with a vectorized environment just because um ppo works with vectorized environment and and uh it's throughput and uh it has relatively it's relatively fast and it's still easy to debug because your code is mostly asynchronous whereas i think in the dqm world you have apex dqm that although runs much faster i feel the asynchronous structure you know it's at least to me you,358,1,1,ed1bqaZGOQw
26,know it's harder to grasp yes yeah um and the the thing is this was a design uh choice that i made a while ago that everything was a vectorized environment because i had that in fact in mind to to allow to have a synchronous environment at the end for any algorithms but first i we wanted to have the baseline working and ready and not make things too complicated already yeah makes sense makes sense looking forward to it music sort of under related notes um have you have uh you two seen uh the recent implement implementation work from domic dominic it's the reproduction of the dqm work uh reproduction of the rainbow dqms uh yeah i thought it was kind of related and super cool at the same time um so for those of you uh who haven't seen it uh this is a work that i personally felt really excited about uh let me see if i can find it real quick yeah i'll post it in a chat so this is a work from dominic and if you read his paper um he basically was able to train the agents using rainbow uh using a fraction of the original rainbow's paper's cost while still maintaining a relatively good performance so for me that was uh really exciting this is kind of um the final result that they have so um you can see that dominic's implementation only runs for 10 million steps yet it really performs um fairly strongly as well whereas um sort of other repositories usually run for longer although you know they also perform really well as,358,1,1,ed1bqaZGOQw
27,well um yeah let me post this in the chat as well there's another question which is in fact quite interesting to to answer the question is is sb3 tightly connected to my research position or is it for the most part a personal project so it started when we started the first version it was quite connected to what i was doing but then it became a personal project as as it was done in another lab and then for the stable basin 3 i got the chance that my my research institute allowed me also to work on that during my work hours so now it's also um connected to my research position because i use it for all the research i'm doing um and but at the same time if i was only working on it during my working hours i couldn't do everything or it it would still miss some time so i'm also considering it as a personal project and working on it at other times so it's it's mostly connected to my research position but it's also part of a personal project and for nc i i will let him answer yeah uh for me it's half and half so i use it for my research but lately it has mostly been a personal approach i like to help people around answering questions so outpounding doesn't have to yeah my my hat's really off uh my hat's really off to youtube uh because i i really enjoy using stable baseline especially some of the utility tools uh for my personal research uh it's like i don't need to maintain them,358,1,1,ed1bqaZGOQw
28,i just kind of impose their baselines and just use the common tools such as the vectorized environments the atari wrappers it's it's just been a really uh it's just been a drawing to to work with yeah and the czech environment is also quite used by people because we had so much issues that at the end we decided to write directly a code to check people's custom environments to avoid uh answering the same question again and again yeah the bacteria is environments i don't know i feel like probably open i should have uh included in open eye uh gym repository long time there is an implementation but i wouldn't rely on it yeah yeah it has different apis as well so i don't know sometimes it's tricky how many uh prs do you do you guys review every day um so pull requests not that much some some what we usually have is sometimes either very small for because we got almost one issue per day at least like or 50 should per month more or less this we got more issues than per request and for put requests it depends because sometimes we just get big pull requests that last three months and this already take quite some time to to ensure the quality like the dict observation implementation it took yeah three to four months to fully between the first working version to the completely um completed version yeah it took us some time so it depends but it's mostly issues nice i guess we can wrap up here uh i want to say thanks again for to costa for,358,1,1,ed1bqaZGOQw
29,running the webinar um thanks to all the great feedback in the chat and a special thanks to our uh guests today for taking the time to talk about your work uh the recording and all the materials will be shared in a follow-up email so be sure to check your inbox in the coming days and hope to see you around in the wnb community take care everyone bye you,92,1,1,ed1bqaZGOQw
0,channel so today we have a python workshop and we will do an introduction to gmail stable baseline foreign music algorithms on simple environments and we will also try to create our own environment which can be maybe useful for research proposals so and third so i have posted the link of a collab i wish you can found some of the functions that i will use in this uh in this workshop and uh if you want to do a function to also follow along which is a good idea uh i have set here um sorry my flight has disappeared so i will paste it in the description also so i have uh just a request of a live review that i needed so you need to have to do i'm sorry but i will achieve it quickly i'm sorry have you tried the space key to go from one slide to the to another uh yes i think it's because music no i think i know what happened i think okay i put my phone on my keyboard and experts okay so just uh i'll put it back in the description so you also have it on the collab so the list of library that i needed if you want to follow along uh okay so let's go um so i will start by uh we will start by looking at gym so jimmy that took it uh for developing and comparing to enforcement learning and algorithm so we so it has been so it was developed and having supported by open ai since 2016.,348,0,0,lZ-F9C6cGIA
1,so it's constantly open source you can go on github and contribute so i should expect the link here so the idea of jim is actually to provide a centralized api for environments um for investment learning control test so you have a simple learning tablet can be implemented as environments so i will explain this a little bit in details just after and with jim you will have a standardized set of classes and methods that you can use to train to to interact with different environments and train different algorithms and provide different benchmarks so the list of all environments on this link here so if you can just to show a little bit of history of available environments okay so you have classic control title for example the control of the modern gap program which are pretty popular and used uh two benchmark different types of control and learning algorithm in the literature and you also have environment based on physics stimulation like mujoku for example but you have to train different animals or um well robots humanities exist so you can train them to to work and by basic music and now it's completely free too so you can have a free license to uh to play with it yeah exactly because before you what happens because this one you have to be a student but now i completely um and so foreign okay and so uh you also have the famous atari games uh which i think it's not the one on which we we have the first huge um progress on the investment learning and i think because with the,358,1,1,lZ-F9C6cGIA
2,deep true learning algorithm that we will use later and so you have all the whole um uh games of all the games uh used in atari and uh i think in the deep free learning paper i actually just took a screenshot of the games and just to be the um a commercial neural network they managed to learn how to play the different games um so you have a huge variety of environment and all of them are completely standardized with the same api and the same methods um so we will see which one um so uh just a little a quick reminder about reinforcement learning uh so in reform smart learning the idea that you have an agent that interacts with an environment and that in the agent will learn an optimal policy which is basically a sequence of action um to to maximize a signal which we call which is called the revolt um so at each step the agent will take an action will send it to the environment and the environment will return every road so the reward will signal with basically describe the agent that's the only signal that the agent will accept to go is learning towards an optimal policy and and the and the the agent uh so we'll uh learn a policy based uniquely on the stage which is the sort of summary statistic of the um of the situation in the environment um so and just by looking at the page it will return uh an action um so so you don't uh necessarily make the hypothesis that the agent can fully observe the,358,1,1,lZ-F9C6cGIA
3,stage so in our case we will continue with this assumption because it doesn't understand too much thing it doesn't change a lot of things for the implementation in terms of coding but um you can also encounter environments that and and encounter situations sorry well the agent only observes um only also partially these states and so we will use this observation of the agent we'll call it observation whether it corresponds fully to this to the actual stage or only through the conservation of it and so the goal and so uh-huh so i don't know if okay so maybe i want to be true to mistake about the size of slide uh so usually you will have i'm sorry but you will have uh music it's actually alternator okay so music okay music to uh music yeah music or something music which is a gamma here and which will discounts which may roll so we want that you will have later in your direction neutral trajectory in the environment will be worthless and the one that you can gain immediately and you go will be to maximize the this this uh discounted uh the sum of this contents move up um okay so i'm going back to my slides hopefully okay so so um in gym this is represented by this set of five measures which really the the first two um let's tell you the most important you can do pretty much anything you you need to we discuss two method so your environment so oh i should i maybe just from the approximity here to say so jim basically really focused,358,1,1,lZ-F9C6cGIA
4,on implementing these parts of this uh on the system so a gym only care about the environment it's completely agnostic about the type of policy or agent that we're using to choose the action um for oh he knows maybe you would simply picking internet from them so jim only implements a class that will um yeah that can take in action and returns a reward under the states so how he does it if today um having so these two methods so you have a method stack that will take an action and returns of the observation so we want to get the learning uh bully and dawn which is uh just an indicator of whether the episode whether we try actually in the environmental stuff right now or not so for example if you have let's say a grid where you have a starting point and a goal as soon as your agent which is the the goal uh it is what we call an absorbing state so the episode is now done because the game is over you have succeeded you can also fail so for example if you have the same grade which you have maybe your case where you have a burning fire and if you win that case while you die well the the agent dies so in that case uh well you will also receive the golian don't is false because you simply have failed and the episode equals so you have the visits method which will initialize and we initialize the environment and return the universe of observation and um so far particularly the two most important,358,1,1,lZ-F9C6cGIA
5,methods then you have the render which will just give you uh an image of your environment so this can be useful for example to record a video if you want to have an id more qualitatively of the type of policy that your agent is following and so when you have a closed method in case you you dealing with an environment that needs to maybe open file or read something so you can close it in that method and then you have a seed which is eaten here but the seed obviously will always use to settle your environment from reproduce reproducible results so in particular new action phase we'll see that you can sample the action space and so this allows you to await control the same action at the beginning um okay so i will show you so an example on a really classic control pass which is the capital so uh okay okay so you have um this is wonderfully see it but you have a class and you have a pool i think you know it so and to the pool well so the cat is uh can move left and right along a track and you're going to make sure that people stays straight uh okay so i think i have the explanation here yeah so you have your uh so dependents are up right and you try to prevent it from pulling up and so every time the pro is applied and you you will receive a reward of this one and uh the exit will end when it's uh when it's falling um so you can look,358,1,1,lZ-F9C6cGIA
6,a bit closer at uh the way it's implemented in the engine so we'll have a environment and so i'm sure we'll have you environmental class and uh it will be associated to a connection space and an observation phase so which are attributes of your um of your character and environmental class and so the action will be in an empire array of shape one and you only have two actions zero and one for left and right and your observation you will have a number of shape four because you will have the caps position and velocity and the ongoing etc uh okay so how now how will we act in this environment how will we interact with it so uh so it's pretty simple so all the environments that are already registered and that so that genes comes with uh um associated to nid so for example the capitalist capital uh v1 so you simply have to uh to choose the id and you do deep dot make and you have your on your environment um so uh so now we have an example so i don't know if maybe you can well you can do it too um and you have so the music methods which give me my first observation yes okay and so i have here the my operation of shape four but it's pretty pretty simple and if i look now at the action page so all my gym environments have an action space attributes and i can see that it's a discrete action space of of size two so because i have two actions left and right and,358,1,1,lZ-F9C6cGIA
7,so all action spaces have a sample attribute which i can use to sample run direction so it's just a uniform uh something of the available space so we have for example like it returns the action one foreign music so now i have so the i have taken a random action and i received a new observation uh which now describes a new state of my system a review b-rolled so one i haven't and i still haven't fallen so i'm still so i will say the first one we bought i still haven't fallen so my if that's not done and the info is empty and okay so we don't have any more instrumental information here okay so now we can do just a quick example uh i start by initializing my environment sorry my first observation and then as until my my episode is done i will simply sample a random action and then step in the environment and i will look at the same of the world that i collect in environments uh but pretty simple and um so one thing that can be interesting that i can also visualize the environment so here uh so i take the the exact same code but i simply uh use my random method here so i have a motivator so i choose the rgb array which gives me just to simply um that i can fit them at the clip and i can look at the evolution of the systems obviously from now i'm i'm simply sampling action at random so i will not last long in my environment okay so i will simply,358,1,1,lZ-F9C6cGIA
8,do so now uh so i have my environment all set up i have um i know how to send action to my environment but i don't not fully picking action in any smart way i'm simply doing a syndrome so i will need an agent that can actually learn a policy in the environment so if you see that i have plenty of ways to to do it so today i will just introduce stable baselines the stable bay lines basically um a library that gives you access to many implementations of uh stable and reliable in implementation survey of deep learning uh different plasmatonic algorithm so the last version which is the will be line free as always implement implementation in python that's the one i will show you today because before i put in terms of it um so i mean so it's pretty uh easy and so it really has a plug and play approach um so i but here the link to the guitar which is a completely open so you can also contribute uh it works on any environment that has a gmail traffic that's where like is genes and the standardized api comes really handy because we just make the assumption that you environment is a class with the step and a reset method and run the method and from now on you can simply it took your environment and feed it to be its own training uh mishaps uh so you can basically you can do pretty much everything in two steps you simply choose the algorithm that you want to train so uh stable ways income we,358,1,1,lZ-F9C6cGIA
9,can now uh eight or ten implementation of really popular um not necessarily stage of the ad but hopefully algorithm and the reason that i often used as a baseline in indonesia ratio and um so you choose a retirement parametization of the movie policy of evaluation exactly um so for example um i will also be showing the example of approximate biceps animation on all types of environments um so okay and music i'm sorry so i'm trying to everyone music started okay okay so uh okay and the same place okay so i'm going back here and so i will um but yeah but i believe that i will explain it quickly uh and then we will do a benchmark of pulling together uh of different algorithms in the environment and um so i will uh just show you how to use it and then we'll try to understand what uh what's happening uh okay so in the collab if you want to do it in the collab you have all the functions that i'm using including the evaluate model um but it's actually really straightforward we just instantiate a model thanks to stable baseline and this model um can uh can someone has a predict method so as we saw earlier with the agent it can take intake and observation and returns an action uh and so we can take that action figure to the environment we save a reward and so what we do we keep doing this until the environment until the episode is done and so we do it here for um i think for 20 episodes and this is,358,1,1,lZ-F9C6cGIA
10,the best to have an idea of the adverse performance of an agent in the environment uh okay so excuse me can't wait we have a question from constantine who asked what is model.project what it does or how it is implemented i think what it does yes what it does okay so what exactly is that it takes and an observation so that she's uh which is uh returned by the environment uh and from it it returns an action so basically it it's your policy when you have an agent that has a policy you want it to be able to look at the state of the environment and uh give what the model thinks is the optimal action for for that space and uh so so it's so it's a kind of model for strategy right model for policy it's a generator of points yes yes yes so uh exactly so when you uh investments we need to program ourselves right model 3d also um with respect to jim yes you need to program it yourself but with stable baseline you basically already have a set of models that uh that are already implemented so that is a set of policies that you can directly um a train so so you don't really so if you use table baseline you don't have to re-implement i mean for the set of base model that exists in table baseline so for example if you want to use some approximate like linear function approximate or neural network it will go in this module or it will go in some other module yes exactly yes so exactly,358,1,1,lZ-F9C6cGIA
11,what it goes so if i want to use neural network or uh strategy generation it will go inside this model right yes exactly thank you okay thanks can you just show back the slide where you were uh defining uh the policy with the mlp uh an appear initialization yeah this slide then maybe just uh the coding like just after where you define your your pto policy and ai this one yeah yes so here so i i choose um maybe spend a little bit of time later if you have an integration of this type of model but basically what you just need to remember for now that so this model will be associated with a policy that will learn that will be shown in the environment and so i'm choosing a type of policy which is simply a fully connected neural network and uh i can give it the environment so it makes all the assumptions that i paid about the um the method available in a gmail alignment and just with these two uh these two arguments i will be able to train the model in in table baseline so i define here type of policy that i'm choosing that i want to learn and the environment that i want to learn them in and so that's pretty much so is there any other questions or in the in the chat looks okay so uh yes so uh that's what i can evaluate my my models obviously now i found out i haven't learned anything i'm just i simply differentiate it so it was initialized with some random parameters so it's,358,1,1,lZ-F9C6cGIA
12,not doing pretty well it is basically saved for only nine steps to strange and then just false and then i can directly train my agents i see that it's really just one line and so uh i simply show that the number of time steps and so stable baseline you obviously take the uh take the policy data you've chosen and update the parameter assumption of what it says when in it interacts with the environment and so uh after it's now i have a lot of more than 300 so the environment consider so i think uh as starting from a fussy market to 200.,139,1,1,lZ-F9C6cGIA
13,yes just to make sure we have 100 episodes and each episode contains 10 000 steps ten thousand five steps yeah uh it's basically a ten talent set is is the number of steps that i will um train the agent for and the number of episodes will be determined by how long i simply managed to stay straight so if you have stuff like 10 000 steps on the previous slide yes so basically the total number of steps that we make in the environment and when the episode ends it is automatically initialized with the wizard function and so it does again so the caps goes back in the middle and then you start interacting with the environment again until it falls and oh which is i think 500 steps and uh so the the number of episodes at the end will be determined by by how long you are able to keep the 80s going i don't know i think i'm confused but well maybe it's just my phone thank you so basically you we will train the agent for attention time set and uh so during one week results right sorry you're in one episode in one episode no in as many episodes as necessary to reach 10 000 time steps okay so so the the duration of episodes here is random and we have a budget of 10 000 time steps right and we use this time budget it is a yes steps budget to train the model you have exactly yeah it depends but of the policy you will learn during your training but yes you have a budget of,358,2,2,lZ-F9C6cGIA
14,ten thousand times seven okay okay okay um okay so uh so now we have a model that seems to work in it and that's we started with something that couldn't last more than nine steps and in the end of the last thing for 300 times that so that's come true but we don't really know what happened uh so we will we can use classes called monitors and callbacks and table baseline to learn a bit more about what what's happening during the training and which is obviously very useful if you're actually doing a benchmark for different products you want to know which one of them is quicker than than the other which one is more stable etc um so i will show you clicking what you can do with the money for class so the monitor is basically what we call a wrapper so in gym which means that you when you have an environment with all the meta that we have discussed you can wrap it in another class which goes to another environment that will let you well inhibit all the classical methods but on which you can add um more uh information on more methods so that's what the monitor does and what it does it actually gives us that every time you take a every time you take a step it simply looks at whether any result is done or not and if it's done it's registered you start the statistic of the environment which i'll do your rewards your total community reward during the episode and just write it in the in a fight um so i,358,2,2,lZ-F9C6cGIA
15,did the same thing but with the monitor and uh so just when you a little bit quickly better because it's probably the same thing you just wrap it up and you just do modern learn and then you can just look at your logs and you see like the progression of your learning so it's really into the internet it's really um well it's it's really noisy and actually what we're doing right excuse me what is the x-axis and the wax so uh is that the social cumulative reward and uh here you have simply the number of epiders of episode so so it's just more questions so it's totally what total discount with reward uh no this one is accumulated we want not discounted not discounted but you you optimize vessels with you know puzzle is somehow that in machine learning community often people right train with respect to one reward but then display another reward that always puzzles me but okay i'm sorry i didn't brilliant understand can you well it's a bit strange you know to train with respect to one uh objective and uh to show another object but well maybe it's the tradition in machine gun yes well yes um well when you use the accumulated reward you also ensure you get that that you have some commercial property that you wouldn't have when you're using simply the community reward uh but at the end of the day what you really want in terms of practice in demo practice uh is to maximize your community rewards so yes but uh um so that's so what we're doing now is,358,2,2,lZ-F9C6cGIA
16,that actually uh looking at the agent that is that in looking at an agent that is still learning and actually still like exploring in the environment but one of the reason why we have visitation noisy results because our agent is learning and exploring at at the same time so that's when we will use it in practice it will actually only extract the best policy possible so to have a better idea of the performance of the agent we would want to look at uh what it learns in terms of the best policy possible at every point at every episode um in in the process so we can do this by using a callback so that a simple class that you can use in a stable day line so i will explain in the in the notebook but simply the shape of the uh of the kelbak and the expected method to implement so in our cases we will simply set an frequency of evaluation and if we music each time set time steps we will simply have a clone of our environment which will be used probably for evaluation purposes and we will simply launch our agents with the best policy extracted at that point in of evaluation environment and see what average we want we can collect and write it down in a file um so so i'm gonna go in details in in the in the line uh but so yes so the idea is i think that you can look um in the same that you did through the monitor you can simply look at both your both your,358,2,2,lZ-F9C6cGIA
17,files and uh print them so we have here the evaluation the evolution of evaluation um so it's always a total simulated reward and not discounted and you can compare it to what you obtain uh when you're following your still exploring policy uh okay so that's a nice way to uh have everything in place to do a benchmark um so i will uh two to three algorithm that tells them but we can compare in environments so the one we looked at uh before was proximal for itself simulation so let's go in detail about the differences but basically but when you have your winter performing problem you can simply uh say well you can say and if we remember um we actually the objective which was to maximize with the sum of this country work and you can look at uh what when you are in a stage and you take an action uh you will receive a reward when you if you keep following the same policy you will have an associated discontent return uh to the action that you take a in your given state s so any satis action pair can be associated this way with a value that you call the true value but once we've put in here so but the first approach is to say we're going to learn these q value for the best possible policy because if we manage to do that then in any state we simply have to look at what actions associated with the best value and we know that if this is the value and the best possible policy so i've,358,2,2,lZ-F9C6cGIA
18,notified the optimal action to take in that state and that's one approach so that's which we will do with deep learning well in deep learning you say well to learn that function i will simply follow me to drive it by a neural network the manual network will try to estimate um my my execution on my my discounted reward for any uh action in anything and or you can try to directly learn your optimum policy where you say well i will simply well every time i take an action and i look at my return i will try to tune a bit my parameters in my policy so that i increase the probability of taking the knife action that led to blue revolved and i did free representative taking the bad action that makes you variable so at the second approach and so these three algorithms that i chosen just because they really popular color in in the literature issue and it can be a very good baseline so we have here the dp learning which is the first approach and here we have the two uh two approaches which are the approach of learning the best quality directly the only difference is that when you do this well when you change your parameters in the policy directly it sometimes actually smooth you can change your parameters a little bit and have a huge change in the action that that you're taking uh so in this last algorithm we're trying to say okay we're actually only moving on the safe zone around the quality that we know works maybe okay so if we,358,2,2,lZ-F9C6cGIA
19,don't end up with an external event policy so that's basically this we can compare and uh so once we have this you can see that so all of them are already implemented disable baseline so you can just simply import the free classes and uh so uh here you have the example of a really uh basically short benchmark when you simply have you simply can define your model and uh you can simply here train every single of them and register that that performance uh the evolution of our performance in training and evaluation during uh 20 20 000.,131,2,2,lZ-F9C6cGIA
20,so so i don't know if you want to just a small question what does iranian um just on the previous slide it was the union starts or something like this oh yeah so actually i i didn't actually i didn't end up oh okay uh yes so let me start this means that you're going to start learning after collecting 500 so here 500 uh transitions after visiting 500 states in the environment this 500 randomly or you visit with the leon police uh with your policy but that is not learned matter with you policy but you haven't learned anything yet so it's almost like okay so it's just few exploration you can see that uh we can say that just your exploration on stage right okay inside information um so i don't know what which one you prefab you can either try to do the benchmark together or we can the code an environment i don't know which one you you laughs yeah food for cooking and environment okay otherwise well again well in this benchmark it was performing but it's also because i didn't do any tuning of hyper parameters so definitely concluded but music you don't want to choose the weights of the neural network one by one by however so um so you can either choose any problem that you would like to turn into inside pakistanian program and try to see what you can get from it i also propose the on the polarity if you have it um a simple game which basically is a really which was kelly he's um so here you have an agent and,358,3,3,lZ-F9C6cGIA
21,you the agent should go toward that goal here in green and uh he does at least doesn't like it statistically so if you either i don't ever go to the bike inside so um i don't know we can maybe try to look at how we could code this or if you have any other id of an environment you would like to let's do the singular jumps yes instead of singulation then i only did single agents how well which kind of problems can you solve with them i get that they serve like the ductwork of them for instance but let's say that i have a problem and i cut the environment do i have a chance solution to the program or most likely so but this can give you is that well you will have a first baseline and also you can work on it so you can so you can start from where they provide and work on it um but yeah so if you have medium pain should be lit bit less pessimistic if you have a simple problem if you have like for example a distance action space with a presentable type for example so you can hope for simple especially just to in stable baseline you have i think eight of ten different algorithms so it's more it's an the stable design basically brings everything you have in terms of tuning in terms of little tricks that you can imagine to make this algorithm work so in terms of what this algorithm can do without any addition it's pretty much is the best that you can find but,358,3,3,lZ-F9C6cGIA
22,this algorithm versus limited because they always well they are mostly been tested on plastic benchmarks that you will be yeah so yeah that's not the standard classic control learning task or simple legend that you're trying to make for you probably will have to try another example well i i'm a bit persuaded let's do it so yeah and which frustrated because i i managed to make it half work but i did i like time to see if i could tune in because i didn't do much tuning i didn't have much time so i didn't know if i could make it work better if i had more time to to to touch it i'm a bit frustrated from but just if you know uh so in your in your example so time is discrete i guess yeah and you so you have an agent which is a circle or something like that or and you the the load moves are just one pixel up uh what are the what other rules is that so uh so i decided that the old move would be the election an action of two with a date and a day so i can very move a little bit like that a little bit like that and so um some limits yes within minus one and one okay so you have the the eight possible uh moves of the king uh yes i do it in continuous months so like i can choose between zero and one okay music they wanted to try with their continuous space i think it's probably you would have been easier with a,358,3,3,lZ-F9C6cGIA
23,disrespective something to do what i think uh managed to do with the continuous space so yeah so i have a limited uh updates um and it's been the the from the point of view of the agents the the state space is uh what exactly uh is the position okay but you all raised to have the same landscape where the cheese is and uh and so on yeah and i'm sorry so it gives you an observation and you said that you don't necessarily observe the true state right so do you have for this kind of problem do you have a way also to know what is the mistake like if it's on the grid for example and you don't really observe the true position i mean to benchmarks yes luckily is that that's a message and that's something that you're going to make when you're going to choose an algorithm you're going to say okay so i'm not observing the full phase of my environment so maybe i'm going to keep track of maybe i know some information probabilities between the different observations to see if i can try to estimate better the end dialing state of the environment so for now the integral baseline they haven't implemented this one now it's only they just assume that you have the four states budgeting but in fact you can implement a partially observable mgp as a gym environment yeah and you can always try to use your favorite line algorithm and uh and see what happens yeah and sometimes it works yeah often but sometimes yeah yeah i mean these are very complex,358,3,3,lZ-F9C6cGIA
24,i mean well the exact one yes in this way more you can um so yeah so i'm obviously there's many many different ways to implement it so what i did is that i tried to um do uh but also i take my information yeah i need to chew and so i just have an observation stage which is just my um my uh position in the environment and i can took some actions when i choose a should be a picture one here but so i can go up and down and so to the steps so what i did is that i think sorry so i take the action that should give me my this my diploma my move thank you so along with my x and y axis and uh i um okay so that's being played in outdated version so it works best and that's an updated version where i did the direction and yeah and the velocity so that's a little bit different but that's the same principle it seems that you you translate it into your move and you know and um and so once you have this you wish you understood a copy of your new uh of your your new position so that's your new observation that you give back to the agent but you also need to give back a reward to tell if the uh if that is done or not and to give eventually supplementary info we don't really need to do this so one way to do it is to uh well use your knowledge of the position of an agent to see,358,3,3,lZ-F9C6cGIA
25,how far away it is from the goal and you say well maybe the distance is less than a completely uh that these things that you choose you consider that the agent has reached the goal and then you say okay now um you can stop and you will give him a good feedback so here i choose 100.,76,3,3,lZ-F9C6cGIA
26,um you can do the same thing for the cliff and give back a boundary role so the main idea is that you shouldn't um set up an environment in tweets youtube more advantages for your agent to simply die on the case because i mean the the agent if it pays extremely long in the environment and never falls in the cliff but always gathers everyone that is negative because it hasn't reached the agent can end up actually find out that the best strategy to simply die immediately which is not what we want but one of the things that you have to think about when you design your uber function because you essentially has to maps to your problems um okay and so yeah so what are these are the simply uh give up give a reward when he's not named when he's made a legal later at the place and regard to the section of a distance to want to go obviously there is a bit of a trap here because you can't really well ideally which is when i'm a bit frustrating but i haven't really spent enough time on it maybe ideally the agents no matter where you would fight it you will learn to go around the cliff because if you say if you only follows the reward function it's for a simple it will fall on the class because you can see i if the equivalent value you could simply cross it on your result foreign music or whatever uh you maybe you don't have such a good evaluation function yourself and you just have the the final,358,4,4,lZ-F9C6cGIA
27,real reward of the game which is the yeah you lost or you you won yeah so to be disturbing to to have to use this kind of trick to i think the idea of the general idea massage in general that we ideally we tend to know the situation where we didn't need to do this and so sometimes in some games actually we it's actually possible to just take a screenshot now of of the game and to just use um the result of the episode in the game as well function and it works on some simple game but uh especially in i think that about industrial application that's not really uh i haven't really worked for now so you generally when you do this kind of now it's really just a simple game when you have to work on the practical application you try to put as much of you you manually need yeah sure but yeah but it's very disturbing to see that even in a very very simple environment yeah which should be an easy one uh you uh i i had exactly the same problem when i when i played around with jim and stable baseline switch which is december for what you do uh we'll just find some kind of reasonable currency if you enjoy what was binary yes like just of the people or not well i try to do it i didn't i didn't manage to make it up but again i don't want to conclude too much of it because i really use the simple classes i didn't do any fine tuning i i'm using,358,4,4,lZ-F9C6cGIA
28,really simple things algorithm so i don't want to conclude much from it but so yeah i think when you look at the literature it seems that you have some algorithm with some kind of functioning also but managed to to work pretty well with me even with this stuff we won't i had work on some algorithm where you really when you basically have no reward for most of the of the episodes in in the game because you need to actually open a door and find the key and the key etc and music it takes a lot of time but then you have to wait until 1 million 10 million 100 million times back in the environment because it was also one problem but then you can you manage to make it fall provided that you have a good exploration policy and strategy etc so yes um music so yeah that's one way i try to do it so that there are many different ways so i try to instantiate my environment this way and i use a really simple algorithm so psych is also a version of the second option so the actual critical option where you try to limited policy i said in fact you try to push the the policy if you are always keeping its option open so you always try to have to maintain a level and entropy on its uh on the action than the policy can choose and um so the next thing that can be useful is that you can register your environment and so you do it very simply every team that and register,358,4,4,lZ-F9C6cGIA
29,you choose an id and you keep important to our your class is and you you can set up a number of max of maximum steps to in the environment okay and then so i'm trying to train my um my model so that's one of the training that i managed to do so i managed to have a lot of 61 which means that it has finally reached the goal but you will see that you're shooting and so basically yes i i wanted to be able to do it in the workshop but so i try to limit myself as something that could be learned quickly so i try to i think back here so but now i'm really i mean i'm i know i'm trying to i think i will explain a little time on it again to see if i can manage to make it work starting here so that's that's pretty interesting so we didn't actually have the time to do coding together okay by talking too much don't don't worry but as a matter of fact it's the end of the hours i think we can go for a round of applause for claire and then,262,4,4,lZ-F9C6cGIA
0,in this video we will be training humanoid with deep learning first we are going to get an overview of how humanoid works then we are going to understand how algorithm works which in this case is proximal policy optimization or po it is just a neural network then we are going to train and visualize results of this humanoid robot after its training so let's start this is internal structure of robot now to move this robot we have to control the robot with some actions we have total of 17 joints with their ids ranging from 0 to 16 each joint gets a torque ranging from 0.4 to positive 0.4 this robot gets total of 17 torqus at different joints to stand or to move every time now the question is how to get those values of torque which can be used to stand or move the humanoid robot for this we get observation from the robot we get total of 348 inputs from robot to generate torqus accordingly to move the robot these values can range from positive infinity to negative infinity the observation space encompasses the robot's body part positions velocities mass inertia joint forces and external forces providing a comprehensive view of its state now the question is how to guide the neural network to give us accurate torqus from robot to stand and move to guide the neural network we add concept of reward this reward is just a numeric value and goal of the algorithm is to maximize the reward first part of the reward is healthy reward this reward is dependent on the center of humanoid robot on,358,0,0,QwJcF08hfs8
1,z-axis this value is 1.4 at start with some randomness if the value is between 1.0 and 2.0 then reward will be five for every step of the play the purpose of this healthy reward is to ensure that the robot remains in an optimal standing position maintaining balance and posture the forward reward ranges from zero to positive value where the reward is positive when the humanoid moves forward in the positive direction along x-axis control cost is a negative reward to penalize the humanoid for taking actions that are too large contact cost is also negative reward to penalize the humanoid if the external contact forces are too large overall goal of these rewards is to guide the reinforcement learning algorithm to learn torqus which will help humanoid to stand and move in positive direction along x-axis now if we apply random torqus you can see the humanoid will immediately fall down to prevent this we need to train a neural network to determine the right amount of torqus at each step to keep the humanoid standing and moving in the right direction second play also uses random weights humanoid falls down again from random torque supplied to its joints for the details you can visit gymnasium documentation where you will get further details of the environment now let's understand the algorithm we are going to use to train the neural network for this humanoid task before going to algorithm there are a few things you should understand consider a simple state represented as s1 this state value is just an array of 348 values this is observation we get from the environment which,358,0,0,QwJcF08hfs8
2,in our case is humanoid to get the action values or in other words torqus we feed those states to the neural network network gives us the action values these action values are just torqus for the humanoid robot you can consider them as policy of the network by taking action we also get a reward starting from the state s1 we take action a1 using neural network and get reward r1 and we move to the new status 2 we take action a2 using neural network and get a reward r two by repeating the process we get collection of stats actions and rewards what we did is take a state from the environment we took action with neural network we got a reward by taking that action and we moved to new state we repeat this process to get all the state action and reward pairs each pair of state action and reward is known as a time step because we move to a new state after taking each action remember all these rewards are collected based on the rule we discussed when discussing the environment these are just rewards we collected from the environment for understanding the discounted rewards i am just using simple reward values consider we just got one reward at the end of episode at all the other steps we got reward of zero these are not actual rewards from the humanoid robot these are just for you to understand the discounted reward concept now these new rewards are what we want to calculate these discounted rewards are calculated based on rewards above here are values of these rewards with,358,0,0,QwJcF08hfs8
3,discount of 0 9 these rewards can easily be calculated using this formula this discounted reward is a very simple concept we take reward at time step t and add it to discounted reward at time step t one multiplied by discount factor when we are at start we will get a reward of one after taking nine steps so at start its reward value is in discounted form because it's at some distance from the reward reward at the end here we got a reward of positive one we can also get reward of negative 1 and this will also change discounted rewards for entire episode we can get rewards at multiple points and calculate their discounted rewards at each point and you can see how discounted rewards change with multiple rewards we will get multiple rewards after episode here episode is one play of the humanoid robot which consists of multiple time steps now how to find if the reward at each point is good or not in other words should we go for this policy in future or not this is the data we collected our goal was to find out if these rewards are good for us or not previously we had one network which takes state as input it produces an action from the given state this is the second network we are going to train which again takes state as input it produces a scalar value as output for given state this value is expected reward if we follow current policy here are all the values generated for all the states using second network we take state st where t,358,0,0,QwJcF08hfs8
4,is number of time step we get action a for this state similarly we get value for time step t using the second network but remember our goal was to calculate whether the reward at a given time step is good or not but how to find out which reward is good and which reward is not good for this we calculate the vantage value for each state this advantage at every step is calculated with this equation this advantage at time step t is calculated using reward at time step t minus the value at time step t remember reward is discounted reward which tells us how much reward we got by interacting with environment with current policy and the value tells us how much reward we expected from current policy now if the advantage is positive it means we expected less and got more reward so we should take this action more often in the future because this is a good action and if the advantage is negative it means we expected more and got less reward so this action is not good and we should take this action less often in the future this action value is deterministic and we get it from policy network but here we want to use stochastic policy which we sample from distribution in po when the advantage is positive the old policy is updated to encourage that have worked well increasing their chances to reinforce good behavior when the advantage is negative the old policy is adjusted to avoid actions that led to poor results reducing their chances to prevent making bad decisions again this helps the,358,0,0,QwJcF08hfs8
5,"new policy improve over time this clip function is just to update policy slowly you can further study this in detail in original po paper to update second network we just take its means squared error with discounted reward this was just an overview of proximal policy optimization algorithm with some basic understanding of deep reinforcement learning our main goal was to train humanoid robot using this algorithm you can study further details of this algorithm now before training this is how results look like remember you are viewing multiple episodes because as humanoid falls new episodes starts instantly so you are viewing episodes in a sequence when we start training the algorithm you can see that rewards started improving with number of time steps after 500,000 time steps humanoids started to stand longer and hence it is getting some extra reward this shows that algorithm is learning from the data now if we train about 2 million steps you can see that reward gets higher that means it should now perform better on results after 2 million times sts humanoids started to stand longer and also started moving along positive x-axis so this means algorithm is working fine another algorithm named soft actor critic algorithm performs much better on just 500,000 time steps",278,0,0,QwJcF08hfs8
0,what is going on everybody welcome to part four of the reinforcement learning with stable bass lines three tutorial series in the last video we were using a custom environment converting it to a doom environment and training a reinforcement learning agent on that environment and the idea there was just to kind of show how you could convert an environment but also to show that you can't just slap something into a reinforcement learning algorithm and expect it to just work right away so in this case we could see that things were sort of quote unquote working but not very well so the episode length did increase because dying was penalizing um and then we also saw the reward was like slowly going up but not how it should it should be should have been improving much quicker i am tempted to continue training this model just for my own curiosity's sake and maybe someone else will as well um but it really isn't it wasn't learning quickly enough so um so what i want to talk about today is the steps that i at least went through when i was like trying to convert this for the tutorial and i think i'm going to skip the like the process of like trying to code it in and then run it and then all that kind of stuff simply because the changes are so minimal i think it would be better for everyone if i just explain what the changes were show it in the code and then show the results immediately so so that's what we're going to do so i've pulled up,358,0,0,yvwxbkKX9dc
1,this other tensor board and this is going to be the baseline so it's basically the same exact thing that we were just looking at a moment ago in this case the the reward really never actually got any better and the only difference here just in case someone is curious is um in the calculation of the reward here it was a much more temporary short-term boost if you did happen to get an apple so in this case the the we were calculating a wee me anyway total reward that total reward was how long is the snake minus three because three is what the length of the snake starts at so we're not gonna reward the snake for showing up and then we were saying that the you know current quote unquote reward is going to be the self.total reward minus whatever the previous reward was so really frame by frame the only way you're gonna get a reward is if you got an apple if you get an apple great otherwise we don't care so so um so yeah and then if we were done the same reward negative 10 basically so a punishment of 10.,260,0,0,yvwxbkKX9dc
2,and that was really the only difference and so we can actually just curiously already see what was the difference between those you know very uh rare and more spread out rewards it really didn't learn anything at all versus what we did in the tutorial which was basically if you ever did get a reward you continued being rewarded for that and then if if you happened to get a couple more like at some point it got like three apples um you would continue getting rewarded for that and again that's why i'm kind of i would be slightly curious to to continue training that one because that might actually work out you know so anyways so very interesting information right there uh continuing along uh the next thing that i wanted to train simply because when i was developing this it really wasn't learning anything was okay what if we punish um like what if we use the distance to the apple as like a reward or punishment so my next thought was to use the euclidean distance so the euclidean distance is just that like straight line distance from one coordinate to another coordinate so the further away from the apple we are the larger the euclidean distance will be the closer to the apple that we are the smaller that euclidean distance will be so naturally it was like well we could just make that a punishment then because it will be larger the further away we are so i modified the total reward to just simply be the length of self.snake position minus three which was what it was before,358,1,1,yvwxbkKX9dc
3,and then simply minus the euclidean distance so the further away we are from that apple the more we're just gonna be just pounded with punishment so so i thought that was a pretty good idea and um maybe some of you are already seeing the error in my ways but but anyways let's pull up the result from that one and compare and in fact we'll just run just that one for now and i think it's these buttons here yes so the episode length one thing you should notice quite quickly is whoa the episode lengths are getting really short and the rewards starting you know very very low basically off the charts low um they do come up but they never exceed a negative 10 what's going on well i'm not even going to play this agent but but basically what happened is the agent essentially is realizing that life is very punishing and the best way to uh it's very morbid but the best way to uh earn the highest reward is to immediately and end the game okay so so so what happened was i was training this and then i walked away and i came back and then the screen like the game screen was just solid black and i was like oh no something broke or whatever and i couldn't figure out what was going on but then i was like oh i saw i could clearly see things were still running but no it was just the agent was just instantly we'll call it running into itself just over and over and over so anyway i think it's,358,1,1,yvwxbkKX9dc
4,kind of kind of funny so yeah terrible idea so then i'm like okay i see so so what i've done is i've just made punishment so high and this agent can instantly end the game so one thing that was interesting about this environment versus the previous and or other snake environments in general that i've played you can't like in this snake environment if the snake is like going left and you just hit the key to go right that will count as like you ran into yourself so it's it you can instantly end the game like before anything happens so anyway cool um so this would be like an unintended consequence like i i didn't even think of this as like oh the agent might just end up doing that because that wasn't even in in like a re it wasn't reasonable um but it's very logical and that's the problem so after that i got to this version of snake so coming down here it was pretty much just okay we still want to use the euclidean distance but then we need something to like offset that euclidean distance so basically we're going to say the reward is just 250 right out of the gate 250 minus the euclidean distance that way if it learns to at least start getting close to an apple um that's going to be way more rewarding than um ending the game so that's cool we could have ended there but my fear was if i just did that it the agent might just go in circles around the apple so instead i also did okay,358,1,1,yvwxbkKX9dc
5,well plus and some sort of apple reward what is the apple reward quite literally 10 000.,22,1,1,yvwxbkKX9dc
6,so then finally because everything was getting very astronomical i just decided okay well after all of that i'll divide by 100 and that will be the um self.total reward and then i believe yeah the reward for the actual step is total reward so no more of these like frame by frame things it's just constant total reward so the more apples that you've eaten um or or actually the longer this or well you'll see in this case it wouldn't even be the longest we're not even taking that into account so really it's just how close are you to the apple and if you happen to get an apple we we want to reward it so greatly that the reward for chasing down apples is higher than just doing a circle because once you get the apple now you're further away from the apple so you're missing out on all these like 250 rewards so this is something that i think needs to be further tweaked honestly from the results that i got but if we check the tensorboard we can rescale rescale and we can see the rewards are much much higher much more appropriate rewards for this agent so now we can actually play this agent uh just to kind of see where it is uh let's take down the smoothing and let's get a time that's episode length let's get a reward that we like why does it still like go off the chart that's annoying um okay so in this case uh step eight eight four so probably like eight eight i guess we would have to go like,358,2,2,yvwxbkKX9dc
7,eight eight oh or i guess 890 so can we get 890 of 80 26 let's see if that's apparently that one didn't oh you know what i did how about i okay so we'll just take the most the latest we'll take 880.,57,2,2,yvwxbkKX9dc
8,so oops so we'll copy that come over here uh we'll go to pl oh that's not what i want i thought that was the arrow key how do i do this okay we'll come over here models uh we'll take that model and then we will do step 880 000 we will load it up we'll just play infinite episodes of our friendly snek game uh python 3 play snake move this over here and yeah so here we have the snake game as you can see he's does definitely he the snake spends time sometimes circling around the apple or near the apple but in general uh it looks pretty good it's getting the apple just ran into itself unfortunately um and yeah so i mean it clearly has learned something now i know that this could be improved considerably from this point um my guess is the observations need to be improved because the snake does run into itself which is weird um and then and the moves that the snake is willing to do are clearly like these like really big arching moves so my guess is the snake actually has not figured out where the where the other bits where the other parts of the snake are so um i think that that's probably something that would be improved but hard to say it's just really trial and error um and that really wasn't my point with doing this tutorial was not to just continue solving the snake game so i'm going to leave it here but feel free to tinker with it and try various ideas and if you get,358,3,3,yvwxbkKX9dc
9,a really awesome model please do share it with me i'm curious to see what people can come up with to do make an even better model so anyway um i think that's all for now uh like i said i just kind of wanted to show some of the things that i worked through just to and in this case it was really just all reward stuff but there are probably tweaks that could be made to the observation as well and really just things you need to think about when you are creating your own custom model because or a custom environment rather because these things won't just be handed to you and even things that you think are reasonable or all you'll need is these things you'll you'll find that you actually have to kind of hand engineer these things so anyways that's all for now questions comments concerns whatever you can feel free to leave them below otherwise i will see you all in another video you,222,3,3,yvwxbkKX9dc
0,so what's up guys today i'm explaining you stable baselines reinforcement learning using tensorflow 2 with bp algorithm now we're installing some dependencies but i have already installed so you using this command to install dependencies we're using tensorflow stable subline 3 gym box 2d or swig stable bliss line 3 is a set of improved algorithm of enforcement learning algorithm based on open ai baseline jim is a project from open ai company to develop and comparing an algorithm swig is a simple wrapper and interface generator a box 2d is a free open source 2d physics simulator now let's start with a dependencies so let's import dependencies music it takes some time now it's imported now make an environment so we make an environment luna lander v2 which is a gym environment now look at this we're using this to train our model its landing is always at coordinate zero comma zero so we train our model to land zero comma zero coordinate so now test our environment oh sorry we do not run this now test it now test our environment that's the test we make a test environment we make a variable episode and it's 10 so we use 10 episode and iterate it we printed a score which lunar lender gave and each episode we have some now run this cell let's see so you can see the purpose the pop-up is here and it's landing in properly it's giving a score each episode it's done and now we build and train the model we make an environment jim make environment name which is describe apple and we use a dummy,358,0,0,LPaFq2KWzLM
1,vector environment which is imported above now we make a model using a pp algorithm so let's run this and see it's printed using cpu device i'm using cpu device you also use gpu it's different now we train our model so now we train our model to show you how it's trained now execute this it's take time so i pause the video and wait when it's done we move further now you can see the cell is executed so let's save our model and evaluate it only for 10 episodes i evaluate our model for 10 episodes let's see what our model run you see it's landed properly music so you can see it's landed properly sometimes it learns from our mistakes what mistake it's done from it so if it's not land properly you run above self again and see what's happening also close the environment using this command enb dot close and save your model so i save our model from here let's see my model is here ppomodo.zip i save my model using model dot save now test your model you,241,0,0,LPaFq2KWzLM
0,"in this course, you will learn the basics of reinforcement learning and how to implement it using gymnasium. gymnasium is a software library by open ai that provides a collection of pre built environments for reinforcement learning agents. in this course, mustafa will show you how to use gymnasium to test and improve your reinforcement learning algorithms. so what will be the objectives of the course. so what we'll be doing is, we'll be learning the basics of reinforcement learning and opening a gymnasium, you will learn how to use the gymnasium interface to interact with different environments. we'll explore various gymnastic environments, so environments could be like a taxi environment, it could be like a gadget environment, you can have different atari games as well. and we'll also understand their characteristics will build and evaluate reinforcement learning agents using given as prerequisites knowledge of python programming language, so you need to be familiar with basic loops. what is a class we'll have different methods. so i'll explain it to you while as we go along basic understanding of reinforcement learning concepts. so whatever the topics are that are required, i'll be explaining it to you. while we are while we start the course. introduction to reinforcement learning, what is reinforcement learning, we will learn the rl technologies. and also we will understand the reinforcement learning workflow. my next section is introduction to opening a gymnasium. so i'll introduce you to open a gymnasium and also we will set up the gymnasium environment. and i'll also i'll explain you the basic concepts that are required for using gymnasium. so it could be the environment agent observation, action and the word.",364,0,16,vufTSJbzKGU
1,"so it could be the environment agent observation, action and the word. the next section is gymnasium environments. so we'll understand the types of gymnasium environments. we will understand the difference between continuous and discrete actions and observations understanding the properties of different gymnasium environments and also the games classic control atari back to the robotics we got different types of the environments available and there are many more available so like jack we have taxi environment we have frozen like environment. so now let the next question is the gymnasium interface will be interacting with environments using the gym interface will understand the gymnasium api. and we'll create some custom environments. now the next section is building reinforcement learning agents. so we'll be implementing rl agents using gymnasium will understand the algorithm q learning will be training and evaluating rl agents using gymnasium. and we'll be improving rl agents with policy search and other techniques. basically, we'll be solving blackjack using q learning this will be the next section. so let me just write it down that will be the tutorial and these are the advanced topics. so after you get a basic understanding of these of their of reinforcement learning gymnasium, and solve some server problem, then it can go and explore the other advanced topics. so the these are the using a porn ai baselines for benchmarking rl algorithms, creating and using rappers to modify environment dynamics, combining multiple environments to create more complex scenarios. so we can have two taxes coming at each other. or you can have multiple players in a poker environment. so basically, you can simulate those environments.",357,16,31,vufTSJbzKGU
2,"so basically, you can simulate those environments. also, we'll be integrating with other tools such as tensorflow and pytorch. okay, so basically, this is the course. now let's look at the conclusion. so by the end of this course, you will have a very strong understanding of reinforcement learning and open air gymnasium, you will be able to use the museum to build and evaluate rl agents. and we'll have a very good understanding of the different gymnasium environments available. this code is an excellent starting point for anyone interested in developing or working with reinforcement learning system. let's start with the basics of reinforcement learning. it is a type of machine learning in which an agent learns to make decisions by interacting within the environment. so basically, as you can see, i have an agent and i have an environment. so basically, let's take an example of a robot to what will happen is that the robot when it takes an action suppose it takes a step forward. so, we will reward either we will reward the robot for that action or either we will penalize it for the action. so how will you decide that? so what my basic policy would be that suppose if the robot takes a step and it banged into wall. so, we will have to penalize it and we will deduct some point for that. or if the robot takes a step, and it doesn't bang into anything, that will be a good action, so we will have to give it some points. this is basically that's what will happen. so, what is an agent?",353,31,48,vufTSJbzKGU
3,"so, what is an agent? an agent is an entity that interacts with an environment in order to learn how to make decisions that will maximize a specific goal or objectives. so, basically, in our case, when we have a robot need it to take something from one place to another place to the city, it has to reach the particular endpoint. so, we will have to train the robot to reach that goal. so, it will take the decision autonomously as it receives input from the environment, perform the actions and receive rewards or penalties based on these actions. the environment on the other hand is the external system or context in which the agent operates. so basically, in our case, everything that is nearby to the robot is the environment. so basically, it could be a warehouse to the full warehouse will be the environment and it will be interacting with the robot will be interacting with the environment. so it can be a simulation of physical system or any other system that the agent interacts with. the environment provides feedback to the agent in the form of rewards or punishments. when the agent uses to learn how to make better decisions. the agent receives feedback in the form of rewards or penalties. so as we discussed earlier in the robot kills, when he takes a step forward, and it doesn't matter to wall, we will give it a reward. and our goal is to maximize the total reward. so if it takes a step forward, and doesn't bind it to anything, it will eventually reach the goal.",351,48,62,vufTSJbzKGU
4,"so if it takes a step forward, and doesn't bind it to anything, it will eventually reach the goal. so it is a trial and error process, observing the consequences and adjusting its behavior based on the rewards it receives. over time, the agent learns to identify the actions that lead to the highest rewards and avoid those that lead to penalties. so as we keep training the robot, it will eventually learn what decisions to take autonomously. so it adapts to new situations and environments. and it will adjust its behavior based on the feedback it receives and improve the performance. and it will become efficient decision maker, which leads to better performance as well. so let's also look at some real world use cases of reinforcement learning. so basically, the example that we covered is robotics. we could perform tasks such as grasping objects navigating and interacting with humans. for example, rl has been used to train robots to play tennis, table tennis, where the robot learns to adjust its movement based on the position of the ball. so let's look at another example of which is the game playing. so like the most famous example is of atari games. so like deepmind launched atari game module. reinforcement learning has been applied to game learning, particularly in the development of artificial objects. or we can also talk about blackjack, we can talk about chess, so a lot of games, atari games alphago, a computer program developed by google deepmind, which is reinforcement learning to learn to play the game of go at a world class level. also, we can use reinforcement learning for autonomous driving.",360,62,78,vufTSJbzKGU
5,"also, we can use reinforcement learning for autonomous driving. reinforcement learning can be used to train autonomous vehicles to make decisions in complex and dynamic environments. for example, reinforcement learning can be used to train a self driving car to navigate to the traffic, world obstacles and make safe and efficient driving decisions. also, another very good example is of personalized recommendations, which is basically netflix or google recommendations. reinforcement learning can be used to provide personalized recommendations to user based on their preferences and behavior. for example, we can use rl to optimize the recommendations of a video streaming service, learning what content to recommend to the users and to maximize user engagement and satisfaction. so now we have learned about the agent and the environment. so now let's move on to gymnasium, which is a toolkit that will help us simulate these environments for our agent. so basically, it is a set of a collection of environments, or tasks that can be used to test and develop reinforcement learning algorithms. these environments are typically game like with well defined rules and reward structure, making them useful for evaluating and comparing different reinforcement learning algorithms. so what i'll do is i head over to the documentation page and show you the different types of environments that are available. so this is the introduction page. so basically, it is just two lines of code to set up an environment and you can interact with the environment as well. so i encourage you all to go and check out the documentation and also explore the different types of environments that are available. so we have a cardpool environment we have a mountain car.",366,78,92,vufTSJbzKGU
6,"so we have a cardpool environment we have a mountain car. also interesting environments. we have a car racing environment as well. basically, for gymnasium, there is only one single agent. but if you want multi agents, then we have another library called petting zoo. we have a bipedal walker here. and another interesting environment is obliging environment. so in this tutorial, i'll be covering blackjack environment. and another environment is a taxi environment. so basically, this is like our cell running example. also, we have a frozen lake environment, forking, these are all the very interesting environments. so it could range from games like pong, or record to more complex simulations like robotics or autonomous driving. the nasm environments are designed to be easy to use, and come with a standard interface for interacting with the environment. now, basically, what all steps do, we need to start interacting with gymnasium. so let's check it out. so we need to define the environment that we want to work in p to create an instance of the environment. so that's what we did here. so basically, we use them that make and use the environment that we want. we will define the agents policy, how it decides which action to take, interact with the environment, taking actions and receiving rewards, update the agents policy based on the reward it receives. so just to go back, we have an example of a robot. when we take that example and apply these rules, we'll be able to interact very easily.",334,92,112,vufTSJbzKGU
7,"when we take that example and apply these rules, we'll be able to interact very easily. so let me just to show you how we can do that to an environment will be the warehouse environment, an instance of the environment we'll create first, we'll define the policy. so in our case, the policy was that once the robot takes a step forward, and if it doesn't crash into a wall, or even cat class into another word, that is a successful step, and then we reward it, otherwise, we'll have to penalize it. so basically, this is the fourth step, we have to update the agents policy. so what we have to do is we need to update it that once you take a successful tip, you need to keep taking successful steps and you will in the end, and we'll keep repeating it until our performance is satisfactory. some of the main concepts we will need in open ai gymnasium. so the first we have is observation and action spaces. and our observation space is the set of possible states that an agent can observe in the environment. and action space is the set of possible actions that an agent can take in an environment. so let's take the example of our robot. so what is the thing is that i have my robot right here. and the possible actions that it can take is either it can travel in the front, or it can double to the right or to the left or take a step back.",338,112,123,vufTSJbzKGU
8,"and the possible actions that it can take is either it can travel in the front, or it can double to the right or to the left or take a step back. so this is the possible action space that can take also, in this case, the observation space will be an ideal place to be 567 action space is different than the observation space. this is the full action space and observation space will be changed. this is action space. this is the observation space. let's look at episode. an episode is a complete run through of an environment starting from the initial state and continuing until a terminal state is reached. so let's look at that in our example as well. so i have my robot right here. and i wanted to reach here. so this would be an episode and i can train the robot to run multiple episodes. so it will start at a particular position and it will reach this terminal position. so that will be one episode it will start there and could go here as well. basically, this could be a terminal location or it could this could be the start location and this could be the terminal location. these are all the different episodes that we have. each episode is composed of sequence of states action and towards rapper rapper is a tool in open engine that allows you to modify and environments behavior without changing its code. so we'll look into that later on.",328,123,139,vufTSJbzKGU
9,"so we'll look into that later on. wrappers can be used to add features such as time limits, reward shaping, and action must be benchmark open and jim provides a set of benchmark environments, which are standardized tests that can be used to evaluate and compare reinforcement learning algorithms. so if we have multiple algorithms for a particular environment or a problem, we can use the benchmark to compare them we are done with the basics of reinforcement learning. and also we have learned the agent environment and everything that is needed to get started. so now let's implement the game of blackjack using gymnasium. first, let's get introduced to the game of blackjack. so what are the basic rules, so the basic rules is that i need i will have two cars and the dealer will have two cars. now in this scenario, both will be ai players. so the thing is that at each turn, i will have to decide that i need a new car or do i want to keep my current set of cards, but the dealer has to keep playing until he until the e reaches over 70 the sum of the card which is over 70 and if i have to when i need to be a bigger number, then my some of the cards have to be greater than the dealer's card. so let's go over all the basic rules of blackjack. this game is played with one or more decks of standard playing cards. each dealer is dealt two cards and the dealer is also day two cards with one card facedown. the value of each card is determined by its rank.",365,139,151,vufTSJbzKGU
10,"the value of each card is determined by its rank. aces can be worth one or 11. face cards kings, queens and jacks are what 10 and all other cards our their face value. players have the option to hit and take additional cards to improve their hand or stand and keep the current trend, the dealer must hit until their hand has a value of 17 or more. if a player's hand goes over 21 the bust and lose the game. if the dealer's hand goes over 21 the player wins the game if neither the player nor the dealer bust the hand with the highest total value that is less than or equal to 21 wins to basically these are the rules now let's head over to the gymnasium documentation and check out how blackjack is implemented. so here we are in the gymnasium documentation now let's check out the action space and observation space here. so basically we need to import blackjack we want to create the blackjack environment this is the rules that we discussed no action space the action space is one comma in the range of zero comma one indicating whether to stick or hit. so it's a player's decision with a dealer has to be played the observation consists of a three tuple containing the players current sum the value of the dealers one card one to 10 where one is is and whether the player holds a usable is one or zero.",325,151,159,vufTSJbzKGU
11,"so it's a player's decision with a dealer has to be played the observation consists of a three tuple containing the players current sum the value of the dealers one card one to 10 where one is is and whether the player holds a usable is one or zero. basically this will be the observation space our starting space will be four between four and 11 dealer car has to 11 and other usable is if i ever usable based on what will be the rewards in this particular reinforcement learning problem. so we'll have win game as plus one lose game as minus one draw game is zero and win game with natural blackjack plus 1.5 if natural is true, and this one if natural is false. so basically this is another type of set that is the natural environment. so basically we can understand that with natural is true whether to give an additional reward for starting with a natural backdrop that is starting with an ace and 10. so we can specify that when we set up the environment. now let's see how we can implement it in google collab. now we've covered the basics of blackjack. now let's understand how we can solve this game of blackjack. so let's first discuss a book that is very popular regarding to blank that it's called reinforcement learning and introduction by richard sutton and andrew bartow. it covers the basic applications of reinforcement learning. and also there is a chapter on blackjack, and it also covers other games. and they have modelled it a markov decision process, and how every action influences the outcome of the game.",361,159,171,vufTSJbzKGU
12,"and they have modelled it a markov decision process, and how every action influences the outcome of the game. now also they explain how reinforcement learning algorithms can be used to learn optimal policies for playing blackjack based on maximizing the expected return over the long term. so now this seems really complicated, but this is the beginner course you don't need to learn this. once this course is over and you still have a curiosity you can go check out this book. and because why should you check out this book because it presents several approaches to solving the blackjack problem including monte carlo methods and temporary difference learning. it also covers value functions and policy improvement, which are a very advanced topics. so maybe we can create a course on that but as this is a beginner course, you do not need to know all these. so let's start with the basic implementation. what i will be doing is i will be importing the libraries that we'll be needing for us to install i'll be copying this multiple times and i plot lib, then let's do numpy. see one. so basically numpy is used for data manipulation. as we import i just explained to you when we bought what each library is used for, take udm we are in synchronism. 0.2 7.0. so basically, this is the lt s support long term support version. and let's do matplotlib in line. so, as you all know, when accurate lib is used, it opens a new window. but in collab, google collab, there is no window. so basically, the plots should be in line.",356,171,188,vufTSJbzKGU
13,"so basically, the plots should be in line. so basically we'll do this percentage matplotlib in line 30. so all the plots below the cell. so yeah, it's done, let's do pip install. so i already have it installed, but in my machine, it'll take some time to install by till it get installed. let's also import all the necessary materials that we'll need. so from collections, i will import defaults. so why i am using a default is because it allows us to access keys without checking that if the value is accessed, its value exists from input map plot lib.pi plot as plt. so this is just also write down why we'll be using access to you find the keys keys that do not exist. check for us, instead of us having to always check if that keys exist, it will just take it for us. we'll be using this for going clots from mac lot plot lib dot patches. so basically this is for creating shapes or shapes. let's import numpy. so numpy is very popular. michigan numpy is for array manipulation and image manipulation. manipulation there's two input c one c one is a very popular data visualization library as soon as and from tq dm so basically this is for a loader. so whatever training that happens, it will show it as a loader. so to be very easy to understand cleaning covers. so i think we have completed the installation of the basic libraries. now let's so let's so what i'll be doing is i'll be using the jim that make command to create a environment so it will be env is equal to jim.",365,188,207,vufTSJbzKGU
14,"now let's so let's so what i'll be doing is i'll be using the jim that make command to create a environment so it will be env is equal to jim. so i've already imported the gymnasium. okay, i forgot to input let me import it as well. so from import gymnasium. now let's do gym dot make. so our environment name is blackjack, we won. check we want. and what i'll be doing is i'll be setting the sap parameter to true what is sap parameter it is the natural environment, basically the default state of certain and the book that we discussed. so what i'll be doing is and also i'll be setting the render mode is equal to rgb grid because i'll be showing you the training states so exactly what is the position of the card at that particular moment in training? that will be interesting to watch. that is why i will set this render mode to rgb array. now let's go and see how we can set up the environment. so i will be observing the environment what i'll be doing is i'll be setting this as let's observe the environment so we'll be using the nv dot start method to observe an episode so basically we discussed what is an episode in is basically all the steps that it takes for me to reach from the starting point to the endpoint once. basically, that is an episode. and for sitting in episode, we do it as observation, comma info is equal to en v dot reset. so basically, we use the reset method to reset the full board, or you said the particular environment.",365,207,222,vufTSJbzKGU
15,"so basically, we use the reset method to reset the full board, or you said the particular environment. and we'll be keeping done as far as because we have just started the training, let's do let's have some comments. they said no environment first observation so, and what will be the output of this, so, observation is equal to it is a tuple. and as we have discussed before, it will be 16 nine and false. so what is this tuple. so basically, this is my hand, what i have been dealt, this is the dealer's hand. and this is the number of if, do i have an ace, so, i don't have an ace at the start, that is why i caught the output as false. so, it is a boolean and it is like the this value deposits that if i have a as usable without busting so yeah, that is the thing, if i can have an ace, but i can also bust as well. so, this is a an ace which is usable, and without busting. so let's write that down as well. so, what this output let's consistently when is the first value in the current some, the second will be value of dealers sub called the third will be boolean whether the player holds, holds usable is and what is a usable is where it is usable. yes, it comes without busted. basically that is let's see how we can execute that.",322,222,234,vufTSJbzKGU
16,"basically that is let's see how we can execute that. so, for executing an action, first we'll receive our first object observation should be basically that will be the starting space, we'll be going to then after that we'll be using the end third step. and we'll be giving the action to interact with the environment. so in whatever example of the blackjack, it will be that if i want to take a new cart, or do i want to keep my current set of cards that will be passed in the action. this function takes an action as an input and execute it in the environment. because that action changes the state of the environment, it returns four useful variables to us these are next state. this is the observation that the agent will receive after taking the action. it will also give us the reward the agent will receive after taking the action. either this environment has terminated or it will give us truncated that if the episode ended early, or the time limit is reached. this is a dictionary that contains additional information about the environment. the next reward terminated and truncated variables are self explanatory, but the info variable requires some additional explanation. this variable contains a dictionary that might have some extra information about the environment. but in blackjack v one environment you can ignore it. for example, in atari environments, the info dictionary has a fairly short lives key that tells us how many lives the agent has left. if the agent has utilized then the episode is over. so basically, these are the four values that are important.",357,234,249,vufTSJbzKGU
17,"so basically, these are the four values that are important. after we take a step two let's see how we can take a step now. so let's take a new cell and let's take the action. so action equal to nv dot action underscore space dot sample now what is this? this is a sample action that will be taking sample a random action basically, this is the training loop. so that's why we are taking a sample exam. all valid actions. now we have hash action is equal to one. and what we'll do is we'll execute, execute the action, we see how we will be executing it. so we'll be taking observation. so we discussed all the parameters here, right, so we'll be taking all the four parameters, we'll be having reward, we'll be having terminated, truncated, and so forth. so we can ignore info but i be taking input, just in case an action so action is the sample exit from here. execute the action. so we'll be receiving input and receive info after taking it but taking the step. so what will be the output if we run this so the sample output will be the division is equal to so basically, we have learned what this tuple means. this is a 24 is my current sum to basically i've gone bust. now, also, first, i do not have any usable. basically, this is just a sample start. so i have some values here. so i'll explain to you all these values. so and reward is equal to minus 1.0. so basically, i've lost, so terminated will be true. we have truncated and hash info, we call to length.",367,249,271,vufTSJbzKGU
18,"we have truncated and hash info, we call to length. so yeah, this, this is how it's going to be for this particular one simple step. so once we get the word terminated is equal to true, what do we do then the current episode, we have to reset. so if terminated, i had to take a text. needed is equal to true or we should stop the current episodes and begin a new one using en with a reset. so basically, i've talked about this method before as well. and this is what is a single action that we take in this particular environment. now let's build our agent. so now we have committed the basic setup of the environment and our agent. now let's understand the approach that we'll be using to solve like that. so let's look at the epsilon greedy strategy. this is a very optimum strategy to solve that. so let's understand it. in this strategy, the agent takes an action that is either the best action based on the current policy with a probability of one minus epsilon or a random action with a probability of epsilon. this approach balances the exploitation of the current best policy with exploration of new policies, which can lead to better rewards in the long term. so basically, it is a very strict policy. either i take the current best policy or current best approach, or action, or either take a random action. so how you set the policy let's let's look at that later. but let's understand the epsilon greedy strategy.",343,271,289,vufTSJbzKGU
19,"but let's understand the epsilon greedy strategy. in the context of blackjack, the epsilon greedy strategy can be applied to determine whether the player should hit or stand at each step of the game, the agent, that is the player can choose to take the action that is either recommended by the current policy, or it is a random action. the policy is learned over time by updating the action value estimates of each state action pair based on the rewards received during the game. as the game is played repeatedly, the agent learns the optimal policy that maximizes the expected rewards. basically, we as we keep on running the iterations and the agent keeps learning, it will understand the optimum policy that will maximize our expected reward. initially, the agent may explore by taking random actions to discover new strategies. however, as the game progresses, the agent will start to exploit the best known policy, which should maximize the expected reward over time. so this is the approach that we'll be using. now, let's build our agent using this approach. the epsilon greedy strategy okay, so let's take a code cell and create our class. so basically, we'll create a blackjack agent class. and let's do def, we define the initial constructor that is a default constructor. and what all parameters do we need for this, so we'll take the self betterment of the learning rate. basically, this will be passed as epsilon, this will be a float, stick initial epsilon be a float as well. let's take epsilon decay is a way of float as well. let's take final epsilon should be a float as well.",361,289,304,vufTSJbzKGU
20,"let's take final epsilon should be a float as well. and let's take the discount factor and this will be 0.95 in this case, so we have completed the initial garaventa. let's move ahead and create our initial complete the initial constructor. q underscore value is equal to default ic so we'll be using default because we do not want to always check if that key exists to be a lamda method, lambda function and p dot zero yeah, so we can just initialize it as np dot zeros. let's set it at e and v dot action space dot n. now let's read some comments as well just for explanation. so what are we going to do we are going to initialize or also whenever i say rl it is it is a reinforcement learning agent with empty dictionary of state action values. this is the key values learning rate and epsilon. arguments let's understand all the arguments as well. so we'll be calculating the learning rate. so, i think these are pretty self explanatory, let me just explain to you the discount factor. so, the learning rate is the learning rate itself, the initial epsilon value, epsilon decay, the decay for epsilon and this will be the final epsilon value and the discount factor is something that needs to be understood. let's understand the discount factor discount factor for computing the q value so i'll explain to you what the q value is just first complete the basic class of our agent. this is the approach that we are going to be using using q learning to solving blackjack using q learning.",356,304,317,vufTSJbzKGU
21,"this is the approach that we are going to be using using q learning to solving blackjack using q learning. so, now let's complete our consulter basically, this is the learning rate. discount factor is equal to discount factor epsilon be able to initial better know also good epsilon is equal to initial epsilon loop self dot epsilon dk single equals final epsilon going to final underscore epsilon cuts to the training error says training error is equal to blank correct. now let's create another method that is the get action method basically, i need to complete the integration. let's do that. it may complete the end condition yeah i'm just looking at this is a tuple that we discussed observation tuple that we'll be getting, which is intelligible and the output will be an integer. so this will be the get action method where we'll refine the policy. so if np dot random dot random maybe doing random is less than self dot random dot epsilon so i think i could do return in v dot action underscore sample. so we do taking the sample action instead of taking the correct action. yeah, else if the probability is, is good enough. let me take the return of n p dot arg max dot q. so basically, this is the key values that we'll be using your so i'll explain to you in just some seconds. me complete all the methods. so let's let's give it some parameters here. basically, this is this will return return the best action be at one minus epsilon otherwise random action with probability explore for exploration. so yeah, this is the basically the approach that we understood before.",369,317,332,vufTSJbzKGU
22,"so yeah, this is the basically the approach that we understood before. and short exploration. let's create another method first printing of what are the q value is, so basically, let me just write it down here. so the q value function is used to estimate the optimal action to take in each state, the optimal action in a state is the one it is the one that maximizes expected long term rewards. so basically, we've understood the epsilon approach to basically this is how we'll be maximizing it. now, this is a bit complicated the equation there is an arg max, which will return the maximizing value. so i encourage you guys to go and just understand the logic behind it. it's pretty complicated, but it is this is basically the optimal action that we have to take in each step. basically, this will be calculated for us in our in our game, pls. let's move on now we need to complete the update method for it's complete that you have our def update and it will have self observation. you see this is the operation tuple. so like you might be having some errors. so when we compile it, we'll get to know all the actions that do reward the workload to terminated boolean and do next observation. again, and to kill it. so we've understood what the cue function is basically we'll be updating the cue function so let's understand it. future. value is equal to not terminated into np dot max self dot q values of next observation, we will be taking the q value of the next observation and multiplying it with the terminating method.",364,332,348,vufTSJbzKGU
23,"value is equal to not terminated into np dot max self dot q values of next observation, we will be taking the q value of the next observation and multiplying it with the terminating method. so, if it is terminated, then we have to start again, if it is not, then we know, we know what need to calculate this equal to. so, he has the reward. basically, this is the method that is a function. so, explaining to you how we calculate it discount factor into future q value minus admittedly, this is the literal formula that we use to calculate it will explain to you, but let's go ahead and complete this method. observation of action. three, and this is done the queue method, key values done. let's do the training. but now we need to update the q value. now, we have completed the difference now let's do the self dot q values of observations and of the action 2d array is equal to self dot values. so, basically this is what we have to do again, you score values i'll be copying this plus self dot x revisit the learning rate into the temporal difference and this will be self dot training dot append temporal difference now, let's compare the last method that dt d k epsilon. lot salon minus salon decay was ugly we have completed our class like the agent. so, now we've completed the class of our agent three and we've defined all the actions that the agent can perform. now let's see the training of the agent. so let's start the training. we'll be taking it one episode at a time.",362,348,363,vufTSJbzKGU
24,"we'll be taking it one episode at a time. so let's do the parameters for servers we'll keep the learning rate 0.01 visit the learning rate we have the episodes equal to 100,000 we can change it like just keep it 100,000 because that's will give us a very good trained agent, but i'll just reduce it for the tutorial. this is the standard that you should keep if you want to get a real world agent to start shorts by two so we reduce the exploration what time we wanted to take the appropriate path without having to explore too much to this will basically reduce it. then we have the final epsilon which really this is where we need to get by the end of the training. let's create our agent this is this these are all the parameters the hyper parameters doors, we have agent is equal to have a black agent learning rate. learning rate we have the initial which means you can just pop up on intubated is equal to start. start if you don't have one, and basically we need to get to 0.1 from one and we have epsilon decay is equal to epsilon decay. it's past that. now also let's do it the final epsilon. so this is the agent that we need hello. so now, let's turn the training let's turn to training now. so basically we have completed the agent setup now. now let's start the training. so what i'll do is i'll just set up the environment in v is equal to gym dot rappers, not record.",348,363,376,vufTSJbzKGU
25,"so what i'll do is i'll just set up the environment in v is equal to gym dot rappers, not record. so we will be recording all the statistics and i will just show you after the training, how the agent improves over time. so will you plotting some charts as well. so let me complete the training part first. and then we'll talk about what the training has, how the training has improved over time, and what kind of all the statistics would mean. we'll be putting charts and also i will give you the statistics we have episodes and episodes okay, it will be recording the statistics for episode now let's do that. so we have under 1000 episodes, let's make it 10 for this tutorial, just to that it will run pass. so i will also output the environment at particular point so i will so every time that a card is dealt or anything that is decision is made by the agent will be plotting that particular environment and see the what is happening in real time. so i'll be doing that as well. so it's like watching live play live poker in real time. live blackjack and play time. so let's do that. let's let's do that. now. we do end with a reset. now why do we do in winter reset because as we discussed, after every episode, we need to reset the environment. because i have traveled from one point to another point i need to reset the environment.",332,376,392,vufTSJbzKGU
26,"because i have traveled from one point to another point i need to reset the environment. okay, so done is equal to false because we need to complete the challenge need to get to the endpoint need to clear up now i need to import. so what i'll be doing is for every episode, i will be carrying the output because i'm only interested in the current episode and what is happening right now in real time. so i'll import it ipython display input clear output. basically, this is what i need to import. so basically for every episode, this will clear the output. now let's complete third training for one episode. result we play one episode as do while not done, so is also while we are not done, let's run this loop. let's do action equal to agent dot get action of observation. let's do next underscore obs comma toward comma terminated whistle all the information that we'll get after making a particular step should be truncated the info equal to e nv dot step. let's update now let's do agent dot update. we'll be passing all these parameters, we have to pass observation action reward terminated and the next position. next observation frame. so basically i will be rendering the frame so at particular instance what is happening we can just output that as well. so i'll be doing that we will able to end the render loop plt dot i am show frame from python, i'll be showing you the exact location of the cards and exact position of the gods exactly what is happening in real time.",353,392,406,vufTSJbzKGU
27,"so i'll be doing that we will able to end the render loop plt dot i am show frame from python, i'll be showing you the exact location of the cards and exact position of the gods exactly what is happening in real time. now, what we have to do is we have to check if that episode is terminated. so how will we know that we have a parameter called terminated or we have truncated so truncated is that we have a bust or terminated is that the time limit is oh observation is equal to next observation. and we have to do agent. so i am excited to follow me to agent dot d k, epsilon. so over time, it will get smarter. so this is it. so i'll be using the package called annotations because i'll be using subscripts in my methods. so i got an error here. now let's run it and check again. if you're also getting an error, make sure to import the package annotations. now everything's done fine. let's do it. yeah, basically. basically this is like running an ad is not defined. so it is not defined.",253,406,421,vufTSJbzKGU
28,"so it is not defined. so i'd say okay, it should be brutal basically these things happen yeah this sounds fine now that's tribes but agreement equal to equal to where is that here let's see what i can do record episode so it is en mi comma non thought basically these things happen so so there is nothing in the gymnasium that happens so it is rappers no rapper also has no attribute record episode statistics so it is np p saudia obviously, this is episode statistics now in opie is not defined for is not ob, this is our obs, this is observation space. now, we're gonna continue to add episode when they already exist. to see why that happens. everything is done, let's run everything through run all. now if you get some errors, just make sure to check out the integrations or i just link my google colab notebook in the description, you guys can go and check it out. because there were a lot of integration issues before running it. now that's it. so as you can see, for every episode, we are able to visualize it. so divisibility when you 121, then it's a drop in both are 21. let's see it is 12 it's pretty fun to watch. so now currently, i've said the episodes over 100,000, i can change it and keep it as 10. but i'm just gonna let it train. so once that is done, i just come back and let's just visualize the training part of them. because we need to understand the error losses and we need to understand the policy that we have used.",360,421,435,vufTSJbzKGU
29,"because we need to understand the error losses and we need to understand the policy that we have used. this is running, let's just complete the visualization code. so what i'll be doing is rolling land. so we will be understanding all the parameters and all the data relating to our policy and understand what is the appropriate amount of episodes that we couldn't use, and also the loss function. we'll plot it and we'll understand it that way as well. so let's do that. so what i've done is i've set the rolling length is 500 and i've done the finger sizer so i let me just complete the code and i'll explain to you everything what we've done. yeah, just give us five subplots, not subplots. just to access of you know that set title, so we'll just give it a title as episode rewards. so what how does the reward change in each episode we have plotted let's plot that episode rewards. now let's do reward the score moving score average equal to np dot convolve. we will plot a colored graph in lieu np dot array e and v dot return q dot satin so we'll flatten it to dot latin comma np dot once and let's do more is equal to valid and let's do rolling length divided by rolling this will be for every 500 iterations we're doing this now this is completed let's do x of zero dot dot array range then the word moving average the moving average of the reward now what i will be doing is i will be putting it against the reward moving average to that value.",362,435,446,vufTSJbzKGU
30,"we will plot a colored graph in lieu np dot array e and v dot return q dot satin so we'll flatten it to dot latin comma np dot once and let's do more is equal to valid and let's do rolling length divided by rolling this will be for every 500 iterations we're doing this now this is completed let's do x of zero dot dot array range then the word moving average the moving average of the reward now what i will be doing is i will be putting it against the reward moving average to that value. the value here so we are going to plot it now let's do excess of one two is the y axis got set title. so lengths this is done let's do length underscore moving underscore will be our variable that can be the same way to a carnival within do np dot cornwalls. so i'll explain to you why we use the commonwealth method as well we need to do a blanket dot plugin, then we need to do np dot once. rolling and again and mod is equal to same so instead of valid we'll do same now it's to the window same close the brackets, divide this by rolling in not clot length training, moving average three out only ever talked about training error. score moving average comma, training underscore underscore knowing no score. loop plt dot tight layout basically, i've completed the training or visualizing of the training parameters and how the graph looks like. so what i'll do is i'll just wait for the training to complete and then i'll explain to you the graph.",362,446,453,vufTSJbzKGU
31,"so what i'll do is i'll just wait for the training to complete and then i'll explain to you the graph. so the training is complete. now let's look at the different parameters that we have plotted. so they have three different types of charts. let's understand each one of them. so first one we have is episode rewards. so the episode rewards in our case of solving blackjack, and episode is a can be defined as a game of blackjack played from start to finish. and the episode rewards is a total sum of the rewards obtained during that game, it could be the sum of the numbers of wins and losses. and we can add any bonuses or penalties for certain actions. so as you can see, for each episode, there is a very drastic change in the rewards. so basically, this is very interesting to watch that it is not a flatline, basically, we do not have any rangebound moment between it just just too volatile, just keep changing it this is very interesting to watch. so now let's look at the episode lens combat which is plotted against each episode. so basically, this is one complete sequence of playing a hand taking a hit or a stand and receiving the result. now the length of an episode would depend on the rules of the game, the number of players and other factors. so in our game, it could be just two player, but you can have more number of players as well.",332,453,467,vufTSJbzKGU
32,"so in our game, it could be just two player, but you can have more number of players as well. and in a game of blackjack you could consist of it would consist of a single hand or multiple hands with different lengths of each episode. so basically, you can see, in our case, it would be more than one only. but it doesn't go over two. so basically everybody will find in between 1.4 1.3 basically 1.3 and 1.4. so each episode is just between 1.3 and 1.4. that is also very interesting to watch. let's look at the training error. the training error is a measure of how well our model is performing on the training data. now in our context of solving blackjack using kulani. the training error would be difference between the expected value of the q function and the actual q values obtained during the training. the goal of q learning is to minimize this error and converge to optimal q values that maximize the expected reward. a high training error indicates that the model is not learning effectively. and adjustments may be needed to divert the motoring data. so as you can see at around 3000 around 3000 this is pretty good. our training error is pretty low. now after 3000 goes pretty high. so that's it now we are done with our training of the blackjack agent. and we have successfully solved the blackjack game using the q learning algorithm. now let's quickly revise all the things that we have learned while we solve when we started solving that chapter. so we started with the reinforcement learning basics.",357,467,487,vufTSJbzKGU
33,"so we started with the reinforcement learning basics. then i also explained to you the role of an agent, what exactly constitutes an environment, what each action is? and how do we keep track of each each state that the agent is currently in. then we also learned about the observation aspect, and also how we use rewards to keep track of a successful step and also give penalty to the agent when it makes a wrong step. so basically, we have covered that, then i'll also explain to you the real world use cases of reinforcement learning. we saw basically there is robotics game playing autonomous driving personalized recommendations. then we started by with the gymnasium libraries basically, it gives us a collection of environments that can be used to test and develop reinforcement learning algorithms. so here we have made use of the blackjack environment. then we started with the setting, then we started the setup for the gymnasium environment. also, i've walked you through all the processes that we need to successfully solve any problems. this is true for any reinforcement learning problem. then i also explained to you the main concepts of open air gymnasium. to be started with observation and action spaces episode, what is the wrapper benchmark. now we start with the introduction to the blackjack game, basically are the basic rules. the game is paired with one or more decks of tender playing cards. each player is dealt two cards and the data is also dealt two cards with one card facedown. the value of each card is determined by its rank.",349,487,503,vufTSJbzKGU
34,"the value of each card is determined by its rank. it says can we work 111 face cards, that is kings, queens and jacks, what 10 and all other face cards are worth at face value. and basically, we have the option to hit and take additional cards to improve our hand or stand and give the current time the dealer must hit until there were and as a value of 17 or more. if a player's hand goes over 21 they bust and they lose the game. if the dealer's hand goes over 21 the player wins the game. if neither the player nor the dealer busts the hand with the highest total value that is less than or equal to 21 wins the game. so, this is an example road. now we have learned about the action space, the observation space, the starting space, what the rewards and what constitutes an episode and how does an episode end. so, we have the termination conditions here. now, we have started with the solving of blackjack, i also introduced you to a very good book that is reinforcement learning and introduction by richard sutton and andrew bartow. basically, this is the the summary of the book. now, we started with the installing of the so, basically we have started with the installing of all of the all the libraries that we need for the tutorial. so there is matplotlib, numpy, seabourn then we started with our import statements. so, we have imported all the libraries that we need.",335,503,516,vufTSJbzKGU
35,"so, we have imported all the libraries that we need. now here is where we have created our logic we want environment then we have set the environment and started with the first observation then we have observation tuple basically this is the tuple. now then, we start executing the actions, we have the next step reward terminated state truncated state and the info. basically, this is the step. now, we have used the epsilon greedy strategy to solve blackjack. so, what is the epsilon greedy strategy? in this strategy, the agent takes an action that is either the best action based on the current policy with a probability of one minus epsilon or a random action with a probability of epsilon. this approach balances the exploitation of the current base policy with exploration of new policies, which can lead to better rewards in the long run. basically, this was the epsilon greedy strategy that we use. so i explained to you the what is the q value. so, this is the class for our black jacket, we have all the methods we have to get actually meant that date method. now we started with the training. so we have set up some hyper parameters here, i also explained in detail what each parameter does, so you can just play around with these. now we start with the training loop, we have the episode, we have created a number of episodes. now we what we do is while not done, we play the episode. and basically this is an area where i got to we started with the rolling length. so this is the plotting of the graphs that we did here, too.",367,516,532,vufTSJbzKGU
36,"so this is the plotting of the graphs that we did here, too. this was the code for that. and also i explained to you the url, what are all the graphs that we have plotted here? so now we've solved logic using key learning. also. now let's look at some other methods that we can solve that check. so the first one is monte carlo method. monte carlo is a more model free method that learns from experience. in the context of blackjack monte carlo methods involves playing the game several times, and in keeping track of the rewards obtained for each action, the agent then updates its value function based on the average of the rewards obtained for each state action pair. monte carlo methods are suitable for problem like episode tasks like blackjack, so you can use him nasm and try this approach as well. now let's look at the other approach. temporal difference method. it is another model free method that learns from experience. in the td learning the agent updates its value function based on the difference between the predicted and the actual reward. td methods are suitable for problems with continuous tasks like blackjack. q learning is something that we've already solved blackjack. but let's just quickly revise what q learning you're learning is a model free reinforcement learning algorithm that learns to optimal policy. by updating its q values for each state pair. the agents selects the action with the highest q value for a given state. q learning is suitable for problems with finite states and actions like blackjack. the fourth method that we have is deep q networks.",360,532,552,vufTSJbzKGU
37,"the fourth method that we have is deep q networks. so basically we will be solving so in the next example, we will be solving card poll problem that is like an atari game where you have to balance the card poll, and we'll be using deep q networks. so i also explained in detail what are the deep q networks. so let's look at the brief. deep q networks combined reinforcement learning with deep neural networks, dq ns learn the optimal policy by approximating the q values using a deep neural network. dq ns are suitable for problems with high dimensional state spaces like image based games to it like an image from these are suitable for image based games. now, the fifth example that we have is actor critic example. actor critic is a model based reinforcement learning algorithm that uses the two networks an actor and a critic, the actor network selects the actions while the critic network evaluates the actions taken by the actor, actor critic is suitable for problems with continuous action spaces like that gap. so these are all the different approaches that you can use to solve blackjack. and i highly encourage you all to use gymnasium and try these approaches. so you will find you will, you will have to use some math for that. also, you will find that online as well. but these are the different approaches that you can use. so now let's go to another example where we'll be using the environment of the carpool. so now let's understand what what what is the carpool environment, and then we understand about deep q networks.",358,552,566,vufTSJbzKGU
38,"so now let's understand what what what is the carpool environment, and then we understand about deep q networks. so in the carpool environment, we have an agent that can decide to actions moving the cart, right or woman the cart left, right or left, so the poll attached to it stays upright. so as you can see in this clip, we have to balance the poll. and these are can be random actions, so it could just tilt left twice, or it could go to the left five times, and it could go to the right one. so we need to have all the possibilities covered here. you can find more information about the environment at the gymnasium website. so i have also linked the website link here. now let's look in detail. as the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action in this task reward plus one for every incremental time step. and the environment terminates if the pole of the pole falls over too far, or the cart moves over 2.4 units away from the center. so basically, we do not have a lot we cannot allow the pole to fall 2.4 units away from the center. and for every incremental time step, we have pressed one reward, so we have to maximize the number of times you can balance this cardboard. this means meter performing scenarios will run for longer duration, accumulating larger return.",340,566,578,vufTSJbzKGU
39,"this means meter performing scenarios will run for longer duration, accumulating larger return. the cardpool task is designed so that the inputs to the agent are for real values, representing the environmental state position, velocity, etc. we take these four inputs without any scaling and pass them through a small fully connected network with two outputs one for each action. the network is trained to predict the expected value for each action given the input state. the action with the highest expected value is then chosen. so now let's look in detail at how we can implement this network as well. now let's look at all the methods that we'll be using from python. so the first is the neural networks, which is imported using torch.nn. now, in general, this module provides tools for building neural networks. it includes a wide range of layer type, wide range of layer types, such as fully connected layers, convolutional layers, and recurrent layers, as well as activation functions and loss functions. now let's look at in perspective of a dq n, the torch.nn model is used to define our neural network architecture. now let's look at the other method that is the optimization method. now this module provides a range of optimization algorithm for training neural networks. it includes classic optimization algorithms, such as stochastic gradient descent, as well as more advanced algorithms like adam and rms pro. but let's look at in context of the dq n, we'll be using towards the optimization module to optimize our neural network weights. next is torch dot auto grad, this model now, this model in the context of dq n is used to compute gradients during back propagation.",367,578,593,vufTSJbzKGU
40,"next is torch dot auto grad, this model now, this model in the context of dq n is used to compute gradients during back propagation. this module provides automatic differentiation functionality which is essential for training neural networks. why am back propagation, it enables patterns to automatically compute gradients of a loss function with respect to all the parameters of the network allowing optimization algorithms to adjust the parameters in order to minimize the loss. now let's import the modules that we need. so the first is the gymnasium classic control. let's import that. basically that convert that contains a model that we need to cut pole v1 model let's install that pip install nazeem classic underscore control this is the name so now let's also import all the other methods that we'll need so let's import import nasm as gym mat. so basically these are all the inbuilt models method inbuilt is doing random randomization what method is matplotlib this charting library plot lib it's used for making charts there's import map plot dot pie plot as plt i think one of the most common code lines in python code named tuple and that so basically we'll be using these data structures to manage the training maker tools input count meter reading this now let's import all the torch models for torch torch.nn has an opcom opcom import.nn dot functional functional f now let's also create our environment so we need this will be gym dot make card poll click poll new one now also let's set the inline setting for matplotlib.",341,593,600,vufTSJbzKGU
41,"so basically these are all the inbuilt models method inbuilt is doing random randomization what method is matplotlib this charting library plot lib it's used for making charts there's import map plot dot pie plot as plt i think one of the most common code lines in python code named tuple and that so basically we'll be using these data structures to manage the training maker tools input count meter reading this now let's import all the torch models for torch torch.nn has an opcom opcom import.nn dot functional functional f now let's also create our environment so we need this will be gym dot make card poll click poll new one now also let's set the inline setting for matplotlib. in ad lib.net i think i forgot to import this little read that quote because i think we can directly make it in leno's in collab. if we end google collab, we want pyplot.to be plotted as inline. so that will be displayed below the cell if is ipython so basically this is google collab again from ipython. input display because we need the display to display the charts do plt that ion. also, if you do not have a gpu, we can just add a condition as well. and if you do have a gpu, then we can increase the training. so let's set the variable. if we have a gpu, this cuda is the art.cuda.so. basically, this is the method to understand if we do have a gpu available.",327,600,609,vufTSJbzKGU
42,"basically, this is the method to understand if we do have a gpu available. and you can just go to settings in tools, you need to go to settings, and google editor in the run time, i think you need to send it around. so it will change and time time. and you can just add a gpu or so i will do that, i'll do that as well. so it will basically give us a lot more computing power. so now we've completed the basic in code where we have imported all the libraries that we will need. now let's look at another concept that is very important in this in this example, or to solve cardboard. so now let's look at replay memory. so what is replay memory, it is a technique used in d enforcement learning to store and manage the experience of an agent during training. the idea is to store the agents experiences as a sequence of state action reward next, state tuples. so basically, we have just four values, which are collected as the agent interacts with the environment. during training, these experiences are used to update the agents policy and value function. so basically, this is the we collect all the actions performed by our agent. now let's look at what the importance of replay memory. the replay memory allows the agent to learn from past experiences by randomly sampling a batch of experiences from the memory buffer, rather than just learning from the most recent experience.",330,609,623,vufTSJbzKGU
43,"the replay memory allows the agent to learn from past experiences by randomly sampling a batch of experiences from the memory buffer, rather than just learning from the most recent experience. so it's like the, so we instead of just learning from our recent experience, we take a sample of all the past experiences and make a decision based on that. this helps us to reduce the correlation between subsequent experiences, which can improve the stability and convergence of the learning algorithm. in addition, by storing experiences in a buffer, the agent can reuse past experiences to update his policy and value function multiple times, which can further improve learning efficiency. the replay memory is typically implemented as a fixed size buffer or queue that stores the most recent experiences. when the buffer is full. new experiences overwrite the oldest experiences in the buffer. during training, a batch of experiences is randomly sampled from the buffer and used to update the agent's policy and value function. this process is repeated iteratively until the agent converges to an optimal policy. so basically, this is an example it's a basic thing. it's a very basic concept, we store all of the training data until the queue is full. and the buffer memory that we have will keep using it as we go ahead. where does the concept now let's look at how we'll use the concept of replay memory in order to implement our dq n algorithm.",318,623,635,vufTSJbzKGU
44,"where does the concept now let's look at how we'll use the concept of replay memory in order to implement our dq n algorithm. so what we'll do is, let's understand that so we'll store the transition that the agent observed, allowing us to reuse this iterator by sampling from it randomly the transitions that build up a batch or d correlated, it has been shown that this greatly stabilizes and improve the deployment training procedure. now let's also understand how we implement this in code. so we'll have two classes. one is the first will be transition class it is a named tuple representing a single transition in our environment. basically, this is our sample input. so this will be represent one state and essentially map state action pairs to the next state reward result with the state being the screen dividends image as described later on. next will be the replay class, a cyclic buffer of bounded size that holds the transitions observed decently. it also implements a dot sample method for selecting a random batch of transitions pertaining. let's implement our replay memory class. so the first will be the transition state that we have which is a tuple which is the named tuple by the name of transition and then we'll have a state action next we have recently this is our condition tuple. now let's also create our class, which is the deeply mini class. so it will be a class we call replay memory, then object. let's do def underscore underscore in it, self, comma, capacity. and then we'll have self dot memory. so basically this will return the full memory buffer.",361,635,650,vufTSJbzKGU
45,"so basically this will return the full memory buffer. so it is equal to that. i don't know how you pronounce it, but i pronounce it as that it could be called as dq also, depending on the accent here, max length is equal to capacity. so basically, this is our max length. after that it it will overflow and then we'll reset it so we'll use one transition into this memory was here it says no memory will have self comma rem sleep are some arguments as well. basically, it is also comment save so i think it is double save a transition save raishin. memory node append transition star arcs okay, so basically this will append to our memory buffer, let's create another method of the sample. this method will help us sample our buffer remember, before memory cells come up bad size. so we have a capacity and the batch size. so it the capacity could be 50 and the batch will be 10. so this is how the there's no return. random sample. okay. still not memory. let's size define underscore underscore length may give us the length of the buffer at any point. return length or size but memory. so yeah, i think we have done it completed. let's also look through the code again. so we have defined the transition tuple and replay memory class. now we have fulfils your state action next state. i think i need to do that this year awards. i think it is rewarding. and we will have other class as well. so we'll have the push method.",349,650,673,vufTSJbzKGU
46,"so we'll have the push method. this method takes in a state which is state action next id tuple as input and creates a transition object from it and append it to the deck. if the deck is already at maximum capacity, the oldest element in the deck is removed. so this is how the append method works. let's look at the sample method. this method randomly samples a batch of batch size, which is the batch size parameters that were passed experiences from the deck and returns them as a list. so this is the sampling method. let's look at the length method. this method returns the current length of the deck. now let's understand the dq n algorithm in detail and how we use the algorithm to solve the cardpool problem. so it is a reinforcement learning algorithm that uses deep neural networks to approximate the q function in a q learning algorithm. so in short, basically it is that but we need to understand the steps involved to solve the cardpool environment. let's look at that as well. first we need to initialize the q network with random weights. the second is sample and action using an epsilon greedy policy. basically, we have covered this in our blackjack tutorial, which selects the action with the highest q value with probability one minus epsilon and the random action with a probability epsilon. execute the action and observe the next state and reward.",317,673,689,vufTSJbzKGU
47,"execute the action and observe the next state and reward. store the experience to built in a replay buffer, so we have implemented the class already sample a mini batch of experiences from the replay buffer, we've already created the method for that, compute the q target values for the mini batch using the bellman equation. so basically, this is how we calculate the t q value i'll explain to you when we read the code, compute the q values for the mini batch using the current q network. so we'll have that value. and we just need to compute the q value for that. compute the loss between the q values and the q target values, and update the q network parameters using gradient descent. and we'll keep doing these steps until we reach convergence or we reach a fixed number of episodes. so i will explain in detail what what we are going to do to the end of the cardboard. the dq n algorithm uses a target network to stabilize the training process. the target network is a copy of the q network that is updated less frequently than the q network. this helps to prevent the q values from oscillating during training. so that's what happened in our budget. when we are solving that check. in cardpool environment, the dtn algorithm learns to balance the port on the cart by moving the cart left or right. the queue network takes the state of the environment as inputs and outputs the q values for each possible action, the dq n algorithm learns to maximize the q values by updating the q network parameters using gradient descent.",362,689,703,vufTSJbzKGU
48,"the queue network takes the state of the environment as inputs and outputs the q values for each possible action, the dq n algorithm learns to maximize the q values by updating the q network parameters using gradient descent. with enough training, the dq n algorithm can learn to balance the pole on the cart for extended periods of time. so basically, this is all the steps in what and how we are going to solve the godbole. example, we are going to balance out the card pool for a very extended period of time. now let's go to our cue network. so let's take some notes here. so far model will be a convolutional neural network. if you take the difference between the current and the previous screen patches, it will have two outputs representing q, s comma left and q s comma right, where s is the input to the network. that, in effect, the network is trying to predict the expected return of taking each action given the current input. so basically, this is our cue network. this is the full implementation will have two outputs. and the network is trying to predict the expected return of taking each action given the current input. now let's implement our dq n class. let's add the class here. that's right class, the q n and n n dot module. so let's do def, underscore, underscore init, underscore underscore, self comma n, underscore observations. comma and underscore actions. so i'll explain to you in detail that just complete the class first, and then we'll read the comments as well. so it will be very easy to understand.",360,703,721,vufTSJbzKGU
49,"so it will be very easy to understand. we self dot underscore underscore in it. self thought we left three layers, let's do all the three layers here to three, the equal to and then thought linear and underscore observations 128. so we have sizes and 128. let's do that. we'll the next one will be 128 by 128. this is how it will be now and the other one will be the exactly the opposite of this will be 128 by the number of actions comma and underscore options. so we've implemented the new folding in it consulted. let's go and define our method. that is the forward method. and we have to do self dot causes comma x and x is equal to f dot value self dot layer 1x and then we have the layer 2x. and then we'll return the cell got layer 3x. so we'll basically go to three layers what the code now let's understand what are we trying to do here? so we have the cue network which is mighty perceptron, so let's just keep writing it down yet mighty per mighty layer or scepter on any point, you can just use perceptron with three layers. so, this is what we are trying to achieve here. now, we have what the input is it is a tensor. so, the what will be the input, so it will be a tensor of the size and underscore observation is our size which is single this is what we passed here. actions is our input to the network this is basically the state input that is the state of the environment, state of environment to the network.",365,721,738,vufTSJbzKGU
50,"actions is our input to the network this is basically the state input that is the state of the environment, state of environment to the network. so, we have the fully connected layer with 120 neurons, followed by a value activation function. i'll also explain like why we're using that the second layer is also a fully connected layer with 128 neurons and a relu activation function. the final layer is a fully connected layer with an actions where an action represents the number of possible actions in the environment, which represents a certain number of possible actions. environment now, let's look at the forward method what we have implemented here. so, if you take the input as the tensor that is the x tensor, as it passed through the first layer of the network using the value activation function, pass, take input, we pass to two or three layers on the network when. basically, this is what we're trying to do here. and what we are also trying to do is, so, we'll pass it to the state to the network. and what we will get this, the output is the corresponding neuron in the final layer. so, when you pass here, we'll get an output we'll pass it to the next year. and in the third layer, we finally return. so and what will happen here, so during training, the q network is updated using the bellman equation that we have discussed already to minimize the mean squared error between the target values and the q values. yeah, so let's look at that. now let's look at the training of our dq n.",357,738,751,vufTSJbzKGU
51,"now let's look at the training of our dq n. so what we'll do is we'll have a q network, which is updated using the bellman equation to minimize the mean squared error between the predicted q values and the target q values. the target q values are computed using the q network but with the weights frozen and not updated during the current iteration of training. this, this helps to stabilize the training process and prevent the network from overfitting to the data. okay, now let's look at all the other parameters that we will be have. or i can say parameters and utilities as well. so first, we'll have select action, it will select an action according to the epsilon greedy policy, we have again covered this in the blackjack tutorial. in the back when we were solving blackjack using toolani. we simply put will sometimes use our model for choosing the action. and sometimes we'll just sample one uniformly. so there is no steps for that. but we will see how we can do that. now, the probability of choosing a random action we start at eps start and will decay exponentially towards eps and eps decay controls the rate of decay. so, basically, this is the set of we do not have any particular target value. so as we go along, the values will keep changing. and we have to decide whether we want to take a random action or we want to take from the action that we have on the board. next we have plot durations. it is a helper for plotting the durations of episodes, along with an average over the last 100 episodes.",365,751,768,vufTSJbzKGU
52,"it is a helper for plotting the durations of episodes, along with an average over the last 100 episodes. the measure used in the official evaluations. the plot will be underneath the cell containing the main training loop and will update after every episode. so we'll plot this training durations as well. let's now go to the code. so let's implement all the parameters that we are going to need so i'll have pad size, that size equal to 1.8 comma you can do zero point. so i'll also explain like this, just complete all the parameters for your start. start is equal to 0.9, we have eps, and you will do 0.05. so basically this is when we will end. this is our epsilon and lbps decay rate, which is going to be 1000. and we have the tall, which is the one who is 0.005. and our learning rate is going to be one e raise to minus four. yeah, this is our this is what is going to be yep. now, let's read the comments as well, what is going to be the batch size number of transitions sampled from the labor force it is grandma this is what is the discount factor as mentioned, we will do the discount factor. so what is going to be the eps third quarter is the final value of episode yesterday the starting value of the epsilon basically, we can just write it next to it as well. so we don't have to waste the time just writing now, it is a starting value of epsilon. let's look at the eps. and this is the ending value of final value of epsilon.",366,768,785,vufTSJbzKGU
53,"and this is the ending value of final value of epsilon. let's look at the eps became in eps api educate controls to equal mediaspace. controls the rate of exponential decay decay of epsilon higher means a slower rate. so first we'll start with a very slow rate. and then we'll keep improving on our learning rates. so there is going to me higher means slower and slower decay. let's look at our this as well is that is the update rate of the target network? what is tau? tau is the update rate of the target network. and what is going to be the lrt it is going to be a learning rate. so this is going to be the atom optimizer. so let's look at now what is this is the perfect time to understand about the atom w optimizer, atom optimizer and atom optimizer. so basically now let's look at the atom optimizer. so basically we've defined it you're eliminating all these parameters and eliminating it for the atom optimizer. so this is the learning rate for the rm optimizer. so adam, adaptive moment estimation is a popular optimization algorithm that is commonly used in deep learning. it is an extension of stochastic gradient descent. if you do not know about that, it's okay, you can still watch the rest of the video, which is the most basic optimization algorithm used to train neural networks. the main idea behind adam is to combine the advantages of two other optimization techniques, add a grad and rms problem. so if you do not know about that as well, it's okay. in the dq n algorithm.",358,785,805,vufTSJbzKGU
54,"in the dq n algorithm. we use adam optimizer to update the weights of our neural network based on the gradients of the loss function with respect to the parameters, so we'll define our loss function as one minus epsilon. specifically, we use the adam w optimizer which is a variant of the atom. so we have defined it here which is a variant of the atom that also incorporates weight decay regularization. weight decay helps prevent overfitting by adding a penalty to the last function that is proportional to the magnitude of the weights. by adding this penalty the optimizer encourages the network to learn simpler and more generalizable representations. the learning rate is a hyper parameter that controls the step size taken during optimization. it is an important parameter to tune as a high learning rate can cause the optimizer to overshoot the optimal weights and lead to divergence while a low learning rate can result in slow convergence and getting stuck in a local minima. in dq n algorithm, we set the learning rate to one e raise to minus one. so basically, this is what this is why we're going to use that optimizer. so we don't get stuck in a local minima. so, we need to control the learning rate. and for the content to the learning rate, we need that and w optimizer. in summary, the adam w optimizer is is a widely is a widely used optimization algorithm in deep learning. and it is used in dq n algorithm to update the weights of the neural network based on the gradients of the loss function with respect to the parameters, while also incorporating weight decay regularization.",367,805,819,vufTSJbzKGU
55,"and it is used in dq n algorithm to update the weights of the neural network based on the gradients of the loss function with respect to the parameters, while also incorporating weight decay regularization. so this is the very critical role played by adam w optimizer. now, let's so now we will learn about the atom optimizer. now let's learn about now let's read the code for the other two other utilities that are pending. so the first variable that we have is the end actions, this is going to be nv dot action, the action space. so basically, this is going to be the action space of the carpool environment. so let me just keep it up, let's just keep writing the number the comments as well. number of actions from them action space. also, now let's do that net number of state observations, which is going to be state comma info. so, basically, this is going to be that tuple that we talked about the content all the information related to the current state, the current state that we are in and of the reasons is going to be length of state. now let's create our policy net, which is going to be a class it is going to be an instance of the dq n class. we have this ruby and underscore operations comma and underscore actions not to device specific devices that what we have set up to target net as well. targeted net is equal to same thing.",330,819,831,vufTSJbzKGU
56,"targeted net is equal to same thing. now let's load the state target dot score net not load underscore state underscore the policy this coordinate got state i think it is policy and i understand the state. now let's complete the optimizer so i'll just come back and explain to you everything is just complete the optimizer also now basically, we this is going to be the torch method ochem but we added w we discussed about atom w i also explained to you the atom w optimizer and our atom optimizer is going to be policy underscore net dot parameters. okay parameters needed we have already defined small error is equal to capital error. total error is defined here. one equals two minus code from ams guard ams grad, so like listening to you what this means as well. let's complete the code memory we'll be taking the last 10,000 episodes for our memory. do replay. memory 10,000 we passed in to our buffer size is going to be 10,000. let's define the steps done as well. with me, this is where we currently are. okay, let's let's go ahead and just complete all the methods let's do the select action as well. basically this is going to be the method that will help us select any action.",284,831,843,vufTSJbzKGU
57,"so if i have a state, it will just tell us which methods are and steps underscore done sample is equal to random sample i think it is wrong it is random not random and with the pps on the score threshold is equal to eps score and plus eps underscore start we have defined all these parameters before vps the scope and into we have to divide it by offering when you go to the next line so with me goes less mad not mad dot exponent of minus one towards star off steps done by up to k yeah this is going to be the equation and then let's do steps done once we're done with this little loop do steps than possible will rent payment from those sample to them ups underscore threshold and do with torch.no grad so basically we'll be doing that we'll do t dot max one minute and the largest column of each row this column value okay what does it mean this is a row we have the second column second column on max next up where max element was was fun so, we pick action with largest yeah so, we need to like we need to optimize for the larger what will basically this is what we are going to do expected reward let's do return policy score net state dot max not view one comma one now we need to do the as of this as well is the els dot pensar en v dot actions so we'll just return the action space action underscore space dot sample response.",349,844,844,vufTSJbzKGU
58,"so basically this is our we're going to do is we'll see if we want to take a sample one already to keep continuing the path that we are on. do that okay, now with the device is equal to device and command d type call sheet.com okay, so i'll explain to you all these utilities let's just complete all the methods while we're at it. so does equal to episode underscore durations okay, so what we'll do is just go over all the methods that we completed. so, the n actions is the number of actions in the environments, no environments action space will be the gym environments, state comma info are obtained by the setting the environment and observations is the number of features in the state. let's just drill down as well in the state let's do the target net is initiate with the same weight as policy net okay. target net initialized with the same weight as policy net then we have the optimizer which is not an optimizer adam w optimizer optimizer is lm w optimizer. adam w. normally optimizer pytorch with the learning rate and other hyper parameters specified earlier. so we use it to optimize the weights. use to optimize weights, we already passed all the parameters that we need. let's go to the memory. so the memory is an instance of the replay memory, and it has a capacity of 10,000. okay, so let's do a site wide recall you can store intense experiences which will be for training for training just ever done.",340,845,857,vufTSJbzKGU
59,"okay, so let's do a site wide recall you can store intense experiences which will be for training for training just ever done. because it is it is used to keep track of the steps number of texts keep track of number of texts taken by the date. we're going to start with we're going to start with zero. now let's understand the step action method as well. so it takes the current state as input, let's just write down and code this current state and returns an action so it could be either that the action is it could be selected by choosing the action with the highest q value. so this is going to be the isp value. or it could be that we have taken a random so how we're going to decide so we're going to decide by this expression. so we had discussed earlier that we're going to be calculating the threshold value and if there was a sample within the threshold value, and we have no grad then we go to the maximum or too high sql that we have that we find or we take the random random approach. so basically this is going to be the envy that action or sample everything the sample and the device will be either a cpu or a gpu in our game it is the gpu that we have defined earlier. and last we have episode durations interviews to keep track of the duration we can often each episode so now let's also complete the plot durations method which will plot all the duration of each. so net is taken by the agent we have shows multiple calls.",366,857,867,vufTSJbzKGU
60,"so net is taken by the agent we have shows multiple calls. start with the plt dot trigger. mainly we're going to plot we're going to be using biplot here when durations underscore t is equal to torch dot tensor. we have the episode underscore duration. format d type is equal to torch dot float. if so underscore result the plt dot title wizard, we're going to be keeping the title and result as a rule that we have also basically the first time we change the title and then we'll plot the crosshairs so that now training painting graphs plt dot x, x level is going to be episode and then we have plt dot y label, which is duration. then we have plt dot plot durations. obviously, we think this munition under different durations might have taken not numpy will convert it to the numpy array. and then we'll take 100 episode averages. so let's start it on. first, let's do that clip length greater than 100 doesn't equal 200. then means durations underscore t naught unfold zero to slice it and we'll take the mean and then we'll take the other viewers what little means is equal to torstar cat it's going to be towards our cat towards third zeros 99 and then we have the means then we have plt dot plot dot numpy let's add a pause when you have plt dot 0.001 pause so that plots are updated. and then we if we have a display so basically i just write it down so that it works on all the environments. if we just run it in your local vs code also it should work right.",366,867,880,vufTSJbzKGU
61,"if we just run it in your local vs code also it should work right. so it will be show underscore results display dot display dot dcf and display display dot here or wait is equal to display dot display plt dot gcf. so i'll explain to you what all these plots mean. we're done. so we have completed the method. let's now go and write some comments. so but it basically it will be very clear for all of you. so this method is used to visualize visualize the training progress of the dq n basically, this is the method here. now first we'll create a tensor from episode durations. so basically this is the keep track of the lengths of intuition. and then we'll get into using matplotlib. and if we have the show result as true then we'll label label it as dessert or as it will be training. and the x axis is labeled as episodes the y is labeled as duration. and we have a function that is called we have a variable that is called durations underscore t is a blue line and overlays with a red line that shows 100 episode moving average of the durations, so let's just write it down. so the 100 moving average of the durations yep, so while we wait for just a second so that the plots are updated it'll come in real time. and if you have a jupyter notebook, then we'll use can use this method as we'll just use a normal method so now let's start our training loop.",344,880,895,vufTSJbzKGU
62,"and if you have a jupyter notebook, then we'll use can use this method as we'll just use a normal method so now let's start our training loop. so, let's create our optimized model here called model from spaces if length match network size then return we do not need to optimize at the start then we have the transition when we take a memory that sample basically this is the sampling we will do the sampling okay, then we have the batch match is equal to transition so, visibly this looks a little bit complicated, but i'll explain to you like why we do this basically we transpose the batch and it will convert the batch array of transitions to transition a batch or is it converts batch or a batch of transitions to trans transition of batteries now, let's compute the mask of non final states 30 being non underscore final underscore mask equal to torch dot insert then we have that tuple do map iterator than the continue number is says not none match.com device this is done so let's write down what this method will do what we're planning to do, we're going to compute our mask off non final states and the batch limits okay and the final state will be where the simulation and basically that means that let's do i think we have to provide a parameter can be d type sql to towards that boolean now let's do the non final next states recall that cat as far as that's not next step is not null.",343,895,896,vufTSJbzKGU
63,"so, let's create our optimized model here called model from spaces if length match network size then return we do not need to optimize at the start then we have the transition when we take a memory that sample basically this is the sampling we will do the sampling okay, then we have the batch match is equal to transition so, visibly this looks a little bit complicated, but i'll explain to you like why we do this basically we transpose the batch and it will convert the batch array of transitions to transition a batch or is it converts batch or a batch of transitions to trans transition of batteries now, let's compute the mask of non final states 30 being non underscore final underscore mask equal to torch dot insert then we have that tuple do map iterator than the continue number is says not none match.com device this is done so let's write down what this method will do what we're planning to do, we're going to compute our mask off non final states and the batch limits okay and the final state will be where the simulation and basically that means that let's do i think we have to provide a parameter can be d type sql to towards that boolean now let's do the non final next states recall that cat as far as that's not next step is not null.",306,896,896,vufTSJbzKGU
64,next underscore state which will be the final state if it is ending then it will be zero in case the finest gets the state was the final state will be taught not zero match score size comma device is equal to device us now with no no torch or that no grand do next underscore state underscore values non underscore final mask sql to target net it looks a little bit complicated but trust me when i explained to you after writing the code they were pretty clear what's missing there looks code looks intimidating but what we tend to do is pretty simple got max one of zero now let's also compute the expected value expected underscore state underscore action value i think we i forgot that yeah we need to do this first you can do this first cannot underscore values equal to next state values values to gamma let's see what we what that about losses well i'll explain to you what police teams losses criterion is equal to n dot mood one loss that's the name and also the call to state score action score i use expected the school state school action expected sports score action score values squeeze one.,267,898,898,vufTSJbzKGU
65,"now let's optimize the optimizer dot basically our query and the way to do optimizer dot zero underscore grad loss dot backward is in place so it must be looking like it's pretty tough but trust me i'll explain to the medicare then dot dot utils not clip grad value add value underscore policy not net that parameters commander then verse will keep the value now let's do optimizer dot now, let's look at this. so, first we check if this is a mini batch, so, we have the optimize method right. so, this is deep q learning that is working what we already know. so well let's check what we do. so first we replay memory. so, first we check if the replay memory contains enough samples to fill up a mini batch let's write it down if check if we have enough samples for you if you do not then return nothing then do nothing okay and then let's transition. so, first then we what do we do is we take many bytes extract extract extract i mean us me myself transitions from epson comma reward comma next step from that now then also what we do is we calculated calculate the expected q value and it is the sum of the immediate reward. so, basically we have it here too, we have the gamma as well i cannot see grandma yeah, so, basically just search for yeah, this is where we do the expected state calculation.",323,899,906,vufTSJbzKGU
66,"so, basically we have it here too, we have the gamma as well i cannot see grandma yeah, so, basically just search for yeah, this is where we do the expected state calculation. so, what we do is okay good calculate the expected q value expected q value you value for each transition using the target network basically we have created our neural network here. now, if you do not know how to create a basic neural network, no issues so, basically it is the sum of the immediate reward and the discounted q value of the next state. okay, now under this contract is controlled by the gamma. okay, now, let's see the function. now what we'll do is we'll calculate the q value predicted by the policy network for each transition. and we'll also select the queue corresponding to the action taken by the agent. so, basically, what we are doing here, we are also selecting now, we need to calculate the hover loss as well. the upper loss is a smooth approximation of the mean squared error loss and is less sensitive to outliers. so, we need to be less than smooth. what is our loss it is a smooth approximation. we do not understand how the loss knows us you can just look it up. but it's a pretty good metric for calculating data loss. listen that basically, this is our losses. and now finally we back propagate now to the quality network and update the parameters using the adam w optimizer.",332,906,920,vufTSJbzKGU
67,"and now finally we back propagate now to the quality network and update the parameters using the adam w optimizer. and we also have clipped it to a maximum of 100 to prevent the exploding gradient problem, so maximum value is equal to under to prevent exploding gradient problem so now, let's go on to the main loop now maintaining loop. so we have completed the training part of it now we need to create, we need to start implementing the algorithm. let's do that now. so what we'll do is if torch dot cuda, basically, i'm just making the code in such a way that if you do not have a gpu, you can use this code, you don't have to make any changes. so then you need to make number underscore episodes, you can just keep it 50. and also, if it is available, then i will keep it somewhere else. i'll do it now let's loop over the episodes for i underscore episode in range of number of episodes to date, so i don't i think you all know what this does is really set the environment and get the current state and the all the information that we need. yeah, let's just write it down again. visualize the environment. get state to state is equal to torch the tensor state comma d type is equal to call that float 32. comma device is equal to device and let's do an squeeze zero. i think you all know what we're doing here. i've already explained it once. let's look let's create another loop. now what do we have the encounter?",354,920,935,vufTSJbzKGU
68,"now what do we have the encounter? number of counter the reaction is equal to cylinder since we need to select an action here. select underscore action false was for the first time we do not want anything with by the state and we want observation. comma, reward comma terminated comma can get it any other parameters that we get is equal to nv dot step is equal to send your items where you could take action on items. and then at maker step forward, so we'll need a new observation they were terminated and you're truncated well. yeah, now let's give the final reward is well, we've taken a step forward now let's look on the reward or the tensor reward karma device, welcome device. and we have done is equal to terminate it or truncated. and we have if yeah, so we can either have that we have completed the number of steps or that we have the card if the card pool has fallen off 2.4 meters away from the center to easily that could be that next underscore state is equal to none. as we need to do you find the next state? we'll do that. one next state will be we call it a tensor tensor observation variation comma v type v going to torch dot float 32 comma device, the call to device dot and squeeze, zero now let's add this next step in memory. so we can do that using manager push, we have created that in our class action, we already have all this action. next, we have the next video. and we have the reward.",356,935,948,vufTSJbzKGU
69,"and we have the reward. now store, just keep that in comments as well store the transition and then move to next, let's move to the next as well. so basically, we move to the next step, let's optimize the model. now. we have already created the model for that, the method for that. now let's do the soft update of the target network weights. so we already have that new target underscore net underscore state. the score did this mean we need to do it for both the target and the policy next. goal to target and the score net dot state underscore dict. so if you find it pretty difficult, it's okay. i happen to be the first time. but when it comes into code, i just go everything once again. so everything becomes very clear. for kean policy net, i think we have to take this one is the policy net, that's to training that state. and then when to do key. the call to policy net underscore net underscore state underscore of key swing into to multiplied by the tau tau plus target net state t. so the old value also into one minus tau. basically this is what we're planning to do now. one minus tau is what we're doing. now let's see if we are done. we are going to do a software update. and then we are done. episode dot.so we are completed one episode two, i think i've explained to you what is the difference between episode and an iteration.",335,948,970,vufTSJbzKGU
70,"episode dot.so we are completed one episode two, i think i've explained to you what is the difference between episode and an iteration. basically, an episode is the one sequence between a start and an end to basically in the cardboard instance, the start will be when we started the initial state. and then if we fall of 2.4 meters away from the center, or that we have completed the game, basically we have achieved the maximum state that we could we have a number of times we could have, we could have played this game. so this is one, this is going to be one one time that we are going to play the game basically. and we are for if you have a gpu, we're going to do this 600 times. so basically, we have to play this game 600 times. there's going durations and there's to break out. now let's complete it. so finally, we have completed then we are going to plot all the durations. basically what i encourage is we can just keep changing the number of durations and check the results. basically, that is something i want you to do. and i'm not going to do that. i'm just going to stick to the 600 episodes, because i have a gpu i have and plt dot show done with the cart pole environment as well. let's quickly summarize everything that we have learned. so what the agent has to decide it has to decide between two actions moving the cart left or right. so basically this is so we do a loop here.",349,970,985,vufTSJbzKGU
71,"so basically this is so we do a loop here. so as you can see, either the carpet will move left or right and we need to balance it out. so now i've explained all this in detail let's look at all the methods that we have imported. so, we have imported the torch that nn which is used for our neural network, we have imported that thought is that octane method, which is for optimization imported towards an autograph, which is for automatic differentiation. and also i've explained in detail what each of this is you would extend each of this in detail as well. now, we have started with the import of the module that we need also we have started. so, basically we have installed it and here we have imported all the methods that we need, here we have set up the environment. and also i have also made it such that you can run it in the google collab notebook as well also in just any python script as well. so, just accordingly, then we have detected if cuda is available, which is a gpu. now, i have also explained to you what is the concept of replay memory, basically, it is a technique used in reinforcement learning to store and manage the experiences of an agent during training. so, basically, we will make use of this, we have saved all of the steps that the agent takes, and use that to make a future decisions.",326,985,995,vufTSJbzKGU
72,"so, basically, we will make use of this, we have saved all of the steps that the agent takes, and use that to make a future decisions. so we're going to make two classes one transition class and one replay memory class to basically we have to transition here and then we have the replay memory class. and we are going to use the dq n algorithm. it is a deep q network, which is a reinforcement learning algorithm that uses deep learning deep neural networks to approximate the q function, i will end with the q function and we have also learned about the q learning algorithm when we were solving that chapter. so, we have basically i have written all the steps that we will be using to solve the cardpool environment using the dq n algorithm. then we have implemented the class for that as well. then we come to the training section. during training the q network is updated using the bellman equation i have explained the bellman equation as well, to minimize the mean squared error between the predicted q values and the target q values. the target q values are computed using the q network but with the weights frozen and not updated during the current iteration of training. now, we have some important concepts here. so basically, this is the select action, maybe p start and we have plot durations, where we paste the case what i have explained to you all these hyper parameters as well. let's go to a very important concept that is adam optimizer.",344,995,1006,vufTSJbzKGU
73,"let's go to a very important concept that is adam optimizer. so adam adaptive moment estimation is a popular optimization algorithm that is commonly used in deep learning. it is an extension of stochastic gradient descent, which is the most basic optimization algorithm used to train neural networks. the main idea behind it is to combine the advantages of two other optimization techniques added an rms block. here i've explained in detail how we'll be using it in our dq n algorithm. next, we get the number of actions from the geometric space we set up, we set the environment and get the initial state, we set up the number of features that we have, we have the atom optimizer, we have initialized our training neural network as well, we have the replay memory which is a 10,000 buffer, which is contained 10,000 last 10,000 episodes, then, basically this is a training loop, i explained in detail as well, then we have the optimized model, this method will optimize the model. so basically what we need to do is we need to just take a mini batch. so and also we will be using sampling as well. to have gone to that now you have started with the training. so we'll take that if cuda is available that is a gpu is available, then we'll do six minutes because or else we'll only find it episodes. so yeah, this is the training loop. and then we have plotted the end result. this chart i've also explained in detail how we use this chart to understand the performance of our algorithm. now we've completed our solving of logic using q learning.",364,1006,1019,vufTSJbzKGU
74,"now we've completed our solving of logic using q learning. so basically, let's move on to the advanced topics that we are going to do go to cover and get you will explore after learning this beginners tutorial. so you could explore deep reinforcement learning. deep reinforcement learning is a combination of deep learning and reinforcement learning that uses neural networks to approximate the q values in q learning. deep rl algorithms such as deep q network and actor critic have been successful in solving complex tasks such as atari games robotics and autonomous driving. so, basically, this is what you can explore, you can dive into autonomous driving now. policy gradients policy gradient algorithms directly optimize the policy function by adjusting the parameters of the policy to gradient set. this allows the agent to learn a policy that maximize the that maximizes the expected reward without competing the q values. the advantage of policy gradient algorithm is that they can handle continuous action spaces, unlike q learning. so this is pretty interesting. you can also explore this. now let's move on to multi agent reinforcement learning. yeah, so this is pretty interesting. now you can have two three cars in an environment and you can simulate that. and you can use petting zoo for that. so petting zoo is a brother of guinness. and it allows it gives us an environment for multi agent reinforcement learning. multi agent reinforcement learning extends reinforcement learning to the scenario where multiple agents interact with each other in a shared environment. in multi reinforcement learning, each multi agent reinforcement learning agent learns his policy based on the joint reward received by all agents.",364,1019,1037,vufTSJbzKGU
75,"in multi reinforcement learning, each multi agent reinforcement learning agent learns his policy based on the joint reward received by all agents. the opening a gymnasium toolkit provides environments for mrl such as the classic prisoner's dilemma game and cooperative navigation tasks. so there's some pretty interesting imitation learning, also known as learning from demonstration is a technique that trains an agent to mimic the behavior of an expert. this approach has been used in various applications such as autonomous driving, robotics and game learning. and genbrain open ai gymnasium toolkit include environments from imitation learning such as motoko humanoid locomotion task flow, this also sounds pretty interesting. now let's look at the last topic that you could cover in the advanced topics to discuss transfer learning. transfer learning is the technique of transferring knowledge learned in one task to another task. in reinforcement learning, transfer learning can help speed up the learning process and improve the performance of the agent. the albany gymnasium toolkit provides environments for transfer learning, such as the classic inverted pendulum does and card polls wind up just so now we talked about multi agent reinforcement learning, so let's check out putting zoo documentation that i mentioned. so what we can do is here you can check out all the different types of environment that are available. so what are the games we have pulled up? here we have mario bros for multi agents of luigi and mario here on the action space is defined so i've explained all the concepts here so it wouldn't be very difficult for you to start implementing these.",349,1037,1048,vufTSJbzKGU
76,"here we have mario bros for multi agents of luigi and mario here on the action space is defined so i've explained all the concepts here so it wouldn't be very difficult for you to start implementing these. we have space invaders will be basically these are different types of games available. let's check out classic games. we have chest solibri chest as well. so i highly encourage you to go first lex was when you understand the basics, then i highly encourage you to go out and check out the petting zoo documentation and study creating your own environments and your own agents and solve some real world problems.",143,1048,1052,vufTSJbzKGU
0,hey elon you need some help uh landing the starship yeah well uh check this out i was very sad to see that worth a try what's happening guys my name is nicholas renate and in this video we're going to be going through reinforcement learning for beginners and who knows along the way we might even help elon out let's take a deeper look at what we'll be going through so in this video we're going to be covering three key things so first up we're going to start out by installing the stable baselines package so this is going to be a package that makes it a whole heap easier to get started with reinforcement learning then what we're going to do is we're going to set up the open ai gym luna lander environment so this allows us to build a model that allows us to actually try to land a spaceship on the surface of the moon so we're actually going to try to build an ai model and specifically a reinforcement learning model that's able to land that spaceship successfully then what we're going to do is once we've gone and built that model and trained it up we're going to test it out and run it and you'll be able to see how our ai performs in real time let's take a look as to how this is all going to fit together so first up what we're going to be doing is we're going to be setting up our lunar lander environment and you can see on the slide this is a little spaceship that we're going to be,358,0,0,nRHjymV2PX8
1,trying to land between the flags so the main goal is that we'll have our landers sort of pop up onto the screen and the goal is to direct the jets on that particular lunar lander so that we're able to land it successfully within those flags so if our ai and our reinforcement learning model is able to do that then we've completed the task successfully so in order to do that we're going to be training a reinforcement learning model from the stable baselines package and we'll go through setting all that up once we've trained it then we'll unleash our reinforcement learning model on the lunar lander environment so hopefully we can land it between those flags ready to do it let's get to it alrighty so in order to kick off our reinforcement learning for beginners tutorial there's going to be four key things that we need to do so first up what we're going to need to do is install and import our dependencies then what we're going to do is test out our environment so specifically we're going to be passing through random actions to our environment there so if you've seen the reinforcement learning crash course video that i did again link will be in the description below you'll see how we wrote this code but as always all the code that we write in this particular video is going to be available in the description below via my github account so once we've tested out our environment what we're then going to do is build and train our model and this is going to be where we use,358,0,0,nRHjymV2PX8
2,some of the algorithms from stable baselines to actually go on ahead and train a reinforcement learning agent then last but not least we'll save and test it out so we'll actually be able to see our lunar land up model actually working in real time now on that note so our core dependencies are going to be stable baseline so stable baselines is just a ridiculously useful reinforcement learning library that has some really really good algorithms available in it so if you actually scroll on down go to stable dash baselines.read you'll be able to access this documentation but again this link will be in the description below you'll see that we've got a whole bunch of rl algorithms that we can work on so we're going to be working with acer but ppo2 i've had really good results with that and a2c as well the other dependency that we're going to be working with is open ai gym so this is obviously one of the most popular libraries when it comes to reinforcement learning environments and the environment that we're going to be working with is luna lander so you can see that we've got this little spaceship that's trying to land just in general but also in between these flags so if you land within the flags then you get more points but ideally the goal is to take this little purple spaceship that you can see here and get it to land successfully without breaking its little legs so what we're going to do is we're going to be trying to build our reinforcement learning agent to be able to solve,358,0,0,nRHjymV2PX8
3,that problem and we're going to approach this in a reasonably straightforward manner so we're going to leverage stable baselines to help us to do that all righty without further ado let's first go ahead and install and import our dependencies so i'm going to write the line of code and then we'll take a step back and we'll take a look at what we've got alrighty so before we go ahead and run that let's take a look at the code that we've actually written so first up we've got exclamation mark let's just make sure this is zoomed in perfect cool so we've got exclamation mark pip install tensorflow equals equals 1.15.0 this is a key thing to note so if you're working with stable baselines you need to be using a tensorflow version which is before tensorflow 2.0 so ideally what you want to be doing is using 1.15 so what we've got here in this line is we're going to install tensorflow 1.15.0 and tensorflow gpu 1.15 and to do that we've written tensorflow equals equals 1.15.0 tensorflow gpu equals equals 1.15.0 so this pip install line is going to go on ahead and install both of those and this really just covers all bases whether or not you're using gpu or a non-gpu machine then the next package that we're installing is stable underscore baseline so this is going to install all of our algorithms that we saw on stable baselines then the next thing that we're installing is gym so this is open ai gym and we're also installing box 2d dash pi so this is a dependency that you're,358,0,0,nRHjymV2PX8
4,going to need if you're using the lunar lander environment so we're just going to install it just to make sure our lives remain easy as we're going through this and then i'm also passing user so this is going to use the user flag to install it for me as a user just avoids any administrator dependencies if that happens to occur on your machine so in this case we're going to hit shift enter and run that cell and that's going to go ahead and install our dependencies now you can see that that's run pretty quickly and that's because i've got already got it installed so we can go on ahead to our next step which is importing these dependencies so let's go ahead and do that alrighty so those are our dependencies now imported so what we've done is we've written four lines of code there and specifically what we've gone and imported is open ai gym and then a whole bunch of different dependencies from stable baselines so our first line of code that we've written is import gym so this is going to import open air gym and it's going to allow us to create a lunar lander environment that we can then apply our reinforcement learning models to then what we've gone and written is from stable underscore baselines import acer so this is importing the acer policy or asap agent that's going to allow us to train our reinforcement learning agent so think about this as a algorithm so similar if you've done any machine learning before think of this as a specific machine learning type of algorithm,358,0,0,nRHjymV2PX8
5,so in this case we could also sub out and we could use something like uh let's take a look we could do something like dqn ppo ddpg so think of them as different algorithms in this case we're going to be using acer but you could try different ones if you wanted to then the next line that we've written is from stable underscore baselines dot common dot vect underscore env import dummy vec env so this is basically a wrapper that goes around our open ai gym environment that allows us to create a dummy vectorized environment for stable baselines so when we're working with certain stable baselines algorithms it's expecting that our open ai gym is vectorized now in this case we're not going to explicitly vectorize our lunar lander environment but instead we're going to wrap it inside of our dummy vec environment you'll see that once we go to set that up and then the last dependency that we've imported is evaluate policy so to do that we've written from stable underscore baselines dot common dot evaluation import evaluate underscore policy and the evaluate policy method just makes it really easy to test out our model and see how it's performing so we'll use that right in step three where we go on ahead and test out our model all right so those are our dependencies done now what we're going to do is we're going to set up a variable that's going to hold the name of our environment and in this case our environment is going to be named lunalander v2 so we can actually just copy that and,358,0,0,nRHjymV2PX8
6,we're going to create a new variable called environment name create an empty string and then paste that in so this basically is just going to save us time from writing luna lander dash v2 each time we want to create an instance of our environment now that's pretty much step zero done so we've gone and installed our dependencies we've also gone and imported a whole bunch of stuff and we've created our environment variable now the next step that we need to go through is to test out our environment and i've said random environment but really it is a standard environment what we're actually doing though is we're actually just going to go and take a bunch of random steps inside of that environment because if you take a look you can see that our lunarland is taking random steps here in this particular video on the open ai gym web page we're going to try to replicate that just to see what our environment looks like and how it actually operates so what we can do in order to do that is just run this cell which is going to use jim dot make and then it's going to pass through our lunar lander string here to be able to go and head and create our environment so if we run that cell that's created our environment now what we can actually do is we can actually go on ahead and run this block of code so what it's going to do is it's going to attempt to land our lunar lander three times so we've gone and set up episodes equals,358,0,0,nRHjymV2PX8
7,three and this basically means that we're going to try it three times to try to land it successfully we're initially going to reset our environment set done and score to false and zero respectively so this is just resetting variables and then what it's going to try to do is actually go on ahead and land it and specifically we're only just going to take random steps so there's no logic or reasoning behind how it's going to land it's just going to take random steps so you see that it won't actually perform that well when we go and run this code now ideally what should happen is once we finish training our model and we go and apply it in a similar way we'll actually be able to land our lander more appropriately when it comes to doing that so let's go ahead and run this and see how it actually looks like so i'm going to set it episodes back to 10 so this is going to give us a little time to actually see the pop-up um appear at the bottom of our screen so when you run this you'll get a little python pop-up and this is actually going to be the screen that we're able to see our environment in so if i run this you can see we've got that little pop-up you can see our lunar lander right now it's nowhere near landing so if so ideally our goal is to either land it anywhere ideally we want to land it between these flags and our model will learn to do that but right now it's not,358,0,0,nRHjymV2PX8
8,even getting close right it's crashing each and every single time and our score isn't getting anywhere into the positive numbers so those are our goals right so we want to be able to land our lander ideally get it between the two flags and that should mean that our score is no longer in the negatives but we're now in the positive values so now that that's done and we've tested out our code that's step one done or this particular step done the next thing that we want to go ahead and do is actually build and train our model so let's go ahead and write that code then we'll take a step back and see what we've written all righty so we've gone and written three lines of code there and the nice thing about stable bass lines is that it makes it reasonably simple to get up and running so this particular step you could really skip this testing out of your environment but it's a good idea to understand how the environment actually operates these three lines here actually create our environment set up our model and really if we wanted to there's one additional line which we'll do in a sec which is going to go on ahead and kick off our training to be able to train our reinforcement learning algorithm so let's take a quick step back and see what we've written here so first up we've written env equals gym dot make and then we'll pass you our environment name which we had up here then as i said what we did is we're going to wrap,358,0,0,nRHjymV2PX8
9,our environment inside of this dummy vec environment and our dummy vec environment is actually expecting an environment generator as its input so what we've done rather than writing a full-blown function we've just wrote lambda and then we're creating a new environment each time we loop through that so ideally what you're going to get out of this is a number of environments generated in this case it's just going to be one because we're only passing it through once and then we're effectively going to reset our environment variable equal to e and v then what we're doing is we're creating a new instance of our asa algorithm and remember this is the algorithm that we imported up here and if you wanted to you could swap this out for any one of these algorithms that you see there so again play around with them you're going to probably get differing results and differing training times depending on which algorithm you use but in this case we're going to be using acer which you can see here then to that algorithm we've gone and passed three key variables so specifically two arguments and one keyword argument so to do that we've written model equals acer and this is our algorithm that we brought in up here and then we're passing through the policy so think of this as the neural network that sits behind this algorithm in this case we're using a multi-layer perceptron policy i believe that's what it stands for but you could just as easily use a cnn policy which is going to work with image-based environments you could also use,358,0,0,nRHjymV2PX8
10,an lstm based policy which is going to give you a recurrent neural network which is particularly good if you've got sort of time series based environments or environments that rely on previous states to that we're then also passing our environment which we created over here and we're setting verbose equal to one because this is going to give us a whole bunch of additional information when we kick off our training run so basically in a nutshell creating our environment wrapping it inside our dummy vec environment then setting up our model initially so the next thing that we need to do is actually go on ahead and kick off our training so let's go ahead and write that line of code and we'll see what it looks like so that's really the last line of code that we need to write to be able to build and train a model so to do that what we've written is model dot learn so this is akin to a fit method and what it's effectively going to go on ahead and do is kick off that training run now to that we've passed through one keyword argument which is total underscore time steps and then to that we've passed through a hundred thousand so this basically means that our reinforcement learning model is going to go on ahead and try to train for a hundred thousand different steps using our lunar lander model so what we can now go ahead and do is run that cell and it's going to kick off our training run now if you wanted to you could train for a,358,0,0,nRHjymV2PX8
11,shorter amount of time or train for a longer amount of time but ideally what we want out of our model is as high possible explained variance which we'll see in a sec and as high a possible mean episode reward so let's go ahead and kick this off i'll show you what the results look like initially and then we'll let it run and we'll be right back all right so that's our model kicked off so you can see here that as our model runs we get these little bits of information and this is because we set verbose equals to one up here now as i was saying what we want out of this is as higher possible explained variance and as high of possible mean episode reward so right now i explained variance is not that great so we've got in the zero 0.000 399 and mean episode reward is currently zero so ideally we want our mean episode reward to be in the positive numbers and not negatives and explain variance to be as high as possible so ideally uh the closer to the number one we get the better it's going to be so let's go on ahead and let that run and we'll be right back alrighty so in the end what i ended up doing is pausing the training a little bit earlier as we're already getting reasonably good performance so ideally what you want to do is once your model starts performing reasonably well you don't want to over train it because sometimes what that will mean is that your model starts performing more poorly and starts,358,0,0,nRHjymV2PX8
12,to overfit the particular environment so in the end what we did is we got our explained variance to 0.831 which is pretty good and we also got our main episode reward to a 122.,45,0,0,nRHjymV2PX8
13,so this should ideally mean that we're able to successfully land our lunar rover pretty accurately so the next thing that we actually want to go and do now that we've gone and trained our model is we actually want to go on ahead and test it out and this is where the evaluate policy method comes in so what we're now going to go ahead and do is go on ahead and test out our evaluate policy method so the other thing also to note when you're running model.learn is you can also run a callback so this will allow you to go on ahead and automatically pause your training once you sort of get to the optimal level if you'd like to see a video on that by all means leave a mention in the comments below and i'll get to it so the next thing that we're going to do is now run evaluate policies so let's go ahead and write that loan code and see how model is actually performing okay so before we run that let's take a look at our code as usual so what we've gone and written is evaluate underscore policies so this is using our evaluate policy method that we brought in up here in our imports and then to that we're passing through two arguments and true keyword arguments so specifically we're passing through our model our environment which we defined earlier we're then defining how many evaluation episodes we want to run through so that this is how many chances we want to give our model to be able to prove its performance in this,358,1,1,nRHjymV2PX8
14,case we're setting that to 10 and we're specifying render equals true because we want to visualize our performance and our results so what we should effectively see is our ai or our reinforcement learning model trying to land our lunar lander and then we're running env.close to be able to close it off so let's go ahead and run this line of code and let's see our performance so again we should get the little pop-up down below and you can see our model is attempting to land outlander they go so it's landed successfully and you can see it's now more accurately adjusting each one of those boosters to be able to go on ahead and land our lander between the flag so that's two successful landings now so as you can see it's performing pretty well even though we didn't hit the elusive 0.99 explained variance and ridiculously high accuracy or ridiculously high mean episode reward you can see that we're still getting there in this case it's trying to push the booster to try to get it between the flags but in this case it's performing a lot better once it's adjusting while it's still in the sky so you can see this one's obviously performing really well it's getting quite close come on yes all right elon come on you got to come and hire us now okay pretty cool right so performing reasonably well not quite falcon heavy but we're getting there right and so again we've done this on luna lander but you could try this out obviously we had a last minute crash landing there but you can,358,1,1,nRHjymV2PX8
15,see that obviously our reinforcement learning model was performing a lot better than what we had when we were just going and running our random environment up here so now that that's all done the next thing that we want to do is go on ahead and save our model so again this is all part of a good data science workflow you want to be able to reload your model and try it out particularly if you wanted to go and deploy this into production later on so let's go ahead and save our model so to do that we can just write model dot save and then in this case just name it whatever you'd like so we'll call it acer underscore model if we now go into the same folder as our jupyter notebook you can see that it has in fact saved so this is our acer model here we can now delete our model if we try running our model you can see it's not going to work and then we can reload it so again this is all to do with being able to reuse your model later on so to do that we can just type in model.load and then copy this and we also need to pass through our environment when we're reloading it sorry acer.load so again it's our algorithm that we're going to use dot load and then the name of the model that we say so to be able to save it it's model.save to reload name of the algorithm.load so in this case this should reload our model which again we've now reloaded we'll,358,1,1,nRHjymV2PX8
16,just throw it inside of a variable called model equals and now we can actually start testing this out again so rather than using evaluate policy this time i'm going to write out a little bit of a flow which sort of closely mimics this and really ideally mimics how you might put this into production particularly if you got sensors on a particular environment so let's go ahead and write this and see how a model performs there okay so that's our last bit of code done now before we go ahead and run that let's take a look at what we've written so first up what we've gone and done is we've gone and reset our environment so whenever we're going out and testing our new code or testing out our rl model we want to reset our environment back to its base state and this is exactly what this line is doing here then what we're doing is out of that base state was capturing those observations inside of a variable called obs and so the full line is obs equals env dot reset and then we're going through and kicking off a loop so this basically just going to keep running different instances of our lunar lander environment multiple times so we'll actually have to force stop this if we want it to stop running so written while true and then with a colon and then what we're doing is we're using a model that we've trained up here and we're using the predict method to go and generate an action based on the observations from our environment space so think about,358,1,1,nRHjymV2PX8
17,this as our model receiving the inputs from our lunar lander environment then trying to predict what the best action should be to be able to land our rover successfully and out of that what we're going to get is the most appropriate type of action and the current state of our model so what we're then going to go ahead and do is apply that action to our environment which you can see there based on env dot step so we're going to our environment we're taking a step using the action that our model has just predicted and out of that we're going to get new observations rewards whether or not our particular episode is done and any additional info and then we're going to render the current frame so this will allow us to see how model is performing in real time so let's go ahead and run this and then eventually we'll have to kill it off to stop it running and we can then close it so again it should look pretty similar to our evaluate policy except this time it's just going to keep running so let's kick it off and again to open up our window and it's going to try to land our lander and again working pretty successfully you can see that it's landed and it's just going to restart so this is just a slightly different way to go on ahead and kick off your code again that's another successful landing again it's looking like it's performing reasonably well now all right and we'll stop that there and if we wanted to you can see that,358,1,1,nRHjymV2PX8
18,our little icon is still open so to close that we can just type in env.close and that will close the little pop up there so that you can then go on ahead and kick off evaluate policy or that whole loop all over again and that about wraps it up so what we've now gone and done is we've gone and installed and imported our dependency we've gone and tested out our random environment and again all this code is going to be available inside of the description below just check that out you'll be able to grab it so on step two we built and trained our model and really these four core lines were the core components to actually going ahead build our environment and train our reinforcement learning model we then went and saved and tested it out and we tested it using the evaluate policy method and we also wrote our own custom code to go on ahead and test it out and that about wraps it up thanks so much for tuning in guys hopefully you found this video useful if you did be sure to give it a thumbs up hit subscribe and tick that bell for more awesome reinforcement learning content and let me know how you went about building your own reinforcement learning model thanks again for tuning in peace,297,1,1,nRHjymV2PX8
0,hello everyone and welcome to the reinforcement learning tutorials in this video i will provide you an introduction to the open ai gym python library openai gym is a very powerful library for simulating and visualizing the performance of reinforcement learning algorithms before i start i would like to mention the following it took me a significant amount of time energy and planning to create this video consequently i kindly ask you to press the like and subscribe buttons thank you very much okay so let's start the first step is to explain you how to install the openai gym library and i will explain you how to do that in anaconda however if you're using some other python environment or if you're using a simple command line the procedure is very similar so i will click here on environments and i will click over here on base root and i will click on open terminal then i need to install two things and this is what i need to install first i need to install my open eye ai gym library and i'll simply type this command line and execute it secondly i need to install this module okay we are all set so let us close this window minimize this screen and let us run our spider is a very powerful python development environment i like it very much since it reminds me of matlab for example in spider i can simply select a piece of code do the right click and evaluate code lines as i mentioned previously open ai gym is a very powerful library for simulating and visualizing the performance of,358,0,0,Vrro7W7iW2w
1,reinforcement learning algorithms it contains many simulation environments and some of them are given over here for example you have a classical computer game environment then you have other environments you for example have classic control environment that i like very much since i have a background in control engineering and control theory you have a mountain car pendulum inverted pendulum etc however in this video we will not use these environments since they're a little bit complex for people who is just starting with reinforcement learning instead we will use this environment called frozen lake environment i like this environment very much because it's very simple and still complex enough to illustrate the basic ideas of reinforcement learning equal we briefly explain the frozen lake environment basically the frozen lake environment contains 16 fields these fields are enumerated starting from zero and ending at 15.,189,0,0,Vrro7W7iW2w
2,the frozen lake is partitioned into 4x4 grid so we have four rows and we have four columns this is one field this is the second field this is the third field etc s means that the field is the starting field so we start our gain from here from the field zero fields denoted by f are actually frozen fields and we can safely step on these fields the fields denoted by the letter h are whole fields if you step on the whole field we will fall down and we will end our game then we have another field and this field denoted by the letter g is the gold field so our goal is to find the path starting from the starting field or s and going through frozen fields we need to find a way to our goal field in this case if i start over here and if i go like this i will end up in the hall and i will not complete my game that is i will lose now we can only step left right up and down so if you are over here we can only step either in this direction or in this direction if you try to step in this direction you will not be able since this is the edge over here and over here so if you are over here we can step in this direction we can step in this direction we can step in this direction or we can step in this direction if you step in this direction we will reach our hole and the game will be finished so,358,1,1,Vrro7W7iW2w
3,again the main purpose of this game is to find a path that will lead you from the start field to the gold field by only stepping on the frozen fields the fields denoted by age that is the whole fields and the goal field are so-called terminal state fields since the game will terminate if the either step on the whole or if we reach the goal in the case of stepping on the whole the game ends prem prematurely and we lose in the case of reaching the goal state within here we need to mention one very important property of the frozen lake environment namely the frozen lake environment is a completely stochastic environment this means that certain actions will not not necessarily lead us to desired goals this means that there is so-called transition probability associated with every initial state and every action in that initial state for example let us assume that we are over here that is in the state six that is frozen field and let us assume that we apply this action that is we want to go down in a purely deterministic environment we will simply go to this state over here however since this is not a deterministic environment we can for example and in this state even by applying this action in the state number six and there is a certain probability associated with this event so let us explain this very important property of the frozen lake environment in more details so let us imagine that we are over here and that we apply the action down so we will right over here our,358,1,1,Vrro7W7iW2w
4,initial state action down in ideal case if the environment is completely deterministic we will go to the state 10.,26,1,1,Vrro7W7iW2w
5,however there is a certain probability of reaching state 10 from state 6 and by applying the action down and this probability is p1 now if you're at a state 6 and we apply the action down there is a certain probability that we might end in the state number seven this state similarly if we are the state six and we apply action down there is a certain probability that we might end over here that we might end in the state number five p1 for example can be 1 over 3 p2 can be 1 over 3 and p3 shouldn't be independent from these two probabilities since p1 plus p2 plus p3 that is the corresponding probabilities should sum to one the probability is p1 p2 and p3 are called the transition probabilities or the state transition probabilities and they're very important for the design of reinforcement learning algorithms to sew to summarize if we are at the state six and if we apply action down there is a probability of 1 over 3 of reaching this state 10.,235,2,2,Vrro7W7iW2w
6,similarly we are at the state six and we apply action down there is a probability of 1 over 3 of reaching this state or the probability of 1 over 3 of reaching this state okay so this also corresponds to reality so imagine you're sitting on a frozen lake and since the floor is pretty much slippery and your ground is very slippery by applying a certain action you will not necessarily move in the desired direction for example you might slide over here or you might slide over here now that we have a basic understanding of our frozen lake environment let us go back to our python code so here is our python code the first step is to import the library so we import the gym library over here then we create our environment here we specify the environment name it is frozen lake version one there is also version zero that's obsolete so don't use that version and here we specify the render mode and we specify the human render mode then we need to reset on our environment and finally we need to render our environment so let us execute these code lines and let's see what will happen okay so we don't see anything over here we should be able to see over here or we should expect to see our environment so where is our environment now notice over here that i have a window so if i click on this window here's my frozen link here we are finding hat and here's our lake this is the hole this is the hole this is the hole this,358,3,3,Vrro7W7iW2w
7,is the call this is our price and this is again our haul next let us investigate and gather more information about our environment to access the observation space that is the dimensions of the observation space we can simply write this command or this code line if you obtain the output discrete 16 this means that we have 16 states the states start from 0 and and at 15 following the python convention where indices start from zero and this is our observation space so the observation space consists of all the states that we can observe or the states of our current gain so if we are over here the state will be one if you're over here the state or the observation will be 9.,166,3,3,Vrro7W7iW2w
8,next we can obtain more information about the action space the action space corresponds to the possible actions so if you are over here we can perform this action we can perform this section and we can perform this action if you're over here we have down action right up and left and this is in correspondence with the action space so the action space consists of the four elements and the actions are coded like this left is zero down is one right is two and up is three next we will generate a random action we can generate a random action by simply executing environment.action space.sample so let's see what happens over here as the result we obtained number three here we are basically drawing numbers randomly from a set 0 1 2 and 3. let's do it again and let's see the result now our random action is again 3.,198,4,5,Vrro7W7iW2w
9,and number three corresponds to up action okay so let us apply this random action to our environment we can apply this random action by simply typing in via environment dot step and we need to specify our random action now while executing this code line we should look our character and look look at our character and we should see if he moves left right up or down okay so he stays at his position now let us analyze the output argument of this function step let's see what happens okay so we obtain one two three four five output arguments and these arguments should be interpreted as follows the first argument is the observation that is that's our state after applying the action in our case we stay at the states zero so nothing changes because we were in state 0 we apply up we stay in this state the next argument is the reward of course the reward is not defined like this i'm just interpreting the results what is a reward in reinforcement learning setup every action from certain state will give us certain reward and these rewards can be randomly defined or they can be user defined for example if you're over here and if we step over here we should obtain a certain reward this reward can be negative positive or it can be zero generally this p is speaking more favorable the actions are we should obtain a better reward or a higher value of reward what is a favorable action or what is a favorable step the favorable step or more favorable step is a step that will,358,6,6,Vrro7W7iW2w
10,that will lead us closer to our goal however in the frozen lake environment all the rewards are mapped like this so the rewards or the reward by stepping at the frozen field easier the reward that we obtained by stepping at the whole is also zero and the reward obtained by stepping at the goal field is one rewards are very important in reinforcement learning because the whole purpose of the reinforcement learning algorithm is to design a policy or a set of actions that will maximize the expected number or the expected sum of rewards obtained from a certain state until the final state that's why the rewards are very important however in this video we will not dig deeper and we will not explain reinforcement learning algorithms the main purpose of this app this video is to explain you the open ai gym simulation environment okay so let's go back to our python code and let us analyze this return value as i mentioned previously the first output argument or the output value is our state or observation since we return to our initial state we obtain here zero the next return value is the reward again since our field over here is a frozen field we obtain a reward of zero the third output argument is a boolean variable that's kind of denotes if our state is a terminal state or an intermediate state we obtained false over here meaning that our state is not a terminal state remember that the terminal states are either holes or our goal state the fourth argument is not important for this video and i will,358,6,6,Vrro7W7iW2w
11,simply skip it and the fifth argument is a simple transition probability this means there was basically a chance of one over three to go back to this field from the original field by applying the action that's in in our case is random action and the value of the random action is zero so there is one over three probability of going to field zero from the field 0 by applying zero action or by applying the left action okay let us now try to apply a deterministic step so let us say that i want to go down that is i need to apply to this function one i need to write environment.step one so let's see what will happen over here and while doing that i need to activate my epic pen since this epic pen is very useful for sketching or drawing on my screen okay now you have to keep in mind and you need to watch this part of the screen while executing this command line and let's see what will happen okay so we went down we were lucky actually since by applying the action down from our initial state we will not necessarily end in this state so let us analyze the return value and let's see what will happen okay so the state is 4 correct since this is the state number four remember that states are numbered like this 0 1 2 3 and this is the fourth state the reward is zero since we don't get any reward by stepping on the frozen field this is not a terminal state right the state is not,358,6,6,Vrro7W7iW2w
12,terminal this argument is not important and this is the probability okay so let us step once more from this state and let's see what will happen observe this part over here good we are still heading in a good direction okay so we you're going down so let us apply again this step and let's see what will happen aha something strange happens over here or not so strange what did we do we were basically over here and then we said we want to go down however since our environment is completely stochastic we didn't go here instead we went right and let us analyze the return value and let's see what happens okay so our final state is the state number nine this is the state this is the state number nine no reward since this is a frozen field it is neither terminal state and probability of performing or the probability of reaching this state from the state over here and by applying the action down is 1 over 3.,226,6,6,Vrro7W7iW2w
13,so this small example and the small simulation completely illustrates and convinces you that this whole environment is stochastic next let us reset the environment and return to our initial position simply execute this code line and you can see over here that we went from here to here we return to our initial position okay let me now try to explain you transition probabilities transition probabilities are very important for understanding the nature of the system and they're very important for designing reinforcement learning algorithms we can access all the transition probabilities by typing environment dot p standing from for probability the first argument over here is the state and the second argument is the action this means that we are trying to see all the probabilities or transition probability probabilities associated with state 0 that is the state over here and the action one and if you go back and if you see what is action one the action one is down so let us see what are the transition probabilities okay here's the result and here's the interpretation the first value over here is transition probability the second value is our final state this is the reward and this boolean variable corresponds to terminal state if the state the state over here is a terminal state then we will obtain true here since the state zero is not a terminal state we obtain basically false over here now this probability should be been through period as follow follows this probability is the probability of being at state zero applying action down and staying at the state zero since this part over here or,358,7,7,Vrro7W7iW2w
14,this variable over here is the final state similarly this row has the following interpretation this is the transition probability this is the final state this is the state for this is the reward and this is a boolean variable denoting terminal state the interpretation is the following one there is a probability of 1 over 3 off reaching state four that is reaching this state starting from state 0 and by applying the action down and in a similar manner you can access and interpret all all other probabilities for example if we type here five one let's see what will happen here is the result aha something very interesting happens over here so let us go back and let us see our state five our state five is a whole uh-huh okay this means that there is a probability of one that is one hundred percent of probability of reaching state five this is the state five while being at state 5 and by applying the action one down this is very natural since we reached the whole we cannot escape the hall so if you apply the action down in the hole will stay in the hole that is our final stage will be five and then our initial state is 5.,279,7,7,Vrro7W7iW2w
15,next i will explain you how to generate an episode and how to simulate an episode in python however let us go back to our original graph over here and let us explain what is an episode basically an episode is a sequence of actions and corresponding fields that start from s and that and at any of the terminal states for example this can be an episode we are in this state we apply action down we go to state number four then we basically apply any of the actions down right however there is certain probability that that we will reach over here and if it really happens that we reach the hole this would be one episode let us illustrate another episode so the episode can also be this episode and this is a successful or the winning episode since we start at the initial state we go like this and we end at our terminal state and in this case the terminal state is the goal state okay so let us try to simulate this case that is let us try to simulate a complete stochastic process and the stochastic nature of our frozen lake environment so to do that we first need to close our environment don't forget to do that and let us erase our workspace here i prepared another code that illustrates how to generate an episode so what do i do over here i perform the steps that i explained previously basically import the gym i import time in order to pause my code then i invoke or create my environment i reset my environment and i render,358,8,8,Vrro7W7iW2w
16,my environment so let's do that and let's see the outcome okay so our screen is hidden over here and let us continue next i will generate number of iterations so i'm generate 30 simulations and i will iterate over i where i goes from 0 to number of iterations in every step of this for loop i will generate a random action then i will apply this action to our system or to our environment then i will render however you don't even need to render here since the code will automatically render and update the screen over here here i print what's happening i print the iteration and random action then i wait for two seconds and if return value is of 2 that is if my boolean variable denotes that my state is a terminal state i will break the simulation and then finally i will close my environment so let's see what happens over here observe the screen over here or this window and here it is okay iteration one action zero iteration 2 action is two you see that we are going down let's see what happens first aha we reached our called okay so we can reset our environment by typing environment.reset and let us execute the game from the beginning we should obtain a different episode again we execute and let's see what happens okay we are still there we went down we went down again then we are still in our state let's see what happens we are going right you're going back you're going right you're going down you're going right and let's see what will happen,358,8,8,Vrro7W7iW2w
17,over here will we reach our goal state no we are diverging we're diverging because we are not following any optimal policy or our reinforcement learning algorithm is not being applied to this problem consequently you can see over here that we came back to our initial state and let us see what will happen further again we are going to the initial state so nothing happens over here and probably there is a huge chance that we will crash over here and we crash that is we reach the whole state and you can see the random action or actually the return value in the last iteration and you can see basically that our final state is the state number five this is the state and this is indeed a terminal state again you can reset the game by typing environment.reset and you can play the game again okay that would be all for today i hope that you like this video if you like these the videos i create please subscribe and support my channel thank you very much and have a nice day,241,8,8,Vrro7W7iW2w
0,hello everybody welcome back to another video today we're going to be talking about reinforcement learning i'm really excited to talk about this topic so let's not waste any time and get right into it this video is going to be split up into two different sections the theory that will be in this powerpoint that i'll explain briefly and then the code which is the hard implementation with a visual demo that's super cool so if you're familiar with the theory or just want to stick around for the demo you can skip to the demo all the time stamps are down below free feel feel free to skip around alright so in machine learning we have three real sects we have supervised learning unsupervised learning and reinforcement learning supervised learning is learning the function mapping from some data x to some target value y unsupervised learning is being presented with x and y y having no labels and learning to identify patterns within the data so this is looking at some data and identifying patterns this might be clustering or looking at the different correlations between the data while supervised learning is like regression trying to find a line of best fit reinforcement learning that data is a little bit different let's talk about how the data is represented in reinforcement learning so in reinforcement learning you have an agent that interacts with some environment now the environment sends some state to the agent and i'll explain all of this terminology again in a moment the agent takes some action onto the environment based on the state it was given by the environment and,358,0,0,Uc6qBg7mM2Y
1,then based on the action the agent performed in some state it receives a reward so really let's refactor this diagram and these are the main definitions the agent is the decision maker the environment is the world where the agent learns and performs actions the action the agent performs is some process the agent can perform within the environment and we'll provide an example a reward is for every action there is a corresponding reward and just a state is a current situation of the agent within the environment so again the agent interacts with the environment the environment sends some state to the agent the agent takes some action based on that state and receives a corresponding reward now let's think of an example that illustrates this like in real life so a really simple example would be a person with a stick throwing to a dog now the dog is the agent and the environment is the person with the stick in some grassy field okay so the states in this case are sitting running after the bone and stick retrieved the actions in this environment are do nothing run and retrieve stick now notice how the states describe the environment and the actions describe the agent the agent is doing nothing the agent is running the agent is retrieving the stick while in the state nothing's happening basically sitting running after bone stick retrieved that's describing the entire environment not just the dog now the rewards is the dog receives a bone after the stick is retrieved so let's review this one last time states describe the entire environment what's going on,358,0,0,Uc6qBg7mM2Y
2,so the dog is sitting nothing is happening the dog is running after the bone the stick has been thrown and the stick has been retrieved and it is going the dog is going back to the owner now another example is atari games so the objective of an atari game is to achieve the highest possible score the possible actions are up down left and right assuming we're playing some of the more simple atari games the state is the pixel inputs of the game so actually what graphically is on the screen and the reward is just plus minus in the score so the score goes up it's a higher reward as score goes down it's a lower reward so these are two fundamental examples of reinforcement learning that can be implemented in real life now the question rises how do we mathematically formulate a reinforcement learning problem now that we know what a reinforcement learning problem looks like in real life so all reinforcement learning process problems are markov decision processes let's look at this markov decision process so we have two states don't understand and understand and two possible actions study and don't study let's say before a test you don't understand a topic and you decide to study well the probability that you don't understand is point two as you see above the study action the probability that you do understand that topic is 0.8 all right and the reward if you do understand the topic is 1 and the reward if you don't understand the topic is negative 1.,344,0,0,Uc6qBg7mM2Y
3,now if you don't understand and you don't do anything you don't study likelihood that you won't understand is one because if you don't study and you don't know something there's no way of you knowing it it's not just going to pop in your head so this is like a simple markov decision process and every reinforcement learning is a manifestation of this structure so now the question arises okay we had these rewards negative one one zero but we might not want to have rewards 0 1 2 3 4 we might want to calculate them in a certain way so the total reward number is the sum of all immediate rewards we get for taking actions within our environment so that's just an r total number the sum of all immediate rewards we get but there are two issues with this formulation the first one is rewards can go to infinity and we're trying to maximize the amount of immediate rewards we can receive and number two immediate awards and future rewards are weighted the same but you want to kind of be greedy towards immediate rewards because you're chasing over something that will give you a reward instantly rather than someone something that will give you a reward in the future so the way we solve this problem is we have some variable gamma that looks like the squiggly y you can see on the left here and gamma is called a discount factor it essentially makes future rewards worth slightly less than immediate rewards so previously we had our formulation where r total equals rt plus rt plus one and t,358,1,1,Uc6qBg7mM2Y
4,is just the time step so we're just adding all of our media rewards and r to our new r total the one on the right uses the gamma discount factor now i made a mistake here it should be squared so for every next time step the power of gamma increases by 1 to decrease the value of future rewards so now we actually need an equation that tells us what action to take in a given state we know how to calculate rewards we have a basic understanding of the theory but now given a state by the environment to the agent how will the agent know what action to take and this is where policy and the queue function comes into play so a policy is a function that decides what action to take in what state and we want to find a policy that maximizes reward now q function or the q function defines the value of taking action a in state s under some policy pi and the the most basic way of formulating this is let's say we had a table where on the left we had some states and on the top we had some actions and they had a corresponding q value and this q value tells us the value of that action within the given state how valuable is it like will it yield a higher reward will it uh bring us closer to the terminal state to the state at which we want to end at all right and we create this table and we'll be able to use this table to say okay i'm in,358,1,1,Uc6qBg7mM2Y
5,state one what action that maximizes the q value or the reward in this state so now we come to the epsilon greedy q learning algorithm and this is the algorithm we're going to be implementing today now what does epsilon mean all right so epsilon basically defines the balance between exploration and exploitation so we're going to have to find the given rewards and q values for every action state pair remember we have these pairs action state pairs we're going to have to find that q value and the only way to find that q value is to explore in the environment do a bunch of different actions and see what happens that's called exploration exploitation is looking at the table and finding the highest q value and determining okay i'm going to do the action that yields the highest q value and now you're probably asking why don't we exploit every time why don't we just find use the highest q value in the state tape in the action state pair that will yield us the highest reward every time well our q table is empty when we initialize the environment we need to explore the environment fill out the q table and then at a certain point start exploiting our q table but until we haven't explored we can't exploit anything because that q table is empty so epsilon greedy essentially has some variable epsilon that is a number that tells us how much we should explore versus how much we should exploit and once we start filling the q table that number goes lower and lower and lower and tells us,358,1,1,Uc6qBg7mM2Y
6,to exploit more and more and more now the point of this tutorial is to give you a very very light introduction to reinforcement learning there's a lot more math behind the epsilon greedy q learning algorithm and a lot more about bellman the bellman equation there's just a lot of math involved but i really want to give you a light introduction so i've omitted a lot of that now that we've reviewed the principles and kind of the theory of reinforcement learning i feel fully equipped to teach you guys how to implement it in code and so the problem we're going to be attacking today is the carpool problem it's a very classic and famous problem in reinforcement learning basically what we have to do is teach a cart to be able to balance some pole so the pole has to stand upright and the card has to move at some velocity or some speed to keep the pole within some range theta and it's a pretty complex problem but once we actually implement it in code and see how our algorithm is improving and improving and improving over time it's going to be really really cool to see the results so let's get right into the code believe it or not for today's demo we only need four libraries so we're going to need numpy obviously i think you predicted that all right numpy's np we're going to need gym time and math let me make this bigger all right so the way you can install these libraries if you just press the blue blue plus button you press that and then,358,1,1,Uc6qBg7mM2Y
7,go to terminal all you have to do is you're going to mainly want to install a gym you might have uh math and time pre-installed just do pip install jam and i already installed it so it'll save requirement already satisfied but for you i'll say gem and then some loading bar so if you have any if you have any issues installing gym just let me know and also install numpy so same process pip install numpy and math and time should be pre-installed but if they're not you can install those is everyone good on installing all those libraries good great okay so now let's take a look at what our environment looks like this is where the cool stuff starts so we'll define our environment as env and we'll use the function gem.make and the name of our environment is cart oops d1 all right so the first thing we want to do is understand what our action space is validated our action space is just the amount of actions we can take so action space up and just to know it's okay we have an amount of action so if we print that out let me just import this first if we print that out that will be two so remember we discussed two possible actions left or right so everyone got on that yeah what's up gem library is going to be for our visualization so that's the invite that's the library that imports all the environment uh related stuff so you'll see that there's going to be like a lot of visualization here and that's what the gym um,358,1,1,Uc6qBg7mM2Y
8,environment does and believe it or not the people who developed these reinforcement learning libraries are from open ai i don't know if you guys are familiar with open air it's a really famous company uh that deals with ai solutions and made gpt3 codecs a really a bunch of really really cool stuff all right so now uh let's kind of experiment with like what our environment might look like so i'm gonna go to this file here let me scroll up i'm going to take this code here this is just example code you don't have to copy it i just want to show you what the environment looks like um so this these are commands that you're going to have to make yourself familiar with observation observation is going to tell us the action that we're currently performing the environment the state the reward whether we're done or not and also some other important information about how our environment is reacting reacting to some actions okay so this is just an example of how we can test the environment so to render the environment just env.render as we defined it previously environment and then action this is just taking some random action action space some random action i encourage you to copy this code because you're going to be able to i want you to be able to visualize this on your own computer and then our reward and whether our program is done or not basically updates based on what action we take we print our observation to reward whether we're done on some people about the environment if we're done we,358,1,1,Uc6qBg7mM2Y
9,print our program has finished after x time steps okay music so this yields a pretty cool visualization and will get you familiar with what our environment looks like so take some time to copy this down and then we'll get to the algorithm itself looks like we have something in the chat can you be closer to the laptop it's difficult to hear your voice when you're away yes unable to find blue the blue plus sign should be on the top left part of the page ah sorry about that um anyone have a windows computer who's running this right now you see the blue plus sign right it's at the top left i think someone's having an issue with that um let me see but this person's having trouble accessing the terminal make sure you have uh ravi make sure you have that tab pulled up on the left side you might have to drag it out so has anyone run this code yet if you run the code you get this really cool visualization this is what our problem looks like yeah make sure to uh pip install pi i forgot too so it should just be like this pip install high glut it's a dependency for gym okay so no you don't have to import it so if you run this restart the kernel if you run this you're going to get a super cool visualization and these numbers here let's talk about what these represent these are states so the first state is the position of the cart the second state is the velocity of the car the third state,358,1,1,Uc6qBg7mM2Y
10,is the angle of the pole and the fourth state is the angular velocity of people all right and there are certain ranges in which these fall so like from here to here okay um can i see what yours looks like and right you got it uh so restart the kernel again so it's yeah it just starts swinging because you're not actually doing anything in your environment you're just initializing it at some random point and letting the pool just swing and swing you're not actually doing anything anyone having trouble visualizing that getting that ready you guys good you got it good good okay music yeah that's just the name of the environment like are you referring to gym.maine carpool yeah right so yeah so um open ai gym has different environments um so it has like a mountain car environment a cardboard environment um there's just a bunch of different environments we just specifically uh chose this one it's a simple environment to deal with reinforcement learning and visualization is pretty easy but they're environments with more states more actions that i encourage you after we finish this go explore those and try to retrofit the q learning algorithm we're about to build to different problems you'll see that there's some there's some pieces of code that we have to write that are specific to this specifically the amount of actions and states that we have in this so remember four states two actions let me review the states one more time card position card velocity full angle and full angular velocity one two three four this is the reward this is,358,1,1,Uc6qBg7mM2Y
11,whether the game is done or not right now let's get into the implementation of the algorithm beforehand again does anyone have any questions i just want to make sure okay let's get into it all right so there are going to be a lot of variables that we're defining initially here that we're going to be using a little bit later i'll give you a brief introduction into what each variable means and then you'll see how we implement them a little bit later all right so first we have a learning rate define that as lr and just set that to 0.1 that's just a standardized value and machine learning learning rates are usually set to 1.1.01 music we just chose 0.1 it's an arbitrary choice you can adjust learning rate based on uh what you see in the results it's just a choice right now so we'll define the learning rate as basically the size of uh of step we take in adjusting our actions all right so the learning rate is the size of step we take in adjusting our actions and that corresponds to how we adjust our q table so the next variable we need to define is our discount factor does anyone remember the name of that variable the little squiggly y that's right gamma and we'll define that as 0.95 all right and this discount factor just tells us like the importance of immediate rewards versus uh future reports the specific algorithm we're implementing is called an epsilon greedy pro policy because it's greedy towards immediate rewards versus future rewards um all right so now there's some other,358,1,1,Uc6qBg7mM2Y
12,basic variables that we need to define so epochs is traditionally how many iterations something runs we'll say 60 000 iterations total time we'll define a zero because we're going to be needing total time to measure uh basically how long it takes for the cardboard to solve the problem or how long a certain action takes we also need a variable called total reward so we can calculate the reward for each time step we'll set that to zero as well and then we need a variable called previous reward so we can compare our firstly calculated reward and our previously rewarded see the discrepancy so yeah if you want to follow along with them just because you can't wait so yeah if you need any help i can come help you while everyone is coding a little bit later all right so those are some basic variables we need again epochs the amount of iterations you have 60 thousand total total reward previous reward pretty self-explanatory all right now we're gonna get into something that's a little bit more complicated we'll define a variable called observation and right now it's just a list with four inputs um can someone give me a guess like what this might correspond to yeah the different states right so whole full velocity full handle stuff like that and then we need another array or yeah array called step size all right and i'll explain what the function of this is it's a little abstract to explain oops it's a little abstract to explain but i'll try my best to explain it so observation and step sides define that,358,1,1,Uc6qBg7mM2Y
13,list and that array and then we need to finally define our epsilon value and our epsilon music ek val we'll set that to 0.9995 all right what is epsilon let's remind us it's associated um with how random an action is all right and then exploration is basically decaying and we will get to a point where um we're only splitting right so epsilon basically tells us how random action is exploited so it defines like whether we're exploring whether we're doing random actions or whether we're exploiting whether we're doing um actions that are most valuable to us and this decay value will tell us how quickly x along the case how quickly epsilon decreases and there's a whole another formula for that um so the next part of the code is not as intuitive but before we do that let's just run this little block of code it's really important that you run each block of code because you might write a bunch of code and if you don't run it it's going to give you errors and you're like and do anything wrong so make sure you have all this written down do you guys have any questions about the variables yeah sure what do you need to see uh this what do you need to see this all right i'll go back to that the previous part before you want to take a picture by the way once you're done with this this is uploaded on github so you guys can check it out on github if you want um yeah do you want to take a picture of that okay,358,1,1,Uc6qBg7mM2Y
14,now let me go back to the uh definitions part of the code so um is everyone done with this part i'm gonna need um well there's a equation that we're going to get into that uses these two variables to calculate u a new epsilon value this is really i'll explain it a little bit okay so now let's initialize our q table so i'll create a variable called q table okay and we're going to use a function called dot np.random.uniform let me explain what this does np randomly uniform there's going to be arguments here it basically gives us values from from a uniform distribution all right random values from a uniform distribution between 0 and one we need to use this just to randomly initialize their like values in our q table we're not interested in initializing like all zeros or all one table we just want completely random values right all right so did you guys get that part or do you want me to say it again yeah so um np.random.uniform is a method from numpy that allows us to grab random numbers from a normal distribution um between zero and one and so there are a couple of uh arguments here so because it's between zero and one our low variable will be be defined as zero our high variable will be defined as one and then the size of our q table will be observation plus env dot action space dot n all right it's not too intuitive but the basic idea is let me just get the parentheses right the basic idea is we want to get,358,1,1,Uc6qBg7mM2Y
15,random values in this q table and we want to initialize this q table to the length of the observations which is the different states and the width of the action space all right so length of the observation lace length of the observation width of the action space all right hopefully you guys um and if we hold on if we print this we'll see that we'll get a bunch of literally random numbers all right random numbers between 0 and 1 and this array format and the arrays are too large but they're of size four by two okay so we've initialized our q table is everyone caught up till this point great now we're getting the exciting stuff get hyped let's get it all right now the last boring thing we have to do is discretize our state so right now the format that open ai gym gives us our state is not something that we can manipulate it's going to give us a box format so the box format is just a different format that openai gym provides for displaying state spaces we need to change that box format to a discrete format and that discrete format will use to import input into our q learning algorithm so we have to make some method called uh discrete state all right and our inputs are just some state and then this is just a one-liner discrete state you can you can define the variable however you want equals state divided by step size summary and then we just want to return why are we doing this we're trying to change the type of,358,1,1,Uc6qBg7mM2Y
16,our state to a discrete state and essentially i just misspelled as type essentially we want to return a tuple with four values in the format that openai jim likes to process all right and that's how we do that i don't want to get too in depth with this because we have a little bit more complex stuff to get to but basically what we're doing is we're taking the format of the state that we get from the environment and converting it into something that open ai gym likes to deal with rather than this because if you don't uh convert the state to discrete then you'll get an error in the code okay let me see i saw a question from zoom see what we got here uh yeah so step size will be used a little bit later uh step size will be used um in relation to learning ray um and how how uh basically how much we adjust our q values you'll see that in a minute okay so now we are getting to our algorithm all right so i'm gonna comment everything for you guys so you understand what each part means let me sit down here all right so first we're gonna do some iterations so we're gonna say for epoch in range epochs plus one remember that epochs we had 60 000 epochs and here we're just iterating through music all right first thing we want to do is take the time all right so we have some we'll define some variable time initial we'll say t initial equals time dot time make sure you have that,358,1,1,Uc6qBg7mM2Y
17,library imported it's really important to use time.time because that way we're going to measure the length of each action right the time length of each action all right then we're going to have to use discrete state the method that we just uh used so i'll say discrete state this is how we get our state all right so let me just say this here all right so we're getting the discrete state for the restarting environment so we know what's going on so we're restarting the environment where we say okay yes the environment is completely clean what's going on what's the discrete state of this environment okay now we need some uh variable some boolean done we'll set that to false if you guys have ever worked with games you'll know you always need some boolean so that if some action triggers the quit uh functionality you'll just exit that while loop and the game will end all right so this is just uh some control boolean all right we'll be taking advantages a little bit later and then we have a variable called epoch reward so for every epoch for every iteration we have some reward that we're going to be adding to all right and so um for every 1 000 epochs we are going to be printing out some string all right so epoc modulo 1000 so equals zero will print episode um so basically we're going off the number that each episode is a thousand epochs an episode is just us saying epochs are the amount of iterations we're doing but episodes are a thousand multiples of epochs so,358,1,1,Uc6qBg7mM2Y
18,episode plus string talk all right any questions still now i'm just on some basic coding here um if you guys uh don't understand some of the control structures here i'd love to explain them if you have any questions good okay so now we're getting into the real stuff oh chat question can you explain the last epoch oh the print statement yeah so we're just basically taking the amount of epochs we have and dividing them into bite-sized pieces because we're not going to measure the reward or the time for every action in every epoch we're only measuring the reward and time for every action for a set of epochs for every thousand epochs for example all right because printing that out over and over again would be too tedious all right so now we're going to start the game loop so we'll say while not so well done equals false that's also a possibility if np let's indent that if np dot random dot random some random number is greater than epsilon here comes our epsilon greedy stuff if some random number is greater than epsilon then we'll take the arg max of our q table with our state okay discrete state what are we trying to find here we're trying to find the max the maximum reward for some given state so we're going through all of the actions and saying what action has the maximum reward are you able to see that should i move it for you let me move this right scroll it up there we go better okay so we're trying to find for some given state,358,1,1,Uc6qBg7mM2Y
19,what is the maximum reward for a given action and we we assign that to an action right and then if that random number is not greater than epsilon then we'll say action equals some random action now we know our action space is too but maybe it's variable and we want this code to be reusable so we'll say action space dot n we really know that action space dot n is two but we can just say random zero to two okay so here we're just deciding if some random number is greater than epsilon we'll define our action as regular action that's the maximum uh action for some given state this is exploitation this is exploration let me let me denote that for you guys so you kind of understand that this is exploitation finding the largest reward and this exploration all right some random action okay now what we want to do is we want to update the environment right do you have a question yeah okay so q table of the script is going to give us just this argument right this argument is going to give us a list of actions then we want to look through the list and choose the maximum reward and the maximum reward will correspond to the best action right good yeah okay and then this is just some random action and we're using numpy uh basically numpy methods these are really useful for array manipulation space manipulations anything numpad is super useful now we're going to update the environment oh we have a question here why are we doing numpy dot random dot random,358,1,1,Uc6qBg7mM2Y
20,epsilon the reason why we're doing this is we're just saying is we're just saying is some random number greater than epsilon and if it's greater than epsilon take the arg max it's just some arbitrary way it's just some way to determine whether we're doing a random action or a discretely like motivated action based on the reward the largest reward using epsilon so we're taking advantage of epsilon to randomize some number compared to epsilon and if it is greater then we take the max and if it's not we just do some random action now we update the environment now we update the environment so we'll say new state reward done equals environment.step or action i want you to recall i'm sorry i just moved in the middle but i want you to recall to back here how we how environment.step some action returns four variables observation reward done and info okay we're not really concerned about info so that's why we said this all right we're not actually going to call on it so the variable name doesn't matter so that's why we had those four four inputs to one uh to one method call so environment dot step action environment dot step means take x action all right so there are two actions let's take this action and assign it to our new state or reward and whether we're done or not all right and this is going to be able to update the new state calculate some reward tell us that the game is still going on all right here's where it gets interesting so we are going to say,358,1,1,Uc6qBg7mM2Y
21,epoch reward we're gonna now update the reward epoch reward plus equals reward we're gonna define our new discrete state and we'll just use our method discrete state new state okay now our state has been updated let me scroll up for you guys just tell me to scroll up yeah um now our state has been updated our reward has been updated now what we want to do is render the environment so we'll use the same if statement as before but instead of renting out episode is whatever we want to render the environment okay so we'll do environment dot render okay and this just initializes the visualization for this epoch all right is this all understood you have any questions about this guys let me check the chat any questions yeah what's up sure so the real point of discrete state is to take the box type the box type is the type of the state that we get as our input and do some calculation to convert it to the discrete type all right so discrete type and box type these are types that are coded into openaiden's library we need to do some manipulation to our state to make it uh basically usable within our code because we don't do that manipulation we can't access the values within the state list so this way this allows us to be able to access the number values within the list of states i know it's a little confusing but if you go to openi openai gyms documentation there's a lot more information about box types discrete types and stuff like that all right,358,1,1,Uc6qBg7mM2Y
22,so now we render an environment great question by the way um we render our environment the only thing that we need to do now is just update our q table right so we're going to say if not done again if the game is still running we'll have some max new queue all right and we'll say np.max q table our new discrete state all right the purpose of this is to get a new max action all right and then we need our current queue which is just our q table with our discrete state that's our action space all right and i'll explain all this in a minute i just want to get all this code done so we can explain fully and then our new q value 1 minus our learning rate multiplied by our current queue plus our learning rate multiplied by our reward plus gamma times max mu all right and then what we'll do is we'll finally update our q table and we'll say q table discrete state plus action okay this is a little confusing but i want to go back to the pseudocode let me make sure that everyone is done writing this let me go back to the pseudocode that we reviewed previously so if we go to our powerpoint presentation and we go to the pseudo code really what we're focusing on is this little part we're selecting an action take the action observe the reward in the next state and we're updating our q table for some state in action using this formula all right this variable is the learning rate this is a,358,1,1,Uc6qBg7mM2Y
23,reward this is gamma we're taking the max of q and some new state a and subtracting it from our current state from our current q value with state s and action a all right so this is the way we update our q value in our q table using this specific equation so if you look at the code you wrote it's not going to be exactly the same but it's going to be pretty similar to what this looks like all right and the final thing that we didn't do is we need to set our state to our new state all right so the way we do that is oops i shared the wrong thing let me share this the way we update our state as we say discrete state equals nu now we define new discrete state right here so we're just updating our state all right so what we just did was we calculated a bunch of variables so we could actually input that into the equation that allows us to update our q value all right and you can see here that we're only updating one q value so we did all of these calculations we updated our q value we updated our state the only thing left to do is deal with epsilon okay so how does epsilon play into this whole equation so really what we just did here we look back at the pseudo code we saw it's kind of corresponding in some way the calculations must be a little different but really let's break this down one more time one minus the learning rate multiplied by,358,1,1,Uc6qBg7mM2Y
24,the current q value plus the learning rate times the reward plus gamma times the max nu q now remember we introduced gamma because we wanted to discount our future rewards all right and that's what we're doing here we're multiplying gamma by our future reward and adding it to our current reward which really corresponds to our original equation which was r t plus one plus gamma times r t plus two plus gamma squared times r t plus three and so on and so on and so on so now how does epsilon play in the equation let's get to that so we'll start with an if statement if epsilon is greater than .05 and then if our epoch reward is greater than our previous reward so pre-reward that's how we can find it right and our epoch is greater than 10 000.,188,1,1,Uc6qBg7mM2Y
25,all right then we'll recalculate epsilon using our epsilon decay value so math.pow allows you to do exponents and then we'll just do epoch minus thousand all right uh yeah you can um you you can technically but math.pow allows you to just input variables a little bit easier and i just wanted to show you guys how to use math here but yeah two two asterix is also good yeah sure um you can do that if you prefer and then we'll print we'll print our epsilon value every 500 epochs and there's more math associated with epsilon choosing different values how we recalculate epsilon um manipulating you know these epoch numbers in that and that number in that initial if statement but really the point here is to show you that epsilon is some variable that we use to determine whether we're acting randomly or if we're acting with some purpose and as the model gets better we continue continually update and decrease epsilon until we get to enough to a number where epsilon is so low that we're only really taking the arc max of our q table and at this point we're done with the algorithm so good stuff good stuff we're done now all we need to do is just print out a couple of more things like our time initial or time final calculate some of the episode rewards and then you'll be able to see your long-awaited fully render rendered animation of how your model continually updates over time i'm telling you this is the one of the most rewarding things about machine learning seeing the results it's going,358,2,2,Uc6qBg7mM2Y
26,to be really cool so the last thing we need to do yeah sure scroll up down sure all right so we'll calculate the time final by just saying time final equals time.time as we did previously with time initial we'll say the episode total time is just final minus initial all right and total time plus equals episode total all right now we'll calculate the total reward and we'll just do plus equals epoch reward our previous reward is just our current reward now again we iterated here always we got to write that but we iterated for every we're doing stuff every thousand epochs so we'll say if epoch medullo a thousand is equal to zero and we'll calculate the mean time um for actions within that a thousand epoch range so we'll say mean equals total time divided by a thousand pretty self-explanatory and then we'll just print um average time it should be plus all right and then um what we want to do is set total time equal to zero because we're calculating total time for every thousand epochs and then we'll do the same thing for mean reward right so mean uh reward equals total reward oops equals total reward divided by a thousand again for every thousand epochs and then we'll literally just copy this do the same thing average reward and then music set the total reward to zero and then we exit the loop completely and just write environment uploads all right um should be good let me run i don't know if that will change anything for us but nope yeah you're right that's it that,358,2,2,Uc6qBg7mM2Y
27,changes everything for us yeah demo works all right so now if you run your demo you'll be able to see the car balancing the pull it you know it takes a bit of a while to render each time so there's delay between the amount of deposit runs but check this out how cool is that guys are you guys able to run the simulation music it's slow you can actually play with the numbers so it renders every epoch so if we want to play with the numbers and let's say have it render every epoch so go to the code and eliminate the piece of the code that says if epoch modulo 2000 equals zero and we just press we just say environment.render and then it's just going to give us this really jittery thing oops what was that oh this slash here it's going to give us this really jittery yeah that's what it looks like this you can see how how phenomenal this is the progression of the card balancing on the pole you'll see that the time for every uh for every iteration gets longer and longer over time sometimes it's shorter and shorter but you'll see after 60 000 iterations the model will be like fully trained and will be able to balance the car in the pool it will take a while for it to get to where we want to be that's why we only rendered every 2000 iterations but you're basically witnessing live a model learning with reinforcement learning and i really think that's really cool like how we went from the conceptual explanation and visualize,358,2,2,Uc6qBg7mM2Y
28,it within code there were a lot of errors on the way uh i appreciate you guys helping me with that but i think this is pretty cool now what we'll do is we will exit this and put back that clause all right and we'll increase it to 5 000 all right and then we'll see the model being trained if you want to see cool machine learning videos that make demos just like this make sure to check out these videos,107,2,2,Uc6qBg7mM2Y
0,hey guys today we're going to try to train this little guy to find the best path across the frozen lake if you haven't installed this reinforcement learning library do pip install gymnasium if you need more help installing check out my install guide the link is in the description let's start with the basic code to launch the frozen lake environment let me execute this all right i put a breakpoint here so the little guy stops moving let's go over the code up here i imported the gymnasium library i'm declaring a instance of the frozen lake environment i'm using the 8x8 map i'm turning on the slippery flag we'll talk about what that means in a sec and then i'm turning on rendering so we can actually see the map here we're calling reset here to reset the guy up to the initial position the reset function returns two parameters we're just interested in the first one which is the state the state's goes from 0 to 63 0 on the top left and then one two and so on and then 63 at the bottom there are two conditions that ends the simulation terminated becomes true if the little guy falls into any of these holes or if it actually reaches the goal and then truncated if the guy wanders around for more than 200 steps the simulation also ends so we do a while loop while those two conditions are not true we select a random action the available set of actions are zero to represent left one is down two is right three is up after selecting the action we'll,358,0,0,ZhoIgo3qqLU
1,execute the action return the new state a reward if there's any nd2 simulation ending conditions so let's say the selected action is right so it's going to go this way it's going to go from state 0 to state 1 so the new state is one the later reward structure works is that the little guy gets a reward of one when it reaches the goal otherwise there are no rewards or penalties anywhere else now that we have a new state we're going to assign it to the current state and keep looping until one of these conditions are true and then we're going to close the environment now let's talk about the slippery flag the slippery flag actually makes it so the step function doesn't always honor the chosen action so here's an example let's say the guy is here if the chosen action is to go this way there's actually a chance that he's going to slip up or slip down if he's here and he wants to go down there's a chance that he's gonna slip sideways this actually makes the scenario much more interesting because she can't really solve this using a traditional pathfinding algorithm lastly if you're wondering what happens if the guy tries to go off the map he'll just stay in the same spot okay now we're ready to talk about q learning let's start with the end result that we're looking for in q learning we're looking to construct a lookup table so in this case we have 64 states by four actions so we're gonna have a 64 by 4 array we're going to use,358,0,0,ZhoIgo3qqLU
2,the q learning formula to calculate values for each one of these combinations after training the table should be complete here's an example of how the q table is going to be used after training so let's say we're in state 62 say 62 is right here let's say the q values look like this so when we're in this state the ai is going to come to the table find stage 62 find the biggest q value so it's gonna go right let's initialize the queue table we need to import numpy use numpy to initialize a 64 by 4 array with all zeros the q learning formula depends on two parameters in future videos we'll talk about how these two parameters affect the formula for now we'll just use the numbers that work for me now put in the q learning formula after taking a step the formula updates the current state action pair with the reward that it received for taking the step and also the largest q value in the new state right now the assimilation ends after one try we need to enclose this code inside a for loop episodes is the number of times we're going to train we'll pass this in through the function right now we're selecting actions randomly but over time we should start following the path dictated by the queue table to do that we'll implement the epsilon greedy policy so i'm going to add three new variables one is epsilon when epsilon is one it means we're picking 100 random actions over time we're gonna decrease the randomness we'll use the epsilon decay rate to,358,0,0,ZhoIgo3qqLU
3,do that here i'm just declaring a random number generator if the number that we generate is less than epsilon we'll take the random action otherwise we'll follow the q table after each episode we're going to decrease epsilon all the way until it gets to zero the epsilon decay weight is another parameter that we can adjust again we'll talk about how to tune these numbers in the future this number actually has a direct impact on how many episodes or the minimum number of episodes that i need to train so right now i have it set to 0.0001 if i want epsilon to get to zero i'm going to need to train for 10 000 episodes because i'm subtracting this number from 1 after each episode so it's going to take 10 000 episodes to get one all the way down to zero so let's go ahead and train maybe 15 000 episodes and finally i want to reduce the learning weight this helps stabilize the q values after we're no longer exploring we're pretty much ready to start training but let me add some code to help with tracking how the training progresses we'll keep track of whether we collected rewards for episode or not and then add this code to get it on a graph the graph is going to show a running sum of the rewards for every 100 episodes we need to import the graphing library and let's control the rendering to the function we'll set the render mode to human if the render flag is true otherwise we'll set it to not render before we start training i,358,0,0,ZhoIgo3qqLU
4,found a problem here make sure the epsilon adjustments and all this is within the for loop and also rewards per episode this needs to be initialized outside the for loop after the training i want to save the queue table to a file so that i can reload it and use it again i'm going to use the pickle library to save the file for the first training i'm gonna turn off the slippery flag just to keep things simpler okay we're ready to train i'm going to hit f5 it's running right now okay it's been a minute and training is done let's check out the files the x-axis is the number of episodes so from 0 to 15 000 the y-axis is the number of rewards per 100 episodes we can see from the beginning where it's 100 exploration rate the amount of rewards collected per 100 episodes is almost non-existent as epsilon decreases to about 50 percent near five thousand the little guy is using more and more of the q table so you can see that the number of rewards is increasing from about five thousand to ten thousand we see a significant increase in the amount of reward collected at 10 000 episodes epsilon is zero so there is no more exploration the little guy is following the q table 100 of the time since the slippery flag is off when it follows the q table it's going to get to the goal 100 of the time so you can see that from ten thousand to fifteen thousand it's a hundred percent reward the q values are saved to this,358,0,0,ZhoIgo3qqLU
5,file since it binary file we won't really be able to read it what we can do is modify the code so we load the queue table from file and then see how it performs i'm going to add some code to distinguish whether we're training or whether we're using a trained model let's add a is training flag if we're training then initialize the queue table otherwise we'll load the model back into the queue variable if we're training then we do the random steps otherwise we'll look up the q table and then only update the queue table if we're training and only save the queue table before training okay now we can run the model let's run it for a thousand times service training to post f5 and it's done let's check out the image since we're doing a running sum of 100 from 0 to 100 it is going to look like a increasing line so this is fine and now we can see that all 1000 episodes is getting 100 100 reward now let's turn on when doing and see what the guy is actually doing i would just do it one time okay now let's do the training again with the slip refactor on same thing 15 000.,278,0,0,ZhoIgo3qqLU
6,and turn this back on and f5 okay it's done let's check out the image with the slip refactor on we can see that the first almost seven to eight thousand episodes are barely getting any rewards then as it starts following the q table more often it's just collecting okay ml rewards and finally at 10 000 we're following the q table exclusively we're getting about between 60 maybe 60 to 85 reward now let's try the model we'll do it one time to see what it looks like you can see with the slippery flag on he's doing a whole lot more back and forth let's see how it does in 1000 episodes okay let's check out the image looks like in actual tests the model is not as great it's hovering between maybe 40 to 60 percent so the slippery factor really make things a lot more difficult if i run the training the results are more in line with the testing i think the first time where i got a pretty high rewards that's probably luck there's also instances where the little guy never reached the goal in all 15000 episodes so it never trained so a better model could be had by adjusting the amount of episodes and all those other parameters that can be adjusted it's a trade-off between time and am i happy with the results that i have right now in the future we can talk about how to make this better i think we're good for now thanks for following along,338,1,1,ZhoIgo3qqLU
0,so hi guys this video is a continuation to my machine learning playlist so basically in this video i'm gonna teach you how to make your own reinforcement learning game alright before that let me just recap the concepts of reinforcement learning so basically the enforcement learning is also called as reward based learning wherein you give your machine learning model rewards when it does something and you want it to do and you give it negative rewards when does something that you don't want you to do for example if you're playing a game of chess the state of a checkmate will be assigned a very high reward and other states will be assigned consequently lower reports so a machine learning model tries to figure out actions that lead to higher rewards now it's important to understand that these rewards are assigned by your environment for example in the environment of a chess game the state of checkmate is assigned a higher reward by the environment right there are certain states in the environment are more desirable and certain states are not desirable right so when you're making a machine learning game or a reinforcement learning game you need to assign certain states higher rewards and uncertain states lower rewards now this task of making an environment has been made easier for us because there is a library called gem library which is specifically made for simulating models for example let me show you today so let me show you a library called jim which is specifically made for simulating the reinforcement learning models so it is a toolkit and has cool environments like a,358,0,0,DgE-r_rF3Nc
1,random and agent where the aunt tries to balance itself and the card pool environment where we have a card and a pool on top of it and the basic goal is to balance the pool on top of the card then we have lots of other games of pendulum and we also have pac-man and space invaders basically lot of stuff to exclude so rina the game of card pool is one such game which is very easy for us to figure out as beginners so that's why i've been going to practice on that so so here's how the card pool environment looks like we have a card to the pool on top of it and our entire goal is to balance the pool on top of it by accident if the card is left of the night so the condition is that the pool should be more so the condition is that the bull should not be more than 15 degrees for a vertical if that happens in this episode ends and the card should remain within 2.4 units from the center as well right if these two conditions are not satisfied the game ends so let's try to make a virtual learning game for this so you need python installed and a line recall you jim so this is sort of this how the code looks like you start off by importing the gem library after that for importing the card pool environment using the gem nutmeg function and we store that in a variable called bmp after that we are resetting the environment using env .,352,0,0,DgE-r_rF3Nc
2,reset function now they're performing a hundred iterations you have a for loop because you want to see how our model is performing over 100 iterations right so environment or trend it basically plots whatever environment it sees on your console now the next step is very important because environment dot step is the function that decides what action you take right you have to supply an action as an argument and it performs that onto your model c is what change happens and clears out the change in the environment right so what we do in this we have to supply an action so we use the environment or actions ps to get a list of all available actions in this case it's either accelerating towards the left or to the right and odd sample basically selects a random action right we're not specifying any action as of now so it basically taking any random action and giving that to the environment and seeing how the forms right so environment what step basically takes an action as an input and outputs the current state of the environment right so let me just show you what the output looks like after the fall of any swift was in the environment using environment or flows so if i simply run this code so you can see that we have a cart that was trying to balance itself and after that it moved out of three right so we have a poor reinforcement learning model as of now right and you can see you have an area of four values this might seem confusing but these four values,358,1,1,DgE-r_rF3Nc
3,are simply the observations from the environment element which have the velocity of the pole the x-coordinate of the pole the angle of the pool as well right how do these four values in an array only the third one is important right the third one is the angle that the pool is making with the y-axis that is what we need to monitor to know if the card is falling off or not and how to take appropriate actions for that right so this was just to show you how the card balancing pole works for its own right so let me show you the other cool which we have cleaned so this is their why school right we have so so this is the revised code the first line is the same with importing the library and storing it into environment variable after that by defining a function called basic policy what this does it tells a machine learning model what action you take depending upon what angle is the pole making with your y-axis right so the name of function is basic policy and the argument is the observation from the environment which is the area of four values what we're doing is restoring the third item third item has an index of two so we're storing the third item of observation array into our because that is the angle that apple is making now if the angle is less than zero we'll return zero which is basically accelerated towards the left and if the angle is greater than zero which means it's inclined towards the right then we action towards the right,358,1,1,DgE-r_rF3Nc
4,which makes sense right if your pool is fallen towards the right we have to add state towards the right so that the cause of inertia it comes back to its original position similarly if it's falling towards the left we have to accelerate towards the left right that's how basic inertia works now that we have given the model instructions on how to take an action given the angle we are ready to run the iterations again so what we're doing this making fondue of twenty so that we can check how our model is performing in 20 different episodes right and then before starting every environment every episode resetting the environment and storing the observations from the environment in the variable calab solution right and in every episode by running hundred iterations right to see if the model is able to balance itself in 100 iterations environment or enter again plots out the environment and show that on to your screen then we call the basic policy function that we are defined supplier observation and store the resultant action in the variable call action right so earlier we were selecting a random action now depending upon the angle we're taking a suitable action right so we're providing that action to the function and environment not step and environment or sub basically turns four values number one is the observation right which is the area of four values number two is something known as in word right so that is the base of reward-based learning because this model assigns a plus one reward if the cart is running and a zero reward if your card,358,1,1,DgE-r_rF3Nc
5,has stopped moving right so the entire girl of the machine learning model used to keep the cart moving and as long as the cart is moving we have to balance the pool right so that's why we define actions to take depending upon the rv and the machine learning world already tried to keep the cart or anything as long as possible to achieve maximum rewards right so it also returns environment or stem also turns a mineable called done which becomes true when you have two sodas and it which means either your pool has fallen or haircut has moved out of a given range right an invoice certain other than an invoice or another information about the environment which is not necessary as of now so basically we're printing the observation so that we can see how our model is doing in each equation and if at any point in between the loop done equal to which means the episode has landed we write that episode has finished and we want to check how many iterations it to restart straight right so we print t plus 1 and then we break out of loop and close the environment so let me show you how this machine learning is model how this this machine learning model is performing so let me just turn the school so you can see here the card has now learned to balance the pool it's trying to balance itself and whenever it moves out of range yeah so earlier the card was not able to by herself do not know what actual thing now we have told it depending,358,1,1,DgE-r_rF3Nc
6,upon the angle you have to take this action right so that was the basics of reinforcement learning right you have a reward and the machine learning model tries to achieve or take actions that max i threw one right so that was how basic reinforcement learning works you can move on to different environments like pac-man as well or balancing a bowl on top of a mountain environment or space invaders is it for this video i will bring out more videos on this topic on the machine learning so if you did like this video do like share and subscribe and if you have any doubts tula we do leave that in the comments you,152,1,1,DgE-r_rF3Nc
0,"most rl tutorials out there use either mountain car, cart pole or luna lander as their environment and assume an understanding of gym. so, let's use this video to get familiar with gym. hi there you beautiful being how are you today? i'm saasha nair, and welcome to this channel. today, we'll be going over what is gym and why it exists and how to use it for our implementations. reinforcement learning presupposes the existence of an environment. it is built on the premise that an agent learns through its interactions with its environment and that is where openai gym fits in. if you need a quick refresher on the basic working of reinforcement learning click on the video linked above. openai gym was born out of a need for benchmarks in the growing field of reinforcement learning. it thus provides a diverse range of environments or tasks to test out your rl agents. it makes no assumptions about how the agent is structured and instead focuses on providing abstractions for the agent to interact with the environment, while prioritising ease of use and accessibility. with that out of the way, time for more fun things. let's look at how to use gym for implementations and let's switch into the console view. let's start with this newly created conda environment. as you can see gym is not available in the environment. to install, we simply use pip install gym . let's test our installation.",317,0,16,cxMuWd83fI8
1,"let's test our installation. environments in the algorithmic and toy text categories work right off the bat, but if you wish to use a box2d environment or an atari environment you'll run into issues to fix this, you need to perform an extra pip install step and install the corresponding environment category let's test this once again... ... and there, we have it working now. time to switch into sublime to get an idea of the code. we already saw gym.make while testing our installations, which is used to create the environment. every environment must be initialized before the agent can start interacting with it, for which env.reset is used, which returns the initial state of the environment. for the agent to perform an action, we use env.step and pass the desired action as an argument. however, since we don't have an agent, let's test by sampling a random action. the step function returns the 'new state' of the environment that has resulted from the agent performing the action, 'reward' that will help guide the agent's learning, 'done' which is a boolean indicating if the terminal state is reached or not, and 'info' which is usually empty. let's examine what happens when we run this... going back to the code, we had ignored action space earlier. 'action space' specifies the range of actions available to the agent. similarly, there is 'observation space' that specifies the type and range of the observations available to the agent in the state variable. for example, in the carpool environment, two discrete actions are allowed and the observation space is a continuous array of size 4, as indicated by box 4, .",360,16,30,cxMuWd83fI8
2,"for example, in the carpool environment, two discrete actions are allowed and the observation space is a continuous array of size 4, as indicated by box 4, . alright, so till now we saw how to create and interact with the environment, but we performed only one step, while in reinforcement learning we need to perform millions of steps of interaction. so let's modify the previous code such that it works in a loop that iterates over game episodes. this means that as soon as a terminal state is reached, as indicated by the done flag, the game is reset and a new episode begins. fingers crossed that it works... and yay, there it is! this is great and all, except that it is just all text. with rl, it can help to see how the agent is interacting with the environment. so let's modify the code. for this, we just need to add env.render within the while loop. also, don't forget to add env.close otherwise it might throw errors. this is perfect now! you can run your agent with any gym environment. let's recap. in this video, we saw that gym is a toolkit that makes it extremely convenient for you to start testing your rl algorithms. using any of the available environments is just a matter of plugging the name of the desired environment into the argument for the gym.make function. it thus allows developers and researchers to focus on the learning algorithm instead of worrying about the structure and the working of the environment. so that's all for today's video. hope you found this helpful, and thank you for stopping by.",356,30,48,cxMuWd83fI8
0,what's up guys it's shown here and today on the computer scientist we're going to be exploring some of the deep learning puzzles on opening a gym as well as caught up an agent to play them open a our gym is pretty much like a gym for testing our different reinforcement learning algorithms on various simulated environments with the overall goal of maximizing the reward from interacting with that environment so let's start by checking out the homepage for open-air gym there are different environments to try out ranging from the simple grid world type environments rendered on the command line to ones with more complex dynamics and then the more difficult environments with many complex variables so today will mainly be looking at the classic control environments first we need to install gym with pip installed gym which i've done already and then we just import gym so next we will create an instance of the environment we want to play through so let's have a look at the classic car pool environment so we'll then save the name as available and then call to make passing in that name so envis now our handle for interacting with the environment which we can use to simulate a random playthrough now we first we set the environment state with mdot we set and then iterate over a number of time steps where at each time step we get a random action by calling in photon actions based on sample and then apply that action to the environment by passing it to an vinod step finally we call mdot render to display the graphics okay so,358,0,0,8MC3y7ASoPs
1,if we want to design our own agent to play optimally in this environment we will need to start by understanding more about the interface of the environment object so let's check out the wiki for the kart pool environment on the github page so we see that the environment has an observation table which is essentially the current state of the environment and this specifies the possible values for the karts position along the line its velocity and also the angle and the angular velocity of the pole we then have an action table which tells us what actions correspond to each action index so zero is push the card left and one is push it right we can actually access all of this information from our end object so looking at the attributes of this object we can see an observation space and when we print it it comes up as box four and this means that it has four values each within a continuous range then if we look at the action space attribute we see that it is a discrete two which means that it has two discrete action index values normally which start from zero and go up to the number of different actions available so now let's create an agent class that uses this information from the environment to make custom actions so within our constructor we can save the number of actions available as an action space dot n and also print it to see what this is then we can define a method for choosing an action from the available actions so in this case of a discrete,358,0,0,8MC3y7ASoPs
2,action space this would just be an integer from zero to the action size exclusive so for now we will just pretend in random integer in that range then we just need to create an agent object and call its get action method to get an action for the environment now what if we wanted to define our own simple policy for choosing an action based on the current state of the environment or the couple so if we think about how he would balance the carpel we would push the car in the direction that the pole is leaning to try and counteract its tilting so looking at the observation table we see that the information about the pole angle is at index two of the observation space so that means we need to pass the current state to our agent when choosing an action for the next time step so it turns out and reset actually returns the initial state and then n f dot step returns a tuple containing the next state or reward for the last time step whether or not the episode reached a terminal state and finally some additional info so having a terminal state essentially provides a finite ending point for each episode so it doesn't go on forever and the conditions for an episode terminating are included in the wiki as well so now that we have the state we can pass that to our agent and then our agent can access the pole angle at index two of the state and push the card to the left if the angle is negative otherwise push it to the,358,0,0,8MC3y7ASoPs
3,right music so now you have an idea of how to set up a basic interface for interacting with an environment on open a i jim in the upcoming episodes we'll be building on the setup to eventually train an agent to make the optimal moves for an environment so make sure you subscribe if you want to stay up to date with those anyways thank you so much for watching this video if you found it helpful be sure to give it a like and i'll see you next time on the computer scientist bye music,126,0,0,8MC3y7ASoPs
0,on earth the simple rules of natural selection and competition led to the evolution of increasingly intelligent life-forms today we ask if comparably simple rules at multi-agent competition can also lead to intelligent behavior in a new virtual world these agents are playing hide and seek these agents have just begun learning but they've already learned to chase and run away this is a hard world for a hider who has only learned to flee however after training and millions of rounds of hide-and-seek the hiders find a solution the hiders learn to use rudimentary tools to their advantage by grabbing and locking these blocks they can create their own shelter the seekers are locked in place for a brief period at the start of the game giving hiders a chance to prepare even so the hiders must learn to collaborate accomplishing tasks that would be impossible for any single individual the hiders are not the only ones who can learn to use tools after many generations of failing to break into the shelter the seekers learned to jump over obstacles using ramps however after many millions of rounds of having their shelter breached the hiders learned to take away the primary tool the seekers have at their disposal note that we did not explicitly incentivize any of these behaviors as each team learns a new skill it implicitly changes the challenges the other team faces creating a new pressure to adapt we've also put these agents into a more open-ended environment randomizing the objects team sizes and walls in this world they learn to construct their own shelter from scratch requiring that they arrange multiple,358,0,0,kopoLzvh5jY
1,objects into precise structures to prevent seekers from using the ramps the hiders move them to the edge of the play area and lock them in place we originally believe this would be the final strategy that the agents learned however we found that after more training the seekers discover that they can jump on top of boxes and surf them to the hydra shelter in the last stage of emergent strategy that we observe the hiders learn to lock as many boxes as they can before constructing their force in order to defend against box surfing so how do agents acquire these skills they're trained using reinforcement learning an algorithm inspired by the way animals on earth learn the agents play thousands of rounds of hide-and-seek in parallel for many days they train against each other as well as past versions of themselves using an algorithm called self play coevolution and competition on earth led to the only generally intelligent species known to date humans while this world is far less complex than earth we have found evidence that simple rules can lead to increasingly intelligent behavior from multi-agent interaction we hope that with a much larger and more diverse environment truly complex and intelligent agents will one day emerge music,275,0,0,kopoLzvh5jY
0,what's happening guys my name is nicholas turner and in this video we're going to take a look at how you can build your very own custom reinforcement learning environment using open ai gym let's take a deeper look as to what we'll be going to so in this video we're going to build a custom reinforcement learning environment using the open ai gym class we're then going to take some previously written code and train up a dqn model to interact with it and then we're going to test out this model let's take a look as to how this is all going to fit together so we're going to start out by downloading some reinforcement learning starter code so we wrote up this code inside of the reinforcement crash course tutorial we're then going to build up our custom environment within that same jupiter notebook so that we can play around with it we'll then train a dqn model using tensorflow and keras rl and then we'll test that out to see how well it performs ready to do it let's get to it alrighty so in order to build our custom reinforcement learning environment we're mainly going to be working inside of a jupyter notebook but in order to keep this short and sharp what we're going to be doing is we're going to be reusing the code that we wrote in the keras crash course tutorial specifically for the dqn model or the actual training part of this tutorial so if we actually take a look at that code which is on github the core thing that we're going to be doing,358,0,0,bD6V3rcr_54
1,is changing this environment here so in that tutorial again link in the description below and i'll include a link somewhere above the core thing that we did in that tutorial is we trained a reinforcement learning model on the cart poll environment so you can see that it's cart pull rather than doing that we're actually going to build our own custom environment but the large majority of this code will remain unchanged so we'll be able to reuse this so in order to get things started what we're first going to do is clone this repo down and again i'll include this link in the description below so it's just forward slash forward slash github.com forward slash knick knock knock forward slash tensorflow keras dash reinforcement learning but again i'll include in the description below and also inside of the jupiter notebook this will all be available um just check the links below all right so we're going to grab this and then what we're going to do is open up a new terminal if you're on a mac or a command prompt if you're on a windows machine and we're going to type in git clone and then our link i think i included this space in there so git clone and then the link to that github repo so in this case you can see that i've got that link copy there and hit enter and this will clone that repository into the same directory that my command prompt is currently at so in this case i'm on my desktop so it's going to be on my desktop and if i actually,358,0,0,bD6V3rcr_54
2,open that up you can see that i've got my deep reinforcement learning jupyter notebook there already but what we're going to do is we're going to open this up inside of jupiter so if i just scroll back to jupiter you can see that that folder is appearing there as well and if we step into that we can open up this deep reinforcement learning tutorial notebook which has a lot of good stuff now you can see here that this has got a sort of full-blown walkthrough to build a reinforcement learning model for a predefined environment specifically as i was saying card pole we're going to be focusing on actually changing this and building our own custom environment now you're probably thinking what custom environment are we going to be building well let's take a look so the scenario that we're going to be solving revolves around the bathroom now i'm sure you've been in this situation before you've gone you've woken up you brushed your teeth you've gotten ready to go have a shower now what happens is you end up trying to find the optimal temperature to have a great shower you're just messing around with the dial you can't seem to get it right in eventually you just end up fed up we're going to be building a reinforcement learning modeled to solved this and specifically we're going to be building the environment to solve this so our goal is to build a reinforcement learning model to adjust the temperature of our shower to automatically get it inside of an optimal range so our environment is all going to be,358,0,0,bD6V3rcr_54
3,to do with actually changing the temperature of our shower to get it within our optimal range now in this particular case our optimal temperature is going to be between 37 and 39 degrees and our shower is going to go for 60 seconds so if you've ever worked with any of the other open ai environments like card pole like space invaders you know that they have an episode length so in this case our episode length is going to be 60 seconds so we're going to try our best within that 60 seconds to get within that 37 and 39 degree range now you're probably thinking this is easy just make it 37 or 39.,152,0,0,bD6V3rcr_54
4,well we're actually going to include a little bit of noise in there so we're going to have temperature fluctuations so say your mum or your dad or your girlfriend or boyfriend go and open up a tap and then drop the temperature of the shower we're going to be accounting for that in this particular case we've got three actions that we can take so we can either turn our temperature down we can leave it the same or we can turn it up so it's going to be three different actions and really our task at the end of the day is build a model that keeps us in that optimal range for as long as possible so again we're going to be building this up step by step so if you haven't fully grasped what we're sort of doing here i'll walk through it as we're actually building okay so now what we're going to do is jump back into our jupiter notebook and we're going to start kicking this up so if you don't have a reinforcement learning environment set up already then i'd highly recommend running through this dependency install so in this particular case we're just going to hit shift enter in this cell and this will go on ahead and install all of our dependencies so just give that a second and that's now done so over here what we've done is actually installed 4k dependencies so we've installed tensorflow 2.3.0 we've installed open ai gym so this is going to be the main environment that we're going to be working with or that the thing that allows us,358,1,1,bD6V3rcr_54
5,to build our customer environment keras and then keras rl2 so keras is a deep learning environment that makes it pretty easy to build ai models or deep learning models keras rl2 gives us a number of predefined agents to actually build reinforcement learning models okay so we can minimize that now in our next cell we've got a few imports now we're going to tweak this slightly and import some slightly different dependencies okay so we've imported some slightly different dependencies there and specifically we've imported the env class from jim so the emv class is going to be sort of a placeholder class that allows us to build our custom environment on top of it we've then imported two different spaces so a discrete space and a box space from jim dot spaces and so this is going to be what allows us to define the actions that we can take within our environment as well as the current state of our environment then we've imported numpy so to do that we've written import numpy as np and then we've also imported random so this is going to allow us to test out our random environment or what random actions look like down there okay so now it's time to start building our custom environment so in this case we're going to be building an environment class so we can instantiate this anywhere and we're going to be defining four key functions inside of that so the first function is going to be an initialization function and this is going to be the function that gets run automatically when you create a new instance,358,1,1,bD6V3rcr_54
6,of your function we're going to implement a step function and this is what runs whenever you take a step within your environment will also implement a render function but we're not actually going to do too much in there because we don't want to visualize or we're not going to bother visualizing and then the last function that we're going to define is a reset function so this allows us to reset our environment so let's go on ahead and create these placeholders and then we'll actually build them up step by step alrighty so that's our template environment set up so you can see here that we've gone and written what is that uh so it looks like eight so nine lines of code so the first line of code defines our shower environment or our shower class and in this case we've written class shower env and then we've passed through our gym environment here so this basically allows us to leverage a lot of the environment capabilities that come from the gym environment so sort of some standard python class based functionality there and then we've gone and implemented four different functions within or four different methods within that particular class so the first one is our init method in so inside of here we're going to be initializing the actions that we can take the observation space we're going to be initializing our space so specifically our temperature and we're going to be initializing our shower length then we've got our step function and our step function basically defines what we do whenever we take a step and how we treat,358,1,1,bD6V3rcr_54
7,actions our render function we're not actually going to implement anything in here but if we wanted to do visualizations for this particular environment this is where we'd implement them and reset is where we reset our environment after each training run or after each episode so the first thing that we're going to do is start by setting up our initialization environment actually one more thing so i've just written pass against all of them at the moment because we're not actually doing anything but we're going to get rid of these in a second and actually start writing up these functions so the first one that we're going to do is our initialization function so we're going to get rid of this pass and then start writing up some stuff so let's do it okay so that is our initialization function done so we've written four additional lines of code there so the first one is setting our action space so in this case we've defined an attribute to our environment so by defining self dot action space and we've set that to equal discrete and then we've passed through three to that discrete space so a discrete class actually comes from our gym spaces so what we had up here and discrete basically means that you're going to be able to have three different values so zero one or two so when we actually go and apply this to our step function we're going to be treating zero as dropping the temperature down one as leaving the temperature steady and two is increasing the temperature we've then defined our observation space and inside,358,1,1,bD6V3rcr_54
8,of our observation space we're basically going to hold our current temperature and so this is going to define where our shower is currently at and so we'll be able to use that to tweak and produce our reward then we've defined another attribute so this case we've defined our state and we've set our starting temperature to 38 degrees plus a random integer so to do that we've written random rand int minus three and then three so this gives us our bounding box and then last but not least we've set our shower length to 60 seconds so by defining self dot shower length equals to 60 we're basically saying that our shower is going to take 60 seconds just a little bit more on this observation space here so the first space that we used was discrete and discrete is going to give us three discrete values so zero one or two because we pass through three now a box is going to allow us to have a continuous value across a whole range of spaces so in this case we've defined a our low value so in order to do that we've read a numpy up we need one of those there so in order to do that we've written low equals numpy dot array and then to that we've passed through our low value which is zero and then we've specified our high value so to do that we've written high equals numpy dot array and then we'll pass through a hundred i'll show you what this looks like in a much better example when we actually set up our environment,358,1,1,bD6V3rcr_54
9,so you'll actually be able to see the difference between what using a discrete space versus what using a box space looks like all right so that's our initialization function done the next function that we're going to implement is our step function so this is going to run every single time we actually take a step within our environment and it's akin to actually taking an action on our environment so to do that we're actually going to pass through our action and then we're going to implement the rest of our step function alrighty so that's our step function done so the first thing that we did is we passed through a new parameter which was our action and so this is going to be 0 1 or 2.,170,1,1,bD6V3rcr_54
10,which we've defined in our action space up here and then what we're doing is we're applying that action to our state and remember our state is really our temperature so in this case if our action is zero we're actually going to subtract one and drop our temperature by one if our action is one we're going to subtract one and then we're going to leave our temperature the same so effectively what's going to happen is if we pass through action zero then what we're actually going to be doing is subtracting 1 which means there will actually be subtracting negative one from the temperature if that action is one then we're going to be subtracting one and this is sort of what this negative one is here we're going to be effectively be doing nothing and then if our action is 2 then what we're going to be doing subtracting 1 again which means we'll raise our temperature by one so this is what this particular line is doing here so it's applying that action to our current state we're then reducing our shower length by one second so in order to do that with rinself dot shower underscore length minus equals one so this is sort of some python functionality to drop a number and so this is effectively reducing our shower length by a second what we're then doing is we're then calculating our reward now you can play around with this but in this case i've kept our reward calculation pretty simple so what we're doing we're checking if self dot state is greater than equal to 37 and self.state,358,2,2,bD6V3rcr_54
11,is less than equal to 39 we're setting our reward equal to one now remember self.state is our temperature so what we're basically saying if our temperature is greater than equal to 37 degrees and our temperature is less than equal to 39 degrees then our reward is going to be one so that means we're in our optimal temperature range if not then our reward is going to be negative one so effectively what you're going to see is our model is going to try to converge so that our temperature is always within this range then what we're doing we're checking whether or not our shower is done so to do that we're writing if self.shell length is less than zero then we're setting done equal to true else we're going to be setting done equal to false or leaving it as false so this basically checks whether or not our shower length or shower time has expired so remember we're reducing it by one over here so ideally once it gets to zero we're going to be setting done equals to true which means our shower is done then we're applying a little bit of temperature noise so to our state remember that's our temperature we're going to be adding a random number between -1 and 1.,286,2,2,bD6V3rcr_54
12,and so this will serve to fluctuate our temperature up and down in reaction to people maybe changing the temperature opening a tap so it sort of mimics real life a little bit more in that sense we're then setting a placeholder for our information so this is something that the open ai gym requires and then we're returning all of that information so in order to do that with written return self.state remember self.state is our temperature we're returning our reward that we set up here whether or not our episode is done which we've got from up here and then we're passing through that info dictionary that we've got here so this means we've now gone and implemented our initialization and our step so the two key functions the last one that we're going to do is a reset function so in this case our render really this is where you'd implement uh visualization stuff to say for example you're working with pi game actually let me know in the comments below if you'd like me to do a tutorial on how to do this with pi game or a more visualization heavy environment so the last thing that we're going to do now is actually implement our reset so let's go ahead and do it all right that's our reset function done so basically what we're doing is we're resetting our temperature so we're setting self dot state equals to 38 plus or minus 3 degrees remember same line that we had up here and then we're resetting our shower length to 60 seconds and then we're returning that state so now that,358,3,3,bD6V3rcr_54
13,our custom environment class is done the next thing that we actually want to do is actually going ahead and test it out so in this case we can test it out just by creating a new instance of our shower environment and all things holding equal we should successfully see this work so we're going to type env it goes shower a and b and it doesn't look like we've got any errors there and this basically means that we've now gone and successfully created a new instance of our custom environment so we can actually go and play around with this now so remember we set up our action space and our observation space and i said i'd explain this in a little bit more detail well the best way to see the differences is by actually sampling the environment in order to do this we can type in env.action space dot sample and this will give us examples of the results within that action space so remember our action space here is discrete and two that will pass through three so ideally what we'll get back is three different values so we've got two we've got zero we've got one and you can see just by sampling we're getting different values within a discrete number space so a whole number now if we change this and wrote observation space you can see that to that we're getting a continuous value which is between our low value which was zero and our high value which is a hundred so what we're basically saying here is that our temperature of our shower can be anywhere,358,3,3,bD6V3rcr_54
14,between zero and 100 degrees so ideally 100 is going to be super hot but you sort of get the idea now the next thing that we actually want to go on ahead and do is actually play around with our environment so what we're going to do is we're going to delete this cell here which is legacy from our carpal environment and we're going to delete this as well and we're just going to change a line of code within this sort of sample run through so what we're basically going to do is rather than leaving our action equal to random choice zero equals one we're going to type in env.action space dot sample and so what this is basically going to do is it's going to run through 10 different showers so we'll set episode equals 10 so it's going to do 10 showers so first i'm going to reset our environment set done equal to false set our score equal to zero and then what it's going to do is it's going to render our environment so we can actually comment that out because we're not actually going to be rendering anything we're then going to take a sample action so either 0 1 or 2 we're going to apply it to our environment and then we're going to calculate our reward so if all things holding equal this should run successfully so you can see there that our environment's gone through really really quickly and it's gone and run through a number of different showers now in this particular case remember that if our shower isn't within that optimal range,358,3,3,bD6V3rcr_54
15,so 37 to 39 degrees then we're going to get a reward of negative one so in this case we can see here that our score is negative 60.,37,3,3,bD6V3rcr_54
16,so that means that for the entire 60 seconds of our shower we were outside of that optimal range you can see in this case be taking a couple of random steps we've got minus 24 and then the best one that we got was -8 so that at least means that for some of the steps we might have been within that optimal range now again this entire code is inside of the full reinforcement learning tutorial so if you want to see how we wrote this up by all means check that out we're not going to spend too much time on that we're actually going to test out our environment now so this is our shower environment completely done now there's one last thing that we need to do in order to actually get this to run with our keras rl model so within our old model function here we're actually going to import all of these dependencies then what we're going to do is we're going to comment out this model dot add flatten step and we're going to define a couple of key things so we're going to define our states and our actions so let's go ahead and write these out all right so that's done so what this basically gives us is the shape of our states as well as the number of actions that we've got to play with so in this case our states we've really just got one value and with our actions we've got three different values that we can place so in this case we're going to get rid of this layer and we're,358,4,4,bD6V3rcr_54
17,going to update our deep learning model and that should run successfully when we go through the rest so what we've basically done here is we've removed our flattened layer and then we've just skipped straight through to this dense fully connected layer so again if you want to see in more detail how we actually wrote this up by all means check out the crash course tutorial we've then gone and passed through our different states so this basically means that we're going to be passing through our temperature to our deep learning model and then we've left everything else the same so ideally if we build up this model so by running the next cell you can see here that we've now got a deep learning model that's going to take in our temperature as an input and it's going to produce three different actions so zero dropping our temperature one leaving our temperature the same or two increasing our temperature we can then pass this through to our keras rl model without too much change so if we run through these cells here oh and if you get this particular area here so sequential object has no attribute compile time distribution strategy just roll back up to your model up here delete this by typing in del model and then rebuild your model and that should now run so you can now see that we've now taken our custom environment and we're now able to train a dqn model again this how we built this up is all in that crash course tutorial but you can see now that we've now successfully built,358,4,4,bD6V3rcr_54
18,a custom open ai environment and we're now training a model to get our temperature right in that sweet spot so ideally what you should see is that this particular reward here increases over time so you can see that right now it's on about -58 ideally you should see it drop a lot more it's probably never going to get perfect or get into the huge number of positives because remember that we take a random step and if we're outside of that optimal range then we're going to get a negative value but let's let this run and then we'll test it out alrighty so that's our model finish training so you can see that we started off with a reward of negative 0.6308 and we sort of end up right at the end with negative 0.5816 now this model probably isn't going to hit a perfect run rate because remember we've got some random noise being applied and at the moment our model only allows us to go up and down one degree at a time so we can't do a huge change to our tap so this is potentially where you might be able to improve this custom environment later on in the future but again i've tried to keep it really really simple great so if we now go scroll on down we can now go on ahead and test our environment so this particular cell here is actually going to go on ahead and test out our dqn on our custom environment so you can see by writing it it looks like our average reward is about -58 minus 60,358,4,4,bD6V3rcr_54
19,and again we've got that randomness applied in there so at this stage it might not be performing the best but that about summarizes how to build up a custom environment using open ai gym now just to recap so what we've done is we've gone and copied down our template repo from github we've then gone and defined our shower environment we've then gone and built up our dqn and updated our build model function and gone and trained our model and tested it up now again you could play around with this environment you could tweak how you actually make the adjustments you could reduce the noise so say for example you didn't want any noise at all you could actually get rid of this completely here and that about wraps it up thanks again for tuning in guys hopefully you found this video useful if you did be sure to give it a thumbs up hit subscribe and tick that bell so you get notified of when i release future videos and let me know how you went about building your custom rl environment using open ai gym thanks again for tuning in peace foreign,257,4,4,bD6V3rcr_54
0,"in this lesson, we are going to get started with our first reinforcement learning task called cartpole. this task is a famous toy problem, and it will help you understand the gym api. and in this lesson in particular, i'm going to show you how to start the simulation of this task in gym. but before we do that, let's go to this task's webpage in gym, and take a look at what the simulation looks like. the task involves a black cart, which has a freely swinging pole attached to it. the pole tends to fall under its own weight when the cart moves, and the task is to learn how to move the cart so that the pole doesn't fall, and to maximize the duration that the pole stays upright. so i've written down the task over here, and the duration is capped to a maximum of 10 seconds, which means that if the agent manages to keep the pole upright for 10 seconds, then the task is considered done. here is a simple diagrammatic representation of the task, and of course, moving the cart is the action. the environment consists of the cart and the pole. so it's called the cartpole environment. but notice that we don't have any mention of a cumulative reward! and that's normal, because most of the time, reinforcement learning tasks will be formulated in terms of real-world outcomes such as maximizing duration of the polls staying upright, and it's our job as reinforcement learning engineers to engineer or construct rewards such that maximizing the cumulative reward is equivalent to the desired real world outcome.",353,0,11,XFqGBnXzAoE
1,"and that's normal, because most of the time, reinforcement learning tasks will be formulated in terms of real-world outcomes such as maximizing duration of the polls staying upright, and it's our job as reinforcement learning engineers to engineer or construct rewards such that maximizing the cumulative reward is equivalent to the desired real world outcome. i've also shown the duration the maximum duration of this task in this diagram, and as you can see, it's capped to 10 seconds. an agent which is successfully trained using reinforcement learning will act like this, and as you can see, the pole stays upright for the entire 10 second duration. so let's learn how to tell gym to start the simulation of this reinforcement learning task. and this involves three steps. the first step is to visit gym's environment page and i will put a link in the video description , and find the reinforcement learning task. so let's go to this link. and as you can see, that in the classic control section of this page, we have the task that we are looking for. so that's the first step: find the task. the second step is to note the name of the task in the webpage. and this is what gym calls this task. as you can see gym calls this task cartpole-v1. and then the third step is to write the following code. so first, we import gym. then we call the gym.make function and pass the name of the reinforcement learning task which we found out from the webpage as the argument. so gym.make , and pass the name as a string to the function.",358,11,26,XFqGBnXzAoE
2,"so gym.make , and pass the name as a string to the function. and this will return an object representing the simulation. in reinforcement learning, the task and its simulation is usually called an environment , and therefore we are going to store the return value in a variable called env , which is the short form of environment . every environment defines an initial state, and our next job is to ensure that the environment is reset to that initial state. and we can do that by calling env.reset . now this command returns something. but don't worry about that; we will discuss this return value in the next lesson. but the main point is that once you call env.reset , then the simulation starts. so, env.reset basically starts the simulation. so after the simulation has been started, you can visually inspect the environment anytime by calling the env.render function. so let's do that. and as you can, see this pop-up window appears, which shows you the state of the environment at the initial state of this simulation. the important thing is that the render function only works if reset has been called before. otherwise you will get a black screen, because if you don't call reset then the simulation won't have started. so that's why env.render will return a black screen instead of this. one annoying thing about this pop-up window is that you cannot close it by merely clicking the cross button in the window. the only way to close this window is to call the env.close function.",339,26,42,XFqGBnXzAoE
0,what's up guys welcome back to this series on reinforcement learning over the next couple of videos we're going to be building and playing our very first game with reinforcement learning we're going to use the knowledge we gained last time about cue learning to teach an agent how to play a game called frozen lake will be using python and open a ice gem toolkit to develop our algorithm so let's get to it so as mentioned we'll be using python and open ai gem to develop our reinforcement learning algorithm the gem library is a collection of environments that we can use with the reinforcement learning algorithms we develop jim has a ton of environments ranging from simple text-based games to atari games like breakout and space invaders the library is intuitive to use and simple to install just run pip install jim and you're good to go really easy as that the link to jim's installation instructions requirements and documentation is included in the description so go ahead and get that installed now because we'll need it in just a moment we'll be making use of jim to provide us with an environment for a simple game called frozen lake we'll then train an agent to play the game using cue learning and then we'll get a playback of how the agent does after being trained so let's jump into the details for frozen lake wait frozen lake like the frozen lake in music sorry but no the frozen lake will be playing won't have us fighting any white walkers and seriously if no one gets this reference then you're spending way too,358,0,0,QK_PP_2KgGE
1,much time learning deep learning and not enough time vegging out on well let me know in the comments if you know where this scene is from alright let's get into the real details for the actual frozen lake game we'll be playing i've grabbed the description of the game directly from jim's website let's read through it together but with an accent you know to add dramatic effect winter is here you and your friends were tossing around a frisbee at the park when you meet a wild throw that left the frisbee out in the middle of the lake the water is mostly frozen but there are a few holes where the ice has melted if you step into one of those holes you'll fall into the freezing water at this time there's an international frisbee shortage so it's absolutely imperative that you navigate across the lake and retrieve the disk however the ice is slippery so you won't always move in the direction you intend the surface of the lake is described using a grid like you see here well that was fun this grid is our environment where s is the agent starting point and it's considered safe for the agent to be here f represents the frozen surface and is also safe h represents a hole and if our agent steps in a hole in the middle of a frozen lake well yeah you know that's not good finally g represents the goal which is the space on the grid where the prized frisbee is located the agent can navigate left right up and down and the episode ends when the,358,0,0,QK_PP_2KgGE
2,agent reaches the goal or falls in a hole it receives a reward of 1 if it reaches the goal and 0 otherwise so pretty much our agent has to navigate the grid by staying on the frozen surface without falling into any holes until it reaches the frisbee if it reaches the frisbee it wins with a reward of 1 if it falls in a hole it loses and receives and no points for the entire episode cool alright let's jump into the code first we're importing all the libraries will be using not many really we have numpy jim random time and clear output from i pythons display next create our environment we just called gem make and pass a string of the name of the environment we want to set up we'll be using the environment called frozen lake v-0 all the environments with their corresponding names you can use are available on jim's website with this end object we can do several things we can query for information about the environment we can sample states and actions retrieve rewards and have our agent navigate the frozen lake we're now going to construct our cue table and initialize all the key values to zero for each state action pair remember the number of rows in the table is equivalent to the size of the state space in the environment and the number of columns is equivalent to the size of the action space we can get this information using end observation space thaw in and in the action space thought in we can then use this information to build the cue table and,358,0,0,QK_PP_2KgGE
3,fill it with zeros if you're foggy about cue tables at all be sure to check out the earlier videos where we covered all the details you need all right so here's what our cue table looks like now we're going to create and initialize all the parameters needed to implement the cue learning algorithm let's step through each of these first with num episodes we define the total number of episodes we want our agent to play during training then with max steps per episode we define the maximum number of steps that our agent is allowed to take within a single episode so if by the 100th step that agent hasn't reached the frisbee or fallen through a hole then the episode will terminate with the agent receiving 0 points next we set our learning rate which was mathematically shown using the symbol alpha in the previous video then we also set our discount rate as well which was represented with the symbol gamma previously now the last four parameters are all related to the exploration exploitation trade-off we talked about last time in regards to the epsilon greedy strategy we're initializing our exploration rate that we previously referred to as epsilon to 1 and we set the max exploration rate to 1 and a main exploration rate to 0.01 the max and min are just bounced to how large and how small our exploration rate can be lastly we set the exploration decay rate to 0.01 the rate at which the exploration rate will decay now all these parameters can change these are parameters you'll want to play with and tune yourself to,358,0,0,QK_PP_2KgGE
4,see how they influence and change the performance of the algorithm when we get there speaking of which in the next video we're going to jump right into the code that will write to implement the actual cue learning algorithm for playing frozen lake for now go ahead and make sure your environment is set up with python and jim and that you've got the initial code written that we went through so far also come check out the corresponding blog for this video on d poster comm to make sure you didn't miss anything and while you're at it check out the exclusive perks and rewards available for members of the deep lizard hive mind let me know in the comments if you're able to get everything up and running and leave us a thumbs up to let us know you're learning thanks for contributing to collective intelligence and i'll see you in the next one well that agent lost music,210,0,0,QK_PP_2KgGE
0,"music okay. um in this module we are going to implement a small project in uh open aai gym that's a great framework for um reinforcement learning and specifically we will be using a q-learning algorithm in that and the game looks like uh the grid game the s is the starting state and that's a grid this grid is basically 4x4 so total number of states are 16 states are there um wherever there is the f written uh that means it's uh allowable cell and the reward is zero there uh sorry zero there and g means the goal state where the agent wants to reach and these h u these are holes if the agent uh drop in there the agent actually loses so these are holes basically why this is called frozen frozen lake uh in fact uh agent can move uh like up down right and left and uh there is a probability that the agent moves left and actually happen to appear at right and that they justify that the the lake is slippery uh it has it is icy and slippery and in slippery uh ice if you're walking on slippery ice then you may step somewhere and may slip and um appear somewhere else and similarly at certain places the uh ice has melt and there card holds in there. if you drop in there, you are done. uh you lose or you can have a goal state. so all the states except the goal states has reward zero. um so let's see how we can implement this particular game.",348,0,5,fqo1-G0xDI8
1,"um so let's see how we can implement this particular game. first we will apply uh q-learning for training the agent and then we will see how the agent can play uh this particular game. so in the next video we will actually code that uh in python. okay, let's um let's implement how the reinforcement learning uh particularly in the context of frozen lake in open gym. so first of all import gym. um if you have not installed gym just write pip install gym and that will be okay completely okay. uh next we import uh numpy snp and uh we can import time uh because we will render environment and to see the outputs we will use the sleep function and from ipython dot display uh let's import uh clear output whenever we uh really want to clear the output. so i python dot display import clear output. so let's see everything is working. yeah. so uh let's make the environment using env equals gym dot make and the name of the environment is frozen lake um zero. there are so many other environments as well that are available in gym built in uh and available in gym. you can explore those. uh let's render the environment and see what happens. so this is basically the environment and the shaded uh cell is the cell where currently the agent is. um next we see for this particular um environment how many actions are there and how many uh total number of states are there. obviously the total number of states are 16. the total number of actions are four uh uh up down left and right.",362,5,22,fqo1-G0xDI8
2,"the total number of actions are four uh uh up down left and right. but let's uh num actions num actions equals uh environment dotaction space dot n that will give me the number of actions um and num states or the size of the state space that is environment do observation space n. so these will give me the num actions. so if i just print out the num actions and num states i will get that there are four actions allowed and there are total 16 states. now uh next we build our q table. so let's say q is equal to np.0 zeros and uh we build this table as num states by num actions num actions and that's our q table if we just print out the q table for now it contains all the zeros so yeah so this is all zeros uh now the environment is set now um let's set the total number of episodes uh the termination for one episode if it does not see the goal state or whole uh set the learning rate discount rate and uh the epsilon for um epsilon greedy policy in q-learning let's set those kind of things so number of episodes let's say we set total number of episodes being uh let's say 10,000 um max steps per episodes or um the the max number of uh actions or allowed per episode. let's say max per episode is let's say we define that as uh total number of maximum number of steps per episode. let's define those to be 80.",344,22,29,fqo1-G0xDI8
3,"let's define those to be 80. let's say these many uh learning rate let's define the learning rate alpha or uh let me write that as alpha. let's define alpha as um let's say 0. two or one or something. um and uh let's define gamma to be the discount rate. let's define that as 0.9. that's the discount rate. um let's define epsilon as uh right now let's start epsilon with uh with one which means uh explore more and more and do not exploit. and then let's set the epsilon decay rate. epsilon decay rate um to be some value which with which we will um set the decay of the of the epsilon. so let's say that is 0 um 0.001. let's say that's a decay rate. so uh these are um some parameters you can play with these parameters. um but let's say these are the parameters for now. um next we uh check for episode in range of number of episodes. that's the loop for all the episodes. uh let's start with the state env.reset. before starting every episode, every new episode, you have to reset the environment. reset all the states. and uh reset also returns the uh the starting state. then uh whether we are done or not for a particular um episode that's that will be required when we will be looping inside the episode. so for step for step in range uh we have already find max uh per episode. so, mp that and here um we generate a number if uh np.trandom dot rand if that number is greater than epsilon.",352,29,51,fqo1-G0xDI8
4,"so, mp that and here um we generate a number if uh np.trandom dot rand if that number is greater than epsilon. if that number is greater than epsilon uh obviously the epsilon starts with uh one epsilon starts with one. so currently the number is not greater than one. so it will go to exploration rather than exploitation. but if this condition is true then it will exploit or go with greedy uh strategy which means or np.org max um q and that is basically the uh state action. so if that is true else if this is not true then action is random. so, env dot action space dot sample. so, um this is basically the greedy strategy. um if if the value of epsilon is too small, uh then then the agent will go greedy and if epsilon value is large then it will go like um more randomly. so this is the exploitation phase. this is the uh exploration phase. so depending upon this random number and epsilum value, it will sometimes explore, sometimes it will exploit. now, now we have sampled our action whether uh through exploration or exploitation. let's uh now uh step in that uh let's take that action. so new state new state comma new state comma reward um reward comma um are we done and info is basically equal to so done info um that is equal to uh environment dostep action.",313,51,65,fqo1-G0xDI8
5,"so new state new state comma new state comma reward um reward comma um are we done and info is basically equal to so done info um that is equal to uh environment dostep action. so in this particular uh action you take a step now um because now we have taken the step we already have reward for that let me write that in complete reward for that we have new state let's update our q table so q for um state and action state action that is basically equal to the q for state action action plus alpha the learning rate asteric. so asteric um so this is basically uh if you remember that is reward this particular reward we get plus gamma if we have defined gamma earlier. yeah, gamma a static um the q value for the next state and the next state here is um as you can say music um a new state and we have to take the max of it. so um the gamma q um and here we have to um new state and this and we will take the max of it. so np dot arg np do max. so that will give us the maximum value the q value and uh minus um the state and action. so that's basically the q value. um that's how we will actually update our q value. there are a couple of other ways of expressing the same kind of formula um using um using 1 minus alpha and alpha rule there. there are a couple of ways. so let me write uh that as well.",354,65,76,fqo1-G0xDI8
6,"so let me write uh that as well. um so q state action is equal to 1 um q state action into 1 minus alpha plus alpha times reward reward plus discount rate the steric np dot max the maximum or cube for the next date and all. so there are a couple of ways of updating this q table. this is this particular formula is more often uh used in q-learning. so after that we are now because we have updated our table we will now set our current state to the next state new state. and if done is true which means we are either we have either seen a hole or we reach the goal state then we have to break this loop. so that's the uh whole loop. that's uh the whole thing. but one thing uh from episode to episode this actually break the episode loop uh for the current episode. from current episode to the next episode, we actually have to um decay the epsilon because uh there's an epsilon. we have to decay that. so uh let's decay the epsilon and see um from one one um from from one episode to next episode how the epsilon is um how the epsilon changed. so epsilon is equal to whatever the epsilon was epsilon static the decay let's decay it exponentially np.x x minus epsilon decay rate into the episode number which is right now it's just e.",317,76,88,fqo1-G0xDI8
7,"so epsilon is equal to whatever the epsilon was epsilon static the decay let's decay it exponentially np.x x minus epsilon decay rate into the episode number which is right now it's just e. so as the number of episodes will grow um this epsilon basically will uh will decay further and further and further um because what happens is um we can we can actually yeah so we can actually uh play with this number but anyways uh let's have this number so this epsilon will decay um and as the epsilon will decay um it will exploit more rather than explore. so let's see if this uh code works. that's a training code. um oh, it has some error here. np. oh, that's a function. that's a function. yeah. so this one. so it is getting trained. so after training, let's see how much time will it take in the training. so training is done. yeah, let's see the q table. uh let's print the q table. and see. oh, the q is all there. what happened? the q doesn't change. um, what is what is going wrong here? um, that's primarily is the reason of this um epsilon decay rate. let me increase uh this decay rate further and uh let's see uh what training it brings now. uh because depending upon this decay rate if if decay is too fast then it may cause a problems uh as we are seeing right now. so let's see the q table after this. so the q table yeah now the q table has certain values. um so um that's the training part. now let's see how the agent actually plays.",366,88,114,fqo1-G0xDI8
8,"now let's see how the agent actually plays. uh let's for example let's make the agent play for let's say five episodes. so for episode or for e equals for e in uh range let's say four episodes and uh state environment dot reset and um done is false. so um so let's check uh if done is false. again done means either you either the agent uh dives into the hole or um it actually um it actually goes to the goal state. so next we print the episode number. so episode is so e let's print that and uh to visualize to visualize the outputs let's put a sleep of let's say 0.5 let's do that and now uh in each episode let's let's uh dive into step in range max per episode. and here we clear our output in the in the jupyter notebook. we clear our output and then we render our environment just to see render and after that we again to visualize we let's do a sleep of let's say 0.2 two or three. and then what we do, we pick an action based on greedy policy org max q. uh whatever the state is, we take that action and we uh get new state. new state reward done and info being environment dot step for that action. if done then there are two cases either we are in a hole or we are in uh we are actually we reach the goal.",323,114,126,fqo1-G0xDI8
9,"if done then there are two cases either we are in a hole or we are in uh we are actually we reach the goal. so clear output bait is equal to true and um we render the environment and uh if we get reward equals to 1 then uh print when else print or whole or i mean yeah whatever so so either goal or whole so let's write goal or whole yeah that's cool um and um then we can have u for after each of these printing we can just see what happens to call a sleep function just rather than getting it disappeared quickly. let's do that. time dot sleep 0.3 or or any other parameter. and we also have to clear our output if we dive in the hole. so clear output bet is equal to true. now that's it. um um then we basically um if if this happens uh whether the reward is one or this we have to we have to break that. so what we really do if if for example the reward is one or the or or it is a whole either way uh this particular loop should should break uh if we are done. so if we are in this if we are done the loop has to break. so we write break and after that we set the state to be new state and um at the end we close our environment. so let's see uh whether if the code is right it should play uh env is not defined. um, why not? we already have our am here. so, so i'm retraining everything. it is mitting. yeah, it got trained.",369,126,142,fqo1-G0xDI8
10,"yeah, it got trained. music so, evn. oh my god. evn. where is evn? env. it should be env. uh, evm. so, that's env. so, state. s a t e state. yeah. state. so, yeah, it's working. it's it's going there and moving there. as you can see, maybe you change different parameters and it somehow move somewhere else or yeah. so maybe i should uh maybe i should uh keep this sleep parameter to be a little larger so it's in the whole um yeah so maybe i make this sleep parameter a little larger. let's say one. and um maybe i do these sleeps to be a little larger just to see what has happened. so let's see now how the agent plays. so hole. so next time it goes to hole again. uh let's see next time it goes to where? let's see hole again. the same hole again and again. and next time pull again. that's really a bad playing. um maybe we should change certain parameters and get more uh get better values. for example, maybe use this gamma to be uh 0.9 more futuristic. maybe we use the learning rate to be 0.1 and maybe the things change. so let's see. so it's done and let's see the q table. now the q table contains these values and let's see how the agent plays here. so let's see how the agent plays again. a hole goal. so now it learns to go to the goal. um again hole and next time maybe it's somewhere the goal. okay. anyways, so um playing with these kind of learning parameters and these they can drastically change the performance.",365,142,180,fqo1-G0xDI8
11,"anyways, so um playing with these kind of learning parameters and these they can drastically change the performance. um and by the way the initially the agent doesn't know how to move where to move and it it learns actually through q-learning but um keep in mind that these parameters uh i mean they are very very sensitive what parameters they should be how you can move and stuff like so they're very very sensitive to go with that. so that's just a glimpse of uh code um a little game in u gym. uh you you can build very large um capacity or quantity games and and reinforcement learning kind of tasks uh in gym or in other frameworks as well. so i hope you like this uh see you next time. okay. um now that you have seen frozen lake implementation with the help of openai gym for rendering the environment as well as taking the steps and sampling the actions and all that stuff it would be nice if you implement the q-learning for frozen lake completely in numpy. um you may use uh open i gym open a gym for rendering the environment or for seeing how the agent plays at the end but to sample the actions and taking the steps and all that these things uh it may be great for learning if you implement that in numpy. so the this activity actually requires you to implement frozen lake uh in plain numpy uh where you can render the environment using open aigm but you cannot sample the actions using open and all that stuff. so uh have fun. music",358,180,190,fqo1-G0xDI8
0,hello everyone in this video you'll learn all about reinforcement learning how to train chat gpt and build the world's best go playing program but let's start with something simpler remember pong it was one of the very first video games the player on the left has a simple policy it just follows the ball up and down let's train an ai pong playing agent to compete with it to do this we'll need to define a policy that takes the state of the world as input and outputs an action up in this case as input we'll take the ball x and y positions and the two paddle positions we'll also include the velocities so our policy will get eight inputs to keep the figure simple i won't show the velocities we'll represent our policy as a neural network our network will multiply each of these values with a weight and sum them up we'll add a sigmoid function to convert the result to a value between 0 and 1.,222,0,0,vXtfdGphr3c
1,if the result is larger than 0.5 we'll move up otherwise down that's it great let's play some pong we'll start with random weights a random pond player isn't very good it just hides in the corner this is like if you've never played tennis and you're matched against serena williams you'd probably hide too or maybe you'd ask your opponent how to play suppose your partner told you exactly what to do for every step we can use this kind of supervision to train our policy let's say our coach says to follow the ball hear the balls above our paddle so the right answer is up but suppose our network outputs 0.2 which means down we'd like to change that 0.2 to a 1.0 here's a different case where the right answer is down and we'd like to drive the output to zero a more confident down answer we can accomplish this by defining an error function and optimizing the weights to reduce this error instead of numbers let's draw these weights as colored lines positive voids are blue and negative weights are red stronger weights of thicker lines and the weights change a tiny bit after each optimization step if we train on thousands of actions the weights on the right start to converge now that we're trained up let's play some games our pompling agent is pretty decent now although it doesn't win very often how can we train our agent to win more well you can think of the game as a sequence of actions which may lead to a loss or a win suppose we lose we'd like our coach,358,1,1,vXtfdGphr3c
2,to tell us which actions were at fault maybe we messed up at the end then we could retrain our policy based on this feedback as we've been doing before training a network like this is called supervised learning and you need access to a really good coach but suppose you don't have a coach all you know is that this sequence of actions resulted in a loss and this one produced a win this kind of problem is called reinforcement learning we don't know which actions were responsible so we'll just penalize all actions we made in the loss and reward or reinforce all actions we made in the win this converts it into a supervised learning problem just like we solved in the first part of this video actually we can do a little bit better by penalizing later actions more as the game losing the stake typically happens towards the end and similarly for rewards after training on 3000 games our agent has improved to play about as well as the opponent the score is pretty even and after about 12 000 games our agent figured out that hitting the ball with the corner of the paddle makes it go faster cool and now it wins most of the time reinforcement learning is cool you learn by trying stuff out and sometimes you discover things that even the coaches don't know but there's some challenges with this approach our neural network tries to minimize an error function you can think of this function as a landscape and we minimize it by rolling downhill using an approach known as gradient descent gradient descent can,358,1,1,vXtfdGphr3c
3,get stuck in local minima so we need to explore a wider range of policies to find the best one so let's make the policy probabilistic a 0.7 now means 70 chance of moving up so the agent will actually move down 30 of the time this bit of randomness allows our agent to get out of some of those local minima to sum it up here's the approach we've described so far we'll start with randomly initialized weights we then train with supervised learning and refine with reinforcement learning amazingly we can skip the middle step and run reinforcement learning from random initial weights it takes longer to train but can still work for many problems and we can do something even more amazing so far we've assumed the ball and paddle positions are provided as input to the policy suppose instead all we had was a photo of the screen the computer will see this as a bunch of numbers it has no idea that this number is the ball and these are the paddles in other words the policy has to learn not just how to act but also how to see it's helpful to encode velocity information as well one way to do this is to subtract the previous from the current frame now positive values are from the current frame and negatives are from the previous one instead of a 2d image we'll stack all the rows into one long vector of pixels and we'll connect all these pixels to a neuron now we'll add more neurons 10 in total we'll use rectified linear activation functions these are pretty standard they,358,1,1,vXtfdGphr3c
4,connect to a sigmoid neuron at the end and it will output the probability of moving up this may look complicated but it's actually really small for a deep net there are only 11 neurons in this network compare this to the human brain which has billions of neurons in the visual cortex how can you see with only 11 neurons let's train the network and find out we'll start with completely random weights it took about 6 million games to learn a policy that beats the computer on average that's almost a week running on my macbook air but what's this network actually learning does it really learn how to see let's examine the first neuron it has a weight for every pixel in the image remember these pixels form a 2d image and each one has a corresponding weight so we can visualize the weights as an image in the shape of the pong screen it looks like noise because i've randomly initialized the weights the white ones here have the largest absolute value now we'll start training the policy by playing games and you can see that it converges to a specific pattern after 1 million and 6 million games these streaks correspond to ball trajectories that this neuron is attending to and it's particularly interested in these paddle positions it's also looking at the opponent's paddle which is a good indicator of ball position this is just one neuron the others look for different patterns so our simple network has kind of learned how to see this approach to reinforcement learning is called policy gradient and you can make it even better,358,1,1,vXtfdGphr3c
5,by limiting the size of each update in a variant known as ppo together they power many exciting applications we'll talk about two of them alphago and chat gpt in the next two videos stay tuned,46,1,1,vXtfdGphr3c
0,a great pleasure to be here i'm gonna be talking about research that's done both at mcgill university as well as deep mind on the reinforcement learning agents and just as a quick introduction for those of you who maybe haven't heard of reinforcement learning it's a way of learning from interaction with an environment that's inspired by animal learning theory and and psychological theories and so you have in the picture there are a little mouse that's doing a reinforcement learning task it's basically pressing a sequence of lovers in order to get to its reward which in this case of food in automated agents reinforcement learning proceeds roughly the same way by the agent interacting with an environment observing the state of that environment and taking a sequence of actions and then receiving positive and negative rewards and so the goal of the agent is formulated as maximizing the expected cumulative return that it would get over time and now you know the perhaps the poster child applications so far and big success of reinforcement learning has has been this alphago program which was developed at deepmind which is the best goal player among people and computers and it's a very clear application of reinforcement learning in the sense that you have a system that perceives the state of the board plays the game by taking legal moves and at the end of the game receives a reward which is 1 if it's lost if it's 1 and minus 1 if it's lost and there's no other rewards along the way so we formulate the task through this very distant signal that the system,358,0,0,hZeeWlLfDHk
1,is receiving and it's learning to optimize this by playing lots of games against itself and training from trial and error so that's very exciting it allows the system to invent new ways of playing superior to those that that people know about you know some people are scared of this i find it really really exciting but then the question is are we done and in some sense as exciting as go is it is a somewhat limited application in the sense that it's a game it's deterministic it's perfectly observable and does not have all the messiness of real world so i don't know what you dream of when when we about aii personally dream of clicking robots and and that's not just because you know this is a big chore in in my family also because i'm really bad at it but also from the point of view of ai it's actually a really interesting task because there's lots of things that go wrong you need to troubleshoot as you go along it's definitely partially observable and you need to learn how to do it effectively from one stream of data and and adapt to to new situations and so people don't even think that this is a hallmark of intelligence right we all kind of do this reasonably well but for for ai machines it's actually a really complicated task so what i want to really talk about is can we actually use reinforcement learning not just to learn how to solve a problem but actually to build the kind of knowledge that people use to solve cooking ok to build it,358,0,0,hZeeWlLfDHk
2,in an automated agent through reinforcement learning means and so to do this i just want to revisit a little bit the kinds of knowledge that a reinforcement learning agent might have and then sort of express some ways in which we could taste these types of knowledge and and learn them more flexibly in an ai agent so if we look at alphago it really has inside of its brain two types of knowledge one is what we would call procedural knowledge how to play the game and that's represented by a policy which has a deep network associated with it this network looks at the board outputs probabilities of different actions and then the other kind of knowledge that we need is what we think of as predictive knowledge ok predicting something about the state of the world now i now forego this is a very simple prediction it just wants to predict whether it's going to win or lose the game and this is the signal that's then used to train the policy network so we have procedural knowledge and predictive knowledge we would like to have or reinforcement learning agents acquiring these kinds of knowledge from data but we would like to do this more flexibly rather than saying the agent has one policy and one value function perhaps an agent needs to learn about many different ways of behaving and needs to learn how to make many predictions about the world right just like we do in everyday life so i'm going to focus on these two types of knowledge we're gonna think of procedural knowledge as being represented through skills,358,0,0,hZeeWlLfDHk
3,or goal directed behavior we're going to think of predictive or empirical knowledge as knowledge that predicts the effect of actions and we're going to try and build these in a way that allows us to express many things in a way that is learnable from one stream of data and in a in a way that is composable so by composability i mean if the agent knows several things can it quickly put them together in order to follow and formulate a solution to a new problem possibly in a zero shot way so in terms of procedural knowledge one of the way that i've been thinking about this for a long time is in terms of a particular framework called options and one way to think about options is that they're controller is that start and stop and so more formally we think of an option as a policy that has an initiation set it starts only in certain states it has an internal way of choosing actions and then in terminates possibly in a stochastic way depending on the state of the environment and so for example if you had a robot going around its initiation set for a navigation option might be is there anything in front of the robot if there's nothing in front of the robot then it can go forward and can go forward until it bumps into a wall or until it's too close to an obstacle and so the way we think about the decision-making process is that the the reinforcement learning agent now has decision points at which it can pick among these options options would,358,0,0,hZeeWlLfDHk
4,be called they execute and then they terminate and unlike an usual reinforcement learning these decision points which are sort of the the circles in these pictures the open circles they are not evenly spaced in time right because the robot only makes decisions at a time that that an option has terminated one of the interesting open questions is how would we decide what options are works worth creating and worth keeping around and so that's what we sometimes think of as a discovery problem now in sort of industrial robotics this is not left open it the designer of the system who builds controllers in a in a sort of in a way that that incorporates prior knowledge about the domain but we would like these options to be acquired automatically from data so the system should learn its sub-goals from data and so there's been lots of heuristics and lots of thinking that's gone into this what kind of sort of strategies we might use i'm going to show you something really really simple which is actually automatic random generation so this is work that we did a few years ago where we simply allow the system to pick some random sub goals and then from the vicinity of the sub goals the system knows how to reach the sample so these are these little lily pads that you see in this navigation task the system sort of sprinkles them around randomly and from within each of these ellipsoids it knows how to go to the center and so we're comparing in this picture a system that just uses primitive actions with a,358,0,0,hZeeWlLfDHk
5,system that sort of uses the lily pads is allow it's allowed to jump around and what you see is that by introducing the structure even though it has no prior knowledge really that's incorporated into it and we get a system that can achieve the goal not optimally right because the knowledge that that decides on is actually pretty rudimentary but it is it is able to achieve the goal whereas the system that just uses primitive actions is still stuck in the corner because it's exploring it's exploring randomly and random exploration in this case just does not take you very far there's a more sophisticated way of doing this which is to say the system should have an optimization objective and then try to to work with this optimization objective to create that sub goals and so this is an approach that we've pursued for a few years called option critic and it's really inspired from this actor critic architecture which is a standard reinforcement learning architecture by sudden and barto in this case you have an agent that has sort of two components a policy and a value function pointers working the policies on the top it chooses actions the value function simply provides the policy with gradients in order to update the representation and so we build an architecture that's very similar but reflects the fact that the agent is now making choices among options and so the policy structure on top is a little bit more complicated has these slates that the agent can choose from and and the valley function also needs to provide a little bit more signal,358,0,0,hZeeWlLfDHk
6,because the policy structure is more complicated so we have more information in the valley function as well this is work that's been published in 2017 and we have the code available online as well if you want details so i just wanted to show you some results with this system these are results in the atari games that are often used for benchmarking reinforcement learning agents and so what you see there the horizontal red lines are performance of the standard dqn agent that's used in these games and the green line is option critic and you know on the face of it you think well why are you so excited about this right it just learns to be roughly on par with dq on but the reason this is very exciting is that we're actually learning a lot more than just how to solve the task we're learning a set of options that are potentially reusable in other tasks we're learning a set of sub goals and the predictions associated with these sub goals as well and doing this from the same amount of data as dqn is a win in the sense that we're acquiring more knowledge leveraging the data in a better way the other interesting thing on the bottom graph you see some qualitative results from one particular game called c quest we're looking at what kinds of things are being learned and actually this agent learns sort of two pieces of knowledge one is that sometimes the submarine that that is playing the game needs to go to the top of the water because it requires oxygen and sometimes it,358,0,0,hZeeWlLfDHk
7,needs to go down to pick up a diver and so we can actually look at these agents and interpret a little bit better what it is that they do and this is behavior that again is consistent in many of these games now there is one problem with this approach which is that in order to keep it very general we allowed our agents still only to optimize the reward function of the task at hand and not anything else and so as a result they actually will acquire options but then often if we let them run long enough they will kind of dissolve them back into primitive actions and that's because we know already from markov decision process theory that optimal policies really can be expressed in terms of primitive actions and sold you know in fact this is the optimal behavior but we of course would like to keep these options around because we believe that they would be useful later on and so in order to do this we use a framework again inspired from psychology theory called bounded rationality the idea is that an agent would find it more difficult to make a choice than to just stick to whatever it was doing before and so we penalized the agent for deliberating over his choices of option and hence encouraged it to stick with to an option once it's been chosen and so this is an illustration of this approach in a game called a mead are the farthest picture there shows what happens if you introduce no bounded rationality and no regularization the agent basically just gets some very,358,0,0,hZeeWlLfDHk
8,small pieces of behavior but once you allow it to regularize using bounded rationality you actually see the agent doing long trajectories that belong to each option and terminating in interesting places so that where it tends to terminate is either around hallways where there is a choice of where to go or in areas where there's an enemy that comes and so you might need to terminate and switch direction and so again we see a little bit of interpretability emerging from these types of methods and now in this case we basically introduce an extra term that is used to condition the termination of an option and so there's a general question of could this be done in other ways as well and actually we had a paper this year at ai stats with collaborators from deep mind led by an entire to nyan which gives a general theorem that allows us to decouple the objective of the policy inside an option and the objective of the termination condition and again we can sort of look at the effect of this approach on some simple navigation tasks and so what you see is if you use vanilla option critic these options you know off will indian terminate a little bit everywhere whereas if we encourage terminations to be focused only in specific states we obtain this effect where the termination function is focused in just a few places and performance tends to be better although large-scale experiments are still ongoing with us so this is sort of the procedural knowledge we're going to think of it as as being options now the question is,358,0,0,hZeeWlLfDHk
9,what kind of predictive knowledge do we want reinforcement learning agents to have and so in order to do this the reinforcement learning way we're going to think of the basic building block of predictive knowledge in this framework which is the value function and so there's a little bit of math on the slide but it's actually interesting to think about it what's the value function the value function expresses an expectation and the expectation is of a long-term quantity which is the sum of rewards that the agent will receive over time and typically we also introduce a discount factor which essentially tells us when the agent is going to continue on when it's going to terminate right and so we can think of this discount factor essentially as a continuation function the probability that the agent will live and see another day so the value function is a very general quantity in some ways it is also somewhat limited in the sense that it is an expectation it's not a full probability distribution for example but the advantage is that we understand it very well and we've spent a lot of time right roughly thirty years building very efficient methods for learning value functions from samples doing bootstrapping algorithms leveraging dynamic programming ideas in order to compute these effectively and so what i'm going to propose to you is that in order to build predictive knowledge we may actually consider a simple generalization of a value function that actually allows us to learn about many things so we're going to generalize this in two ways one is instead of just thinking of numerical,358,0,0,hZeeWlLfDHk
10,rewards and cumulating numerical rewards we're going to think of general cumulants which could actually in fact be vectors or could be matrices or could be tensors okay the only property that we requires of course activities of we can do the cumulative part and so see in this equation is a is a general cumulant as in usual reinforcement learning we think of it as depending on the state and action and possibly the next state that the agent ends up in the other ingredient here is going to be a slightly more general continuation function which instead of being a fixed probability of the agent surviving at every time step is now going to become dependent on the state and so the agent is going to continue or to terminate depending on the state that it's arrived at the first work that actually proposed ideas of this type is it's very very old it's the pandemonium architecture which was proposed by oliver selfridge back at the beginning days of ai in the 50s and 60s and his idea was basically that you can have one stream of data but an ai agent will have a bunch of little demons in its head that are all looking at the data and predicting different things and shouting out their predictions and there will be a controller on top that listens to all these predictions and then decides what to do and so this in some sense is an implementation of this idea we're going to have one stream of data we're going to have many different value functions sitting on top of the stream of data,358,0,0,hZeeWlLfDHk
11,computing expectations for very different cumulants for different time scales and all of these predictions will be available at the same time for an agent who tries to to leverage them and so we we had in our incarnation of this architecture back in 2011 and the idea is that we're looking at now are essentially extensions of this architecture now one way to think about these value functions is that they're almost like little lego building blocks of knowledge where you have cumulants coming in you have predictions going out you have continuation functions coming in and so you can start thinking of taking these value functions and hooking them up together right where for example a cumulant could be actually the output of a different value function or or our value function could be provided in features for a different value function and so there is no longer the case that we have one value function inside the age had in fact we're going to have many this is an idea that's also been explored in in a more simplified form through auxiliary tasks in the work of maxia de burgh so there's actually lots of different things that can be expressed in this remark and this is just a little laundry list for those of you who know about the successor states and successor features option models feudal networks value transport so you know it's an interesting thing to think about this framework as being very very general and expressive now the question is is this just interesting conceptually or can we do something with it and so i'm just going to show,358,0,0,hZeeWlLfDHk
12,you one potential use of this which is actually illustrated in a nurbs paper that we have coming up and this is the use of general value functions for synthesizing behaviors and so the way we think about this is an agent might have multiple goals at different times okay maybe sometimes it's hungry sometimes it's thirsty sometimes it's tired okay so imagine this agent having at its disposal a keyboard and each of the keys corresponds to a goal and the more i press the key the more the agent wants that goal now you know how do we decide what to do well one way to think about it is the agent could play this keyboard one key at a time one goal is active the agent achieves that goal then another goal is active and so on and that is the way that usual option execution would have worked right you pick an option you stick to it until its completion then you pick some other option now if you're a musician you know that it's much more interesting to do chords right so to play possibly multiple of these keys at the same time right so an agent might be somewhat hungry and somewhat thirsty right then maybe it's not at all tired and so that will create a new way of behaving and so essentially what we do in this paper is we implement this idea by by using linear combinations of cumulants where cumulants can be positively weighted or negatively weighted and the agent decides on the weights on this of this combination and once the weights are there because,358,0,0,hZeeWlLfDHk
13,we've already learned how to achieve each cumulant independently we have a single short way of into sizing the behavior corresponding to it and so once we're given a weighting of the cumulants there's a general box that produces a policy that will achieve this particular weighting and so this is just one simple illustration there's an agent that's in a little round arena it learns three different pure cumulants which are going in these sort of three directions but then if we place a goal anywhere arbitrarily randomly the agent can figure out a weighting of these directions that will take it to that goal and it can even learn to avoid a particular direction if if necessary because we're allowing negative weightings as well and so what you see in the picture is the red curve is ddp g which is a standard policy gradient primitive action algorithm the green curve is q-learning using the pre trained options and it achieves a certain level of behavior but our approach doing this kind of automated synthesis is no more expensive and it actually leads to better performance so and i just like to point out in the discussion that these reinforcement learning agents actually already have a lot of interesting ways of creating knowledge by using options and by using generalized value functions and really a lot of the research going on right now is more having to do with the objectives that we should optimize while we are proposing cumulants while we're proposing sort of the structure of the representation there is however also one bigger open question which i would like to discuss,358,0,0,hZeeWlLfDHk
14,which is how do we actually want to evaluate empirically these agents and when we are doing sort of pure reinforcement learning tasks like oh that's actually fairly simple there's a clear objective function there's one reward and we're trying to get the agents to maximize that expected reward in the case of the agents that we're building now that have this kind of flexible knowledge what we would like ideally is an agent that does continual or lifelong learning that's in a situation where it interacts with the environment over a long period of time and keeps acquiring knowledge and using it and so that is a much more difficult task to assess okay in some sense we want to get at what does the agent know and what does it not know and right now the way we do this is by measuring its return which is a very crude right single number kind of measurement of how well the agent is doing and by looking at the qualitative behavior which is interesting but not always illuminating sometimes it's actually quite hard to qualitatively assess what these agents do and so what i would like to point out is that perhaps as a field we're going to need to rethink the way that we do empirical evaluation in order to gain more understanding of these agents there used to be a time where we could do theoretical analysis and gain this understanding now these agents use deep nets that's very hard to analyze theoretically but from an empirical point of view we may still want to sort of allow for an evaluation way that,358,0,0,hZeeWlLfDHk
15,doesn't look at the return as the only measure but actually tries to run targeted experiments where we make a hypothesis about what these agents might know and then we run an experiment specifically to test if the agent has that kind of knowledge and it's especially important when we are allowing gaugin to to learn this kind of predictive knowledge so we might for example ask does our agent know about object permanence and in that case we may run an experiment which is a sequential task to try and assess if that knowledge is present in our agent so i really hope that in the long run we will go to to a method that is more like this more like the scientific evaluation method thank you i'll be happy to take questions applause,176,0,0,hZeeWlLfDHk
0,"hello everyone. uh i'm going to present my uh project called deep q-learning implementation for atari games. the games which i chosen is uh riverate and that is present in the atari games. so in the next few minutes, i'll walk you through my deep learning project that trains an ai agent to play uh the atari river rate using deep learnings. so here's what i'll be covering today. uh first, i'll explain the complete workflow from environment setup to final evaluation showing how we transform random button mashing into intelligent gameplay. second, i'll dive into technical architecture including the dq network, gpu optimized replay buffer and custom environment wrappers to make that learning possible. third, i will demonstrate the hyperparameter optimization process that identify the best configuration from multiple candidates. then i will show you the actual training progression with real metrics and learning. finally, i'll present a detailed result analysis comparing our train agent against baseline performance, showing exactly how and why we achieved that improvement. uh throughout this this presentation, you will see the actual game play footage, performance charts, and the specific engineering decisions that made this implementation successful. let's begin with this technical workflow. so, the project follows a systematic six-phase workflow. the phase one would be the environment environment setup using the gymnasium with custom pre-processing. so here we can see uh so this is our river rate and the environment setup includes the required package installation which are gymnasium torch and uh so these are the three main packages that are required here.",338,0,14,UVknj2Ifwmg
1,"so here we can see uh so this is our river rate and the environment setup includes the required package installation which are gymnasium torch and uh so these are the three main packages that are required here. then after installing all these steps, we are going to import the libraries and set up the necessary gpus in order to uh make use of the gpus as well. uh so here you can see uh uh right now i run with the cpu. but if you only need to use the gpu, we can use the gpu as well. then uh the next one would be presenting the custom uh wrapper implementation. so this wap implementation uh gives uh the minus 100 penalty for the life loss uh teaching self-preservation and also it uh takes all the game scores which are uh shooting a ship jets and other things will give you a positive points and then uh shooting the fuel will give you a negative points and losing a life will give you a minus 100 penalty which is a huge but uh this is needed in order to train the uh deep learning uh concepts as so uh the next would be uh getting the uh fact environment function. so we are going to create a environment which is uh from the gymnasium. previously it's called a gym but right now it's called gymnasium. so that's the newer version of it. so both are same but uh since we are going with the new version that will be much easier. so i'm going to initialize the evaluation and video recording function where uh after the training is completed.",364,14,24,UVknj2Ifwmg
2,"so i'm going to initialize the evaluation and video recording function where uh after the training is completed. so it will evaluate based on the results and then it will give you the uh video footage of the game it played. so that will be present in the google collab itself in the cell output. so that would be a great way to uh show our model that uh it played well. then we are going to have the evaluation with the random agent. so random agent test that it gives the uh couple of actions to the environment and it will see how the game played as you can see. so uh this is the uh game it so it took uh the best score as,1540 and 1254 is the average score. so we tested this game for the random movements and we are going to implement now the dq architecture. so the there are like three convolution progressive uh scores which are like uh 32 filters 64 and 64 and these are the kernel sizes for the dq1 uh architecture as well. so um this uses the dq architecture with 1.67 million parameters uh separating value and advantage estimation. so here you can see it uses all the gpu uh usages and it use the memory flow optimization as well so that uh it runs very uh efficient on those gpu and as well as on the cpu. so i'm implementing the replay buffer as well here. uh after implementing the next would be like action selection and the training loop. so this is the main uh architecture that has the all these things.",358,24,37,UVknj2Ifwmg
3,"so this is the main uh architecture that has the all these things. then the next step would be uh hyperparameter configuration. so first we are going to get the best parameters of the uh like of the architecture and then we are going to use that to train our full model. so right now i'm using a four different kinds of hyperparameters. so one is the standard aggressive patient and explorer. here you can see i'm tuning the these four parameters which are learning rate gamma alpha and epsilon final. so after uh giving these ones so these are the full configuration of our uh u dq and architecture. so uh the main thing to note that we are having a buffer capacity as 50,000 and the number of episodes which are going to run is for the thousand ones and for every 4,000 we are going to get the target updated. so the learning starts is like 20,000 because um now we need to initially first run for the few uh steps in order the model to train it and then we can go ahead with the uh sorry first exploring and then we need the model to train it as well. so the after running the hyperparameters the best episode will be present here. then the we are going to have the base model training which is like without any learning we are going to train the uh give the scores of the model. uh so that model we have the score as 913 which is the average score. so which is uh before the model training here.",352,37,49,UVknj2Ifwmg
4,so which is uh before the model training here. so we're going to implement with the parameters which i used from the hyperparameters and then we are going to use that one. the next one would be u memory optim hyperparameter search implementation. so after implementing uh we can find that the best configuration is the learning rate is uh 025 with these hyperparameters as you can see. uh so these are the average rewards for each and every uh trial and we can see this one has the good results as well. then we are going to use that parameters to train the full model. as you can see for every training steps we have written to average rate and the loss and the steps taken to complete this and masel then we are going to visualize it. so this is the entire visualization which we got. so these initial parameters because we uh train the agent to do the uniform explanation that's why we have a very uh uneven rewards and then once it uh start learning it progressively increases and it's the same for the training loss as well because once the model gets learned it will uh get all the details for it and then it will produce the results and it means that it's learning from the game. so the evaluation would be uh after doing we are giving a final evation for the train model. so i'm going to play a game. so so this is the model which play the game. as you can see uh it uh passes through very well and it has the maximum score of 2210.,360,49,61,UVknj2Ifwmg
5,as you can see uh it uh passes through very well and it has the maximum score of 2210. now comparing all these results we can find that uh uh so the green would be the train model. as you can see the all the scores uh for the train model is very good compared to the random or the baseline models. so as you can see uh which is very uh good and it has trained from the model. so for now since we played for the thousand episodes and in the future if we are going for the other uh episodes i think it will uh learn even better. so as you can see like uh the project successfully demonstrated that deep learning uh with the significant achievements uh the 30 performance improvement uh validates our architectural choices. so key technical contribution include the gpu optimized replay buffer uh two-phase uh strategy and other things as well. uh this project demonstrate that uh successful in in reinforcement learning requires not just an algorithm but careful engineering of all these uh components as well. so thank you for watching and uh all the codes and everything is present on my github. so feel free to uh check the entire one so that uh we can have uh further topics. thank you.,288,61,71,UVknj2Ifwmg
0,"hey everyone and welcome back to the channel. today we're diving into the exciting new release of autocode agent version 1.8.0. this update introduces some powerful capabilities particularly around making our retrieval augmented generation or rag systems much smarter and more adaptive. the headline features are a brand new versatile reinforcement learning agent and its application in a system we call rl meta r a. this new system dynamically chooses the best r a technique for your specific query. sounds cool, right? but before we see how it works in practice, let's understand the core concepts behind this new rl agent. so what exactly is reinforcement learning? at its heart, rl is about learning through trial and error, just like humans or animals often do. imagine an agent that's our learner like the q-learning agent in autocode agent. this agent exists within an environment, the system it interacts with. the agent observes the environment's current state, which is a description of the situation. based on this state, it takes an action. the environment then gives the agent a reward, positive if the action was good, negative if it was bad, and transitions to a new state. the agents goal to learn a policy, a strategy for choosing actions in different states that maximizes the total reward it collects over time. one popular rl algorithm is q-learning which our agent uses. q-learning focuses on learning the value of taking a specific action in a specific state. this is called the q value written as q of s comma a. a high q of s comma a means taking action aa in states seems like a good long-term bet.",360,0,18,hvm13n3xVHY
1,"a high q of s comma a means taking action aa in states seems like a good long-term bet. a low q of s comma a means this action probably won't lead to good rewards from this state. the agent learns these q values by repeatedly. first being in state s. second choosing an action a, sometimes randomly to explore, sometimes picking the best known one. third, observing the reward r and the next state s prime. fourth, updating its estimate for q of s comm, a using the q-learning formula. this update considers the immediate reward r and the estimated best future reward it can get from the new state s prime discounted by a factor gamma, how much we value future rewards. the alpha parameter controls how much new information overrides old estimates. now, our q-learning agent is special because it supports two modes for handling these q values. first, the simple mode tabular q-learning. this uses a straightforward dictionary like a table to store q values for every state action pair it encounters. it's easy to understand and guaranteed to find the best policy if the number of states is small and manageable. think of simple games or grid worlds. the downside, it doesn't scale. if you have too many states, the curse of dimensionality, the table becomes impossibly large and it can't generalize to states it hasn't seen before. second, the neural mode, deep q network, dqn. this is where things get powerful. instead of a table, it uses a neural network, a brain, to approximate the q values. you feed the network the state as a vector of numbers and it outputs the estimated q values for all possible actions.",370,18,37,hvm13n3xVHY
2,"you feed the network the state as a vector of numbers and it outputs the estimated q values for all possible actions. its pros, it can handle vastly more complex problems with huge or even continuous state spaces like sensor readings or complex feature vectors. crucially, it can generalize. it can make intelligent guesses about states it hasn't seen based on similar states it has seen. its cons, it's more complex to set up and tune. can sometimes be unstable during training, requiring techniques like experience replay not in this basic version and is less interpretable. it's harder to see why it chose an action. it also typically needs more data and computational power. so that's the core idea behind the new rl agent, a flexible learner that can use either a simple table or a powerful neural network to figure out the best actions based on experience. okay, theory is great, but how does autocode agent actually use this new rl agent? this brings us to rl metar a. think about different r a techniques. standard vector search like basic llama index might be good for simple keyword queries. highde hypothetical document embeddings might be better for questions needing inferred context. adaptive rag might excel at complex queries by first classifying them. which one should you use? it depends on the query. rl metar treats this selection as an rl problem. the agent is our q-learning agent specifically in neural mode. the state is a set of features extracted from the user's query. eg its complexity type keywords generated using an llm. the actions are the different rag techniques available. eg. action zero equals llama index. action one equals highde.",365,37,61,hvm13n3xVHY
3,"action one equals highde. action two equals adaptive rag. the reward ideally comes from how good the results were using the chosen rag technique. this can even involve human feedback. the goal is for the agent to learn a policy. for this kind of query state that rag technique action usually gives the best results, high q value. it's meta rag because it operates above the individual rag techniques orchestrating them intell. as you can see in the video demonstration, i make a request to activate the rl meta r a tool. the system initializes the q-learning agent in neural mode. initially, it attempts to select the r a technique with the highest q value. however, because we don't have sufficient training data yet and the prediction error td error is consequently high, we've implemented a fallback mechanism. in this early stage, the choice of rag technique is actually made by an llm suggestion after receiving the response generated by the chosen r a technique. the user has the option to provide a rating from 1 to 5. this rating serves as the crucial reward signal for training our deep q network dqn. this reward is used to update the network specifically shaping the q value for the particular state, the query features and the selected action the r a technique used. by using this one to five reward, we train the model to learn the expected long-term value q value for that specific state action pairing. okay, so let's really dig into this q-learning agent and see how it ticks. basically, it's built on those q-learning ideas we talked about.",352,61,78,hvm13n3xVHY
4,"basically, it's built on those q-learning ideas we talked about. it figures out how good an action is in a particular situation, that's the q value, by trying things out and seeing what rewards it gets. it uses that standard q-learning update formula, tweaking its guesses based on the reward it just got and what it thinks it can get from the next situation. we've got the learning rate alpha which decides how much weight to give new experiences and the discount factor gamma which is all about how much it values future rewards compared to immediate ones. now the cool thing about this agent is that it can work in two different ways. first up is the simple mode. this is your classic old school tabular q learning. think of it like the agent keeping a big spreadsheet or a dictionary. for every possible situation or state and every possible action it can take, it stores a q value. how does it work? well, it maps states which need to be something simple it can use as a key like grid coordinates to a list of q values, one for each action. what's good about it? it's really easy to understand. you can literally look inside and see the value it's learned for taking a specific action in a specific state. plus, if your problem isn't too complicated and you let it learn long enough, it's guaranteed to find the best way to do things. what's the catch? it hits a wall if there are too many possible states. this is called the curse of dimensionality. the table just gets way too big to handle.",358,78,96,hvm13n3xVHY
5,"the table just gets way too big to handle. also, it can't really make educated guesses about situations it hasn't seen before. it only knows what it's experienced directly. that brings us to the second way it can work, the neural mode. this uses what's called a deep q network or dqn. and it's designed to handle the problems the simple mode can't. why do we need this? because lots of real world problems have tons of states or states described by continuous numbers like sensor readings or complex features. a simple table just won't cut it. so, how does this work? instead of a table, it uses a neural network, kind of like the agent's brain, to estimate the q values. you give the network the current state represented as a bunch of numbers, and it spits out the estimated q values for all the actions you could take. now, a really important part here is figuring out how to represent the state. you need to turn whatever your state looks like, maybe text, maybe an image, maybe some structured data, into a fixed list of numbers that the neural network can actually process. this might involve scaling numbers using one hot encoding for categories or creating embeddings. the agent actually has a specific part, the underscore pre-process state method designed for this, and you usually have to tailor it to your specific problem. what are the advantages? dqn's can handle huge numbers of states, even continuous ones. and the big one, they can generalize. they can make pretty good guesses about situations they haven't seen before based on how similar they are to situations they have seen. what are the downsides?",367,96,116,hvm13n3xVHY
6,"what are the downsides? well, they're definitely more complicated to set up and get working right. sometimes the training can be a bit shaky, although this version keeps it fairly basic. it's also harder to look inside and see exactly why it's choosing an action. they're less interpretable. and usually they need more data and more computing power to train compared to the simple mode. so besides these two modes, the q-learning agent has some other essential parts. there's the setup part sakuran in it a where you tell it how to behave. things like the learning rate, discount factor, how often it should explore randomly, epsilon, whether to use simple or neural mode, how many features are in your state if you're using neural mode, how many actions it can choose from, and where to save its progress. it also needs to save and load what it learns. save data, load data. for the simple mode, that usually means saving the q table. for the neural mode, it saves the trained neural network. itself, plus other info like how much training it's done. we already mentioned state pre-processing, pre-process state, super important for neural mode to turn raw states into numbers the network understands. then there's getting the q values. get q values. you give it a state and it tells you the current estimated q values for all actions. simple mode just looks it up in the table. neural mode runs the state through the network to get the predictions and the core learning happens when you update the q values. update kar value train step after the agent tries an action and gets a reward and sees the next state.",367,116,136,hvm13n3xVHY
7,"update kar value train step after the agent tries an action and gets a reward and sees the next state. you call this simple mode just plugs the numbers into the q-learning formula and updates the table. neural mode figures out a target value based on the reward and its prediction for the next state and then it nudges the neural network's weights a little bit trying to make its original prediction closer to that target. finally, choosing an action, choose action. this handles the balance between exploring new things and exploiting what the agent already knows. most of the time, it picks the action that looks best based on the current q values. but sometimes with a probability called epsilon, it just picks an action randomly to make sure it doesn't miss out on potentially better options. usually this epsilon value gets smaller over time as the agent learns more. just to make it clearer, think about that simple grid world example again. if you used simple mode, the state might just be the row column coordinates. the agent learns values like the q value for being at 2, three, and moving right is. if you used neural mode, you'd first have to turn row column into numbers. maybe like scaled row number, scaled column, number. the neural network takes those numbers in and gives you q values for moving up, down, left, or right. training means showing the network lots of these number vectors and the target values they should have produced.",330,136,150,hvm13n3xVHY
8,"training means showing the network lots of these number vectors and the target values they should have produced. so understanding these pieces, the two modes, why state representation matters for neural networks and what these key functions do gives you a really good picture of how this q-learning agent operates and how you could use it for different kinds of learning tasks. all right, so how does this q-learning agent actually power our advanced r a system? let's talk about the rl meta r a implementation. the core idea here is pretty neat. instead of just picking one rag technique like llama index or highde and sticking with it, we use our q-learning agent specifically in its neural mode with that deep q network to dynamically choose the best rag approach for whatever query the user throws at it. the whole point is to learn the best way to pick a tool over time so we get the highest quality information back. you can think of it like an intelligent ra orchestrator. here's a breakdown of how it actually works. when a query comes in, first up, llm powered feature extraction. this is where it all begins. when a query arrives, we don't just send it straight to a r a tool. instead, we use a large language model guided by a specific prompt to really analyze the query. the llm pulls out key characteristics, things like, is this a factual question or a comparison? what's the topic like technical or historical? are there specific names or places mentioned? it even scores things like complexity, ambiguity, and urgency and notes the query length.",354,150,166,hvm13n3xVHY
9,"it even scores things like complexity, ambiguity, and urgency and notes the query length. all these features get processed, maybe turning categories into numbers, scaling scores to create a numerical state vector. this vector is what our rl agent actually understands. next comes the adaptive action selection. now the system has to decide which rag technique to use. llama index, highde or adaptive rag. these are the actions it can take. and this is where the smarts come in. the system keeps an eye on how well the dqn agent has been doing recently by looking at its average prediction error over the last few times it made a choice. if the agent's doing well, meaning its error is low, the system trusts the policy it's learned. it uses the dqn to estimate how good each r a technique is for the current query features and then picks one, usually the best rated one, but sometimes it explores randomly just to keep learning. but if the agent seems to be struggling, maybe the error is high or it just hasn't had much training yet, the system plays it safe. it falls back to asking another llm directly for a suggestion on the best rag technique for this specific query. this mix and match approach makes the system robust, blending what the agent has learned with the reasoning power of an llm. after that, it executes the chosen r a gn. once a rag technique is picked, the system runs the corresponding retrieval function. then, it generates the final answer.",336,166,182,hvm13n3xVHY
10,"then, it generates the final answer. the information retrieved by the chosen rag tool is combined with the original query and another llm call puts together the final answer that the user will see. now, here's a really cool part. the optional human feedback loop. if this feature is turned on, after generating the answer, the system saves the state vector, the r a tool it chose, and the original query. then using websockets, it sends the answer to the front end and asks the user, hey, how helpful was this? rate it one to five. that rating becomes the reward signal. and finally, the learning step, the actual learning for the dqn agent doesn't happen right away. it usually happens later when that human feedback, the reward comes back. that reward along with the saved state and action is used to train the dqn helping it make better r a selections in the future. throughout this whole process, the system can use websockets to send real-time updates to the front end. you can see the features extracted, whether the rl agent or the llm made the choice, the predicted values, which rag tool was picked, and if it's asking for feedback. this makes the whole thing much more transparent. so you see rl meta r a isn't just some abstract idea. it's a working system using the q-learning agent in neural mode right at its core. it cleverly uses an llm to understand the query, adaptively chooses between its own learned strategy and an llm fallback, runs the best rag tool, and even has a way to learn directly from human feedback to get better over time. the advantages here are pretty significant.",369,182,199,hvm13n3xVHY
11,"the advantages here are pretty significant. it's adaptive, handling all sorts of queries by picking the right tool. it's self-improving, especially if users provide feedback. it aims for optimized performance, trying to get better results than any single rag method could alone. it's etransparent, showing you how it's making decisions, and it's a great example of the hybrid intelligence, combining rl's ability to learn patterns with the reasoning power of llms. this makes it perfect for building advanced chat or search systems that need to deal with unpredictable questions, learn from users, or manage a whole suite of different information tools intelligently. autocode agent 1.80 really takes a leap forward by bringing in reinforcement learning. that dual mode q-learning agent gives us a solid base for learning from experience. whether we're using simple tables for simpler problems or deep q networks for complex tasks like this rag selection, the rl meta rag system is a fantastic showcase of this, creating a r a orchestrator that learns to pick the best tool for the job, adjusts its approach based on how well it's doing, and can even be guided by human input. this really opens up some exciting doors for building smarter, more adaptive, and ultimately more effective ai systems. so, what do you think of these new rl features? how might you use the q-learning agent or the rl meta rag system in your own projects? i'd love to hear your thoughts. drop them in the comments below.",322,199,212,hvm13n3xVHY
0,"wei wei: hi, there. welcome back to this video series of reinforcement learning with tensorflow agents. my name is wei, and i'm a developer advocate at google. as time we gave you a quick overview of reinforcement learning and its applications. in this video, we are going to teach you how to use tf agents to quickly train a cartpole agent. so what is tf agents? tf agents is a reliable, scalable and easy-to-use reinforcement learning library for tensorflow. while there may be a number of open source rl libraries you can use, tf agents is the one we recommend. tf agents is great for learning, reinforcement learning, because it comes with an extensive set of colabs, examples and documentation to help you get started. you can use tf agents to solve realistic and complex rl problems with scalability and develop new rl algorithms quickly. you can easily swap between different agents and algorithms for experimentation. it is also well-tested and easy to configure. tf agents is built on top of tensorflow 2. so all the easiness of tensorflow 2 comes for free. you can use the eager mode to quickly develop and debug your code. keras helps you easily define your neural networks and tf.function automatically and speeds everything up. tf agents is also modular and extensible. you can define your own custom environment or agent quickly. and lastly, our extensive examples and documentation can help you get started really fast. here's an overview of tf agent's components. at a high level, the process to train our agent is we use our agent to interact with the environment so that we can collect experience trajectories.",358,0,20,2nKD6zFQ8xI
1,"at a high level, the process to train our agent is we use our agent to interact with the environment so that we can collect experience trajectories. we then feed the trajectories to our agents to train, so that it becomes better to receive the rewards from the environment. we keep doing this until the agent has good policy to extract the most rewards from the environment. let's talk a little bit more about environments and agents. first, the environment. tf agents supports a wide variety of environments. gym, atari, pybullet, deepmind-control, and bsuite. tf agents also has an interface to mujoco. recently mujoco was acquired by deepmind and is now free to use. next, agent. tf agents has implemented a number of classical and state of the art agents, such as dqn, reinforce, and ppo. they are fully tested and ready to use as black boxes. that's the nice thing about tf agents. you don't necessarily need to know how to implement these agents yourself, but you can still use them effectively for your own needs. now, let's walk an example of using dqn agent with tf agents. remember that dqn is a kind of value-based method. you'll see why in a minute. dqn stands for deep q network. here q is what we call action value function. it basically means if the agent is in state and is going to take action a, what's the expected total reward for that action over the time horizon? an optimal action value function q star gives best value possible for any policy, which is the first equation. so think about this.",351,20,41,2nKD6zFQ8xI
2,"so think about this. intuitively, if you know the reward values for all possible actions given a state, it's pretty easy to pick the best action for that state. now, our job is to represent the q function with a neural network, which leads to the second equation. we're not going into the math here, but basically it tells you how to update the neural network parameters based on the experience trajectories we collected during the training in place. for a more detailed discussion on dqn, i highly recommend the reinforcement learning course by deepmind and university of college london. now, we come back to this overall picture of tf agents. for the environment, we are going to use the built-in cartpole environments. tf agents provides an easy interface to load the environments from gym. we just call load function here and tf agents will take care of it. we call that and we are trying to keep the pole balanced by moving the cart left or right. the agent receives a reward of positive 1 if the pole remains upright at each time step. it's important to understand the environment's specifications since not all agents are compatible with all the environments. here we print out the specs of the observation and action. you can see that our observations is a tensor of four float values, which are the car's position, velocity, the pole's angle, and the velocity respectively. the actions is just an integer that can be 0 or 1, which means we move the cart left or right. now, we define our q network.",345,41,56,2nKD6zFQ8xI
3,"now, we define our q network. as i mentioned just now, we are going to use a neural network to represent the q function. the network is pretty simple and consists of a few dense layers. the last dense layer needs to output a probability distribution over all possible actions, which then allows us to pick the best action. we stack all layers together and we have our q network. next, we define our dqn agent. we basically pass the environment specs and q network to the dqn agent. next, we define how we collect the experience trajectory. all agents contain two policies. agent.policy is a policy that is used for evaluation and deployment. agent.collect_policy is the second policy that is used for data collection. in the original atari dqn work by deepmind in 2015, there's a technique called experience replay to make the network updates more stable. basically, at each time step of data collection, the transition data are added to a circular buffer called the replay buffer. then, during the training, instead of using just the latest transition to compute the loss and the gradient, we can build them using a mini batch of transitions sampled from the replay buffer. this has two advantages, better data efficiency by reusing each transition in many updates and better stability using unrelated transitions in a batch. we also call these transitions trajectories. we'll see how these trajectories look like in a minute. to start an example, this trajectory is for training. we set up a replay buffer. we're going to use a framework called reverb. it is open sourced by deepmind and is specifically designed as an efficient framework for experienced replay.",365,56,76,2nKD6zFQ8xI
4,"it is open sourced by deepmind and is specifically designed as an efficient framework for experienced replay. to set up the reverb replay buffer, we first create a reverb table using trajectory specs, sampling rules and et cetera. then we start a reverb server. next, we create the reverb replay buffer using the reverb server and the table. note that we set sequence length to 2, since this is required by dqn agent. now we create an observer to the replay buffer. we create a replay buffer observer here. it will be added to the driver in a minute so that the trajectories can actually be collected into the replay buffer during the training. the last piece of the puzzle is driver. driver is the utility that runs the experience collecting loop for a certain number of episodes or steps. we pass our replay buffer observer to the driver to collect the trajectories for training. after training data is collected into the replay buffer, we can sample them out as a tf data set. here's how our experience trajectories look like. it contains important information, such as action, observation, and reward. finally, we have all the pieces in place and can start training. we can simply run a training loop here. in the loop, we first use the driver to collect experiences. then, we sample some trajectories from the replay buffer, and lastly, train the agent. that's it. we can visualize the average reward as the training progresses. eventually, the agent will become so good at cartpole that the pole never falls down as you can see in the gif here.",352,76,96,2nKD6zFQ8xI
5,"eventually, the agent will become so good at cartpole that the pole never falls down as you can see in the gif here. to sum it up, today we introduced the tf agents to you and walked you through how to use the built-in dqn agent to easily solve the cartpole environment. although it seems there are many steps to set it up, it's actually pretty straightforward once you understand the overall workflow and the key components. solving other tasks will follow almost the same flow. in our next episode, we'll be teaching you how to build a board game app using tensorflow agents and the tensorflow lite. thank you for watching this video. see you next time. music playing",156,96,103,2nKD6zFQ8xI
0,here you're watching the starset algorithm in the beginning stages the agent is taking random directions to try and figure out what is actually a good action after this clip you will see a later stage in the sizer algorithm where it takes a very precise path that would be deemed a safe path next we have q learning q learning is slightly different from sarsa and we'll get more into that once we get further into the video here you see the agent selecting random actions it does this so it can evaluate random actions to see which action is best given a state now i will show you a quick clip that is of the key learning algorithm once it's picked an optimal path the topic of reinforcement learning is what actually got me into machine learning and so i thought it'd be really cool to actually go through this book called reinforcement learning i'll put the book title somewhere here right as i make my way through this book i'm going to start making videos on the algorithms within it on the various different types of problems so if you're interested in seeing that please subscribe more content will be coming out and so without further ado let's just take a look at how these two algorithms work so this is the environment the environment is a 4x12 rectangle and the guy in the bottom left is the character that the agent controls the agent is the actual reinforcement learning algorithm what we can do here is we can think of every box inside this rectangle as a state a state is just,358,0,0,MI8ByADMh20
1,wherever the agent is at any given time so let's go ahead and make a table for now we'll just fill that table with a bunch of zeros and we'll call this the state table in each state the agent has a choice to go either up down left or right each time the agent makes a move either up down left or right the agent will receive a reward for that movement and so say for example the agent moves up now the only way that our agent can learn from our environment is if the agent gets feedback this feedback is called the reward and so we just observed our agent moving the character up moving the character up will receive a reward of negative one in fact every single movement that the character takes will result in a single reward and so instead of this bottom table being full of zeros allow me to replace it with the reward for moving to that respective square there's something very fascinating about the table i just showed you in the previous clip it's that the only space where the agent receives a positive reinforcement or positive reward is at the end goal square in the bottom right corner and that's because i don't want the agent to run around the map and then get stuck on certain squares so the agent views every step as a penalty up until it reaches the goal also if the agent falls off a cliff the agent will lose 100 points opposed to 10.,339,0,0,MI8ByADMh20
2,this is to show like a sense of danger that if you die you lose more than if you just wander around the map and explore for yourself the best way for us to evaluate our agent is by adding up all the rewards that's gathered over the entire episode this will give us information on how well it's performing and also if it's wasting time exploring or if it's just falling off the cliff this time let's reset our character and allow it to move randomly throughout the environment and will sum up the total award over time while this is happening i would like for you to also notice the observation or the state the current agent is in and the action that it's choosing to take all albeit random there's a really small subtlety happening right here and i want to know if you guys can find it it's most noticeable in the beginning but i think if you watch it enough you might be able to figure it out on your own that's small subtlety is that when the agent begins it only starts off with a state and an action it only receives the reward after the agent has moved and so the sequence kind of goes state action reward state action reward so on and so forth before we get into the specifics of the sarsa algorithm we need to establish some notational consistency so s sub t is the current state that the agent is in a sub t is the action that agent is taking and then r at t plus one is the reward after taking that,358,1,1,MI8ByADMh20
3,state in action so t is the specific position that you're in we know from our discussion earlier that the quality of an episode is determined by the total reward the agent reaches at the end of that episode and so that's sort of all we can start with the action the state and the rewards and so is it possible to develop an algorithm that can evaluate each step to determine what the best action might be that's the underlying question for the sarsa algorithm and hopefully we can derive that in a common sense type of way let's start the derivation by defining something called the return the return is all of the rewards summed up for a specific episode an episode is the agent starting in the starting position and then ending in the ending position and a really important concept for the beginning of this is to note that all of the episodes are finite and so now we're going to start grouping these variables together in a very convenient way so here we'll put parentheses around r 2 plus 2 till the end of the episode and so what we really had there was the return for the episode but shifted over one and so we can replace that with the returns equal to the reward in the very next step the agent takes plus the return for the rest of the episode and so i think i know what you're thinking you're thinking well we don't know what g t plus one actually is because the agent hasn't actually gone through the entire episode and found the end so how,358,1,1,MI8ByADMh20
4,do we know what those rewards are and the correct answer here is that we're going to predict g t plus one that's the entire point behind sarsa and also q learning so instead of having gt plus one we can literally replace that with q and insert the next state and the next action taken the gamma variable there is just a way for us to adjust how much we want to weight the future returns in our calculation so think of it as a value that attributes how significant the q value is in the calculation and so what we like to do is make an algorithm that could actually compute these q values for any given state and action so first we are going to subtract the q value for the prior state and action what this will do is create sort of like a gradient so that we can take the difference between the current return and the prior return so that'll create sort of like a step for us to replace our current q value with let's add on the current q value so you can get the entire equation in front of you and so now you can see that what we're really doing here is replacing the current q value by that gradient that we just discussed the alpha in this case allows us to determine how much of the new step or the gradient will affect our current q value so higher levels of alpha means very large changes a very small values of alpha i mean very small incremental changes trying to find this sweet spot for,358,1,1,MI8ByADMh20
5,that value is an entire discussion on its own and so that really is the meat of the sarsa algorithm the purpose of that algorithm is to evaluate what states and actions give us the most reward as opposed to the key learning algorithm which is a policy algorithm that just means that the purpose of that algorithm is to find the best actions given a state so the equation then changes slightly the change here is that we add a max right before the next q value and so what that does is that when the next state comes around we don't want the q value corresponding to the next action we want the q value corresponding to the action that has the largest q value and so that's it for this video the the purpose of this video is just to sort of get an introduction to this topic as it's my first time learning it as i do more projects hopefully my understanding will get deeper and the actual in-depth nature of these videos might increase as well so also i find the actual learning stages of the agent to be very fascinating so after i wrap this up i will put some time footage of the agent learning from starside also key learning from the beginning some middle stages and then what it looks like at the optimal so i hope you enjoyed this video please subscribe if you did and i'll see you next time music foreign music,328,1,1,MI8ByADMh20
0,in today's video we are going to use deepmind's very own deep reinforcement learning framework acme to code up a couple of simple reinforcement learning agents now as you might imagine acme is an incredibly complex framework as such we are not going to have a whole lot of time to go over all of the intricacies of it today instead in a couple future videos i will look at how to create custom reinforcement learning agents using this framework in particular we're going to implement soft androcritic and td3 as these have not been implemented by the deep mind team now if you don't know anything about those two algorithms never fear i have a couple of videos linked over here in the top left so the first thing we are going to want to do is copy the code from the github so we're going to copy the git link and then head to the terminal and do a git clone so we'll do that first we will get clone and that will clone into the acme directory and then we're going to want to create our very own virtual environment for this reason being there are a number of dependencies for this which will probably mess up your base python installation if you should do this just willy-nilly without a virtual environment so we'll do making our own virtual environment we will then activate that of course we have the cd into acme and then do source spin activate and then it will work then we have to upgrade our pip and then we can begin the process of installing the deep mind,358,0,0,J1XCWjuyRaI
1,acme framework now keep in mind that being google this is implemented in tensorflow but they also have dependencies and code for the jaxx library which honestly i don't know really anything about it so we're just going to stick with the tensorflow stuff so we'll say pip install dmacm tensorflow and then we'll do pip install dm acme envs this will give us some of the base environments we will have to install jim in a moment i'll get to that in a few minutes now acme is built on top of many other packages written by deepmind one of which is something called reverb now reverb is a framework for serving machine learning data and the basic idea is that you have a server that serves data to any number of clients and we're going to be adding data tables to this when we code our own custom agents for the code we're running today it's already implemented for us but the basic idea is you create data tables within this server and then sample that data using whatever strategy you want like say prioritize experience replay or n-step transitions anything like that using any number of servers it's highly scalable it's an incredibly powerful technology and everything we're going to be doing is built atop that and so then we also need something called sonnet now sonnet sits atop of tensorflow as a sort of additional framework for creating deep neural networks i haven't played around a whole lot with it i've used it for these projects but it's not something i've done a deep dive on and i don't really know what,358,0,0,J1XCWjuyRaI
2,all is going on with it so next we're going to want to install tensorflow and i'm going to specify a particular version reason being the 2.8.0 release candidate has some conflicts with some of the other codes so i'm going to specify 2.7 because i know that works now of course if you're watching this in the future maybe the 2.8 full release has no issues maybe they've ironed everything out but for now let's stick with what works next we will need tensorflow probability courses handles probability distributions for policies next we need to install something called checks i don't know what that is honestly i didn't have the energy to look it up next we have trfl truffle that is a tensorflow reinforcement learning agent package and then we want to do an upgrade for jim atari and it's really a downgrade because we're running 0.21 we want to downgrade to 0.19 because as of 0.21 or perhaps 0.20 they quit including the roms with the gym environment so when we try to run pong or any other atari game we're going to get an error so let's downgrade to 0.19 so we should be good to go so now we can do a list go to the examples we can go to atari and then we can see we have a whole bunch of files here we have run dqn run impala and run r2d2 we're just going to run a dqn here to make sure it works and i'm expecting this error here so now this is easy to deal with what it's telling us is that we don't have a,358,0,0,J1XCWjuyRaI
3,particular shared object file so of course i am running linux kubuntu 20 if you're curious and so i haven't set up the ld library path environment variable in this particular virtual environment so we can check that by saying echo ld library library library path and i believe i need the dollar sign here because that's a variable and it tells us it points to uh cuda 10-10.1 so then we can locate that particular file to see where it lives and why it's unhappy so we'll just do live python 3.7 and we can see that it lives in this particular directory so what we're going to do is export ld library path equals that directory colon dollar ld library path and so let me move my face out of the way here the colon ld library path will with the dollar sign we'll append the current variable for the ld library path and we can verify that by echoing it to the terminal and we see right here that it now includes both directories as we want it to do now if you're running windows i don't know the fix for this i don't use windows except for some light gaming every now and again you should probably upgrade to a real operating system preferably linux although probably mac is going to be up more people's alley but nonetheless let's try running this again okay so now it is starting the replay the reverb replay server so i'm saving some checkpoints and i can see it's spooling up my cpu usage now so that tells me it is running okay so we do,358,0,0,J1XCWjuyRaI
4,get some output to the terminal uh you can't see it but it's telling us uh table priority priority tables access without grbc it's not doing any calls over the internet because it's just my local machine it has some code that indicates it is learning by outputting to the terminal and then it also does some output from each individual episode telling you the return the length and all that good stuff so let's stop that code it takes a second to shut down the replay server and then we're going to code up our own dqm agent as well as our own ddpg agent to see really how easy it is to get started with this so we're going to get out of this directory and we're going to go back here just to the base acme directory just for giggles so we're going to them into dqn dot pi and we're going to start with our imports we're going to say from absol import app we're going to use app.run to run our program let me zoom in a little bit for you guys and then we're going to import funk tools you'll see why we need that in a little bit now all of our environments are going to be wrapped if you've taken my course on deep q learning you probably understand that it just means that we're going to take the output from the gym environments and then um apply operations like say converting the screen images from integers to floats by dividing by 255 we're going to stack frames and repeat actions that kind of stuff so acme.agents.tf import,358,0,0,J1XCWjuyRaI
5,dqn so we're going to import the actual agent and from tensorflow we're going to import the networks now as i said um sorry i can't talk and type at the same time as i said we're going to be using all of the building blocks they give us and so the structure of their program is a little bit different than what i typically code on this channel because they are professionals and they know best uh their structure is that they have an agent that has both an actor and a learner so the actor is basically the policy and the learner basically collects uh the data from the reverb data table and uses that to update the deep neural networks and those networks are defined in that package networks excuse me that sub package networks and so we are going to be assembling all these building blocks into a deep q learning agent that we're going to test so the first thing we want is a function to really wrap all of our environments and this is written all of our code is written using uh i guess this type of syntax of specifying the typing came about in like 3.7 or maybe 3.6 something like that which i really like it is one of my biggest complaints about python is that it's not a strongly typed language now of course this doesn't make it strongly typed but it goes a long way towards rectifying that issue so here we're going to specify our level no frame skip dash v4 and they have something called an oar wrapper what that does is it,358,0,0,J1XCWjuyRaI
6,takes the time step from the environment and wraps it into an observation action reward tuple they use it for some other agents but not for the deep q learning agent it's a little bit i find a little bit inconvenient obviously they do it because they find it convenient but for someone from the outside looking in it's a little bit difficult to wrap your mind about around maybe i'm just a i don't know other thing to note is this arrow and then the return type that tells the python interpreter what to expect for a return type now as far as the typing what i'm talking about specifically as a brief aside is that in python you can pass in a variable to a function and change its data type from say a float to a string or from a custom object to a boolean and python never says hey did you really mean to do that and it can result in some subtle bugs down the line if you write your code in such a way that it's not strongly dependent on a particular data type so sometimes strings will evaluate in certain ways versus floats and it can make it inconvenient to track down bugs so i'm glad they at least uh inserted some features to at least improve the type checking capabilities so we want to make our environment uh level full action space equals true i believe what that means is that in the um the ale imports the ale environments themselves you have a total of 18 actions for all the atari games whereas in some of the,358,0,0,J1XCWjuyRaI
7,open ai implementations they reduce it down to just the stuff that actually does something so you don't have like 16 no ops and then two actions that actually do anything so then we'll say max episode length equals 108 000 if evaluation else 50 000 and then we're going to make a wrapper list that just tells the module what we want to do so we'll say wrappers.jim atari wrapper that is going to handle all the good stuff like stacking frames converting to grayscale resizing and then funktools.partial this will apply the atari wrapper and to float equals true that means we're going to convert the observations to a floating point number instead of integers and this zero discount on life loss i believe what that means is it sets the reward to zero the total discount uh to zero at the end of the episode um so that the loss of life actually affects the agent that's my understanding again i haven't mastered everything within this framework so take that in mind if you are a little bit more familiar with it and i say something incorrect please correct me in the comments i don't claim to be an expert on something that deep mind wrote i'm just using it uh to learn how the experts do stuff so then we're going to wrap all of our wrappers into our environment and return it and then we have our main function and that's not a typo you do need that underscore there uh and we're gonna say env equals make environment we're gonna need an env spec uh sorry what that does is,358,0,0,J1XCWjuyRaI
8,um it gives us the specifications for environment like the shape of the observations and action space make environment spec i'm going to pass in the environment now here we're going to kind of cheat a little bit and use their built-in networks so they have one specifically for a dqn in atari evspec actions.num value so we have to specify the number of actions now what this does is that it creates a uh it'll create a convolutional network just like what we see in the deep the deep q learning paper where it's got a bunch of convolutions and then we'll actually implement the dueling architecture from the paper on dueling architectures uh for you so you don't even have to worry about that and then our agent we're gonna need to pass in a an env spec and a network and then we need an environment loop to play the game environment loop env agent and then we're going to run we're just going to do 10 episodes uh other thing i want to note here is that the default parameters don't give you good learning so you do have to do hyper parameter tuning on this that's one way in which you know they don't completely and totally hold your hand and so then we want to run our app now let's it's unhappy expected two lines i upgraded my vim installation with some pep 8 guidelines and it's unhappy about something else undefined name bools that's because it is bool there we go and now it should be okay so let's uh sorry python dqn dot pi and see what happens,358,0,0,J1XCWjuyRaI
9,module acme rappers has no attribute jim atari rapper that's because it is jim atari adapter that's right there we go that is a typo let's try that okay so it's loading our reverb server that's a good sign and then boom it is learning so you see see how incredibly easy it is to get this up and running but we didn't really do a whole lot so perhaps it's not all that satisfying for you in particular one thing we can do to have a little bit more involvement in the process is to actually define our own networks and so to demonstrate how to do that i'm going to show you how to do a ddpg agent which of course has the policy and critic network the actor critic network so let's terminate that okay and then we're going to do deep deterministic policy gradients so we'll say from absol import app we'll need our wrappers again and again they have a ddpg agent which takes the networks handles the interface to reverb handles the functionality to actually add memories to that server the data table on the server has functionality for sampling a tensorflow data set uh as well as functionality for handling the learning so there's a lot that goes on under the hood in these agents and again i'll come back later to do soft after critic and twin delay deep deterministic policy gradients but we're kind of running long already about 20 minutes and so i just want to give you guys a brief overview of how all this works so we say macmi.tf import networks we're going to,358,0,0,J1XCWjuyRaI
10,need some utilities we'll need acme jim numpy sonnet they always abbreviate that as snt and we will need tensorflow next we have our main function we're going to make our environment i have a few different possibilities here i'm gonna use the pendulum v one that is just a swing up pendulum it's a very simple environment but this is um not in a format that that uh deepmind's framework is gonna expect so we have to do a gym wrapper to handle the interface with the dm underscore environment the deepmind environments and one common thing they do is use the single precision wrapper to convert everything to single precision to get max throughput on your gpu then we make our environment spec we're going to need the number of dimms in our action space so we'll say mpprod any spec actions shape and cast it as an int batch concat uh then we have our policy network sonnet sequential this is where sonic comes in so we're going to pass it a a set of layers so we'll say layer norm mlp multi-layer perceptron we'll do 64 by 64 by end dimms and what this does in simple terms is it makes three layers of shape 64 64 and n dimms of a layer norm linear layer basically so what essentially they implement in the original ddpg paper and the final activation will be a tan hyperbolic very very simple very very elegant we don't have to handle compiling or anything like that that gets handled by sonnet under the hood critic network another sequential network will have what's called a critic multiplexer,358,0,0,J1XCWjuyRaI
11,and what this does is that it concatenates the observations and actions as we do in the original paper then we have another layer norm mlp 64 64 and one of course one because this is the critic network it's outputs a single value next we have our agent it's a ddpg agent we have to pass in a little bit more this time i call it env spec up here i did envy spec and then we have our policy network critic network our observation network i'm not entirely sure what this is again i haven't dug into everything yet i'm just kind of excited about this and showing it to you guys as i go along and then whether or not we want to checkpoint our agents learning we'll set that to false because we don't really care at this point then we need our env loop again acme environment environment loop pass in our environment and our agent and run for just say 10 episodes and then app.run main all right so yeah it expects more spaces there and that was my only error so let's go ahead and run this and see if i made any typos ddpg pendulum one not found that's funny i must have different versions in my different uh directories for this obviously i've done this before so and the other one it didn't find pendulum v0 so let's switch that out to v0 and try again and there we go it is running already so you can see how incredibly easy that is they do have one warning here uh this looks like uh they put gradient,358,0,0,J1XCWjuyRaI
12,tape dot gradient inside of a loop or something i didn't do that let me scroll down so you can see that that's something in their own code i'll probably take a look at that when i write the td 3 agent because it'll be relevant to the project but yeah otherwise you can see how incredibly simple it is to run this and it did 10 iterations very very quickly so that is how you code agents like deepmind now obviously some things that stick out to me are that one there is a lot of complexity there because deep reinforcement learning at the scale they do it is incredibly complex another thing that sticks out is that there as far as i've seen and i could be wrong but as far as i've seen there's no simple way of running agents in parallel so with something like r2d2 which is a recurrent distributed dqn agent they run in the paper 256 actors in parallel and the agent they supply here is a single threaded version there's no multi-threaded uh framework to stick on top of it now of course you're free to do stuff like uh python multi-threaded stuff or maybe even mpi type stuff on top of it but it would be helpful to see how they do that but of course they want to keep their secret sauce to themselves they can't you know give everything to the world because they need a competitive advantage right they need to keep something some cards close to their chest which i totally and completely understand other thing is that their hyper parameters are not,358,0,0,J1XCWjuyRaI
13,tuned so i played around a little bit with these two agents and couldn't get very good learning um and so that probably means i needed to tune hyper parameters but for the purpose of this video i didn't get too excited about that so that's something to keep in mind is that even if you do use this it's not totally off the shelf you do have to have to do a little bit of tuning but you know they've done you know 90 of the work already so you know what more do you really want right okay i hope that was helpful for you guys this is an enormously interesting framework for me i've been digging into it trying to implement something called never give up which is a highly complex algorithm and so i found this enormously useful it's something i wanted to share with you guys something i'm going to dig deeper into in the coming weeks and months and i hope you will join me for those videos go ahead and give a subscribe if you haven't already drop any questions down below and i'll see you in the next video,256,0,0,J1XCWjuyRaI
0,"as of r2021a release of matlab, reinforcement learning toolbox lets you interactively design, train, and simulate rl agents with the new reinforcement learning designer app. open the app from the command line or from the matlab toolstrip. first, you need to create the environment object that your agent will train against. reinforcement learning designer lets you import environment objects from the matlab workspace, select from several predefined environments, or create your own custom environment. for this example, let s create a predefined cart-pole matlab environment with discrete action space and we will also import a custom simulink environment of a 4-legged robot with continuous action space from the matlab workspace. you can delete or rename environment objects from the environments pane as needed and you can view the dimensions of the observation and action space in the preview pane. to create an agent, click new in the agent section on the reinforcement learning tab. depending on the selected environment, and the nature of the observation and action spaces, the app will show a list of compatible built-in training algorithms. for this demo, we will pick the dqn algorithm. the app will generate a dqn agent with a default critic architecture. you can adjust some of the default values for the critic as needed before creating the agent. the new agent will appear in the agents pane and the agent editor will show a summary view of the agent and available hyperparameters that can be tuned. for example let s change the agent s sample time and the critic s learn rate.",339,0,12,pN6AVNkQmFY
1,"for example let s change the agent s sample time and the critic s learn rate. here, we can also adjust the exploration strategy of the agent and see how exploration will progress with respect to number of training steps. to view the critic default network, click view critic model on the dqn agent tab. the deep learning network analyzer opens up and displays the critic structure. you can change the critic neural network by importing a different critic network from the workspace. you can also import a different set of agent options or a different critic representation object altogether. click train to specify training options such as stopping criteria for the agent. here, let s set the max number of episodes to 1000 and leave the rest to their default values. to parallelize training click on the use parallel button. parallelization options include additional settings such as the type of data workers will send back, whether data will be sent synchronously or not and more. after setting the training options, you can generate a matlab script with the specified settings that you can use outside the app if needed. to start training, click train. during the training process, the app opens up the training session tab and displays the training progress. if visualization of the environment is available, you can also view how the environment responds during training. you can stop training anytime and choose to accept or discard training results. accepted results will show up under the results pane and a new trained agent will also appear under agents. to simulate an agent, go to the simulate tab and select the appropriate agent and environment object from the drop-down list.",366,12,28,pN6AVNkQmFY
2,"to simulate an agent, go to the simulate tab and select the appropriate agent and environment object from the drop-down list. for this task, let s import a pretrained agent for the 4-legged robot environment we imported at the beginning. double click on the agent object to open the agent editor. you can see that this is a ddpg agent that takes in 44 continuous observations and outputs 8 continuous torques. in the simulate tab, select the desired number of simulations and simulation length. if you need to run a large number of simulations, you can run them in parallel. after clicking simulate, the app opens the simulation session tab. if available, you can view the visualization of the environment at this stage as well. when the simulations are completed, you will be able to see the reward for each simulation as well as the reward mean and standard deviation. remember that the reward signal is provided as part of the environment. to analyze the simulation results, click on inspect simulation data. in the simulation data inspector you can view the saved signals for each simulation episode. if you want to keep the simulation results click accept. when you finish your work, you can choose to export any of the agents shown under the agents pane. for convenience, you can also directly export the underlying actor or critic representations, actor or critic neural networks, and agent options. to save the app session for future use, click save session on the reinforcement learning tab. for more information please refer to the documentation of reinforcement learning toolbox.",344,28,44,pN6AVNkQmFY
0,hey what's up guys i have something that will absolutely blow your mind this is the computer playing super mario bros and guess what the computer has no knowledge about the internal state or the mechanics of the video game rather it's simply looking at the frames and then deciding what buttons to press exactly how humans do it impressive right well if you want to learn how to implement this stay tuned because that's exactly what i'll be sharing with you guys today and i cannot wait this was done using a subclass of machine learning algorithms called reinforcement learning reinforcement learning is the coolest thing ever in fact it's been used to train drones and robots find more efficient ways for matrix multiplication and even make the training process for other machine learning algorithms faster oh and not to mention it was also used to train everyone's favorite chatbot chat gpt reinforcement learning is pretty different from its cousin's supervised and unsupervised learning in supervised learning the model is given a data set where each point has a correct answer associated with it whether it's categorical or numerical the model's job is then to predict these values accurately an unsupervised learning the data points don't have a correct answer but rather the models tasked with extracting general trends or insights reinforcement learning is its own beast the objective of reinforcement learning is for the agent to maximize its rewards inside of the environment we give the agent a positive reward whenever it exhibits behaviors we want to persist and negative ones for behaviors that we don't want it's kind of like teaching a dog,358,0,0,_gmQZToTMac
1,new tricks another thing to note is that in supervised and in unsupervised learning you must have the data pre-collected before you feed it to the model whereas in reinforcement learning we'll see today our agent go out into the world collect its own experiences and then learn from it it's a completely different paradigm when it comes to training i wanted to share an implementation of the ddqn algorithm or the double deep q network i'm not going to dive into the nitty-gritty of reinforcement learning theory i'd rather just share a more intuitive understanding for those that have a base level understanding of machine learning but haven't really been exposed to reinforcement learning before however for those that are interested i'm going to leave some of my favorite reinforcement learning resources down in the description below along with the paper that originally pioneered the double deep q network algorithm i first implemented this algorithm when i was a sophomore in college and i still remember the excitement that i felt when i first watched the model start to train learn and eventually beat the level i was absolutely awestruck i mean i understood all of the math and each line of code that i wrote to implement the algorithm however there's just something that's still magical about reinforcement learning that gets me excited every single time and my goal today is to be able to package that excitement and share it with you guys through this video let's start off with the basics in order to understand the fundamentals of reinforcement learning we need to define these following terms let's understand each of them,358,0,0,_gmQZToTMac
2,in the context of super mario bros first is the agent the agent today will be our neural network that's controlling mario it's the one responsible for making decisions and taking the optimal action the environment is what our agent will be interacting with it's going to be level one one of super mario bros on the nes a state is a snapshot of the environment at any given time it's essentially what our agent sees today we'll be using four consecutive frames from the game the reason we're using consecutive frames is so our agent can see motion for example is the goomba coming towards us or away from us well for us to know we need to know the previous frames oh well hopefully our agent knows to jump actions are the inputs that the agent has available to it to give to the environment today the agent will have access to these five button combinations to press first one is with no buttons pressed and then the agent is able to move right a is used to jump and the longer it's held the higher mario will jump and then b is used to move faster once the environment gives the agent a state and the agent responds with an action the environment also gives the agent a reward a reward essentially helps the agent determine whether or not the action it took inside of that state was good or bad today we'll be giving the agent a plus one reward for each unit to the right that it makes in the video game it'll receive a negative one reward for each second,358,0,0,_gmQZToTMac
3,that passes by in the games clock this is to disincentivize the agent from standing still and then the agent will get a massive negative 15 points if it dies now the concept of a negative reward may seem foreign to some however remember the goal of the rewards is to guide the agent to optimal actions and negative rewards still help the agent do just that everything that we've talked about so far is summarized in this diagram it essentially outlines how the agent and the environment interact with one another the agent gives the environment actions and the environment returns states and rewards an episode for us today will just be one attempt at the level an episode will end whether mario dies reaches the flag or runs out of time a policy is a function that takes in a state and returns an action it's what our agent will use to make its decisions some reinforcement learning algorithms will have the policy be a probability distribution however today in our ddqn algorithm implementation we'll be using the epsilon greedy approach more on that later a value function takes in a state and returns how valuable that state is in reinforcement learning it's natural to view some states as more valuable than others for example the state where you're right next to the flag is probably a little bit more valuable than the one where you're falling down a hole we're not going to be working directly with the value function today however i thought i'd mention it because a lot of other reinforcement learning literature brings it up an action value function takes,358,0,0,_gmQZToTMac
4,in a state and an action and then returns how valuable this pair is as you can see on the screen this state action pair is probably more valuable than this state action pair i'm sure you can see why approximating this function accurately today is our goal because it's going to let our agent take the action inside of in state that has the highest value alright quick review we're using reinforcement learning to train the computer to play super mario bros our agent or the neural network that's controlling mario will receive a state from the environment remember a state is four consecutive frames from the video game the agent will then take an action if the action was good we'll give it a positive reward if the action was bad it will give it a negative reward and then the agent will use these rewards to train itself over time time to take better and better actions eventually we're hoping that the agent will learn how to beat the level and reach the flag that's waiting for it at the end that was a lot to take in thank you guys for bearing with me we have three more concepts to cover first is the epsilon greedy approach second is the replay buffer and third are some more details about the action value function after that we'll assemble the algorithm and then implement it in code the epsilon greedy approach is the strategy our agent will be using today to choose its actions however to understand epsilon greedy we first must take a step back and understand the explore exploit dilemma a problem,358,0,0,_gmQZToTMac
5,that's at the heart of reinforcement learning let's say our agent has learned that jumping over koopas is the most effective way to not die good i mean i guess it gets the job done right however as i'm sure my fellow gamers know that jumping on a koopa actually turns its shell into a weapon that can be used to eliminate other enemies if our agent constantly exploits or takes the action that it deems to be the best in the current moment then it will never get a chance to explore other potentially superior strategies so how do we navigate this explore exploit dilemma enter the world of the epsilon greedy approach a clever solution to the explore exploit dilemma with probability epsilon our agent ventures into the unknown taking random actions and with the remaining probability 1 minus epsilon it'll take the action that it's confident is the best one it's essential to note that this best action of the agent is dependent only on what it knows in the current moment it's not necessarily the best action overall initially when our agent knows nothing about the environment we'll set epsilon to 1 ensuring maximum exploration and then over time as our agent continues to learn about the environment and its dynamics will start to taper off epsilon from one to a very small non-zero number we'll always keep epsilon greater than zero just so our agent is always on the lookout for new strategies the this means that if our agent did learn to jump over koopas we're hoping that with the epsilon greedy approach it'll eventually randomly jump on one and,358,0,0,_gmQZToTMac
6,then realize the benefits of doing so now let's cover the replay buffer it's essentially a storage for our past experiences and rewards it'll be used to train the neural network there are a couple of reasons why we use a replay buffer rather than training on the experiences as we collect them the first is because sequential experiences have a lot of correlation which can lead to a lot of instability when training the model to eliminate this instability we randomly sample from our replay buffer and the second reason is that we're now able to reuse data which actually improves data efficiency in the replay buffer we'll be storing tuples of the state action taken reward received next state and a boolean flag indicating if the episode is done for any reason we'll see how each of these come into play when we sample the replay buffer to train our network now let's pull up the action value function that we mentioned earlier this is often referred to as the bellman equation remember our primary objective today is to approximate this function enabling our agent to make optimal decisions in any given state the goal isn't just about seeking immediate gratification instead the agent aims to maximize its rewards over the long run humans intuitively do this as well for example we may take a slightly longer route with less coins if it leads us to an item box at the end that contains the invincibility star math notation is scary so let me translate this equation to english the value of taking action a in state s is equal to the reward you,358,0,0,_gmQZToTMac
7,get in the next time step plus the value of taking the best action in the next state s prime multiplied by a discount factor the discount factor makes rewards from future states less valuable in reinforcement learning this is important because future rewards are less predictable than the current one due to the environment's stochastic in the stanford marshmallow experiment children were given a marshmallow and were told that they could either eat it right now or wait 15 minutes for a second marshmallow it was to measure their self-control when it came to delayed gratification essentially the researchers were measuring the children's gamma or their discount factor one marshmallow now or two in the future our agent is kind of in a similar situation except for the children their second marshmallow was guaranteed unfortunately our agent does not have that luxury as such it's ideal for it to value a guaranteed marshmallow in the present more than a potential marshmallow in the future a gamma of one means that there is no discounting while a gamma of zero means the agent is completely myopic we want neither but rather to strike a balance between the two this action value function is recursive in nature this means that the value of a specific state action pair is not just based on the immediate reward rather it also depends on the value of the subsequent state action pair dive deeper and you'll see that this subsequent value in turn depends on its immediate reward and the value of the next state action pair and the chain continues we assume that we take the optimal action the,358,0,0,_gmQZToTMac
8,action with the highest q value at each recursive level thus the max function when you unravel this recursive structure you'll find that the value of any state action pair is essentially the sum of its immediate reward and the accumulated discounted feature rewards until the end of the episode this first equation once you assume you're taking the best action at each time step condenses down to this after you distribute the gammas appropriately the further out a reward is the more times it's multiplied by gamma still a little bit confused don't worry i was too when i first learned this let's try to tackle this from my more visual approach take each ribbon on the screen to be a reward the agent will get until it reaches the flag remember that these rewards are granted after each step the agent takes in the environment and the number on the screen is arbitrary the ribbons are getting progressively smaller to signify the discounting assuming the agent is currently in state s and takes action a the value of this state action pair is the sum of this entire sequence of rewards discounted appropriately if s prime is the next state after taking action a in state s and a prime is the best possible action in s prime then the value of s prime a prime will be this sum again discounted appropriately we know the reward r we got from going from s to s prime so we can use that to help tune the estimate of the current state action pair's value to train our agent we compare its predicted value of,358,0,0,_gmQZToTMac
9,a state action pair to the target value remember the target value is the immediate reward from a move plus the disc discounted value of the best possible next move essentially we compare its predicted value of the sum of these ribbons to the predicted value of the sum of these ribbons plus the ribbon whose value we know with certainty because the agent just received that reward while both the prediction and the target are estimates the target is considered to be more reliable why is that the case well because the target is based on fewer future assumptions as a component of the target is the reward directly observed from the agent's recent action making it grounded in immediate experience rather than speculative projection once we have our predicted value and our target value we can use the following equation to update our estimate for those of you familiar with gradient descent you might recognize this to be exactly that alpha is our learning rate or the size of the steps our network's parameters take as they converge towards their optimal values the part inside of the square brackets is the error or the difference between our predicted and target values this is exactly the derivative of the mean squared error loss function i'm sorry if all of that was overwhelming i know it was for me the very first time i learned it however the intuition of the action value function is the hardest part i swear it gets easier from here so how do we even begin to approximate this function that has such a vast space of potential state action pairs,358,0,0,_gmQZToTMac
10,well we turn to our friend the neural network remember neural networks are great at approximating functions and extremely high dimensions the specific architecture we'll be using today is the convolutional neural network or a cnn cnn's excel at extracting meaningful visual features from its inputs whether its enemies holds or the flag if you're not too familiar with cnn's i highly recommend nvidia's learning deep learning book it'll cover literally everything you need to know about cnn the input size will be the dimensions of our state which we'll see in just a sec the number of neurons in the output layer will be the number of actions available to us which in this case will be five the value in each neuron represents the predicted q value for that associated action paired with the input state when running this algorithm we're actually going to have two identical copies of our neural network the first one will be the online network this is the one that will actually be training the second one will be the target network the one we'll be using for the ground truth for our predictions to give to our loss function the target network won't be trained however we'll be intermittently copying the weights from the online network over to the target network this is to increase stability during the training process to train the online network we sample the replay buffer to get a random tuple containing a state action reward next state and done flag we then pass in the state to the online network to get the predicted q values for the five actions the different sized,358,0,0,_gmQZToTMac
11,controllers represent how the network values each action differently however we only care about the action that was taken and is present in the sampled tuple this is our predicted value for this state action pair to see how accurate it is we calculate our target value to do so we pass in the next state into our target network to get the predicted q values for the five actions again this time we take the highest predicted q value represented by the largest controller to compute our target value we take the reward from our sample and add it to the predicted q value of the next state multiplied by gamma our discount factor we would then pass these two values into our loss function and perform one learning step for our online network this technique is known as bootstrapping because our poor little network is essentially pulling itself up by its bootstraps it's using a crappy target network to train our crappy online network and then as the online network improves just a little bit we'll copy over the weights from the online network to the target network and now we're using a slightly less crappy target network to continue training our online network we'll repeat keep this process tens of thousands of times and eventually we'll start to see our agent actually start to learn i know this seems very jank like we're using a moving estimate to train another estimate however in reality we'll see how stable it actually is so let's put together all of these moving parts and finally assemble our ddq1 algorithm we start off with initializing our starting,358,0,0,_gmQZToTMac
12,variables and resetting our environment which gives us the starting state second given the state we'll choose an action with our epsilon greedy approach third we'll use this action to take one step inside of the environment as mentioned previously the environment will return a new state and a reward we'll then take the current state action reward the new state and the done flag and store them all in our replay buffer then we'll do one learning step which involves sampling the replay buffer and training the online network then we'll take our next state and assign it to our current state we'll repeat all of this until the episode is done and finally we'll repeat this inside the loop for however many episodes we want to train for so let's finally get to coding this up for the nes emulator we'll be using a library called gym super mario bros it handles all of the complexities of the emulator side of things for us it gives us neatly packaged python objects for the states along with a simple way to input actions it does the this via a simple api that's based on openai's gem api openai's gym library is a wildly popular collection of different reinforcement learning environments that researchers or enthusiasts like us can use to try out different algorithms they're incredibly easy to set up and use these days the most common approach is to use gymnasium which is a maintained fork of openai's gym library by an outside team i wonder what openai is working on these days also all of the code will be linked on github right down,358,0,0,_gmQZToTMac
13,in the description and if you ever need to reference it throughout this video or even afterwards please feel free to do so so let's start off with a simple example let's first import the libraries we'll be limiting our agent to only press these following button combinations which is why we're importing write only then we create our environment object and wrap it with the joypad space wrapper ensuring that the actions in write only are the only ones the environment can accept then we set our done flag to false and reset our environment this while loop will run for as long as the agent is still alive first let's have our agent only press right on the d-pad we'll then take one step in the environment with that action and get our done flag ignoring the other return values from the step function for now and finally because on line 6 we have our render mode set to human calling env.render will display our environment to our screen so we can see what our agent is doing as expected our agent is not doing so well to add some dynamism let's have it randomly choose an action from our action space unfortunately our agent still sucks good thing we have an entire reinforcement learning algorithm up our sleeve let's implement the wrappers first rappers are essentially a way that we can modify our environment's outputs if you want some more information about writing your own wrappers i highly recommend checking out the gymnasium documentation which is linked below we're going to be using four different wrappers today the wrapper we're implementing will skip,358,0,0,_gmQZToTMac
14,frames meaning it'll just take the action inputted for the first frame and reapply it for however many frames we want to skip we'll aggregate the rewards over those four frames this is useful for us because consecutive frames have a lot of overlap so it's redundant to reprocess everything the next three rappers are already implemented by gymnasium resize observation will change the dimensions of a frame from 240 by 256 pixels to 84 by 84.,100,0,0,_gmQZToTMac
15,this is just to reduce the computational load similarly grayscale observation will turn the frame from having a red green and blue channel to just one reducing the amount of data our network will have to crunch finally so our network can see motion we're going to take four consecutive frames and stack them on top of each other to finally create our state object that will pass into our network each final state object comprised of a stack of four processed frames encapsulates data from 16 original game frames due to the combined effects of frame skipping resizing and stacking in our pre-processing steps the code for the wrappers is pretty straightforward as we're only implementing one of them we first override the constructor to take the number of frames to skip then we override the step function whenever step is called we use the for loop to take skip number of steps aggregating the rewards the apply wrappers function applies our wrappers one by one encapsulating our base environment like the layers of an onion at the end it returns our fully wrapped environment object we'll use this function in our main.pi let's next create our neural network with pi torch as mentioned previously we'll be using a convolutional neural network we're going to have three convolutional layers followed by two linear layers the input shape will be 4 by 84 by 84 which is the dimensions of our state the number of actions will be five the underscore get conv out function performs a dummy forward pass through the convolutional layers to get the number of neurons we need in our first linear,358,1,1,_gmQZToTMac
16,layer it's 3136 with this specific architecture but the function allows us to dynamically calculate it where we to change anything in the future the freeze flag allows us to prevent pie torch from calculating the gradients which we'll need for the target network remember we only use the target network to calculate the correct values we want our online network to predict then finally we add our forward pass and then move the network to whichever device we're training on whether it's the cpu or gpu let's next create the agent class these will be our imports along with pytorch and numpy we'll also import our neural network that we just created for the replay buffer we'll be using pi torch's built-in tensor dict replay buffer which will hold python dictionaries with tensors as the values the storage mechanism it'll be using is pi torch's lazy mem map storage which will use memory mapped files for easy access to our experiences along with alleviating ram usage we could have just used a python list for our experiences from which we sample but that means sampling would be really slow and we'd be using a lot of ram for the constructor we'll take in the dimensions of our state and the number of actions our agent has access to we'll also use a learn step counter which keeps track of how many times we've trained our network we then set up our hyper parameters first is the learning rate or alpha this is the size of the steps the network will take when it's updating its weights second is gamma which is our discount factor remember,358,1,1,_gmQZToTMac
17,we want the agent to discount future rewards third is the starting value of epsilon which will be set to 1 initially to encourage exploration fourth is the epsilon decay factor which will multiply epsilon with after every time step fifth is our minimum epsilon so we always maintain some likelihood to explore sixth is the batch size for our training and finally is how often we'll sync the target network weights with the online network after the hyper parameters comes the networks again making sure the target network's parameters are frozen after adding our optimizer and loss we create our replay buffer with the capacity to hold a hundred thousand experiences then let's write our choose action function that uses the epsilon greedy approach as we can see when the random number is less than our epsilon value it'll choose a random action since epsilon starts off at 1 the action chosen will always be random as the value of epsilon decays the probability of using our online network to choose the action with the highest q value increases also because we want the index itself not the estimated value of that action we're taking the arg max our decay epsilon function multiplies the current value of epsilon with our decay factor and ensures it doesn't go below the minimum epsilon threshold the store in memory function takes in the tensors that we want to put into our replay buffer organizes them in a dictionary and adds them to the buffer the sync networks function checks if enough learning steps have passed and if so it'll copy over the weights of the online network to,358,1,1,_gmQZToTMac
18,the target network now for the most important function the learn function in the agent class we first validate that there are enough experiences in our replay buffer to sample a batch from then we call the sync networks function we then clear our gradients followed by sampling the replay buffer we store the results in the states actions rewards next states and duns variables we passed the state's tensor through the online network to get our predicted values we index by the actions that we actually took because we only perform back propagation on those values then we calculate our target values we pass in the next states through our target network and get the value that the best action yields we then multiply it by gamma and added to the rewards we got in the current state the one minus duns part sets all future rewards to zero if we're in a terminal state we then calculate the loss using our predicted and target q values performing back propagation to calculate the gradients and then perform a step of gradient descent with those gradients we also then increment our learn step counter and decay epsilon finally let's create our main dot pi in this file we'll set up our environment as we saw previously and then create the full training loop again starting off with our imports this time adding our apply wrappers function and our agent class we then create our environment applying the joypad space wrapper plus the other four that we talked about earlier we also create our agent object and pass in the dimensions of our state and the number,358,1,1,_gmQZToTMac
19,of actions from our environment finally the training loop looks very similar to our pseudo code for each episode we collect experiences and learn from them until the episode is done first we choose an action given the starting state again this will be completely random at first then we take a step in the environment and store the state action reward new state and done flag in our replay buffer we then perform one learning update then we set our current state to our new state and repeat fair warning training for 50 000 iterations can take a while i have a rtx 3080 and it took me about two days of training non-stop however i'm excited to see what optimizations you guys can uncover by tooting the hyper parameters but now the moment you've all been waiting for after we finish training this is the final result thank you well we did it guys we finally trained the computer to play super mario bros i hope you guys feel the magic of reinforcement learning that i've always felt in fact even bigger than that i hope you realize that machine learning is so much bigger than just predicting labels from a data set or trying to cluster some points anyways if you have any questions about anything that we talked about today please leave a comment down below and until then see you guys next time,309,1,1,_gmQZToTMac
0,in this video you will learn how to create a machine learning model and how to deploy it using flask from scratch watch the video till the end so that you will understand everything step by step we are going to do two steps the first step will be building the ml model that is the machine learning model then the second step will be to deploy it using flask and in the process of building and deploying we will create several files and the first file will be model dot py this is going to be a python file and this file will contain our machine learning model once we create our model we will convert this model into a pickle object then we will make one pickle file that is the pickle of our machine learning model then we will create one more python file app.py and this file will contain our flask application then we will also have one more file index.html and index.html will contain the html codes which we will see on our server then we have one more file this is the iris.csv this is going to be the data we are going to use this data to develop our machine learning model we will use pycharm ide to develop our ml model and deploy using flask you can use any other ide of your choice such as visual studio called or spider first i'll create a folder in my c drive i'll click on this pc i'll go to c drive and i'll make one folder here and the folder name will be ml model deployment and inside this,358,0,0,MxJnR1DMmsY
1,folder i will put two files the first will be the data file and the data file is on my desktop so i have iris dot csv i'll copy this from here and now i'll go to the folder and paste it here so this is going to the data file on which we are going to develop our machine learning model the second is the template folder so i have this template folder i'll copy this and paste it in my folder here i'll give you all this file i'll put the link of my github from there you can download this code so in our folder templates we have a html file and this you can see that we have this html file i'll show you the code inside this i'll right click on this and then i will open this with notepad and you can see that these are the html codes so we have given our separate length sample with petal length petal width so these are our independent variables in which we will put the values and then we will get the output so you don't have to worry about this html file i'll share this with you you just have to copy paste and then put it in a folder like this so you have to make one folder where you will have this template folder and inside this this html file then you have one data file now i am going to open the pie chart and i will tell you step by step what we have to do so i'll type pyjam here now i'll click on open,358,0,0,MxJnR1DMmsY
2,then i'll go to the c drive so first i'll click on this refresh button now i'll open this c folder now here we have made one folder ml model deployment you can see here i have made this folder right now ml model deployment so i'll click on this and click on open so we have opened our folder in pyjama and you can see here we have iris.csv and the folder templates already here to create a python file you have to click on this folder name then right click then choose new and from here you have to click on the python file then you have to give the name and our first file name will be model dot py so this is our model dot py file and in this file we will create our machine learning model let's import all the libraries that are needed to develop our machine learning model first i will import pandas as pd to read our csv file import pandas as pd then from sk learn dot pre-processing import standard scalar we will scale our data independent variables from sklearn dot ensemble import random forest classifier we are going to apply this random forest classifier on our ies dataset from sklearn dot model selection import train test plate and this will be used to trade and split our data set into train test now we are going to load the csv file i'm going to explain you each and everything step by step so you will understand everything and you will be very confident after watching this video i'll make one variable df and now from,358,0,0,MxJnR1DMmsY
3,pandas i will call the method read underscore csv and inside this i will pass our csv file name in double course and our csv file name is iris.csv you can see here so i'll write iris dot csv now we will look at the top five rows of a data frame df so i'll write df df dot head using head function we can look at the top five rows of our data frame so now i'll right click here and then i will choose run model because our file name is model you can see here so i will click red model it has been run successfully and here you can see that process finish with exit code 0 but we did not see the head because we have to use the method print here so i'll just type print and then i will cut this code from here and put it inside this and now i will run this again so i'll right click and click on run model in the output you can see that we got the top five rows so these are the head we have columns such as sepal length sepal with petal length petal with n class class is a target variable while sepal length sepal with petal length and petal width are independent variables we are going to use this independent variables to predict our class whether it belongs to satosha or any other class will separate our independent and dependent variables in x we will have all the independent variables and in y we will have all the dependent variables select independent and dependent variable so,358,0,0,MxJnR1DMmsY
4,next we will have all the independent variables so now here i will call our variable df and then i will make a list inside a list and then i will pass all the independent variable names here so first is going to be sample length second sample with third petal length and fourth petal with these are our independent variables now i will make one variable y and in this we will have our dependent variable and our dependent variable or the target variable is the class variable so i'll call our data frame df and then i will type the variable name class so we have got x and y now we are going to split the data set into train test split the data set into train and test i'll make variables x strain x test y train and y test so first is going to be x strain then x test then we have white ring and white test now i will call this method train test split so train test split now inside this we have to pass our independent and dependent variables so we will pass our variable x and y x contains our independent variables and y contain our dependent variable then we have to pass some parameter the first parameter is going to be test size that is we are going to build the model on 70 of the data and the remaining 30 of data will be used for testing so here i will type 0.3 that means i am saying that 30 of the data will be used for testing then there is one more parameter,358,0,0,MxJnR1DMmsY
5,random state and here you can give any number here i'll put 50 if you want your result to be same as mine then you also have to put 50 here next we will do feature scaling i will call this class standard scalar to standardize our independent variable i'll make one variable sc and i will call our class standard scalar sc will be an object of this class now i am going to apply this standard scalar on x strain and x test first i will call this variable x strain then i will call object sc and from this i will call the method fit and transform then i will pass our variable x train we will do the similar process on x test so i'll call x test then i'll call the object sc then we have transform and i will do this on x test so in extreme we are doing the fit as well as we transform and in next test we are only applying the transform method here we have done the feature scaling the next step is to instantiate the model and we are going to use random foreign classifier to develop our classification model to develop our machine learning model so first i will instantiate the model i'll make one object classifier like we have done here we have made the object of the class standard scalar and here we are making an object of our class random forest classifier now i will call the class random forest classifier so this object classifier will contain our class random forest classifier now we are going to fit the,358,0,0,MxJnR1DMmsY
6,model so we will call this object classifier and from this we will call the method fit and here we have to pass our x strain in y drain so we will pass x strain and y train so here we are doing the fit on the model we are applying this fit method and once this fit method is done that means we have built our machine learning model and the next step is to make the pickle object of our model so here you can see now we have two files model.py and iris.csv and now we are going to create one more file that is the pickle of our model object so let's make pickle file of our model and for this first we will need to import the pickle library so here i will call import pickle now now i will call this pickle and i will use the method dump inside this we will have to pass our object this classifier so we will write classifier so whatever you are used for fitting your model here we are using this classifier random call classifier to fit the model so in pickle dump you have to use that same classifier here so that's why i'm writing classified here because we have used this to fit our model that's why in pickle.dump i am using this classifier because we are going to dump this classifier then we have to pass parameter open and then we have to give our model name so we will give our model as model and then the name will be model and then we have to type the,358,0,0,MxJnR1DMmsY
7,extension since this is going to be a pickle file so we will write dot p k l this is the extension of pickle file and then we have one more parameter wp we have called pickle class and from this we have used the method dump and then we have dumped our model classifier which we have used here to fit our model and then we have given the name to our model and the extension is model.pkl pkl because we this is a pka pickle file so we are using the pickle extension here now i will run this code and after running this code you will see here we will get a new file and that file will be model.pkl so i'll run this code you can see that we got a new file model.pkl we have successfully converted our model into a pickle file and we can also verify this from our folder i'll open my folder here and in the folder you can see that we got this pickle file model.pkl let's verify what files we have created we have created model.py we have also created model.pkl we already have index.html we already have iris.csv the only file left is app.pui this is the flash application and this is the last one now we are going to work on this flask application we will create our last python file that is the app file which will contain our flask code i'll click on this folder ml model deployment right click new python file and the file name will be app dot py now this file will contain our flash code let's,358,0,0,MxJnR1DMmsY
8,import all the libraries import numpy as np from flask import flask request jsonify render template when you use the flask environment you will get this message here in pycharm you are using the flask framework and we are going to make use of all the class here flask request json if i random template now first we will create the flask cap to create a flask f you just have to give one app name so i'll give the variable as app and now from flask we will call this underscore name that's it now you have created an app you just have to give any name here i am giving a name app here now we are going to load the model this pickle model we have to load this model here because we will make use of this model to make predictions so first we will load this model load typical model i'll make one variable model now we will load this model we also need to import pickle so i'll import pickle here now i will call pickle dot load we will use the method load from our class pickle and then inside this we will pass our method open and inside this we have to pass the model name and our model name is model dot pkl so we have to write this here in double quotes model dot pkl and then we have one more parameter rb to read our file so here in this line we are reading our we are loading our model and saving it in the variable model so whenever we want to make prediction we,358,0,0,MxJnR1DMmsY
9,will use this variable model next we will define the home page and we will use our app to define the home page so i first call at at the rate and then i will call this app from this i will use the method route and inside this in double quotes i will put one slash here this means this app will take me to the home page and then i will define a function define home so i'm defining a function for home page and this once it takes for the home page it will call this function home and this home function will return me render template and inside this i will pass our html file which we have in this folder template we will define one more method and that method is going to be predict method first i will call this at the rate app dot route and this time this will take me to the page where the predict method is defined so i will write here double slash slash and this time i will write the method name predict so when i will call this this will take me to this predict method page and the method is going to be post in flask we have two types of method the get and post method and here we are using the post method because we are going to receive the independent variable values so that's why we are using post here and now we will define our predict method define predict so when our app will go to this page predict this predict method will be called and first,358,0,0,MxJnR1DMmsY
10,i'll make one variable float underscore features so i'm making this variable float features because you can see here that our independent variables had got float values that is the values are in decimals 5.1 3.0 1.3 the values are decimal that's why i'm making a variable float features where i'm going to convert all the numbers into the float variables so i'll make a list comprehension and i will call a loop here follow float of x for x in request dot form dot values here i am saying that when i am going to receive the request that is the independent variables values that is these values when i will receive these values i am going to take those values and convert those values in float and then save the those values in the variable float features so someone will send us these independent variable values these values will be sent and they will send an integer so we are going to convert that integer into float and then save it in the variable float features and the next step will be features i'll make one variable features and now i will make one more list comprehension and now here i am going to convert this float into arrays so i will call from numpy dot array and then i will pass our variable float features which we have saved above here so i'll pass this variable here float features so here in this step i am just converting this float features into numpy arrays now the next i will make one variable prediction now i am going to make the prediction and to,358,0,0,MxJnR1DMmsY
11,make prediction we have to call our model and our model is saved in this variable model so we will call our variable model and from this we will have to use the method predict because to make prediction we have to use the method predict so we are calling the method predict here and inside this we will pass our variable features here because features contain the values of our independent variable so these are independent variables and this feature will contain these values for example 4.9 3.0 1.4 0.2 so whenever we pass any number here those number first will be converted into float and then those float will be converted into numpy array and store in this variable features and then using this feature we will make the prediction and save in the variable prediction here now once the prediction is done we will call we will use the method return and return will call us render template this class random template and this will take us to the index html index index.html page and there we will show our values so we will use prediction underscore text this is a parameter and here we can type anything for example uh because this model is related to iris flower so i will write the flower species is then dot format then i will call prediction what i am doing here is once i get the prediction it will take me to this page index.html and then it will print this text as the heading the flower species is and then the species name so whatever species we will get in the prediction that,358,0,0,MxJnR1DMmsY
12,species name will be printed and along with that this statement will be printed we can put any statement here i have to remove this l from here and i have to put l here everything is completed we just have to write one more line and this is going to be if name is equal to main then we will call our app app dot run and debug is equal to true that's it we have completed our flash cap now i'll right click i'll right click here and then click on app i got type error type error init got an unexpected keyword argument method here instead of method i have to type methods i have to put s here i'll put s here and now i will run this code again right click click on run app and now let's see so now our app has started and it is running on this port so you can see here we have uh some numbers here http dot 127.0.1 is to 5000 so this is the port on which our app is running and this is the default port on flask now we have to click on this port so i'll click on this http and this will open me in a new browser i'll click on this now here you can see that we got the app here flower class prediction if i go back here you can see we have successfully run our app and it is running on this port now we just have to put the independent variable numbers here and this will predict class for us for example i,358,0,0,MxJnR1DMmsY
13,put for 1.2 5.1 4.8 and 0.1 and if i click on this predict this will predict the class of our flower iris i'll click on this method predict in the output you can see that it is saying that the flower species is virginica and this statement the flower species is which we have defined here you can see here in prediction text i have defined that the flower species is so that is why here we are getting the flower species and then we got the prediction here whatever prediction we are getting here we are calling this along with this statement now if i put some different number let's say i will put 0 0 0 0 0 0 now here you can see that we got the flower system virginica if i click on this method predict now we got the flower species is versicolor so we have successfully made the flask app and we have deployed our machine learning model so here you can see that these are the steps first we have to develop a machine learning model then we have to convert that file into a pickle object once we convert that into a pickle object we have to make one flask cap and once we make a flask cap in flask cap we have to load that model that pickle object then we have to define the predict method and then using that predict method we can make the prediction i'm sure after watching the complete tutorial now you are comfortable on developing and deploying your model if you face any problem you can write down in the,358,0,0,MxJnR1DMmsY
14,comment section and i will try to help you i hope you enjoyed this video if you like my video please subscribe to my channel thank you for watching,37,0,0,MxJnR1DMmsY
0,hello all today we'll be discussing about how we can deploy a machine learning model by using a flask this is one of the video that everybody most of my subscriber will requesting for that i'm really sorry that i've just put up you know it just got this little bit delayed because i was busy but today i'm going to deploy i'm going to show you how you can deploy a machine learning model by using flask it is a very simple technique all together so make sure that the most important components that you require in this particular session is basically that you should have one model building file that basically means that one py file will be responsible for creating your model and this also involves feature engineering all the data pre-processing and the type of model that i have basically taken is a very simple one because the main agenda of this particular session is to show you how you can deploy the model the second thing is that you need to have one index dot html file so that this will basically act as your front-end web app so that any request that you give it to your model which will be in the form of api which we are going to host through flask it will interact with that get the output from that particular api and then there are some stylings also i've included it i have i'm going to put down this whole code in the github and the description will be given in the video in the description box about the url of this button edit hub,358,0,0,UbCWoMf80PY
1,length so you can refer it from there then i also have one more py file one is called as apt of ty5 in this what i will be doing is that i will be writing my flask for where and i will be creating the api is like hawaii yes url look like like our localhost address slash reading okay and another api is basically slash reddit at moscone epi so we'll discuss first about model dot dy where we'll be seeing that how the model is actually created so for this i have taken a data set which is called as hiring dot csv data set i'll just tell you i'll just tell you about this particular dataset what this data set is all about so let us just go and see about the detects that altogether now here you have some fields like experience test score interview score and based on all these parameters you have to provide a salary to the technical guide was comfort this particular interview here you can see that you have something like experience field s coroner interviews underscores for field and this is basically your independent features salary is your dependent feature in experience you have the number of years of experience that you basically have in terms of spread so one feature engineering work that i'll directly show you is that first of all you need to replace man with zero because experience if it is none that basically means the person is having zero experience and all those strings you have to basically convert it into integers now in the tests code you can see,358,0,0,UbCWoMf80PY
2,that there is also nance then we will try to fix this by finding the out the mean of all this particular test score and introduced for also mean of if there is no nan in the interview score so you did not do that one more point i want to note on to make you note that that test score usually ranges between 0 to 10 similar interview score also used usually raises between 0 to 10 and based on that you will be able to predict the actual salary that should be given to the candidate now this is a small data set case i just wanted to show you how the main deployment is done not just taken of small data set applied a large linear regression to this and try to show okay so let us go go ahead with the code as said that for the experience i am going to fill the hemi value with 0 so here it is then for the test score what i am doing and trying to find out the mean wherever i have had the value i am going to replace with the mean and also taking at is in place is equal to true now in the x i have all my independent features let me just execute this 3 line i am also going to convert that first experience filled from string to integer so for that i'm making a function which is say at a scan mode to end here are all my words and i am going to just apply that by using a lambda function in my experiences this,358,0,0,UbCWoMf80PY
3,is all feature engineering guys don't worry about it you can download this particular code from the github link that i provided in the description here you can see that these are all my independent features in ian's test run will score score interview score now you can see that this is basically being replaced the manuel is basically replaced with a mean of this and instead of man here i have zero zeros so that's perfectly fine then my wire which is my dependent feature i'll just use i lock feature and take the minus one feature that is this is my future with respective and after this what he can do is that you can apply a linear regression you already know how to apply a linear regression i guess so then i will do a set after doing a fit my model will be ready after my model will be ready has just used picol dot dub this pickle is basically coming from the pickle library so this pickle helps you to create a pre-compile format model name which will just be like a file which will have a extension like dot detail so here you can see that i am having that particular model dot decal so finally if you want to test it what you have to do is that model because dot load and you can test it but anyhow your model dot tkl file is ready now you make sure you execute this and just put the create your model dot eql fine so this is the one thing that you require now let us move ahead and try,358,0,0,UbCWoMf80PY
4,to see how we can create our flask environment with where we will be creating our api is where we will be reading this pkl file and then we will give the input to the peak alpha and then finally from the peak alpha will be still get output so for that i have created a file called as ab dot t1 so in this r dot toi i have three import statement one is numpy will be used under to use people and i am going to import use flasks if i am using floss basically to host my model okay so from the flask can be using fast jason if i had render underscore template render and also template will actually help us to you know redirect to the first home page that we basically have initially then in that particular home page will try to put some inputs and then as soon as we submit the submit button we will get the hope now so far as usual always make sure you write this particular first line of code because you have to initialize the flask app then right pickle dot load open this particular pickle file in in a read mode okay bytes mode so once you read it your model is ready now okay then you have two functions that are created one is defined home basically this is basically the root node where it should root api url well should go it will directly rewrite you to the index dot html file which is minus five which is my first file which will be just like my home page and,358,0,0,UbCWoMf80PY
5,what i do is that i create my another api that is slash predict and always use this app dot route route because in flask you have to use this in order to create any number of you are eyes with respect to the api okay and the method that you are going to use is post after this what you do is that now you know that during predict what will happen we will be providing three inputs because in my variable my independent feature has three inputs so i will be providing these three inputs as soon as i provide these three inputs this will be you know reddit since this is a post request i will be reading it from request or form not values so each and every values will be available over here i will convert that into an array and finally i will just do model or predict or final features but finally i will just output i will just get the output and not round it off then after this i will return that render on this foot template in index dot html i will say prediction underscore test should be replaced with this particular value now where should this value get replaced one let us go to the index dot html here you have a placeholder called as prediction on this foot test okay and this predict this particular placeholder will get replaced with the value that are you want to pass it over here that is employ salary should be so-and-so dollars it and this output will get basically replaced with this particular this is one api,358,0,0,UbCWoMf80PY
6,that i have created i'll also create one direct api where i will just directly a sort pass on the hard-coded json so that our naming it has predict telesco yeah so this json also we should get we should collect it from somewhere right so for that i have created one request dot d by five here i am giving the url and i am just saying that request or post url and i am just giving the json values that experience is two years as for is this this and i'm just printing it are gorgeous as soon as this is you know this is running the command prompt what will happen is that you'll go and in this particular url gave out this particular json values and in this app dot api this will run it over here and you will get down put it so always remember whenever you are creating a flask whenever you are creating a flask kind of a pr deployment there will be one main function okay and then with respect to that you always have to import this flask request json file in a template they are various ways to create root directory that is slash which is my default or home page then you know you can create any number of api is based on that you can just write a logical now the main thing is that how do you run this all right how do you run this all now make sure that you have your file explorer like this okay this is your static you have a css over here this is your,358,0,0,UbCWoMf80PY
7,template you have index over here i have app dot t why my csv file model k model dot a pk you need not run model dot gui again because i have to take you inside all you have to go is that go to a command prompt okay and just write python python and just see which is your model i mean the flask api file this is app dot p wise it's so as soon as you write ab dot e y in just executed and it's a warning do not use the deployment server but it is fine so that will run in your local machine now just go and copy this and open the browser over here and just go and hit it now this is your home page that you can see away it is very very much simple i just use three fields it is very very simple to see okay just write your years of experience like i'll say i am eight years of experience my test code should be between zero to ten so my test score is nine and my interview score is 8 now it will go and directly predict yourself that you salary should be seventy four thousand six fifty dollars and that is how you have you can do one more way is that i will go and directly and predict underscore api and remember there from the hard code jason i have already given it but you cannot run it directly so what he can do is that close this okay that is my predictor then what you do is that you can,358,0,0,UbCWoMf80PY
8,just write python you can just this particular application a pest control see stop python and you can just read it request dot e one take some time to execute okay you're getting some error but it is fine what you can do is that okay this cannot be running like a flask api because this is just for the test analysis don't worry about this request dot t why it is just like you can see to your console itself main thing is you just try to run this app for t 1 let us verify once again let me tell you suppose somebody is 12 years of experience is not done well in desk or if he is getting six and into v score is 5 you can do the predict oops i'll just run it once again after t why because my you know i close that app dot py flask api so it is just giving me some error saying that the site cannot be reached but now to be able to he what is the error this now here it is now what i'll do is that i'll just say that the person is 13 years of experience - that person sim test score is 5 into be score is that right now you can see that before i had written eight years of spinosad road test score at nine interviews for as nine give me the output of somewhere around seventy five thousand now it is less than that even though the experience was high so this is a basic example guys you can apply any model use the generate,358,0,0,UbCWoMf80PY
9,template you will be able to get it that's it it's very simple you can also design your own html you if you want but make sure that each and every field is properly handled into it and the type of input that you are giving to the model you make sure that it is properly back so i hope you like this particular video guys i'm extremely sorry for the delay of this particular video of deployment part i'll be uploading more about deployment times so you better model together so don't worry about that please do subscribe the channel if you're not subscribed and i'll see you all in the next video have a great day and never give up keep on learning god bless your thank you,169,0,0,UbCWoMf80PY
0,so in this tutorial i'm going to show you how you can deploy your machine learning models um and we're going to do this using uh flask so this is python based so in my example uh on the right here you can see that the user can select a file so in our case let's select the dog um and when they hit this button it returns some text and it says that this is a pug uh along with how certain it is um and we can do the same with the cat as well and it says you know it's a cat and it's 50 certain for that one um so to get started uh with flask uh let's first create a file um in fact this one on the right here i'm running uh because i've already built it so i've already i'm running it just to show you so let me just stop that one okay there we go so now if i refresh that you can see it won't it won't load it just gets stuck loading so i'm going to show you how to do this from scratch so let's first start off with creating a python file you can call this anything you want and then we want to import flask but before we do that uh just do pip install flask i've already done it so it's gonna say that i've already got it here um but for your case just do just run pip install flask um and then once we've done that we can import it so from flask let's import flask and then the,358,0,0,0nr6TPKlrN0
1,standard flask stuff so app equals flask i'm just underscore name like so um and then we want to create roots so if you've used flask before uh we do this by doing the following so app.root and we want to say when we go to our just the the forward slash so not not adding anything to you to the url so we just go to localhost 3000 uh forward slash then let's uh let's accept a get request so we can do that in here oops okay and then for now let's just return hello world so we create a function and we can return some kind of text here so uh let's just do hello world and you'll see that oh in fact before we do that we just need to run it so so we can do if name equals equals main then app dot run and then we can set our oops our port to be 3000 you can set it to any port that you want uh and let's just set debug equal to true uh and then if we run this now so we can do am i on the right folder yes so we can do python app.pi and what have we got here we've got an error ah sorry i just realized what my mistake was this is a list right so the method isn't the correct term it's not the correct name of the parameters methods so if we run that again finally this time it worked and if we go to the page you can see it just says hello world so what we really,358,0,0,0nr6TPKlrN0
2,want to do is we want to build some kind of template and floss does this for us we want to have like a template page where we can then add some more data to later so to do this with flask let's just create a new folder called templates and then within within templates let's create a new file index.html uh you can call it anything you want so in here we can say let's let's create some standard oops some standard html here uh if you're new to html i've got a short video about that if you want to get up to speed but let's just create some standard html here and we've got a head and we can put a title let's call it uh i don't know tutorial real um and then our in our body we can have some kind of uh h1 tag and we can say image classifier in fact yeah that's what we called it before as well so if we save that and then give this a refresh ah so the reason why it's not there is because we want to return uh this template page instead so what we want to do is we also need to import something called a render template and what we can do here is instead of returning the text like we did before we can do render template and then the name of our file we don't need to specify the path to it we can just put in the file name and it will flask automatically looks in the templates folder for this so if i save that,358,0,0,0nr6TPKlrN0
3,you can see now we've got an h1 tag this doesn't look very nice so for the sake of this tutorial i'm also going to add in some bootstrap you don't have to do this it's just um it looks nicer so i've just copied this link and it allows to access all the bootstrap css classes so if you refresh this now you can see the font looks a bit nicer um i think in the example that i had before we had some we had it centered so let's just do text center and then if i refresh it you can see it's in the middle now um so back to the important stuff uh we want to be able to that there's kind of a couple parts to this we want to be able to have the user to upload data in our case the image we then need to do some pre-processing to it so get it into the correct format for our model and then we want to create a prediction and then finally we want to display that prediction to the user so what we want to do let's start off with the first thing so allowing the user to upload an image so we want to create a new route and this time we're going to use a post request we can still keep it uh to our to our home page to forward slash you can change this if you want uh but this time let's add a post request now some people will add it in here and you can have a if statement to to change,358,0,0,0nr6TPKlrN0
4,you know if it's uh if the method is post then you can all have it within one function uh for the sake of some for the sake of simplicity i'm just going to create a new function for this um and let's just call it predict um and then so so what we want to do is first let's add the html uh whether the user can can input some stuff i'm just going to copy and paste what i had before here so this is just a sorry you can't see that so what we've got here is a form um and it's when when the form is submitted uh it will send a post request to our root um and then this is our encoding type uh by default this is something different but because we're uploading an image we want to change it to this and then we've got a button and the upload section so if i save that you'll be able to see this so you've got your choose file and your predict image here so what we then need to do is being we need to be able to uh get the image save it into a folder and then do all of our pre-processing and whatever else we want to do to it so you can see that i've i've named this upload bit image file so in order to access this using flask we can do image file equals or whatever variable you want and then request dot files and actually first we need to also import requests here um and then sorry request um and then,358,0,0,0nr6TPKlrN0
5,request.files and then the name of that tag and we called it image file so that this bit here uh sorry this bit here is our name here oops and so now that we've got the we need to also be able to now save it so let's create a path in fact let's create a folder as well we'll just save it into an images folder so let's say image path is equal to and we'll go into the images and then to get the file name we can do image file dot file name and this will return the name of whatever the file was that the user uploads and then we can do image file dot save and then the path so now if we save that so now you'll see that if we choose a file and let's say it's the dog hit open and hit predict oh we've got an error so the reason we've got errors because we haven't returned anything so let's just return render template index.html and if i if we refresh again if we choose a file we hit the dog uh and then hit predict image so you can now see if we go in our images folder the dog is there and the same if we were to do the cat as well you can see the cat appears there and we can just delete these now uh but you can see that that's the the image does get saved uh when the user if you go when the user uploads it um and if you were gonna do this in production like if you're,358,0,0,0nr6TPKlrN0
6,gonna actually have a live website you might want to have some additional checks to make sure you know the user's only uploading images and you know they're safe as well um so the next thing is then having the model so at this point you want to load your model and then pass the image or text or whatever you're trying to predict uh you know into into this part now i'm using a keras model that's already been pre-built so i'm using this this model here vgg16 and this is just like a pre-built image recognition model that's that you can get with keras um so for the sake of like simplicity for for this tutorial i'm gonna use this um and the following code um here so let me run you through this so we're going to load the image and then we're going to try and pre-process it into the correct shape that the model wants and then what we're going to do is that we're going to make a prediction and then we're going to return our prediction here and then we want to actually put put in our prediction and give it to our template so we can say prediction is equal to classification um and in in this case you can use really any you can use sklearn or you could use keras with tensorflow or whatever you want to load your model here and then the output of your model you can pass into your template and what we'll need to add into our template is some kind of paragraph tag or some kind of text that will,358,0,0,0nr6TPKlrN0
7,uh only be there if we pass a prediction so that's what this is here so in in flask if you you can use this kind of notation here um to say if uh predict there is a prediction uh then display this paragraph onto the screen and that prediction is being passed in here and it won't get displayed when we just do the standard get request because we're not passing anything in here so just to show you what that looks like now so let's save that and save this um and then hit enter to refresh it and then if we wanted to look at the dog again and then hit predict oh we got a we've got an error so what's the error name and model is not defined um ah yes my bad i forgot to add the model in so we can just do this at the top here uh if we save that now so this is this model is from uh the import here so it's a pre-built model um viewers if you're using i think if you're using sklearn you could un-pickle your model and if you're using tensorflow you can uh you can use a they've got like a load model function you could use so let's try this again so let's choose our file we can we can look it up we can see if it works and yet it says image is a pug um maybe i'll try this out with some other images but you can kind of get the idea as to how you could adapt this uh into your own thing,358,0,0,0nr6TPKlrN0
8,it's relatively simple there's only two files here um but yeah if if you're not too familiar with html i've got a pretty short tutorial on that um you don't have to include this bootstrap stuff but it looks quite nice um let me just make that bigger um but yeah uh if if you get stuck on any of this then feel free to to ask me and i'll try my best to answer any questions but otherwise thanks for watching you,107,0,0,0nr6TPKlrN0
0,hello everyone this is sashmin here in this video we're going to see about how to deploy a trained model using python flask so i'm going to use iris dataset project for this experimentation so i'm going to run this notebook if you didn't see how to do this a whole project means you can click on the right top corner of this video to see the whole project until the training and testing so now we are going to deploy this model using flask and get the input from the user and predict the class label so that is the objective for this video now i am going to run all the cells so i will just say run all so this is just a small data set so it will be uh running very much quickly and here we are going to use only one model so let's see so it is currently uh doing the training so we are getting our different accuracies for for the model so we are not going to worry about the accuracy here we are going to train with any model so i'm gonna go with our decision tree classifier so the model is already trained i'm going to save the model so save the model so for that i'm going to import a module so import pickle so using this only we are going to save the model so we will have a file name file name equals in a string save saved model dot sav so that's done now we are going to save it using pickle pickle dot dump the model that is this model,358,0,0,2LqrfEzuIMk
1,and open the file so file name comma right binary so this is a string so write binary so this will save our model run this so it will be uh saved now we have to run this in visual studio code for running so usually when you are deploying a model means it it can be a command line python file or you can run it in a normal file so until the training you can use this jupyter notebook so this is the command for saving the model so here i'm going to load the model so i will say load model equals pickle dot load open the file name comma read binary so this will load the model and okay the model is loaded let's predict something so load model dot predict a double list here we have to set the parameters so let's see what are the parameters we are having i will have x test dot head so these are the things we are having so 6.0 2.2 4.0 and 1.0 let's run this so we are getting the array that is 1 so we are having the class in 0 1 and 2 because we have used label encoder here maybe we can also train the model without a label encoder let's comment this this is just to get the actual label directly so it'll be easier for us i'm just going to load the data set once again so let's load the data set and drop the id let's split the input and train the model so here predict it so here it is directly showing the class label,358,0,0,2LqrfEzuIMk
2,that is iris versicolor so this is how you are going to predict the class name using the user input so this input we are going to fetch it from the user and predict it and display the output so now this is how the whole flow works let's jump into the deploy.pi okay now in the deploy.pi we have a few files so this is the notebook file this is the dataset this is the saved model in this deploy.pi we are going to do the back end code for flask and in templates folder we are going to have the html files so first let's go for the back end code that is this deploy.pi so for this we are going to import a few modules from flask import flask render template render template and request request and i think that's enough so this is the import and apart from that we have to import pickle in order to load the model so import pickle and now let's initialize a few things if underscore underscore name name equals nascar underscore main so this is like a main function so when you run this program it will automatically uh run this so here app.run and debug equals true in order to show the errors so now let's initialize this app and load the model so app equals flask of underscore underscore name and after that load the model so load the model so i'll say model equals pickle dot load open the file name that is saved model dot sav and this will be read binary the model will be loaded and after that we,358,0,0,2LqrfEzuIMk
3,have to display the home page so for that at app.root slash so this will be the home page now after this we can create a function so def home so here return render template off so here we are going to load the index html page index.html so this is the home page that's gonna load and music so this is good and after that in the index.html page when we give all the values and click on predict it will show the output so for that we will create another function so that will be def predict so here this is the function and here also we have to specify this app root at app.root the path slash predict and we also need to set the methods that how we are going to get the data so for that methods equals post so you can also use get like this so you can use any one of them if you want only post means you just specify post alone so this is the initialization part and after that we need to get the variables so for that let's leave some space what are the variables we are having we have a sepal length sepal with petal length petal width so sepal length equals request of sepal length so this is i'm going to get it from a form so similarly i'm going to initialize all the other variables so sepal width equals request of sepal width so similarly we have to do it for a petal so this will be music petal just paste it all the places okay using this we can able,358,0,0,2LqrfEzuIMk
4,to get all the data we need and after that we can do the prediction so here i am going to say result equals model dot predict of so this is the values we are going to pass sepal length sepal with petal length and petal width and uh here it will display as an array so i am going to get the first index that is zero let's also check whether this is the right format so we have to specify the input according to the columns so sepal length sample width petal length and petal width so the order is right now after all the things have been done we will written again this render template html here i am going to pass all the locals so this result also will be passed to the index.html so it will be easier for us so currently i am just focusing on like the baseline so you can go to some other page you can render other results in a different page if you want so that is up to you so this is like a baseline backend code so this is how we are going to do the prediction now let's create a file new file index.html now here we are going to create a html code so html and body inside it we are going to create a form action equals slash predict so this is the route we have specified so it will call the predict function here and method method equals i'm going to use post so inside the form i am going to specify all the input we need so the,358,0,0,2LqrfEzuIMk
5,inputs are sepal length colon and here we will have the input input type equals text name equals sepal underscore length so using this only using this name only we are going to get the value and close this have a break tag and similarly do it for the remaining so we are going to have four input four inputs so sample length and sepal width petal length and petal width okay that's done and next we have to change this name also so this is simple width and this is petal length and this is petal width petal width so using this we are going to get the input and after that we can display the class colon and here i am going to display the class so currently if you want to display a variable so you have to use this double curly braces so here it will display the result if you have any so after this we will again have a break tag and finally we need the submit button so input type equals submit and here value equals predict so this is the value it will be shown on the button i think this form is completed and all the values are in text so we have to convert it into float here float off so so similarly we have to do it for all the other values float off okay all the things are done so here when we are rendering template we will have the result as empty and pass the local variables so we have done everything so this will get the input predict the result and render,358,0,0,2LqrfEzuIMk
6,it and here also we are passing the results i think we are good to go so just open the terminal so in this terminal you can run the command but here this is like a function so i just made a mistake here just have it as a function so this is like a function calling and and here also this is request dot form because we are using a form to get the values so this is also form and dot form this is the last one now everything is good to go let's deploy the model so python deploy.pi run this so this will deploy the server so this is the local server we are having control plus click now it is all loaded the website so it is showing 200.,174,0,0,2LqrfEzuIMk
7,let's go to the website so this is the website uh we are having so sepal length sample width length and petal width predict see now it's showing the class name so for this input values it is showing the class name you can also have a separate uh web page in order to display the results for the corresponding input currently i'm just having it as a small index html let's also get out so this is other websites so this is how the website is looking so this is the form we have created and here the server is running and everything is good to go you can also try some other input and check the values that's it guys this is how you can uh deploy your uh trained model in a server so this is just a local server you can do the similar setup for any kind of cloud platforms instead of this local host you will be uh having a separate ip address that is the public iep address with the corresponding port anyone can access the website and get those results so for better practice you can create a multiple html or javascript files and display all the results so i'm just going to go through a quick walkthrough how we have built so here we have created a form with action as predict so this will call the function here that is the root and this is going to pass the data using post and these are the input attributes and i am getting it as a string so that is a text and here i am just,358,1,1,2LqrfEzuIMk
8,displaying the class variable so result will be shown here and finally this is the submit button so the value will be predict that is here you can clearly see separate length sepal width petal length and petal width these are the inputs you can have different values also four five three and five these are like random values predict and it will show you the class so now let's see the backend function so these are all some default attributes so these are all flask related attributes so here we are loading the model and when we are deploying the server so this is the first page it will load that is the index.html so we have created a function called home and after that whenever we click the button we are going to predict and in the predict we are getting the values from the form using request.form according to the name and we are converting all the attributes to float and we are trying to predict from the model so in this model it will written an array so from that array i am just getting the first index that is the class and this is the result so finally i am just displaying the result here so this is how the whole flow works so similarly you can do the same process for all the other projects you want so i will try to deploy it in some cloud platform also if i have access to that resource so currently this is how you can deploy it in the local server and i think that's it guys if you like this video,358,1,1,2LqrfEzuIMk
9,hit the like button and stay tuned for the next video,14,1,1,2LqrfEzuIMk
0,welcome everybody back to this awesome video tutorial where we are going to build this awesome machine learning website using flask and python programming language we are going to make prediction on our iris flower data set as you could see this is really responsive and it is working of course first of all you need to install the flask module for installing you need to say pip install plus mine is already installed i'm gonna close it before starting anything let me actually explain you some basics of flask first of all we usually import plus class from this already installed plus module and it's a class so it will gonna have capital f in it then we create an instance of this blast class which we have already imported then it expects an argument which is underscore underscore name now what usually it is it is basically returns mean if you are at python script whereas if you are using it or importing it any other python script then it will gonna return the name of that script so it is basically that then uh then you would have seen this line so many times which is uh this if underscore score name is equal to is equal to underscore underscore main as i already told you that if we gonna return main if we are at current python scripted so in that we usually say app dot run which is the instance of this class class and it contains a sub function called run and you would also have seen debug equals to true what did what it means that if we make,358,0,0,i3RMlrx4ol4
1,any changes to this file we do not need to rerun this python script again and again it will gonna automatically reload everything and we just have to need to press this refresh button one more thing which you should know is at the red app dot route and it also expects a argument which is basically this slash basically this slash means if we are at current base directory of our web page as you as we are over here so in this we usually defines a function with any name whichever we want i'm going to name it like home and it returns something to us so for an example let me show you that if i return heading of x by saying h1 hello then if i run this app.basics dot py so if you wanna return me this hello heading let me show you okay so if i refresh it you you can see over here it is hello all right now one more thing okay first of all if i write here like slash home and it will gonna automatically refresh as the debug is is equals to true and if i refresh it you're gonna return me 404 error not found whereas if i put here slash home then i will get that output all right i hope that should be clear to you one more thing you would have seen is a render template which we which we import from this plastic class what happens we instead of returning a line of code of html we returns whole html file which resides in this templates folder which is all,358,0,0,i3RMlrx4ol4
2,in lower case so as an example i have this home dot html okay let's run this first of all okay over here finally we got it so what happened i am saying that return this whole html file instead of typing it over here i have i have wrote whole code in another html file and accessing it using render template these are some basics which you should have which you should know before jumping to create this website now you know these basics let's jump into the folder structure what folder structure should be first of all we have static folder in which our images resize which you want to show to the end user we have templates all in lowercase and over here we have our html files which we want to show it to the user we have a file called app.py you can name it any name you want usually people name it as app.py and in that our whole code resides which i have just shown you before then over here are my three files which is my machine learning model and my saved pickle model and my data set okay let's first see let's first build our machine learning model what is happening i am importing the pandas library for importing my data set i am importing numpy for creating arrays and all that stuff and i am importing pickle for saving my model then i am reading my data set using pd.read csv let me quickly show you how that it looks like it has four values and the last value is the class of that flower now,358,0,0,i3RMlrx4ol4
3,these are in string keep in mind so over here i am saying x equals to these first three columns uh sorry four columns and y equals to last column now our machine learning model usually works on integer data set so we need to convert it to integer as i have shown you it is a in string format so i usually do it using label encoder from sql dot pre-processing and fit transform to y this this will gonna uh convert this to zero and another one to one another one two two and so on then i am using support vector classifier as it's a classification problem kernel equals to linear and fitting that x strain and y train and i am saying pickle dot dump and dump this model in the i r i dot p k l in the in the right binary form all right so now we are in our main app dot py so we are importing flask and render template and i will talk about the request later i'm reporting pickle to load this model which we have already saved in a moment before and i am importing numpy for creating arrays and that stuff this you already know what is the meaning of this and i have already shown you what this do it says if we are at base then run this home.html which is right here let's see what is inside home dot html first what is inside home dot html is our four input fields and one submit button and one big form of all these input fields now what happen if i,358,0,0,i3RMlrx4ol4
4,am saying that if i press submit button then use the method post and action equals to url for whom what does it mean url for home usually mean that whenever i press submit button you search for this home function inside my app dot py and run letter function what is happening inside my home function is i am requesting for my a d c d input field values which are this a b c d i am saying access these values and pass these values to data 1 data 2 data 3 and data 4 variables and create an array of these values and send them for prediction and and store that prediction value to this spread variable okay so i have told you uh what this return mean i am saying return this after dot html let me quickly show you if i pass these values to it and if i press submit button this button then it will gonna take me to this function slash predict route and it will gonna return this after dot html function and it will gonna also send this data to this html let me show you how you can access this data for using the if for such code inside html we we have to create a code block by saying and by saying two curly braces and this percentage sign and in that we pass the statement which you want to run like if or for or whatever you want for example over here i am i am using if statement and at the end you need to say and if like or add for,358,0,0,i3RMlrx4ol4
5,whatever you use and inside then we pass our whole code which you want to run similarly i am doing it over here i am saying if data is equal is equal 0 what is data data which i have my trade variable i am saying if if i am saying if data is equal is equal 0 then it is i reset also and show this image to user as it is ids versicolor and show this image to user now i know i have one more class but i have not defined it over here you can define if you want and i'm saying and if and i'm saying for getting back to main home page which you could see link over here if i press this i get back to home page by sending a slash href to this anchor tag that is whole cool how you can create this machine learning website now you can use bootstrap and css styling or javascript for creating awesome stunning website you can get this whole code and down description github repository make sure you hit the like button and subscribe to my channel for more such awesome machine learning class videos with this side i would like to end this video and i'll see you in next video,284,0,0,i3RMlrx4ol4
0,hello everyone i'm sadan in this video let's understand how we can deploy a trained machine learning model as api using flask in python let's say that you working in a company in some machine learning use case and often times there would be a separate team would be working on frontend so this front end can be anything it can be a website or it can be a let's say an android application or a iphone application so now we need to understand how we can uh deliver this machine learning model so that the front end can use that let's say that you are working on a diabetes prediction app and you would be working on building this model training this model data collection and all the stuff and the front end people will be working on the user interface and related stuff so again it can be a website that can be built using let's say reactjs or node js and so on so we have angular and other stuff right and if the you know user interface is basically application let's say android application then the programming tools like cotlin or android can be used in order to build those things so now let say you have trained your model and there is a front end now we need to serve this model so that the front end can use this and that's where we can deploy this machine learning model as a api and we have already discussed about how you can deploy ml models using fast api library but in this video let's understand how we can build this uh,358,0,0,MvTqi2Mb_PM
1,you know apas that's going to serve your machine learning models that's going to make prediction using flask library so this will be the agenda for today's video and let's get started so for this let's work on a simple use case so so we have already worked on this diabetis prediction several times that has this kind of like basic data and so on and then we would have outcome column that would predict whether a person is diabetic or not so let's consider the simple use case and build a simple model so i'm not going to focus more on this data pre-processing you know model tuning part and all that so i'll just like kind of build a very simple model with a let's say a very simple scaler and then using these files we will try to deploy this as api and see how we can test this api so this is what we are going to do today and for this these are the libraries that i'm going to work with so one is numi to handle this data and all that and then we have pandas to load this into data frame and work with that and then s could learn to train the model and stuff like that so pickle should already be installed uh in your machine if not you can also do a p install for pickle but again it should be uh you know installed in the standard python installation so you don't need to worry about that so these are the main libraries that we'll be working on and these are the versions when i,358,0,0,MvTqi2Mb_PM
2,was working on this code but if there are latest version you can also update these libraries as per your choice okay so that is the first step so as you know i have this diabetes.,46,0,0,MvTqi2Mb_PM
3,csv file in the same directory as my notebook file ml model training for flask which is this notebook and the data set file is is also in the same directory so the first step is i'm going to import the required dependencies so i'll say import npay as np and import pandas as pd and then from from sk learn do model selection import train test split so the next thing is from sk learn. preprocessing input standard scaler so once we have trained this model once we get this pi f later we will be working uh you know on building this api code on pyam again you can use vs code or other ids as well but i'll be using pyam so the first phase is training the model in a notebook second phase is in know in pam again you can also you know build a script that's going to do all the steps instead of doing in a notebook it's no compulsion that you should train your model in a notebook file only so that's okay and then from sklearn do linear model import logistic regression so this would be a classification task so we'll be using logistic regression model and then from sk learn.,273,1,2,MvTqi2Mb_PM
4,preprocessing input standard scaler so once we have trained this model once we get this pi f later we will be working uh you know on building this api code on pyam again you can use vs code or other ids as well but i'll be using pyam so the first phase is training the model in a notebook second phase is in know in pam again you can also you know build a script that's going to do all the steps instead of doing in a notebook it's no compulsion that you should train your model in a notebook file only so that's okay and then from sklearn do linear model import logistic regression so this would be a classification task so we'll be using logistic regression model and then from sk learn. metric input accuracy score and finally we can import pickle so let's understand the use of this libraries i mean by this point you would already know but if there are someone who are not kind of sure about this libraries so npi we use in order to basically work with r and so on so in some cases we would use this and then we have pandas to create pandas data frame that's going to be useful for your data pre-processing and and stuff like that and then we have sk learn.,295,2,3,MvTqi2Mb_PM
5,metric input accuracy score and finally we can import pickle so let's understand the use of this libraries i mean by this point you would already know but if there are someone who are not kind of sure about this libraries so npi we use in order to basically work with r and so on so in some cases we would use this and then we have pandas to create pandas data frame that's going to be useful for your data pre-processing and and stuff like that and then we have sk learn. model selection train to split which is going to split your data into training data and test data where you train your model on the training data and evaluate it on the test data and then we have standard scaler in order to scale this data so the next step is you have logistic regression model which is a simple classification model and then accuracy score to just like look at the accuracy of this and so on and then pickle to save this model file and the scaler file as as pickle files so those are the libraries that we'll be working on so the next step is load loading the data set so i'll say data is equal to pd. read csv as we have a csv file and the name of this file as you can see over here is diabetes. csv you can give the full path or i mean in this case it's in the same directory right so i'm just going to say diabetes.,341,3,6,MvTqi2Mb_PM
6,csv you can give the full path or i mean in this case it's in the same directory right so i'm just going to say diabetes. csv and that's going to load it so okay so it says no search file let me just copy the name okay so there seems to be a slight issue over here oh i think this is in a different file so that's okay okay let me just copy all this this should be in a different directory so that's why i was not able to do this i'll close this one i'll close this diabetes. csv file as well and then we have this data is equal to pd. read csv diabetes.,154,6,9,MvTqi2Mb_PM
7,read csv diabetes. csv so now this worked so next if you want you can just look at the end of this data just this just print the first five rows we have pregnancies glucose blood pressure and so on and then we have outcome column which is the target column again as i said i'm not going to do a lot of data analysis and pre-processing and all those things as the agenda of this particular videos just to deploy the model so let's just just work with this data just build a simple model uh let's not you know focus on this you know class distribution and all those things so in this particular data set right so there is a class distribution for zero i i think the number of zeros is more than compared to one so if you want to build a proper model so you should probably like handle those class imbalance and out layers and all those things but for now i'm not going to look at all those things just use this data and just train a model so let's just focus on that so i'm going to split this data into features and target okay so i'll say x is equal to data.,275,9,10,MvTqi2Mb_PM
8,drop and within that i'm going to pass a parameter called as columns and i'm going to drop my outcome columns outcome column so this is something that i don't need because this is a target column and then y is equal to data of outcome same outcome so basically what we are doing is here is like outcome is the target column so that goes to your y and from your x you're just like dropping this outcome column and using all these things so the idea is the machine learning model would try to understand all these features and see for what features values like the outcome will be one and for what feature value the outcome is going to zero so that's what it's going to learn in the training process so now we have separated our x and y so if you want you can print this x and now you can see we have pregnancies until age we don't have our uh you know outcome column if you just print your y so that's going to have your outcome column so that's how you can split these two things so the next step is you have your features and target now basically your x and y now we need to split this data into training data and test data so i'll say split the data to training data and test data so here also maybe i'll mention a comment saying that splitting the data to features and target and split the data into training data and test data so here i'm going to say x train comma x test y train,358,11,11,MvTqi2Mb_PM
9,comma your y test so here i'm going to call this train test split the method that we have imported from sk learn.,29,11,11,MvTqi2Mb_PM
10,model selection and in this train test split i'm going to sp in my x and y and say test size is equal to 0.2 maybe stratify that's going to split commonly your classes so that also can be based on your y and then you can have your random state is equal to let's say 42 so this is it so xtrain is your training data features so basically your x and y both are kind of splitted into two kind of different parts so xtrain contains your training data features and the corresponding labels for these features are traded in are stored in this white train and x test contains your test data features and the corresponding labels of your target are stored in your y test so we using this train test spit we are passing this x and y test size 0.2 is nothing but 20 of test data stratify y is basically stratifies or kind of have a similar distribution in your original data to you know your uh training data and test data so it's just that make sure that not all one goes your training data or not all one course your test data it's just to have a you know common split between your training data and test data and then you have random state which is just for reproducibility if you use the same number 42 your data is going to be splitted in the same way as my data if let's say you're using a value as one or two then your data is going to be splited in a different value so here you can,358,12,12,MvTqi2Mb_PM
11,use any integer value so that's fine so that's the other thing so this has been executed so if you want you can also print and look at this shapes i'll say print y do shape which is the entire data and then you can print this white train do shape to look at how many data points are there in the training data and finally white test.,88,12,12,MvTqi2Mb_PM
12,shape so this is the number of data points in your test data okay let's run this so totally there were 768 data points so 614 is your training data and 20 of the 768 154 goes in your test data so this is it so the next step is uh scaling this data so for scaling we can use minmax scaler and the other scaling is your standard scaler that's kind of going to have your uh you know you know make this data into a uniform distribution so i'll say scal this again we have already discussed all those things i'm not going into detail of like what exactly happens in the scaling and all those things so i'll say scale the features using standard scaler so you usually the linear models right so the logistic regression linear regression these kind of models requires your data to be scaled so basically it converts your data so that the distribution is uniform or normal distribution the build shaped distribution without any skew but if you're working on tree based models like random forest xute xd boost and so on so you don't have to use the standard scaler so those models are not kind of affected by this scale so here i'll create another variable called as extra scale so that i can save this in a different variable here also i can use the same variable but i just want to keep this original data point separately so i'll go with a different variable name called as exrain scaled and extra scaled is equal to scaler uh do fit transform and here pass your,358,13,13,MvTqi2Mb_PM
13,exrain so you would be kind of like scaling only your uh features not your target as it's just one and zero in this case and then you would have your x test scale x test scale is equal to scalar do transform so this is a key thing right so the scaling happens right so it's based on your training data so we say fit transform so first the scaler uh the standard scaler is fitted on your training data and then we transform the data based on that scale that we get from here and for scale uh scaling the test data we shouldn't again perform fit transform so the scaler only should be based on the training data that we have got in the first place so basically in your training data you do fit transform of your training data or training features for test data the same you know fit that you got from your training data is used so for training use fit transform and test use scaler do transform of this should be x test okay so here we have scaled our training data and test data okay the next step is training the model so here i'll say train the uh logistic regression model so i'll say model is equal to logistic regression so i'm not going to work with you know several parameters and so on so here we are creating an instance of this logistic regression and then i'm going to say model.,327,13,13,MvTqi2Mb_PM
14,fit so fit based on your scale data otherwise you may get a warning that you need to scale your data so i'll say exrain scale and we would also pass our training data target which is my y train so it pass these two things so this is the step which is basically like training the model so i'll run this so this has trained the logistic regression model as it's it's a simple model and the data number of data points is also small and the number of columns is also small it's it didn't take like much time it just took a few seconds so yeah this is a train model so now we need to make predictions on our test data and and just like kind of look at the accuracy so i'll say y pr is equal to model. predict ex test scale let's say a is equal to accuracy score that we have imported from sk.,210,14,15,MvTqi2Mb_PM
15,metrix here pass your y test which is your true labels and your predicted labels that is your wi rate and then print accuracy and when i run this i get the accuracy as 71.4 percentage if you just want to round this you can just say round accuracy this should also be within a quote so i'll say round accuracy comma to so this this is going to round this to two decimal places so the accuracy is you know 0.7 if you just do the same thing for training data right so this is for test data prediction but if you do this for training data exrain scaled and this should be your xtrain your wi let me call this as wip train so it's highly likely that there's going to be a war fitting issue over here because because of the class distribution and other stuff so this is your let's say training accuracy this is your test accuracy so wi is your test uh prediction and your wi train is your training data prediction that we do on extra scaled so you have white train and y train okay so this should give you training accuracy so the training accuracy is 79 test accuracy 7 so actually there seems to be an under fitting issue but again you can just like work on this and try to see what is the issue over here but again if your main focus is just working on the deployment then you can just like skip these stuff as again the focus is different in this case so now we have got a train model that,358,16,16,MvTqi2Mb_PM
16,is nothing but this model that we have trained over here so the next step is we need to save this model not this model alone we need to save this scaler as well because when we make a new prediction we cannot pass that raw data we have to scale this in the similar way we have scaled this over here so now we need to get two files one is your your model pickle file and the other one is your scaler pickle file so here i'll say save uh the train model and scaler as pickle file so for this i'm going to say with open name the files i i'll just call this as diabetus i mean you can give any name over here but i'll call this as diabetes model.,175,16,16,MvTqi2Mb_PM
17,pkl the pickle file extension and we are going to write this file so say dou b so you're going to kind of write this as bytes in the binary format and and then say as model file and i call this pickle. dum so this is going to create your pickle file and call this model which is the train model that we cau over here so that is what we are passing over here so this is going to serialize this and save this model file so this is the syntax for this code so we are opening a new file called as uh diabest model so this is like we are writing the file so that's why we are using this wb and when we are reading this we would mention this as rb so we are opening this file as model file and to this model file we are dumping this model trained model data so that's the purpose of these two lines of code and i'll just copy these two things and do the same thing for scaler so i'll call this as scaler pk typical file this can also be the same or you can also changes to scaler file and for scaler we need to copy this the standard scaler that we have created and and we have fitted on the training data so this same scaler should be used again you shouldn't fit and transform it you you you should just like transform the test data that the user is going to pass so this is it so we have diabetis model.,351,17,18,MvTqi2Mb_PM
18,pk we are writing this file as model file and dumping this and scalar pk we are kind of saving this as scaler pk and dumping this particular data so when i run this here we don't have those files right so when i run this it's going to create two new files the one is diabetis model pckl scaler dopel so those are our pickl files i'll come back here and click on this refresh and as you can see 2 seconds ago we have created two files diabetus model pk and then you have scaler pk which is like very small failes as the data is small and the model is is also a simple model but again based on the data on the model the size of the model can also change so later we would download this and kind of serve this as api so that's the idea uh but before that right i i'll just show show you in this notebook itself or you can make a prediction it's basically a predictive system code that we can later use in order to uh you know use in our api so here i'll create this as a txel markdown and say code for prediction so here we need to import pickle and i mean we have already imported it there but maybe i'll import this again so that it's easier for you to look at in one place so these are the two things that we probably need need so import nump as np and first thing that we need to do over here is we need to load this file so,358,19,19,MvTqi2Mb_PM
19,i'll copy this same thing so in this step we have kind of return this file we have created this file but now we need to read this file so i'll say with open diabetes model.,46,19,19,MvTqi2Mb_PM
20,pk rb as model file this again you can change this name or you can just keep it as it so that doesn't matter over here so here i'm going to change this to loaded model so let's say that we have trained a model and saved this file and later we are reading this file and storing this in a new variable called as loaded model so the previous model that we had is model this is the same model but we are reading this from the pickle file so i'll say pickle. load so for dumping it you use pickle do dump for loading this again use uh pickle. load so this is for saving the model this is for loading back the model to the variable so say model file so this model file is coming from this particular aspect next the as the similar step you would read this scaler uh.,201,20,22,MvTqi2Mb_PM
21,pickle file as well so i'll say scaler dople file read this as let's say scaler file and copy this and paste it over here and this we can you know call this as loaded scaler okay so when i run this it's it's basically like reading those files and storing those in this variable called as loaded model and loaded scaler so now i'm going to take a new data point and and you know make a prediction so for that i can just say new data point or maybe i'll just use this over here okay let's just put this in a new cell itself uh new data point is equal to let's this in a nump array so i'll call this nump array or i think you can just use a simple list instead of you know having any confusions i'll open this diabetis csv i'll copy this first uh value so we have 6 148 72 35 0 33 point okay this this first values i'm going to copy everything apart from this it's getting copied as image let me see okay let me just look at the values then so the values are 6 148 72 148 72 35 33.6 35 0 33.6 and then we have this diabetes pedigree function as 0.627 and 50 0.627 and 50 okay so these are the input values so we have 6 148 72 35 0 33.6 0.626 627 and 50 okay okay this is my data point so the next step is we need to transform this data using our scaler so for that i can just say uh scaled data let's call,358,23,23,MvTqi2Mb_PM
22,this a scale data is equal to scalar or maybe let's use this loaded scalar loaded scaler do transform again as i said you shouldn't fit this here you should just use this transform so i'll say loaded scaler do transform new data point but you shouldn't just mention it over here instead you should just create another list and within that pass in your new data this is just to tell the model or the scaler that i'm making prediction or i'm processing only one data point right so that's why we need to put this within a list so you shouldn't just like say this as you know new data point so that's not going to work instead create another list over here within that pass your original list that is your new data point so run this so here you get a warning saying that x does not have valid feature name so was fitted with feature name so this is basically that previously when we fitted that there be the column name such as pregnancies glucose and so on but when we are now trying to fit this we are not pausing any feature name so if you don't want this error you can just like pause those corresponding feature names as well but again to just keep things simple i'll just like ignore this warning for no but the scaling would have happened so if i just print this scaled data right so this is how it's going to look like so previously we had 6148 72 and so on so this data has been scaled to this particular uh thing,358,23,23,MvTqi2Mb_PM
23,like based on this scaler that we had previously so i'll delete this so now we are going to make a prediction so i'll say prediction is equal to uh loaded model and within this you need to pass your uh scale data so similarly as we did over here you shouldn't just like pass your scale data within this parenthesis you should create a list within that and then you know paste your scale data and that's going to give your prediction object is not callable oh okay so it should be loaded model do predict because we are predicting it so i'll say loaded model do predict found array with dimension three log expected less than two okay this seems to be a slight issue over here okay so we have loaded model or i think we should be good with just passing this scale data as this would have created an array okay so here we have previously used this right so i think it has already created an ire ar with like proper formatting so if i just like show you here right so we have this array and this within this array it's it's this list is present so we don't have to again create a list this way so you can just like pass this scale data alone so yeah it seems to work with this itself so when i say this print this prediction it says ar one so i mean one basically means the person have diabetes and the label zero means like the person doesn't have diabetes if you open this if you look at this it,358,23,23,MvTqi2Mb_PM
24,it's one but again as this model is not highly accurate there is a chance that it's not going to make correct prediction most of the times or let's say 70 of the time but yeah for now let's let's not focus there so this is my prediction to access this prediction to look at whether it's you know one or zero say prediction of zero okay so the value is one so you can also try printing it over here so that you're not looking at this data type okay so the value is one so this is how you can just like look at the label value so now what we are going to say is if prediction is i'll say if prediction this zero is nothing but this is a array right so i'm accessing the first element this only has one element but we can use the zero to say that extract the first element so here i'm going to say if prediction of zero is equal to 1 then say the person is diabetic right else what happens is right so let's say it should print non diabetic okay so in this case the prediction is one so we say diabe if else is the casee it would say non-diabetic so this is how we can build a predictive code the main aspect is first we need to load this pickle file and then we need to scale this data using this loaded scaler and finally we can make a prediction and and return this value so this code is really important because this is what we are going to use,358,23,23,MvTqi2Mb_PM
25,in our flask api that we are going to build so let's understand how we can do that now so now i'll open my p charm and create a new project so if you're again using this pyam for the first time or if you have already worked on it you can just follow this particular process and i also made a video on how you can use pyam you can refer to that if you have some doubts again alternatively you can use vs code or some other ids like spider as well so no issues there so i'll go with this new project this is like the directory where you want to save this so there is a directory called as cod prep ml model flas api again you can choose the location of your choice in your device and this again creates a subd directory so this name sometimes you won't see this name option here so the ent directory you can just give it over here so here i'll create this subdirectory name as let's say flask api uh you know fl ml ap code okay so let's say that this is the environment and and the project folder that i'm going to create so i'm going with custom environment and i'm going to generate a new uh virtual environment and this is the b python so if you are working with this for the first time you would also see an option to download python so that's okay as well so for the first time you can do that and i'll cck click on this create this window so it's going,358,23,23,MvTqi2Mb_PM
26,to replace this window with the new one so this is the new project that we have created and then yeah let's wait for it to complete okay so now you can see this project directory here and you have this v env which is your virtual environment directory so the basically the idea of virtual environment is that every project that you work on may require like different uh versions and let's say that you're working on a project this month and you you have not touched it for like let's say next two months and then you work on it you install the libraries there may be a issue that the libraries have been updated and your code works no more sometimes we kind of stick to having like specific versions of libraries so we create this virtual environment and install the libraries over here so what happens is your systems uh you know based libraries are not affected by the installations that you do here or the different project would have a different virtual environment so all those kind of basically kind of get container right so there is no conflict between the libraries so for this particular project this v andv within this folder is my uh environment library library that's going to have my libraries so the first step is we need to create a few files so the first file that i'm going to create is requirements.txt so this requirements.txt file basically this is used in order to uh you know basically mention the libraries and the versions that you need and then we can install all the libraries at,358,23,23,MvTqi2Mb_PM
27,one go so instead of like doing a paper install for all the libraries separately you can just like use this in order to mention like what are all the libraries that you need and the corresponding versions of those libraries so let's first do that so here i'll say flask is equal to 3.1.0 and maybe i'll just install pandas and nay as well just in case we need it so i'll say num is 2.2.1 and let's say pandas is 2.2.3 again replaces with the latest versions if you need citore sorry iphone learn so the pyit learn version is 1.6.0 so this is a very important thing because when you have different version of py could learn the place when you have trained the model and in your ap environment if you have a different version then it's going to give you a warning saying that there is a chance that it may not work as expected so make sure that you have consistent library versions across these two places so we have flask in order to build the apa and then we have nay and pandas for data pre-processing and to work on it and theny could learn to just you know use that model of logistic regation and so on so these are the files or or the libraries that we need so now we need to install this in order to install this you can go to this terminal i'll type in this clear that's going to clear all the previous things so now the virtual environment is not activated so you can see it's ps this particular directory and,358,23,23,MvTqi2Mb_PM
28,so on so this is just like you know running something on your windows powershell so we need to activate our virtual environment that's present over here in linux right so i've worked on ub tu so in ub tu what happens is like the virtual en environment kind of automatically gets enabled in pyam but if you on in window windows and you are not able to kind of work on that you can use this v env which is the name of this directory and scripts activate so that's going to uh you know activate this virtual environment and you would see this virtual environment uh word over here so that means your this is currently in your virtual environment and you can install your libraries so that's about it and sometimes you may get a error when you are running this code so that that may give you a you know permission issue so let me know in the comments if you're facing this particular issue so i've made a video on you know how to get started with py charm in a different channel i can share you that link but this is the process so let me know if this particular code doesn't work for you and you are not able to see this virtual environment so with that being said next step is we can install this libraries so you can just type in this ls that's going to give you all the files that's present over here so here we have this requirements.txt file right in this particular directory ls is basically list all the files that present over,358,23,23,MvTqi2Mb_PM
29,here i'll again clear this and say pip install iphone r requirements.txt so this pip install iphone r requirements.txt is basically going to read this requirements.txt file and install all the libraries present in this requirements.txt file so press enter and you will see that it kind of starts to install all the libraries that we need you know flask nay panda learn and there would also be like other libraries dependent on these libraries so all those would kind of like get installed so that is the next step so in the meantime these libraries are getting installed i'll create my uh you know main.py file which is basically the apa code so i'll click on this new and go to this python file and name this as main usually this is what the naming convention is the main file that you're working on or the base code kind of goes in this main.py if you have modular code you would just like create some other py files and just like import it as class or functions and so on so yeah that's about it so as you can see the libraries are also getting installed okay in the meantime what we can do is i'm going to uh go to my j notebook i need to download these two files the diabetis model.,292,23,23,MvTqi2Mb_PM
30,pk and scaler pk so you can click these two things click on this download so that's going to download these two files so this is kind of with this one you know number because of duplicate things so i'll just like copy these two things diabetus model 1. pk and scaler pk so you can open your pam click on this folder and press contrl v and that would paste it alternative way you can also open this particular directory in your file manager and then there also you can paste it so that's okay in this particular project directory both are okay and this i'm going to click on this right click i'm going to rename this and remove this one in scaler and diabetes model so click on this uh you know right click on this diabetes model go to this refactor and click on rename so here i'm going to remove this one and just press enter or i can click on this def factor and then in scaler also i can just leave the other things as it is so go to refactor rename and just like remove this one and click on this refactor okay so now we have the train model and the scaler file so diabetes model.,279,24,25,MvTqi2Mb_PM
31,pk and scaler pk so here the libraries are also installed so you can just like click on this terminal again and that's going to minimize this and currently i'm in main.py file so now we can just get started to work on the apa code so here i'll say uh import pickle and then from flask import flas comma request comma jsonify so this is going to you know usually the format that we would kind of send the data is in the json format so it's kind of like a standard format so we can use this json ify in order to like wrap this you know json format to basically make that the output of this api in the json format maybe i can also import this nay and pandas in order to handle these pd and n as np in order to handle the data better also we can use this flask in order to give like proper error messages as well so i mean if it works well it's it's kind of going to work well but if it's not working well we need to send like proper error code status code and then mention like what's the issue and so on so here we need to sa say app is equal to flask the flask with upper case f that we have imported and within this uh mention your name okay and then we have to load the train model and the scaler file we can just use the same code that we add over here uh yeah this one so we are going to read this file copy this,358,26,26,MvTqi2Mb_PM
32,paste it over here and this maybe you can use loaded model or you can just like change to model as well so model and scal instead of you can remove that you know loaded uh word you can keep it as it is or you can use it so that's completely up to you so the next part is say at app.,81,26,26,MvTqi2Mb_PM
33,route and within this parenthesis you can pass in your forward slash so say df and say return diabetes prediction app is running so what we are doing here is when we deploy this ap right so when we run this currently this is going to run on local h but actual uh you know in deployment phase we would let's say put this in a ec2 instance or or virtual machine on assur in some server right and that would kind of generate a url when we go to that base url so it's going to say diabetus prediction app is running when we put that on on let's say browser but usually we don't you you know put this apas on browser instead we would use that on let's say our front end code and so on and then we would call this api but this is just to make sure that when someone goes to this particular url it says diabetes prediction app is running so the next step is the main step so that is uh app.,235,27,27,MvTqi2Mb_PM
34,route uh put your forward slash and say predict so this is what we call as the end point so i'll say predict so this is a predict end point and in a single ap you can have like multiple endpoints so here we have one endpoint for this woman the other endpoint for predict and so on similarly you can have other endpoints to you know send some other messages and so on so here i'll say this predict and then uh for method i'm going to use a post method because we need to get the input from the user right so basically a json or a dictionary format of data is kind of posted to this endp point we receive that process that and we then make predictions on so that's the idea so here i'll say define predict again this can be in a different name as well but i'll just like use the same name predict function okay similar to we would kind of work in fast ap just like a few changes would be there so here i'm going to add a try and exception block in try i'm going to say get the json data from the api request so when a user right user in the sense when when a person is working on the front end is is kind of calling this api with the data that the user has passed they would send this as json data so i'm going to say data is equal to request.,334,28,28,MvTqi2Mb_PM
35,get json so that that's going to get this json data and then check if input is provided so we check if the input is like kind of not empty so we say that if not data return jsonify the json that we have imported over here and we just say a message saying that you know error input data not provided and then we can also send a status code so you can put a error message saying that 400 which would represent like uh you know input data there is some issue with the input data so if not data that means data is not there the expected data is not there expected data in the sense we we should get this values the pregnancy values like all the values that we are seeing except for outcome value we need all this data if the user has not like sent uh you know data they just sending some null data then we return a error message saying that input data not provided with the status code of 400 now they know that what's going wrong in the ap level and they can just fix that from ui only if they are not setting proper data and so on so let's say if this is not the case right if the input data is not provided the return statement would work and the other pieces of the code other lines of code won't be executed but if this condition is not satisfied that means when we actually get some data then we kind of execute this other cells so this is uh you know other,358,29,29,MvTqi2Mb_PM
36,lines and this is to validate input columns or fields whatever it is right the pregnancies glucose and all those things so here i'll say required columns for our prediction so required colum are nothing but like all the columns that we see over here so i'll just like put this in this thing minimize this i'll i just want to copy all this or maybe i can just open this right so i'll minimize this open my csv file that's present over here i'll open this with my notepad these are the things that i need so i don't need the outcome thing so i need until age because outcome is something that we need to predict so i'll close this come back to my py cham paste it over here and make sure that you put everything in codes you can just double click it and press your double codes that's going to create you know opening close and closing codes okay so the p pip standard is that nothing should go beyond this line so if you want to kind of follow that you can just like press enter somewhere over here so that we are creating this list in two lines but that's okay you can also have this this this way so no uh issues over there so that's completely fine so here i'll just break this line okay so these are the requ column so now what we are going to do is say if not or call in you know input called input data.,339,29,29,MvTqi2Mb_PM
37,data frame so i'm putting this within this data frame so make sure that you're putting this within a list because now it knows that this we are creating a data frame that has like one uh column with all those colum sorry with one row with the column names corresponding column names so here we are getting this data that the user is sending which is basically all the features that we have just seen and creating a data frame called as input data and now in this input data right we need to check if all these columns are present so i'll say for uh you know if not all column in uh input data do columns so input data columns is like basically print all the columns in your data frame for call in required columns this is a simple piece of code i'll just explain you this in a minute if not return again a error message saying that you know uh required columns missing and you can say requ columns or re fields anything i'll say that requ columns is this we can put a f string and say take columns okay let's try to understand like what we are basically doing here so the first er handling that we did is when the user is kind of not sending any data in that case this if condition is satisfied and we send a error message saying that input data not provided and in the next step we need to before calling the model we are going to evaluate that all the required columns are present because if all the columns,358,31,31,MvTqi2Mb_PM
38,are not present then we are going to get a error right so that's the issue so we we are just like saying that these are the required columns and in this if condition we are checking if all these columns are present so if this condition is right so what we are basically checking is if not all columns that means if any of the columns that's present in this required columns are missing from this input data that we get from the you know ap's data if some of the columns are missing then this if condition is satisfied because we use a not condition here so this would this line would only run when one of the requ one or more required columns is not present so if not all these columns are present in your uh data that you sending to this api then we say that required columns missing and we also send a message saying that these are the required columns required columns is basically coming from here so pregnancies glucose blood pressure so we basically send this value and then send a error code of 400 so this is how this would work okay and again so these are the two main checks that we have one is to check if the data is empty and in the next step if it's not empty we make sure that all the requ columns are present if not we say that you you have missed some columns and these are the columns that we would expect and send a 400 uh you know status code over there so if both of,358,31,31,MvTqi2Mb_PM
39,these issues are not there only then the next step of line is going to run because when there is a return right so when a return statement is executed none of the other lines are going to kind of run in a function so it's kind of basic function concept right so if it encountered a return and it's returning something then the other pieces of code are not executed so if any of these return statement are executed that means we have some issue either in the data or the columns of are not present right and then we would kind of check that if both of these issues are not there then we can just move uh to the next step that is like you know scaling the data making a prediction and so on so that would be the next step so here i'll say scale the data and here i'm going to say scaled data is equal to scaler which is something that we have already loaded above this function so understand that we are loading this above this function this is because if you put this within this predict function then for every call it's going to load that model so we don't want that we want this model and scaler to be loaded when the ap is active so you don't have to do that every time because if the model is larger then it's going to take some time so don't put this loading operation within this predict function so that's another take away key take away here so here i'll say transform and pass my input data,358,31,31,MvTqi2Mb_PM
40,so here as we did in the previous predictive code right so here we add this data in a list so that's why we put this uh new data point over here which is a list within another list but in this case this input data is a data frame when it is a data frame you don't need to put this within a list you can just simply say input data you don't have to worry about that so it automatically knows that it has one data point and it has to scale it for one data point similarly for model prediction as well so that's the other thing so here we have the scale data scale data is equal to scaler uh do transform input data which is my input data frame df and then we are going to make a prediction so this prediction will be model.,195,31,31,MvTqi2Mb_PM
41,predict you can use this uh you know predictor proba so that's going to give your prediction probability so if the prediction is one so what's the probability that the model is thinking that it is one that is also like another good thing that you can send so this is the prediction that i'm kind of going to send and then we are building the response apa response that we get from this model so say response is equal to create a dictionary format because this would be later converted into a jsn formula which is the front is kind of expecting let's say so here i'm going to say prediction is the key and say diabetus if prediction of 0o is equal to 1 else no diabetes so this is basically what we did in this step so when the prediction of zero that means this first value within this prediction array when it is one we say it's diabetic else we say non diabetic right so this is the exact same thing that we are doing but we are just like doing this in a single step so here we are uh you know we have two strings one is diabetes so if the prediction of zero if this label predicted by the model is one then we say it's diabetic else right if the value is not one that means if it's zero it would say no diabetes right so this is how this would work so in the prediction is one it's going to say diabet and if not it's going to say no diabetes and this goes into the key,358,34,34,MvTqi2Mb_PM
42,called as prediction so this is basically a simple dictionary that would have something like this either it would have prediction as diabetes right or this would have the value as you know no diabetes so this is basically what we are sending from the model so return this particular json okay so this is it and in the last step i can say return uh json ifi your response right the response that we have created so that this dictionary has been converted into a json format and then we have this try block right so after this we need to put this except saying that except uh ex exeption this is like again a common syntax as e return jsonify error saying like basically what the error is these are like really good practice while you're building this ap to tell like what's basically the issue and finally you can just use this if name is equal to you know you can just like use this as a standard piece of code so that ap is active when you are running this name is equal to main say app.,248,34,34,MvTqi2Mb_PM
43,run debug is equal to true so to make sure that we are getting like messages so that you can easily debug this okay right so these are the aspects so here we basically have a tri block and and the exception block so let's understand this first we get this data from this request so request is the user is sending the request from that you get the input data as json and then you store this in a data frame called as input data and then here we have two checks one first check is if the input data is empty if it's empty we say input data not provided and the next step we also check if the all the requ columns are present if all the rec columns are not present then again we are going to raise the error and tell them that these are the columns that we are expecting and then we are also sending like corresponding error codes and then we have if these two conditions on status satisfy that means like no issues are there then we scale the data make a prediction and then send a respond and jsonify that response as well and finally we have this accept block saying that if any error kind of happens in this step right so that error can be anything here we know that these errors are due to input data but within that if you get some error then we are not like breaking anything in the api we are also sending this response saying that it's a 500 that's basically your internal server error saying that,358,35,35,MvTqi2Mb_PM
44,"this is the error that kind of happened and and so on basically let's say that if the data has been sent in a string format then it's not going to work when you transform it and so on right so it's it's a case where your internal server error is going to be triggered so that's like one example okay so this is how you are going to develop your api and the last step is i mean when you run this your ap is going to be active but let's also understand uh you know how this uh you know it's going to work when you call this so either you can use this postman and and other things in order to test this but let's test this in python as well in python we have a library called as request and that can be used in order to call this api send some data get a value and so on so let's use that to work with so here i'll run this and i run this it's going to start this api so it says warning this is a development server do not use it a production deployment user production wsgs ser so again so warning but we don't have to worry about that for now so it's saying that for production we have to use like a different server and so on this is okay for development though so that's what they're saying and the ap is running in the port 5,000 and 1271 is my local host and 5,000 is the port it's running and you can also configure the port",358,35,35,MvTqi2Mb_PM
45,"if you want to run this on port 4,000 or 8,000 you can also do that and this is the p number in which it's kind of running so i'm going to copy this url and paste it in my browser see it says diabetes prediction app is running so this is what i said uh you know while creating this you know app.",83,35,35,MvTqi2Mb_PM
46,route home so when we go to that exact url it's going to say diabetes prediction app is running so that's what is present over here so as you can see there is a green dot here that means the app is kind of running so you see here 200 that means the you know uh request is successful 200 is like working successful code so it's it's running right and if you stop this right your ap is not going to work you need to keep this running and then it would work the other way is you can also run this from here you can say python main.py maybe i'll show you how you can do this i'll stop it here instead of running this from here you can also run this from a terminal so here i'll say python space main.py and that's going to again run this file this is to start it from terminal instead of running it from this id you you can also start from a terminal windows powershell or some other terminal so this is basically what we do when you are deploying this in a u to server or a linux server and mainly like almost all the deployment that we do would be in a ub to or a linux server we don't mainly kind of like often use a windows server in production and so on so we would use that and it's a good practice to understand all these terminal commands and codes and so on so this is how you can start it from your terminal both would work in this case though,358,36,36,MvTqi2Mb_PM
47,you can either use this python m.p or you can run this from here so both is going to run your ap from this particular ular uh inpoint and your port so now i'll come back here i'll create a new notebook and i'll call this as test api t diabet api and now i'm going to say pip install request library so request is equal to the version is 32.3 i'm working on this 2.,98,36,36,MvTqi2Mb_PM
48,"32.3 okay so now i'll work on this code to call this url and send a data here okay so now i'll just un command this and say import request and the url that we need to send is http 127.0 5000 and so on right slash predict because predict is the endpoint that's going to you know make a prediction and send us the label thing so say 5,000 after this you should put a forward slash and say the inpoint name that is predict in this case and the next step is you should pass the data so data is let's put this in a dictionary so first is this pregnancies right so i'll again open my you know csv file let me open this in notepad open with notepad these are the i can use this one that's pleas in p ch so we have this pregnancies glucose and all these stuff right so i'll just go with this one on my browser this tab as api so this should be a dictionary and we should send the corresponding values let's do that in a minute and let's use the same values 6 148 72 35 0 33.6 0.627 and 50 it's have these values so these should be sent as pregnancy is six glucose value is 148 blood pressure value is 72 skin thickness value is 35 insulin value is 0 bmi value is 33.6 and diabetes pedigree function value is 0.627 and finally your age is 50 okay so these are the values so we have pregnancies glose blood pressure skin thickness insulin bmi diabetes so this is the data that",358,37,37,MvTqi2Mb_PM
49,i'm going to pass so now i can say response is equal to requests.,18,37,37,MvTqi2Mb_PM
50,json if you just print response it would just say status code of 200 say it would run successfully so run this okay we have to put a parenthesis over here so it says prediction of diabetes so what happens is when you post this data to this url so this is kind of called and you can see this over here so it says 200 that means the request is successful so this is the time at which the request has been made and so on so this is how you can call this api and test it let's also try to understand if it would work if we send this send send a empty data let's say that this is a empty data run this error input data not provided right if you also print this response right maybe i'll show this to you the error code as well so if you just print this uh response so so it would say response is 400 and the error message is input data not provided so this is what we did when we uh you know build this error condition saying that if not data that means the data is empty say the input data is not provided now what we do is in the next step i'm going to use all these things but i'm going to remove this age alone now the other errors handling should kind of kick in and it it should say you have not sent all this required columns these are the required columns and the error is 400 so basically we should see this error message message and,358,40,40,MvTqi2Mb_PM
51,the status code should be 400 similar to this one so i'll run this it says here reed columns missing reed columns are pregnancies glucose blood pressure skin thickness and so on so this is how you can kind of uh work with it and also make sure uh you know your ap is like handling all the error messages as well so i'll save this and again you can just test this out with the actual data this particular one with all those key value pads and so on and send this and you would see a response of 200 and says prediction diabetes if the label is uh you know zero it would say that the person is not diabetic so this is how you can build this apa using flask so i hope everyone understood what we have did here i'll just like give you a walk through of whatever we have done so the first step is just like making sure we have the train model so we kind of worked with this diabetes.,231,40,40,MvTqi2Mb_PM
52,csv file did all those basic processing model training part and we have finally saved our diabetes model file and a scaler pickle file and then we transported those files basically copied and pasted over here in this py cham thing and then we have this flask code so first we read all these files imported the dependencies created this flask app using this app is equal to flask name and then you have this w basically when you call a url that's going to say diabetes prediction after is running and in the predict end point we have this post method and we call this as predict and we process this have all the error and link and so on and we also made sure that every data is like kind of json ified and then we have this debug true so that if there are any issues we can also like kind of see it over here so that's it so i hope everyone is kind of clear with all the things that we have covered here today i'll make sure that i post all these files in a github repository maybe after sometime uh the video is uploaded i'll kind of do that you can refer to that in order to get all these files and work on it so i'll put all these model file main. p file and so on these are the files that you need diabetes main.,317,41,42,MvTqi2Mb_PM
0,so you've built a machine learning model it can swiftly predict whatever you've trained it to predict but that's just a half of the story in order for someone other than you to be able to use this model you need to make it accessible to general public or to other pieces of software that could interact with the model programmatically sending data to that model and retrieving predictions in other words you want to deploy the model today we'll learn how to do this using docker and fast api we'll create an api for this model and test it through a interface and programmatically first thing we'll need is a docker engine and it's real easy just go to the official website and i'll post the link below and download the version for your operating system then install it and you are ready to go it provides a cool app that will give you an ability to control your images and containers but we're not going to use it we're going to do everything through the command line next we want to save our trained model as a binary so that we could place it inside our container which would then load it and use this model to predict on the incoming data so here i have a really simple script for a classification model on the iris data set or it i don't know what's the correct name for this so basically what it does it uploads the data set from psychic learn then it uses a random force classifier model to train on the data and then we use the job library to,358,0,0,vA0C0k72-b4
1,dump the model or to save it as a model.,13,0,0,vA0C0k72-b4
2,job file with real life data of course there going to be a few more steps with your data pre-processing and saving additional artifacts for later so that in the inference pipeline your incoming data could be transformed accordingly but for our proof of concept this is not required now let's create our api i'm going to create a new python file and i'm going to call it server we're going to need three libraries the fast api to create an api job lip to load our model and np so the first thing we want to do is load our model and for interpreting our predictions for this particular model the iris model we're going to need class names and as you can see github copilot is really smart it's already understanding that we're want to create an app using the fest api so i'm just going to hit tab here and uh create an app gap method this decorator tells test api that the function right below is going to handle all the requests coming to this path which is going to be the root of our web app it will be really simple it will just return the message the name of our api now let's make a function for sending data to the model so it would predict on that data and we'll create an endpoint predict with the post method and the function is going to receive the data which is going to be a dictionary it will extract features out of that dictionary make a prediction with a model map the prediction to the classes that we have defined here,358,2,2,vA0C0k72-b4
3,so it would return something meaningful other than 0o one or two and return this response and that's it this is our whole api nothing else we need in here well other than maybe some documentation to this predict function just to give us an example what the request should look like i'm going to create an app folder and place the python code and the model in there because this is our server and everything required for this server is inside that folder and one thing i forgot we're going to need the requirements as well so here in the rout let's create the requirements.txt just make sure that libraries versions will be the same as the ones you've used when creating your model specifically the psychic learn because a different version of psychic learn in the container may not load the model correctly now we need a docker file our application needs some kind of environment to run in specifically some operating system and with the python version in there and the requirements that we have just specified because currently it runs on your local machine where you have all of this installed but when it's going to run as a deployed container somewhere on the server that server might not have all the necessary requirements so in the docker file we're going to specify which is going to be the operating system the python version the requirements and so forth docker file without any extensions just like this everything we're going to write here are basically going to be just commands that are going to be executed when the container is created each,358,2,2,vA0C0k72-b4
4,subsequent command will stack on the result of the previous command and they have to be sequential first command is always going to be the image that you are going to use to create this container this image is going to be pulled from the docker hub there are a lot of different images that satisfy different criteria in our case we're going to use python 3.11 which is going to pull an alpine distribution of linux with python 3.11 installed this next one is just creating a code directory inside our container and assigns it as a working directory since our container is an isolated environment we're going to need to copy all the necessary files that we need the container to execute so let's just first copy the requirements and we'll place them inside the code directory that is going to be created with the previous step now after our requirements are copied we want to install those requirements so we're going to run the pip install those requirements inside the container and copy the application folder inside our working directory code second to last is going to be expos some port because again this is isolated environment and in order to interact with this we need to expose some endpoint to where we can send the data or we could interact with this container and the last one is going to be the command the container is going to run after it is created uicorn is a server gateway interface it's the binning element that handles the web connections from the browser or api client and then allows fest api to serve the,358,2,2,vA0C0k72-b4
5,actual request it also allows you to properly spawn many workers and scale your deployment properly the app.,23,2,2,vA0C0k72-b4
6,"server colon app is really simple it will just run the server.py file from the app directory and initialize our api which in this case is also called an app when we set the network to host a cont container will share the host network stack and all the interfaces from the host will be available to the container and the last one is going to be the port which we have exposed here in the previous step this means that a request to a container on port 8,000 will be forwarded to the application bound to port 8,000 inside the container and this is literally all the code we're going to need to server our model through an api inside a docker container and a good idea is to always add a read me file so i'm going to just paste whatever i created before which explains everything how to operate this api but we're jumping ahead so let me close this for now i'll put a link in the description to the repo where this code is going to be located now let's spin up our container and use this model through an api we're going to need a terminal make sure that you cd into this route where this app folder docker file and everything is located before running the container we need to build an image for this container so the command is docker buil minus t and any name that you want to give to this image i'm just going to say image name and then dot at the end once you run it especially the first time around it's going",358,3,3,vA0C0k72-b4
7,"to take some time because you with this first line of code here you actually download the uh operating system with python version in there and it may take a few minutes and then it will execute all these uh lines one after another and you can see each step uh is being logged into the console so now everything is fine we have successfully built our image now we want to run this image the command is run so uh name of the container that you want to create i'm just going to say container name minus p which is port 8,000 and the image name is the name of the image which you have created in the previous step and it looks like we have a mistake in our code on the server side so let's go here and append a path to the model.",191,3,3,vA0C0k72-b4
8,jobet as an app folder because this is actually where it's stored now once you save it or make any changes whatsoever to your code or to the docker file you have to build it again so i'm just going to say docker build t image name okay and now we can create a container and since we've tried creating a container before this container name that we are trying to use once again has already been used and we cannot use the same name we either have to go into the docker application and delete this container or you can also use the command line for that as well okay so since we have deleted this container we can run this command once again not changing the container name here or otherwise we could have called it container name to or any other name and a second container would have been created okay very good it's telling us that uicorn is running at your local host port 8000 copy this url open a chrome browser and you will see the root of your application as you can see here it tells us the message that we have included into the get method aras model api now we have also created a predict endpoint if we add a predict here we're not going to see anything because our predict function expects a dictionary and we have not sent any dictionary here uh so there are a few ways you can interact with this api first and easy way to test this api with a web interface is going to the docs endpoint we have not defined,358,4,4,vA0C0k72-b4
9,this endpoint in our program but this is the default endpoint that goes within the fest api so any application you have created using fast api is going to have this docs where you can see all your methods so this is our get and post method and you can test them out you can just open it here and send your data through this request body field now let's go back into our code and here we have a convenient example this is how our data is supposed to go to the model as a dictionary with a keyword features and a list of features so i'm just going to copy this and paste it in here execute and it seems that we have some kind of error okay and if you go back into your terminal where you have launched your container you can see the details and i can see the error so one of these brackets has to be right here because we cannot reshape a list we have to reshape a numai aray so all we need to do is replace this bracket over here and we're going to have to close this container and uh build it once again and then run it we're not going to go into the docker app to remove the container we'll just give it a different name so container name two good thing this error happened because here you can see a simple example of debugging this application okay the container is running i'm just going to refresh go to the predict method try it out paste our features in here and click execute,358,4,4,vA0C0k72-b4
10,okay very good response code 200 this means everything happened correctly and this is our predicted class virginica but the real intention usually for model deployment is to use it programmatically because say you have some data in some database that gets collected there and every once in a while you need to send some data to some model for prediction uh this is where api helps now let's just create a simple inference script welcome call it client.py and we'll use this to send the data from python to an api and receive response let's define our data and just going to use the same sample we've plugged into the web interface the endpoint location convert the data into json format and send it to this url as a payload using the request library and store the reply in the response variable and i'm going to print this response let's check it out and virginia of course you may have much more data say this is an extract from your d database that got collected overnight and you want to classify it so just make a simple loop to run through these data points create a payload out of each one of them send it to the model and then receive your predictions and store them in the predictions list and here are your predictions you can also interact with the model via curl request and here i have an example in the read file so just through terminal paste that request with your features and the url of the model and it's endpoint and receive your predictions we have success y deployed a machine,358,4,4,vA0C0k72-b4
11,learning model into a container created an inference api and made it runable on our local machine and this is a good start deployment to the cloud involves quite a few more things like setting up aws or kubernetes on the digital ocean writing scripts that interact with your repository and push your code onto the hosting environment scalability issues and other things if you want to dive deeper into the subject below i will link the best corsera and urmi courses and stay tuned music hey,113,4,4,vA0C0k72-b4
0,this is the fifth video in a larger series on full stack data science in the previous video i walked through the development of a modelbased search tool for my youtube videos here i'm going to discuss how we can take this tool and deploy it into a production environment i'll start with an overview of key concepts and then dive into the example code and if you're new here welcome i'm sha i make videos about data science and entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the videos that i make when we think of machine learning we probably think neural networks or other models that allow us to make predictions although these are a core part of the field a machine learning model on its own isn't something that provides a whole lot of value in virtually all situations in order for a machine learning model to provide value it needs to be deployed into the real world i define this deployment process as taking a machine learning model and turning it into a machine learning solution we start by developing the model which consists of taking data passing it into a machine learning algorithm and obtaining a model from that training process deployment can look a lot of different ways it could simply be making predictions available to programmers and other developers it could be using the model to power a website or a mobile application and then finally it could be embedding the model into a larger business process or piece of software but the key point,358,0,0,pJ_nCklQ65w
1,here is that the model that comes out of the training algorithm and is sitting on your laptop doesn't provide a whole lot of value however the model when integrated into a website into a piece of software or made available to end users through an api is something that provides value natural question is how can we deploy these solutions while there are countless ways to do this in this video i'm going to talk about a simple three-step strategy for deployment that is popular among data scientists and machine learning engineers the first step is to create an api in other words we create an interface for programs to communicate and interact with our model what this looks like is we take our model and we wrap it in this api which is represented by this box here and then people can send requests to the api and receive responses from it and so in the case of a model the request will be the inputs of the model and the response will be the outputs two popular libraries for doing this in python are flask and fast api the next step is to take the api and put it in a container here container is a technical word referring to a docker container which is a lightweight wrapper around a piece of software that captures all its dependencies and makes it super portable so you can easily run that piece of software across multiple machines and then finally we deploy the solution and since we put everything into a container now it's super easy to run that container on someone else's computer some,358,0,0,pJ_nCklQ65w
2,server you manage or most commonly into the cloud the big three cloud providers of course are aws azure and gcp so with this high level overview let's see what this looks like in code here i'm going to walk through how we can use fast api docker and aws to deploy this semantic search tool that i developed in the previous video this video is going to be pretty hands-on so i'm not going to talk too much about fast api docker or aws from a conceptual point of view but if those are things you're interested in let me know in the comments and then i'll make some follow-up videos specifically about those tools okay so here we're going to create a search api with fast api then we're going to create a docker image for that api then we'll push that image to the docker hub and then finally we'll use that docker image to deploy a container on aws's elastic container service so let's start with the first one creating the search api with fast api which is this python library that does all these great things apparently and using it to write this example here it was super easy for me to learn and they have this great tutorial for those just getting started i walked through this to make this example and it probably took me like an hour or something to do it so super easy to learn especially if you've been coding in python for a while anyway coming back to the code first thing we want to do is we're going to make a file called main.py,358,0,0,pJ_nCklQ65w
3,and we're going to import some libraries so we'll import fast api to create the api and then the rest of these libraries are so we can implement our search function like we did in the previous video so we use polers to import data about all my youtube videos we use the sentence transformers library to compute text embeddings we use psyit learn to compute the distance between a users query in all the videos on my channel then i have this other file called functions.,113,0,0,pJ_nCklQ65w
4,piy that has this return search results function which i define here and i'm not going to go into the details cu it's not critical for the deployment process but essentially what it does is that it takes in a user's query and then it'll spit out the top search results for that query coming back to the main script the first thing we do is i'll define the embedding model that we're going to use from the sentence transformers library so by default the library will download the model when it's run for the first time but here to avoid that i just save the model locally so what that looks like is here we have the main python file that we were just looking at and then i have this folder called data and in it i have this folder which contains all the files for this embedding model and then we have this parquet file which has all the data about my youtube videos we can then load the model from file like this we can load the video index using this line of code and this is the same way we did it in the previous video and then we can import the manhattan distance from sklearn which i did like this and again since i talked about this at length in the previous video i'm not going to get into the details of how the search tool works here but you can check out that video if you're interested so everything we just did had nothing to do with the api this was just the implementation of that search function to,358,1,1,pJ_nCklQ65w
5,create the api we create this object called app and and it's this fast api object and then we can simply create these api operations here i'm strictly defining these get requests which allows users to send requests to the api and receive back responses the other most common one is a put request which is often used to send data to an api and load it in the back end for example if we wanted to update this parquet file in some way we could use a put request to do that anyway here i define three operations and that's done using this syntax here where we have this decorator what's happening here is we're saying that we wanted to find this get request at this end point here for the api and it's going to operate based on this python function this is a common practice where you have the root end point be a health check so it doesn't take in any input parameters but anytime someone calls this m point they'll receive back this string response of health check okay and a similar thing here so i created another endpoint called info which just gives some information about the api so it doesn't take any inputs but it returns back the name which i called yt search and i have a description for it which is search api for shots levy's youtube videos this one's not completely necessary but you can imagine if you have multiple users using this api and maybe you have multiple apis having an info endpoint can be helpful but the one we care most about is this,358,1,1,pJ_nCklQ65w
6,search endpoint what's happening here is we're defining this search function that takes in a query from the get request and then it'll pass it into this function return search result indexes defined in this functions.,46,1,1,pJ_nCklQ65w
7,piy file it'll pass it in as well as the video index the embedding model and the distance metric and then we can use the output of this function to return the search results and so a lot of fanciness here maybe it's not super easy to read but what's happening is we use the select method to pick out the title and video columns of the data frame the video index we use this collect method because we didn't actually load the data frame into memory because we used scan paret instead of read paret and then once we load this in we pick out the indexes from this search result and then finally we convert that data frame to a dictionary that dictionary will have two fields one corresponding to the title and the other field corresponding to the video ids and it'll have up to five search results for each field that's the code to make the api was super easy it's great for me as someone who's very comfortable with python and knows very little about a lot of other programming languages especially ones that have to do with web development but now we can run this api on my local machine and we can interact with it so the way that looks is make sure we're in the right direction we see we have this app folder and then we can go into app and we see that we have our main.py file to run that we can do fast api dev main.py all right so now it's running running on this port 8000 had to make a couple of changes,358,2,2,pJ_nCklQ65w
8,to the main file so i had to add this app in front of functions and then i had to remove app from these paths here because it was running from a different directory than a previous version of the code but that should be working now now we'll see that we have this little url here which we can copy and then i have a notebook here that allows us to test the api the url is already here and it's at port 8000 by default and then we want to talk to the search endpoint of the api that we created so we can actually run this you have this query called text embeddings simply explain and then we can pass that into our api and then we can see we get a response so it took about 1 second so that's actually pretty long long but maybe if i run it again it'll be faster yeah so maybe that first one is just slow but run it a second time 76 milliseconds and then we can see the search results here just kind of taking a step back the response in its raw form looks like this so it's just a text in the json format which is basically a dictionary and then we can use this json library to convert that text into a proper python dictionary and then we can access the different fields of it like this so these are all the titles from the top five search results and then we can also look at the video ids so that looks like that now we've confirmed the api is,358,2,2,pJ_nCklQ65w
9,working locally so coming back to the slides the next thing we want to do is to create a docker image for the api the steps to make a docker image from a fast api api is available on their documentation and it's a few simple steps we'll create a app directory so a folder called app we'll create an empty init.py file and then we'll create our main.py file we've actually already done this if we go back we see that the app directory already exists and we have the main.py file and we already have the init.py file taking one step out of that directory we see that this app folder is in another folder with a few other files so we have this requirements.,165,2,2,pJ_nCklQ65w
10,text file which is shown here this is just your typical requirements file that you might have for any kind of python code here you can see we have all the different libraries we used in the main.py file so we have fast api polers sentence transformers psychic learn and numpy we also have this docker file which is essentially the instructions for creating the docker image this consists of a few key steps we start by importing a base image there are hundreds of thousands of docker images available on the docker hub the one we're importing here is the official python image version 3.10 and so we can see that on the docker hub here it's an official image so i guess it's by docker it's called python and then they're all these tags so these are all different versions of this image we did 3.10 which i guess is going to be this one the next thing we're going to do is change the working directory you know imagine you just installed linux on a machine or something so the working directory is just going to start as the root and then we can change the working directory to this folder called code next we can copy in the requirements file into the docker image so we take the requirements file from our local directory here and put it onto the images directory this code directory here and then once we've moved the requirements file onto the image we'll install all the requirements we do that with this line of code here and then i have this line of code to add this,358,3,3,pJ_nCklQ65w
11,code app to the python path this might not be necessary because we actually changed this main.py file so i'm going to actually try to comment this out and see if it still works next we're going to add this app directory to the image so we're going to move it from our local machine to this code subdirectory on the docker image finally we define a command that will be run automatically whenever the container is spun up to build the docker image we run docker build we specify the tag so we'll give it a name i'll call it yt search image test and then oh i forgot this we got to specify where the docker file is so it's going to be in the current directory so we do that and now it's building the docker image okay so now the image is done building you can see it took about maybe a minute to run the run times are here the longest was installing all the python libraries and so now if we go over to the docker desktop app we see that this image is here under the images tab so i have a previous version of the image but the one we just created is called yt search image- test and then we can actually run this image the way to do that is we can go let me clear it out so we do docker run and then we specify the container name i'll put yt search container test and then we specify the port we want it to run at do 8080 yep cuz that's what i put,358,3,3,pJ_nCklQ65w
12,here in the docker file finally we'll specify the image here's the code docker run- d-- name of the docker container and then we specify the port and then specify the image so we can run that that's not the right image name it is yt search image test now the container is running locally we can actually see that if we go to our image here and it says in use this is the container that is using that image alternatively we can just go to this containers tab and we can see all the containers saved locally here we can see that the container stopped running and that means something went wrong and we can see that the folder with the model in it is not a local folder so it didn't run because the model folder wasn't on the python path we could add the data subdirectory to the python path but alternatively we can just go back to the main.py file and add app to these directory names here and the reason we need this is that we define the working directory as code all the code is going to run relative to this directory here that means if the python script is looking for this model path you have to put app here because it's running from code alternatively you could add this python path thing here but i don't want to do that to make this docker file more simple now let's try to run it again so we'll build the image so that was nice and quick and then we'll run the container so now the container is running,358,3,3,pJ_nCklQ65w
13,click this and indeed it is running successfully so we can click on it and we can see that it's running at this url here we can test this if we go over to the super notebook we test the api locally so now let's test the docker container running locally so this should be the correct path so yep and so it's the same thing we have the url and then we have the endpoint name we're going to use the search operation we'll define a query and then we'll make a api call it ran slower because it's essentially talking to a different machine but we can see that the api response is the same we can similarly call the info endpoint or we can call the base endpoint as well we can see those generate different responses now that we've created the docker image next we're going to push the image to the docker hub the reason we want to do this is that once the image is on the docker hub it makes it easy to deploy to any cloud service that you like so here specifically we'll be using aws's elastic container service but dockerhub integrates with other cloud providers so not just aws but also gcp i'm sure it also connects with azure even though that's not something i've checked to push the image to the docker hub the first thing we want to do is create a new repository so i already have one called yt search but let's go ahead and create a new one from scratch so we'll call this one yt search demo and then we'll,358,3,3,pJ_nCklQ65w
14,say demo of deploy deploying semantic search for youtube videos and then we'll leave it as public and we'll hit create reposit doesn't have a category so let's just do that i'll call it machine learning ai so that's it the repository is made now what we can do is we can go back to our terminal we can actually list out the docker images like this we can see that these are all the images that i have saved locally what we want to do is push this one to the docker hub first thing we need to do is tag the image so it matches the repository name on the docker hub essentially we're going to create a new image and this is going to have the same name as that repository we just created if we go back here we see that repo is called shahin which is my dockerhub username and then the name of the repo we just created yt search- demo and then the next thing we need to put is the name of the local image so here we had yt search image- test so i actually have it backwards we need to put the local image name first so yt search image- test and then we need to put the dockerhub repo name we've now created a new image which we can see here called shahin t t search- demo and now we can just push it to the docker hub so that's really easy so docker push shahen t yt search demo and then we can add a tag to it as well but that's not not,358,3,3,pJ_nCklQ65w
15,necessary so it's using the default tag of latest and now we can see that it's pushing up to the docker hub so now it's done running if we go back to the docker hub and hit refresh we see now the image is here now that we've pushed the image to the dockerhub the last step is we can now deploy a container on aws using their elastic container service the way to do that is we can go to our aws account if you don't have a aws account you'll need to make one for this tutorial and then we can go to elastic container service so we can just type in ecs and it should pop up once we do that we'll see we come to a screen like this we can see that i already have a cluster running but let's start one from scratch the first thing we can do is go over to task definitions and click this create new task definition i'll call this one yt search demo we'll scroll down to infrastructure requirements we'll select aws fargate as opposed to amazon ec2 instances and the upside of fargate is that you don't have to worry about managing the infrastructure yourself that's all handled behind the scenes and you can just worry about getting your container running and using it as a surfice the next important thing is selecting the operating system and architecture this will depend on the system that you're running for mac they use arm 64 so that's the architecture and then linux is the operating system of our image next we can go to the,358,3,3,pJ_nCklQ65w
16,task size i'll leave it at one cpu but i'll actually bump down the memory to 2 gb and then task roll you can actually leave this as none if this is your first time running it and it'll automatically create this task roll called ecs task execution rle but since that already exists for me i'll go ahead and click that now we're going to specify the container details so i'll call this yt search container demo and then here we'll put the url of the image which we grab from the docker hub so i'll grab this and then i'll add the tag of latest and we'll leave it as a essential container port number we'll leave at 80 we'll leave all this as the same we'll leave all this stuff as default we won't add any environment variables we won't add any environment files and then logging leave that all as the default and then we have a bunch of these optional things that we can set like a health check startup dependency ordering container type timeouts so on and so forth we can also configure the storage so there's ephemeral storage so just like shortterm i'll just leave this as the default of 21 we can also add external storage using this add volumes thing which is good if you wanted to talk to some external data source and there's this monitoring tab and tags tab but not going to touch any of that just going to keep it super simple here and then i'm going to hit create all right so now the task definition has been successfully created now we,358,3,3,pJ_nCklQ65w
17,can go here and we'll see we have this new task definition now we can go over to clusters and we'll hit create cluster i'll call this one yt search cluster demo again we'll use aws fargate for the infrastructure and then we won't touch the monitoring in the tags hit create now it's spinning up the cluster so this might take a bit so now the cluster has been created we can click on this so we see that the cluster is running but there's nothing running on it there are a few things we can do we can create services or we can create tasks services are good for the we service kind of like this api we're creating a task is better for something that's more of a batch process that runs like once at a predictable time increment but here we'll create a service so to do that we'll click services and then click this create button we're going to use the existing cluster the wht search cluster demo click on launch type and we'll leave that as fargate and latest we'll make the application type a service we'll specify the family of the task definition and then we can give a service name call it youtube search api demo we'll leave the service type as replica we'll have the desired tasks as one deployment options we'll leave those as default deployment failure detection leave that as default we won't do service connect i'm not sure what that is service discovery networking we'll actually leave all this the same we'll use an existing security group and then we can enable load balancing,358,3,3,pJ_nCklQ65w
18,if we like but i won't do that here service auto scaling we can automatically increase the number of containers that are running or decrease the number of containers that are running again we can configure the data and what not but we're not going to touch any of that and we'll just hit create so now it's deploying the search api so the api has been successfully deployed it took like 5 minutes or something but now if we scroll down and we click this youtube search api demo something like this will pop up and we can go over to tasks and we can click this task here and we can see that it'll have a public ip address so what we can do is copy this public ip and then i have a another piece of code here and we'll just paste in the public ip and then we'll make an api call so just 100 milliseconds to make the api call it's actually faster making the api call to aws than locally which is pretty interesting so this ran just fine here but one thing i had to do yesterday to get this working was go to the youtube search api demo click on configuration and networking and then go down to security groups that'll open this vpc dashboard thing and i had to add this rule that allowed all inbound traffic from my ip address specifically so if you do that it'll you know have some default security group and then you'll hit edit inbound rules and then you can add a additional rule that allows all inbound traffic from my,358,3,3,pJ_nCklQ65w
19,ip you can also have custom ips which you specify one by one you can have any ip version 4 or any ip version 6 so it wasn't working for me but once i added this inbound rule it was working just fine now that the api is deployed on aws it makes it a lot easier to integrate this functionality this search tool into a wide range of applications to demonstrate that i'm going to spin up a gradio user interface that can talk to the api i'll just run this whole thing and this is essentially the same thing that i walked through in the previous video of the series so if you're curious about the details be sure to check that out but now we can see that this user interface got spun up we can search something like full stack data science and we see that search results are coming up this is the great thing about running the core functionality on aws now we just have this lightweight front end that can interact with the api and return search results through a web interface so you can see that the other videos in this series are popping up in the search results we can search other things like finetuning language models and i had a typo but it doesn't matter and we can see all the content on fine tuning and large language models pops up and i'll just call out that all the code i walked through is freely available on github so if you go to my youtube blog repository and the full stack data science subfolder you'll see,358,3,3,pJ_nCklQ65w
20,that all this code is available in this ml engineering folder and then you can check out other videos in this series and all the medium articles associated with this series this was supposed to be the last video of this series but then i got a comment from cool worship 6704 on my video on building the data pipeline for this project and they were asking how would you automate this entire process and so that's a really good question and it wasn't something i originally was going to cover but since you have this question here i assume other people have the same question and so just to recap what we did here is we took the search tool wrapped it in an api put that into a docker container and deployed it onto aws so now users and applications can interact with the search tool but one limitation of how i coded things here is that the video index the videos that are available in the search api is static it's a snapshot from a couple of weeks ago when i made the video on making data pipelines so the ob vious next step here would be to create another container service that automates the whole data pipeline on some sort of time cadence whether it's every night or every week or whatever it might be and then feed the results of that process and update the search api so that new videos will be populated in the search tool so that's going to be the focus of the next video of this series so that brings us to the end this video,358,3,3,pJ_nCklQ65w
21,was a lot more hands-on than a lot of my other content i'm experimenting with new new format so let me know what you thought in the comment section below if you want me to dig deeper into any of the tools or technologies discussed in this video let me know and i can make follow-up videos on those topics and as always thank you so much for your time and thanks for watching,96,3,3,pJ_nCklQ65w
0,music hello and welcome one and all in this session we will convert the machine learning model to an api this is a good way to integrate your machine learning models in other applications by building a rest api for a model we can use the model in different platforms such as a website or a mobile application also this separation allows us to eliminate the dependency on either application we can independently modify or update either of the applications we will convert the simple linear regression model and deploy it as a rest api using flask restful if you haven't checked out the simple linear regression video then this will be a good time to view it i will briefly cover the model in the simple linear regression video we developed a model that predicts the sale of a company based on the marketing budget and let me give you the preview of the finish application this is the front-end web application that consumes the api we provide the input the marketing budget and behind the scene the api call is made and we see the input budget amount then we display the predicted model value i am displaying the data used for this model but that's just an extra functionality this is the final application and if this packs your interest then let's start building this app the model is available as a pickle file on the github repo i'll grab it and copy it to the base directory of my project so we can reference it in the api now that we have the model file let's deploy this as a rest api,358,0,0,AZfJ8buL5II
1,i'll go ahead and create a new pie charm project pycharm automatically creates a virtual environment for us and in pycharm we can use python's built-in package manager pip to install the libraries let's issue pip install commands to install the required libraries first we need flask to create a flask application then we'll install flask restful to create a restful api and next i'll install pandas we'll use pandas to process the incoming parameter and use it as input for the machine learning model finally i'll install the flask course library this will help us with the cross origin resource sharing or when we make an ajax request from the web application as usual let's import the required libraries at the top to build a flask api we will need flask so from flask we'll import flask and request and for the api we will import flask restful and from it we will use the resource and the api next we will import pickle and we will use this to load the model we'll use pandas for some data processing so i'll go ahead and import pandas and finally i'll import the first course i'll create an instance of flask and save it as an app i'll call the course for the cross-origin resource sharing and supply it the app let's go ahead and create an instance of the api for our app this will help us with the url routing and to this object we'll pass the app instance of flask to get the model prediction let's define a prediction class this will take resource resources are the main building block for the flask,358,0,0,AZfJ8buL5II
2,restful apis each class can have method that correspond to http method such as get post and put get will be the primary method because our objective is to serve model predictions to a web application in the get method we pass the incoming budget value from the web application i'll print the budget amount for testing purpose and convert it to an integer i'll go ahead and convert the budget amount to a data frame and provided the label marketing budget we will use this data frame as an input to the machine learning model now it's time to load the model we will do this with the load function from pickle make sure the model file is in the same directory as your python script i'll save it to a model variable now we can access the predict function from it and to the predict function we'll pass the incoming value that we converted as a data frame let's get the prediction from the model and from it get the first index i'll convert this value to an integer as i don't want the decimal places and finally from this class we return the predicted value as a string let's go ahead and create an endpoint and we can do this with add resource function from api so i'll call the endpoint prediction and this endpoint will take a parameter and it will respond to a get request this endpoint will respond with model prediction based on the incoming budget amount i'll add the boilerplate code if name equals main then run the application this essentially is all the code required for the model,358,0,0,AZfJ8buL5II
3,prediction to serve the data to web application let's create another class and i'll call this get data this will also respond to a get request and in this class we read an excel file and save this file into a data frame i'll convert the data frame to a json object and we will return the json from this class let's also map this class to an endpoint and once again we'll use the add resource function and i'll map this to an api endpoint we are done coding the rest api we can test this using a browser or postman application i'll go ahead and save the code let's run the api and issue flask run command in the browser i'll call the api endpoint and we get data in json format so our api endpoint is working as expected as we get data back from this endpoint while we are here let's test the prediction endpoint as well to this end point we will supply the budget value okay we get a response back perfect our api is ready let's integrate it into a web application i have a vanilla web application written in html with some css and javascript the main components are the form with an input for budget amount this is where the user would enter the value and on the button click i am calling a javascript function we display the input and the model predicted value in this section here the values are set by javascript function so let's scroll down to jquery first i am checking if the input field contains a value if it does not,358,0,0,AZfJ8buL5II
4,we prevent the form from submission and in the else clause we remove the values that are displayed in the section above we get the new incoming budget amount from the input and set it to a h3 tag with section budget id then we make an ajax call with the api endpoint and append the budget amount to it if the call is successful then we access the return value and set it to the section prediction id this is the meat of the jquery ajax which integrates our api in this application similarly we can invoke the api endpoint to get the data and we add this data to the table on the web page i'll go ahead and save this page and open it in a browser we have our web application up as well as our flask api is up and running so let's input a budget amount and click on the predict button we see our input value displayed on the page and we see the model predicted value as well this is how we can integrate a machine learning model in a web application we also have the data printed in a formatted table below once this api is deployed it can be consumed in multiple application you can use it in web in desktop application or in mobile applications i hope you enjoyed this session as much as i did this is all for now like share and subscribe take care and i'll see you in the next video,332,0,0,AZfJ8buL5II
0,hello everyone in today's video we'll talk about how to deploy machine learning models right so we built our model successfully if you see our you know previous videos so we use linear regression and built a model right now obviously what whatever the linear regression model which i build need to serve the customer's business right to serve the customer business first my model need to be deployed in some environment so that our model can take data from different sources for prediction right so in this video we'll expose our model as a rest api using flash package which you know we discussed in our last two videos so we'll be using job clip package to export uh save our model the reason why we are using jobly packages because i don't want to save my model as a file right so whenever i'm saying right saving a model saving a model as a file so we are obviously saving the coefficients and constants right so when you talk about the linear regression formula by equal to mx plus c we are storing m and c in a file right so because whenever i want to read that file where the coefficient is stored so there will be a d serialized that may decrease the performance i want to store my model as a python object so that it will be faster to read as d is not required right so when i am loading my model the best part about job lib is that you can load your model into a ram your primary memory so that the performance will be faster right,358,0,0,6pcoCcliToA
1,so whenever you want to predict right so your model will be stored in a ram so it will be faster so you might have heard about the word serialized and things it has deserialized right so we will try to understand what are those right let's say when i want to write any of my python object like list dictionary tables data frames numpy so even our national learning models into a file or a database so what what we'll do is we'll convert our python object to bytestream and then we'll write into a file or database or any any other system in fact right so this process we call it as a serialization right and the reverse way right so when i want to read the data from a file or database right so we call it as a deserialization which means we are converting a file into a python object so this conversion usually will take time that's why what we are doing is we are storing our machine learning model as an python object itself using joplin so that the dc realization is not required so now we'll jump to a jupyter notebook and we'll see how to you know export our model as i said we'll use joblit package so we need to install like pip install chocolate i already installed it so if i want to save my model so there is an option called dump so this is my model right so we fitted it here and i want to save my model with this file right okay noted i need to run this file let me run,358,0,0,6pcoCcliToA
2,this so i'm just running from first yeah so now it should work yeah so now if you see here our last model yeah the model successfully saved right few seconds back now obviously uh we exported our model so whenever uh you know we want to predict we'll import this model you know we'll load this model using chop clip and we can start predicting it right so now we'll go to uh spider we will start coding here so obviously i need to import my job package i also need to install flask obviously because we are exposing this rest api class quest i am a team oops sorry jason what's happening yeah import music now now we need to create a flask object let me save this now i obviously need to load my model using job click so we use dump to you know save our model now we are using load to load our model right so it should show somewhere here linear model yeah oops my bad right so now i loaded my model in this object called model you can name anything i just given a name model now will start writing our code so my url would be predict and obviously i will be using a post method because i don't want to you know use get because if i use get all my values will be exposed in a url right so let me use post and i'm creating function i'll name it predict so i'm getting my data and storing it in the event variable so we discussed this json.loads request our data in our previous,358,0,0,6pcoCcliToA
3,videos please feel free in case if you don't you know see those videos please go back and visit those videos so now let me print this event first right so i'm running this oops we have to initialize right should be double equal to now let me try to run this yes it trans successfully now let's go to postman tool yeah so this is the url and i already have the data so let me copy and paste it here let me run this okay we got some error let's let's check this okay so we have to return something obviously right so for now let me written one okay it's automatically restarted okay now let me run this yeah so this is how we are getting so our actual values is in the key call values is a dictionary obviously right so and we are getting our values which we want to predict as a list if you see this right so now let me remove this print now i'll say values is equal to event of values because we know our actual request is present in the values right so obviously right if you see the type this is obviously a list right so but i need to convert this to numpy before i predict it right so what i'll do is i'll convert this to numpy right and and and before i convert to the numpy so if you if you observe this right this is string right it's coming in a double quotation first i need to convert it to the float right because obviously it's not a string it's,358,0,0,6pcoCcliToA
4,a it's a valid number right so i'm using map function map mp dot load and i'm typing my list no need to print we don't now music i'm creating a new variable and i'm converting my listing to numpy array right let me save this okay so now we need to reshape this numpy array before we you know uh predict so why we need to reshape it we'll see we'll try to understand it let me open the jupyter notebook right so what i'll do is maybe x underscore test so i'm taking the first record here right so i'm obviously converting to list right and now if i say such a p shape obviously shape won't work because it has to be numpy i mean list won't have the shape object yeah so if you see it's 13 comma there is no columns right so usually how how you know this shape should look is right so it should be number of rows by number of columns right so we have 13 columns here right we have only one row obviously we are taking only one record right so let me let me show you right we are taking only one record so it should be one comma 13 not that in comma one right so that is the reason why we are reshaping it right so reshape one comma minus one okay spelling mistake let me see the shape now can you see this now as i said number of rows we have one rows and we have 13 columns number of rows by number of columns right so let's do the,358,0,0,6pcoCcliToA
5,same thing here so equal to p dot 3 shape of 1 minus 1 right now we know right so we we loaded our model in an objective model right so i'll say model dot predict which we already know this object right right so let me print what i am getting in the response yeah it's restarting let me run this yeah so now it's successfully predicted we are getting in a list so when i am returning it maybe what i'll do is res of zero simple right now yeah we're restarting it because we did some changes right so now it's running now let me run this yes we got the value now for this values which we are passing right and it predicted the price will be 19 right so if if you're not uh you know clear what i'm talking about so please feel free to visit my previous video so that you'll get more clarity right so yeah so now i can give this rest api to anyone so whoever wants to you know uh apply the predictions on this housing loan house pricing sorry so they can use this rest api i can send this 3 13 columns right they can send this 13 columns and they can you know predict the values right so these are the columns which i'm talking about right so okay so now when you see error here right so you see a warning there should be a warning somewhere here maybe let me start it again you should be able to see some warning yeah one this is a development server do not,358,0,0,6pcoCcliToA
6,use this in a protection development so flask is used to expose as a rest api right so now i have a sdp obviously right so i need some server to deploy my rest api so whatever you are seeing now is a development server right so it is good for you know testing purpose so the the major problem with the flask is it's a blocking function which means no let's say for our api right so we got 100 request right at a time so it can serve only one request at a time the remaining request will be in a queue and will be processed one by one because as i said flask is a blocking function which means so if you see this function right if i get two requests at a time the first request will block this function because of that the second request cannot use this function right so once the first request is processed successfully then only the second request can make use of it right now if we make this blocking function as a non-blocking function then obviously it can solve more than one request right so we have many options to make is a non-blocking function so one of the best option we have is a unicon server so which will be discussing in our upcoming videos and also we'll see some complex you know the machine learning deployment using aws in our upcoming videos right so hope you enjoy this video a lot right so in our next video we'll discuss some quite interesting topics like you know the logistic regression how to derive the,358,0,0,6pcoCcliToA
7,equation for the logistic regression right so thanks again thanks for watching this video stay tuned and don't forget to subscribe to this channel thank you bye,35,0,0,6pcoCcliToA
0,hello folks in the last video we developed a small flask sample app to get acquainted with the you know flask of that framework so in this video we are going to create the backend of web app for our profit prediction model and which actually is will take inputs from the user in terms of state where the startup is situated r d spend administer spend and marketing spend values and once we click the submit button on the web page it will show the predicted profit amount for that startup thereby helping venture capitalists to take decisions of whether they should invest in the startup or not so we will be loading of multiple linear regression models pickled or saved file which we created in the or and saved in the previous videos so the idea is to load the saved model to provide predictions okay so let's create the app dot py file for this particular model prediction web app so this will be the actual back end app for our project okay so let's come out of this not terminal in fact from this command so control c is the command to terminate this running of this particular app which we ran earlier by calling python app dot py right so what we will do is let's do one thing for us so okay what we'll do is you will make the changes in this particular app only but what we will do is we will comment out the no code which is not required here okay so first of all what we need to do is we need to install certain,358,0,0,AMgp_1AXEE0
1,packages okay so pip install found us then we have a lump i spy get or side by 4 instead of quests okay so this package basically involves in getting and posting the a request to the web so okay for what else so these four packages for now or maybe what we can do is job link is internal to it so i think yeah these for for now and we'll see if any other packages required being installed them in there okay so till the time these packages gets installed what we will do is we will make changes in our app so that we can make it for our actual project we will comment this particular portion and in here so i told you in the last videos that whatever logic you want to get executed on the webpage that comes in between this particular statement and these two statements so whatever comes here that will be executed on the webpage so for this particular let me create a home page so slash to present the home page of it and i will change the name of the function as okay and here okay let's import import fonda's as pd and import numpy as p okay and in here we'll also include render template function okay okay so the render template what it will do is we will call a particular map page using this would return random render template so basically what we will do is in the return section in return section we will do name of the html file so hole dot html now this home thought i was tml,358,0,0,AMgp_1AXEE0
2,what we will do is we will create a folder here let the name template okay so what we will do is whatever html files we create for our project we will keep those html files in this template folder okay because there has to be a dedicated place from where these to keep these web pages html pages at one place okay so in here we will create a new file called oh dot html which is nothing but our html page okay for home page basically we will include the code for the home page here later on but for now you're just creating this back end so home page will be our front end and the app dot py is the back end which will run as web service on the web service web server a triple location or the remote web server ok so one piece of corn at sea our packages got same-store no not so it is still installing here okay the next piece of code is will create another decorator here so create another decorator with the name epcot route and this time we will create endpoint as reading okay ready to will be the end point here and the map thirds will be both get and post so cat is to get the response from the web server remote web server and post is to post any values which are present on the web page to the web server for further process okay so this is a work project end point where the predicted value so once we click the submit button on the home page that result,358,0,0,AMgp_1AXEE0
3,the final result or the predicted result will be shown on a new page and at the new end point okay so here and if you will include the logic for that prediction is in this particular place okay so what we can do is we can create again our new function with the name depth predict okay we will say if the quest quest so method is either get or post right so the requests or so in our case we will be submitting the value so once we enter the menus i showed you the demo app so why i entered the values for state california florida new york and forth you know all the other expend values on the webpage so once we click the submit button it will go as a poster request okay so that's why i am including the if condition as post here and then we will and we will write the code like in try block statements so try try catch basically so the idea is to you know if there is any error that error can be caught in these statements so kind of what can i say i don't handling so we are doing error handling here right so try and accept statements okay and we will include our code in between the strikes straight so very generic error okay and once you know the prediction is presented in a successful manner it should be available on the another page so we will be creating another page which will be a prediction web page okay well our final predicted result will be shown so we are,358,0,0,AMgp_1AXEE0
4,creating a new html file with the name predict dot html where the final predicted result will be presented okay so two files in this template folder so what we will be returning just like in home page we are returning the home home page return number ran the template then there is a template so we will be returning prediction dot html file here okay so don't let and name of the witches religion or predict in our case it is predict dot html' okay and further what we will include this so may will be you know predict in basically we will be including the pickled file here in this particular s block and the idea is to basically so once that pickled file is loaded here and we will provide the values from the home page then the final result will be presented to predict dot html file correct so or i will include this piece of code later on okay so in this particular video what we did we installed certain important packages here okay and then we have written setting code state code blocks to predict the values and present them as html on html pages so we created two html pages as well let me include one more statement here because obviously it will be loading the pickle to file so we need to have the job lib method as goes to a scale down dot externals okay job because if you remember we saved our model using job lab man method only job live dot dump right so here we will be using job lib dot load okay so,358,0,0,AMgp_1AXEE0
5,yeah so this is it for this video folks we'll be including the rest of the code statements are covering the remaining code statements in the next video so till then keep on watching thank you,46,0,0,AMgp_1AXEE0
0,hey guys welcome to the tutorial of deploying a machine learning model in this tutorial we are going to deploy a already trained model into live production i will try to keep things as simple as possible and i hope you will be able to follow along when i am going through the tutorial if not please post your question on the chat and i'll take the questions at the very end of the tutorial so with that let's start so for this tutorial i'll be using this notebook so our topic for today is deploying a machine learning model and we are going to create a web application using flask a python web framework and deploy it in the cloud using render you will also learn how to add html templates and see how to style it using css very basic uh by the end of this tutorial you will have a basic understanding of how to create a web application using flask integrate machine learning models into the web application and know how to deploy a web application to the cloud the following topics are covered in this tutorial first we'll set up the project on github and conda environment and then we'll create a simple web application using flask just a simple like print something on the web page then add html templates to the web app then we will use we will see what ml model we are using we will just learn a bit about it we'll not go deep into the ml model and then run the model locally on a web browser publish to github and deploy to render,358,0,0,rgr_aCg-338
1,and finally we'll create an api route for the model so that these are the topics that we are going to cover there are certain prerequisites for this uh for this tutorial i would say very basic you have to you should i think you should have very basic knowledge in python but basically i just mean defining a function i guess a very basic knowledge of html tags like body div headings forms etc and css styles like background color text align etc and some level of understanding of machine learning concepts uh just like if you have if you are given with the model you should know how to predict uh with the value okay i think you will be able to understand even though you don't have don't know some of this so try to follow along okay so definitely we are going to solve a problem uh in this tutorial the problem states that you are given a pre-trained email spam classifier model and its requirements basically what are the things they are in the model uh our task would be to deploy the model and prepare it for end users so anyone can go to the site give a email or body email body and will be able to understand whether it's a spam email or not whether it's not a spammable to achieve this we need to design a web page definitely using html and uh predict using the model whether it's a spam or not the web page interface should be as friendly and simple user friendly and simple as possible and in short we have to design a,358,0,0,rgr_aCg-338
2,form that takes only one input that is the email body the email text and returns whether the email is spam or not okay cool enough of all this um theoretical part let's start with creating the simple flask app for that we'll start with creating a new repository on github i'll go to github.com and i've already signed in you can sign in or sign up to github.com i'll create a new repository here and give a repository name so let's say model deployment it can be anything any unique name i will skip the description keep it public add a redmi file you can add the redmi file later but i'm adding it now a git ignore template this is basically a common template used for different programming languages i'll use the python one and a license you can skip but i'll use the mit license um but just because like it's the best one for open source projects okay so we have created a github repository right so now let's start coding we will be using uh code spaces here but there are different options for code like for coding okay so you can use replit or vs code locally uh for the coding part but i'll be using github courses as it as it is easy to set up i'll just click on code here go to code space and say create code space on main let that open let that set up i'll also have two uh create a conda environment now what is conda conda is a package manager just like pip and it also allows us to create environments,358,0,0,rgr_aCg-338
3,now an environment is useful to manage a static pattern like a certain project uh so if you have a project if you can build different environments for different projects and have the packages required for particular projects in that particular environment okay so this is our repository so we will see if conda is available for first so you can just say conda version and yes conda is already available let's start by creating a honda environment so here is the command you can just start create the environment so i'll allow and conduct create minus n this environment name can be something like let's say model deployment and i've assigned python 3.7.10 but uh you can use the latest version also i'll tell in the towards and i'll tell why i have used this particular version so while it's installing let us also install the python extension to code space yep that is let's install this and yeah it's asking permission to install the different libraries i'll just click yes okay just a minute i think there is an issue you cannot you're not able to see the github screen just give me one moment yep i think it should be visible now so i will just repeat the steps very quickly so first okay i just start from creating a new github workspace right so let's go to that yeah sorry for all the issues uh it was a technical glitch i guess so we will just log into github first okay github.com will start uh here click new repository and let's say model deployment as we as we already have model deployment,358,0,0,rgr_aCg-338
4,uh i will just say we do i click on add a readme i'll click on python i'll select the python getting node template i'll select a license mit okay and i'll click on create repository we are starting again from the github creating a new repository right so now next we will use a code editor we can use code spaces we can use replit we can use vs code so here i am going to use code spaces and in this tutorial i have mentioned the different steps to install like to use replace to use vs code okay so while it opens i will just install the python extension here let it open i guess i will uh stop the other code space because it might take some usage so i'll just go to code spaces yep just give me one moment yeah i'll just delete this code space the previous one and here yeah the code space is now open we'll install the python extension and i'll also check if quonda is available here so ponder minus just as version and yes uh yes conda is available so i will ins create a new conda environment so yeah here is the code contact create minus an environment name python 3.7.10 so here i'll just give environment name let's say deployment it can be anything you can give any name and i've assigned python 3.7.10 so i'll let it install and here i will yep so i'll let it install first and here we'll say yes i'll let it install all the packages yep honda is installed i will try to activate this,358,0,0,rgr_aCg-338
5,but i think it should return some error let's see yep it says conda is not initialized so i will just say quanda in it bash that i'm using bash l cool i think it's initialized i'll have to just reopen a bash shell i'll delete that previous one and now you can see the conda based environment here right so we created another environment called deployment so conda activate deployment this will activate the deployment environment that we have created right okay so now we only have a few packages installed here so you can see this 35 okay and so i would like to install different packages in this environment i'll just select this packages installation tap install and enter this will install a bunch of packages that are required for this tutorial okay while it install let's go to the next steps that is create and run a flask web server right so first we'll need to create a app.pi file so here i'll create a app dot pi file so app.pi file will write all the flask code in this file basically uh everything everything python related stuff will be in this file so let's just right and now the libraries are installed so what we will do first is import flask so from flask import flask full the next step is creating an instance of class so app lask we're just going to underscore name so this will create an instance of the flask class okay and name is basically referring to this current uh file okay a current module cool let's go let's now create a route so what is,358,0,0,rgr_aCg-338
6,a route basically so whatever you see on this website this is a route so joven.com is a route now if i uh go to joven.com notebooks it's another route okay it is basically going to the notebooks uh route so i will create a route like this slash this basically means the home route that is joven.com okay or for here it will be different so app dot route so the route is kind like so the route is defined as a decorator so i'll just say that and i'll create a function called home and i'll return hello world okay so we're trying to create a flask uh application where there will be a text called hello world right so now what we have to do is we have to run this so we have to run the flask application so i can just copy this but i'll just go step by step so if name equals equals mean and here i can just say app dot run cool with this thing with this uh i'll just zoom in the screen yep i think that's visible but that's better okay so with this we can just uh run the python file let's say python app dot pi and you can see that it's running it's showing up on the right side that open in browser i can click that and you will be able to see a hello world here i'll zoom in here yep that's there hello world cool now the next step is if i want to change i want to change this hello world to net something else i'll call this,358,0,0,rgr_aCg-338
7,updated right i'll save it and let's say fresh but it's not showing up the update it's not showing up here so what we have to do is whenever we save uh change something and save it it should show up so for that we can activate the debug mode debug equals true okay and i'll just close this and uh and run the server again to close i'll just say ctrl s ctrl c and run it again python app dot pi now again let's see so you can see the hello world updated and now if i change something here updated version 2.,136,0,0,rgr_aCg-338
8,it should show up here also yep it's showing up so that is a simple basic flask app uh we have created a very basic flask cap and i think it was very straightforward up till now um yeah the notebook is shared on the chat window so you can just access the notebook from there okay that's creating a simple uh flask application but now we will try to render an html template okay for that we will create a new file here new uh folder called templates cause flask uses this folder this template's name for uh better for for their rendering templates right okay so we have created a folder called templates here and then we will create a new file it can be anything let's call it index.html and here we can write our html code so i'll just copy this code here basically what we are doing is we are giving a head a title of my first web page and it says hello world from template now if i save this and if i load it will not update because we have not updated in app.pi okay so here what i will do is i'll i'll have to render the template right so i'll render template and just give the path of the file now as i said render template looks in the templates folder okay so we don't have to give the path exact path like template slash index.html we can just say index.html and reddit template is not imported i mean yeah not imported so we can just import it render template i am seeing some errors here,358,1,1,rgr_aCg-338
9,right so i want to remove those errors that is causing by eslint so for now i will just reactivate eslint yep and the error should go yeah cool so now if i update this yeah this has uh so what happens in code space is you are we are using a free version of code space it reloads or stops some time okay or you can we see a we've got an error here so it has stopped we can just say python app.py again and it's starting yup hello world from template so this came from template right so you can see this template had a title my first web page so this title is showing up here on top my first web page and here the h1 is hello world from template so that is what is coming here but yes we have created a web app and rendered a template html template here cool um the next thing we are going to do is creating a simple form so our end goal is creating an email spam classifier it should have a input text area it should have a heading like this it should have a button so this is the next thing we are going to do so we are going to update this index.html file here and we will give a h1 let's say the heading will be email pam classifier so let's see what we have got here yep this is again reloaded or what yeah i think it's again i have to reload this just i'll stop this and again python app.py okay so i am getting an,358,1,1,rgr_aCg-338
10,error so let's see what the error is first i'll let update this conda environment here so the quanta environment is this one we are using and yep let's see we will try to fix this error let's see we will run this app again and yep it's now loading again okay cool so uh i think i was not i think the conda environment was the issue so we have got the email spam classifier heading now we will create a form in this we will add a text area okay and we'll also add a button here that's what is there in this thing so a text area and a button so this text area let's now look at the web page once yup we have a text area and a button so let's modify it a bit we will give a button we'll call it check spam and this text area can have a placeholder like this enter your email so let's do that enter your placeholder equals enter your email uh body okay so once we load save that and refresh this we got this this is good enough i guess we are getting there so we can say rows equals to uh we we can increase the size of the text box so i'll just modify this rows to let's say 15 and calls number of calls basically rows is the height and calls is the width so i'll say width as let's say 25 let's see uh how it looks now yep it is looking a bit better i can increase the width to 35 and yeah i think this,358,1,1,rgr_aCg-338
11,is better so one more thing i want this check spam below this so if i can create a different block for this what i will do is i'll put this in a div yep that's there and this is looking let's refresh this this is looking fine in this the this is centered so we can also add a style tag here to center everything is the naive way of adding css but i'll be using this for now just in the interest of time so i'll be adding let's say body i'll align the text to center okay now see let's see how it looks yep everything is aligned to the center we can also improve the button so button we can give a background color of blue and a text color or the color of white let's see how it looks yeah this is looking better so now we have our simple form okay but what does this form do this does nothing as of now right if i just click here and write something it will do nothing what we want is we want something this whatever we have written here we want to display it below right so let's do that i'll just zoom in a bit more yeah this is better okay so let's get let's write something here right below so we can just give a paragraph tag or a heading h2 tag i'm giving a head h2 tag here h2 is basically second header and i'll say hi so let's refresh the page again and you can see hi here so instead of hi i want to display,358,1,1,rgr_aCg-338
12,whatever is showing up here right so what we can do is we have to update uh we can use jinja template okay so we have to update two three few things in this first we'll go to the app.pi file okay and here we will give the method so we are basically we are sending a post request by clicking on this button okay so we can update that we can go to the index.html and say this button this form will send a post request the method is post all right cool now the post request is sent okay what we want is we want to get the text from the post right we can say the method so this particular route okay home route accepts two methods it accepts a get and a post request so why get get is basically when you open this web page or whatever this url okay it will fetch the web page from the server and get it okay so that is get and post is on clicking this button it will show something here so that is the post request right so both get and post and i'll just say if request dot method equals equals post there will be a text which is request dot let's see it is request dot form dot get right but the code is here we can just say text equals to request.form dot get okay cool i will return the text here but one thing you can see that there is something called email content i don't know what that is okay so basically email content it should be,358,1,1,rgr_aCg-338
13,the name of the text area so we'll add a name here so we are fetching this email content okay email content so we will be getting text from this email content text area right and request is not present here that's why it's giving the errors we can just say request we can import the request from flask cool so that's great okay but if we still refresh this okay this has i'll have to run the server again so let's see let's run the server again getting some error okay let's see we are getting some error let's fix that so what we have done is we have added a method post okay and it shouldn't give a error let's me just run that server again yep just this happens sometime with the code spaces let's just run the server again so this yep it's running now okay so this problem doesn't happen with your local development vs code okay it happens sometime with code spaces so now if i write something here okay let's say hello world i'll click check spam and it should show hello world here but it's not showing up because we have we are fetching the size till now so we will use this ginger template okay and ginger template engine to get the text from here okay so this we have returned the text in app dot pi this is the text we have returned i want to fetch that text right okay so i'll fetch that text here and that should show up now let's type hello world yep it's showing up here one more thing we,358,1,1,rgr_aCg-338
14,want is we want to show the hello world here okay so what we can do we can just give the hello world here the text here also in the text area okay so now if i just say hello world it's showing up here also and it's still remaining here also it's not showing entire email content okay that's great we have we are midway through now what what else to do we can uh i want to reset this now if i refresh this page okay this hello world will still remain here so i want to reset this but a very nice way of doing this is i will just add a href here okay and i'll give a path i'll give the home route okay and i'll give a reset uh text what this will do this will add a hyperlink or this will add a link okay a preset okay and we'll just fetch the home route okay i'll get the get request so let's refresh and we have got a reset button and let's say if i just click on this reset everything is reset okay so okay this is looking a lot better now hello world check spam it shows hello world and reset okay cool we have now uh fetched like we have now shown whatever is uh input we are giving in the text area we are now showing it on the web page okay so what's the next step now let's learn a bit about the model itself okay so an email spare class fire model spam classifier model is a classification model and uh what,358,1,1,rgr_aCg-338
15,it does is it categorizes either a email is spam or not spam okay that's the basic thing now the data set has a uh labeled emails okay so and uh and it also has some email body so basically the data set has an email body the label that it is a spam or not a spam and it's a classification supervised model okay so i am not going deep into the models and what models are trained i'll link a notebook here you can check that how the model was trained but what are the things to note the things to note is it the model takes a plain text will send the plain text using this in text area okay it will tokenize okay so we'll have to talk uh tokenize the text okay so that step we need to do then it will run the model dot predict okay the tokenized text and it will return either one which is spam or not one which is not spam so that's the basic thing you need to know about this model okay cool now let's head on to running the model locally but before doing that we need a few things we need the model and the tokenizer okay or we are using count vectorizer here we need these two things so i have linked those two things here convectorizer and model uh what we will do is we uh i already have it downloaded okay so i will use that but you can download it from this links now we will create a new folder named model okay inside our repository so,358,1,1,rgr_aCg-338
16,let's create a new folder here so the folder is models and inside this models i already have a couple of files the models i'll just paste it here cool the models are there the models are loaded now what i will do is i will yep so now i will have to load that model right now i have to load that model integrate that model to our web page right and here we have defined this thing in the home route okay we have defined this hello world uh this post response in the in this particular home uh a default route but i'm going to create a new route that will take a post request so let's say app dot route and we will give a new route called predict and this will take methods as post only the post method right so basically this method this route will only be fetched only when we click on the check spam button cool so i will say post here and we'll create a new function called predict cool and we will again return the same template lender template that is index.html template we can use the same template for multiple functions so we are using the same template okay so now we don't need these things this text request dot post okay so basically home the home route will now only get this page if i am in the home route i'll only see this page nothing else right so i will remove this methods also we don't need a post method for the home route and that's it so this is the home,358,1,1,rgr_aCg-338
17,route and we are getting the predict route now in this what we have to do is first we have to get the email text so this text whatever we're giving here you have to get that for that we can say email text equals to request dot get dot request.com dot get yep request.form dot get and we need to specify the text area name here so email content email yep this is the one email content email content okay so we got the email text now so whatever we we are writing here this will be coming here now what we have to do next we have to tokenize this text right so tokenize this text we have to basically let's define a new variable tokenized email we have to tokenize now we have to use this tokenizer this account vector as a cv okay so this is the models on vectorizer model which will tokenize so we can just say cv dot transform okay so this is coming from sqln we use the transform but we don't have cv yet right so first we'll have to import the c a pickle library so it's a pickle file we have to load the cv and model okay so let's import tickle and we have to basically load these speakers so let's do that import pickle tv equals tickle dot load open tv dot pickle uh i can call it let's say organizer and read binary because uh a pickle file is a binary file so we'll say read binary similarly the model can be pickle dot load open uh clf dot pickle and again,358,1,1,rgr_aCg-338
18,read binary so clf is the classifier that is the model and this specific models you can you might not recognize these names because i have created its models and you don't might you might not have idea but when you are having your own model when you want to deploy that okay you will have the particular pickle file name and everything you will have with with yourself right so this is tokenizer this is the model i've uh got this so this request spelling is wrong req est yep now this tokenizer dot transform and i can say email text and again i have to predict the model also so predict predictions let's say predictions equals to model dot predict this is what we do in sqln to write the model tokenized email okay so this is the pre-process text okay and we are passing that uh so you can also call it the x value this is the x okay we're passing it to model dot uh predict fine uh cool now one last thing here is if the prediction is one it is spam and if the prediction is not one then it is not spam so let's do predictions equals 1 if predictions equals equals 1 else minus one so if it is a spam will it'll be one fine if it is already one it will be one if it is not one it will be minus one okay so these things these all these things i have got from the model okay which i have defined so these things might be particular this will change from person to person,358,1,1,rgr_aCg-338
19,whatever models they are creating whatever way they are training okay uh so these things will have to get directly the idea from the model itself correct cool so now we have this predictions what we have to do is we have to pass these predictions so predictions equals predictions and text okay so we will also pass the email text remember i want to show the email text here so so that it doesn't disappear okay so i'll also return the email text okay so text equals email text okay let's call this email text also cool so i think the spelling of predictions is wrong yep that's correct now so i think this is good yeah i don't need to change anything on the app dot pi file i what i have done is i have kept the default route as just fetching the index.html page okay and i have um created a new route called predict okay which will take in at like which will take get the text from the text area will do the tokenizing like we'll do the uh tokenizing part okay and also do the prediction part and it will return the prediction uh to the basically it will return the prediction to the html okay so now here what we have to do is we have to change this text to prediction i hope it's called prediction called predictions okay so predictions and again to save the email text we have changed it to email text so i'll call it email text here in the text area cool let me just go to the new line here yep,358,1,1,rgr_aCg-338
20,so this is called text email text and yep so we are i think this is good now one thing here so we have to give so initially this form was posting to the default route that's why we have not given anything else here but now we are making a post request to the predict route okay so we have to give her actions here so let's call it actions and i will say predict i'll give the route here okay we are basically making a post request to the predict route okay so let's see if this works or not okay i'll have to run this again this has reloaded i'll call python app dot pi okay so it says oh the path okay so it there is an error okay it says the file not found error so i the path is wrong here so i'll have to say models.cv dot pickle model classified vehicle models class salem dot pickle why because we have saved the models in the models directory okay so that's there now let's try to run this again and see if it's working yep it's running let us open it in browser let's give a simple text hello world or this is about email body okay and let's see what we get oh it says method not allowed okay let's see what error we have got it says four zero five that means we are not able to get uh we're not able to make a post request let's see what's the error so here we have given predict as the route the method is post okay one thing,358,1,1,rgr_aCg-338
21,i'll do here is if request dot method equals equals post i will get this ml text although i don't need this but yeah i'll just give this okay so that means when there is a post request i'll get the email text okay that is fine but actions method post actions predict okay method post action spread it i think this should work let me run this again let me see if this works this is a email yep okay finally cool so i think this action was wrong in the previous one so it should be action not actions okay uh but i am still getting an error here it says that um tokenized email cv transform here it is wrong let's see we will fix that error but finally we were able to get this so cv dot transform email okay cool and we are getting the error as none type object right cool so none type object has no attribute lower okay so let's see i have not given any text is it this is a email button okay i'll just run this again python app dot pi okay i think i'll have to open it again python app dot pi yeah this is an issue with code space i'll have to open in browser okay then type attribute has no object lower so let's look at the code again so email we are getting the email here and this we are getting the name is content now so content this should be content okay cool i think this should work yep finally we are we have got the result minus one,358,1,1,rgr_aCg-338
22,so this is not a spam okay so i think this content was wrong okay uh this is now content okay because i've copied the code from here so this was having content and i had to set content here okay so everything else is still the same we are getting the email content from this okay and we are creating the tokenizes we'll bring the organization we're doing the prediction and we're returning the prediction okay and the email text so here we are just getting the email text in the text area and getting the predictions in the predictions in the h2 okay so this is the result now we'll click on the reset to we'll check the state is working or not so if we click reset everything is reset that's working fine okay great so now what we want is now we want to show this as not minus one and one so we are getting minus 1 here if we type this is an email i want to show spam here instead of -1 for that what we can do is we can change this we can add python code here using jinja template so i'll just type here percentage percentage and here i'll say if prediction equals this is just simple python if predictions equals equals 1 okay then i will say then this will be like spam okay allied spam here and else if left that is l if prediction equals equals minus 1 will be this will be not spam i'll copy this yep so this will be not spam correct now i'll end the if so this,358,1,1,rgr_aCg-338
23,is a bit different than python but almost similar so i'll just say this is and alif sorry end if correct okay so this is the format so for writing a if here if predictions results one within this curly bracket and percentage and this will be spam if it is prediction is one if it is minus one then it is not spam correct let's now try to run this once there is an email if i yep it should be and if okay i'll just refresh this once this is an email and just clicking on this button it says not spam okay so we have got it it should be end without space if okay cool so now we can modify this a bit we can add a small style here okay the style let's say color is for spam let's say the color is red and for not spam the color can be green okay let's check what it does yep not spam this this is working okay so i have um kept uh pam email and uh non-spam email example here i just copy and see if it's working everything is working fine this is a spam email i got in my mail folder okay and i'm not saying the model is 100 correct okay it can also return wrong so this is again a part of machine learning okay so i am not going to the machine learning part this can be not a spam email but it showing it must show scab spam here okay according to the model i have created uh again i'll check this and this,358,1,1,rgr_aCg-338
24,shouldn't so spam let's check this once yeah it's not spam okay so great i think we have finally created the model and run the model locally so we have done most of the part now the next thing that is remaining is uh deploying this web application to cloud okay so yeah so the next part is deploying the web application to cloud so what we will do is we will prepare for publishing first okay so we have to have a requirement.txt file in this requirements.txt file we have to save all the libraries that are required for this particular um project okay so we already know that pip phrase shows i'll just let me open let me stop this server once okay i'll show you what pipfish shows so if i do pip freeze here it will show me all the libraries that are required for this particular project okay so what i can do is i will save all these things in requirements.,217,1,1,rgr_aCg-338
25,requirements.txt file requirements yeah the spelling is correct and here you see that all of this are saved okay so there is a library called this i think this is wrong so in this case i'll just uninstall it and install it again uninstall 30 file just uninstall this and pick install 35 okay let's see if it's now it's correct so if freeze greater than symbol equals txt and yeah now it's showing correct okay so now we have all the libraries that are required for this particular project okay so we have created a project we are ready to deploy this okay we'll take it live so next is pushing these changes to github to do that we can check the changes that are made here okay so we have to hit changes in app dot pi fine to cameras.txt settings.json yeah this is looking fine you can just commit it from here we can just give a commit message and commit from here but i'll just do it from the terminal for this time get add dot get commit minus m let's say first commit yep get push origin mean so you might have to set a git config like who is who who is pushing who is trying to commit okay so if you are doing it for the first time i have made uh i've set up like a link here okay so if if it is if you are getting an error while trying to commit you can check this guide cool okay now the next thing is publishing with render so we will directly head on to let's first,358,2,2,rgr_aCg-338
26,check the github repository now okay so this was our repository in the beginning you can make changes in the readme.md file but let's check this first if it is working yep all the changes are here you can see the app dot pi file changes here okay cool now uh let's open render for deployment so render.com surrender uh lets you deploy some websites for free okay uh and uh yep it is very easy to use so that's there also so i'll just open render i'll log in with github that's what i've done uh so i have a few previous deployments here so what i will do here is i will create a new web service and i will configure github here i'll connect with my github account it's not connected you can see so i'll connect with github by pressing configure github and all of these steps are also mentioned in this notebook so if you uh you can follow this notebook also so then i'll go to my account okay and here i will say i'll give access to all repositories for now okay i'll say install yep now uh github is connected with replet let me just refresh this once and let's press new web service yep now it is now github is connected with replied right okay uh so if it was not working you should all once refresh the page because it's already connected so this was the one i have i was working on model deployment v2 okay i'll click on connect okay now here i'll give a service name i'll say model deployment okay i can,358,2,2,rgr_aCg-338
27,select a region so i'll just select i'll let it be the default region singapore southeast asia try to select a region that's nearer to you or wherever the most traffic is the main branch i want to connect with the main branch okay if i don't have any other branch as of now uh then i don't have any road trip directory so i'll not set anything here the my root directory is already the one directory that is there that is the this directory basically okay if you uh have your router it was src you could have given that src or anything else now i am using python 3 as the runtime we have created a requirements.txt file so this is the build command here bb install manager requirements.txt this is fine and also you can see the something here is g unicorn app colon app we have already installed geolicon in our uh while while creating the environment okay so if you check the requirements.txt file you can see g unicorn so g unicorn uh helps you is a you can say it's a library that helps you um deploy your website okay so here there are two things app colon app so this app is the file name you are trying to run okay so our file name is app so this will be app okay and this app is the flask instance okay so we have this in our app we have something called app here this app okay so this is the app that is a flask instance okay i'll select the free instance type that is uh that,358,2,2,rgr_aCg-338
28,only has 512 mb ram and 1.1 cpu and i'll just click create web service okay so now we have created the web service and let's go to the next step okay so all the steps are mentioned here what i have done the next step is awaiting deployment so now it will you can see that deployment will start okay and we'll have to wait okay this this at this point we'll have to wait for the deployment to run it because we're using the free instance it might take some time okay and we might also get some errors so that will have to fix now why we have used python 3.7.10 i mentioned at the very beginning uh that conda created by 3.7.10 because by default render is using python 3.7.10 okay uh so that's why we have used python 3.7.10 as our um conda and like for this in particular project okay but if you want to use the latest python version you can just you have to you have to mention this on the python but like environment file okay there is a dock here on how to uh use the latest python version cool so you can see it says build uploaded so build successful okay so build like all the packages were installed okay all the required packages now it's starting to deploy uh it's trying to start let's see yep the service is live okay cool that was fast okay so the service is now live let's check this okay it's working fine let's zoom in yep so now this link this is my email body okay let's,358,2,2,rgr_aCg-338
29,check once yup this is a spam i don't know why but this is spam so this link is live this is published live and everyone can access it so i'll just post it in the comments and everyone can access this uh but this particular link to check whether the email that i have got is a spam or not so i have successfully deployed my email spam class 5 model okay in the same way you can deploy all other models okay so we are done with the deployment okay now one last thing we are going to cover is api how to uh create an api route okay so i'll just go back to my app.py file again and here basically the very basic way of like what this may what is the creating an api out mean is we are not going to use the web interface this thing okay we are going to send the request as a json okay and we are also going to get the output as a json okay so that is what by creating your api means okay api route so we will do that so let's again create a new route app.rout plus api so now we'll give a api so this is a general convention we can say we can say api slash predict this is the new route is not the same as this predict okay and we can call methods uh again this will be post okay now again this is api we can give a function name api predict the function name can be anything i'm calling it api predict the,358,2,2,rgr_aCg-338
30,first thing is getting the email so email will be request dot get json okay i'll force json okay i'll force that it should it to get json okay so this is called force equals to true then if i got the email body so next part is same okay so i'll just copy this the same thing okay and return again i'll just return a json uh file right i'll not return this render template will not let the template i'll just return the json so i'll say jsonify let's just return the prediction prediction equals prediction okay so it's returning the json okay but uh this jsonify is not we have not imported this 25 so we can import it from flask json e5 it's present in the flask module yep so now we have successfully created our api route okay now let's try to see if this api route is accessible via web page okay so i'll just say python app dot pi and we have got our new this thing this is an email okay and if we click on button you can see it's going to the predict route and it's getting not spam okay but if i just call like this api slash predict we are not getting any things okay so this is this now not how we call an api correct so what we can do is we can use um uh different api tools okay we can use maybe postman insomnia or other tools to call an api we can also call it uh by request or by using terminal but i will use a tone called,358,2,2,rgr_aCg-338
31,thunderclined i'll install the extension thunderclight and while it's installing i'll go back to my notebook okay so here is how you can fetch the api okay you can use command line you can use python you can use any api testing tools okay for example thunderclined insomnia postman okay i'll be using thunderclient here so let's go back i think thunderclient is already installed yeah let me just refresh this code space once yeah and android client should show up here yeah thunderclient is here okay so i will do a new request and here i have to give my url okay so this url so for that first we'll have to run this first we'll have to run our app python app dot pi okay now it's running in this particular link okay so the url will be this link slash api slash predict okay so this is the url yeah i'll just copy this and give the url and we are sending a post request not a get request okay so i'll post and the body so what will the body have right so let's go back to the code once and here you can see we are yep so we have to give a content so let's just go back here yeah request dot get json so we okay so we're getting the data in this form okay we have to get the content so this is the data and out of that we are fetching the content okay so this will be the data we are getting the data and the email is basically that content from the data okay so,358,2,2,rgr_aCg-338
32,this content should be a key in the json okay so now let's see so we will be sending a json with a key called content okay and this is the email body okay now let's try to send this and we are not getting a response let me just run this once again yep i'll close everything and let's run this once again python app dot pi let's say open in browser now this slash api slash predict i'll copy this uh copy and paste this new url here we have this new url and i am sending this so it's showing method not allowed so have i done okay slash api slash predict methods post let me check slash api slash predict method post and force true okay i will just copy this once yep so this should return let me just run this once again yep this is running slash api slash predict i think i will okay let me check new request here change the link and let's send okay i'll have to uh deploy this because i think this is not taking this localhost okay so let me just once again make a git push okay i'll just make a git push once again with the new code so added api route commit yep i'll stage everything and make the changes so what it will do is it will automatically deploy the new code to render let me just go to render okay and here let's just sign in yeah deployment is progress so the new version is deploying now okay so let me let's let's wait for deployment yeah it's,358,2,2,rgr_aCg-338
33,still deploying so you will see when it's deployed okay let's wait for this while uh so one more thing i want to talk to you about is code refactoring so what we can do is so you can see in our code that this part is repeating twice okay so we can refactor this okay and create a new utils file where we will predict our model okay so let's do that so first what we'll do is we'll create a new file called utils dot pi okay and here we will copy a few things first is the pickle part import pickle and uh this cv we'll copy the model models okay we'll copy this and paste it here the model loadings and also import pickle we are trying to uh reduce all of this from this particular file and create another file for the model prediction okay now i'll copy this code tokenized email equals to this and paste it in the utils.pi file okay i'll post put it inside a function called def make prediction with the email body okay and it will return a prediction okay so once that's done what we can do is we can just call this function make prediction here so let's say prediction equals make prediction and we will give email here so the it accepts our email will give email here so you can see make prediction is not available here so we will have to import it will import it from utils will import make prediction okay so now that is done so now we can use this make prediction function to make prediction,358,2,2,rgr_aCg-338
34,we don't need this anymore again we will just say prediction equals make prediction email okay so now you can see the code has is like reduced a lot so we just have like three lines of code in each functions into four lines so that's there now let's check the deployment once yup the deployment is done i'll go to the page here so we have this api right so that was api slash predict so now what we can do is we can go to thunderclient we can try to make a new request with this api slash predict url make a post request and in the body we will say content this is a email body okay and uh yeah i think that's it it takes the content from the data i'll send this request yep finally we got the api um uh call okay so fine uh so we got a prediction of one that means it's a spam okay so yep so that's how you can create an api also in fiction if i think it was not working because we are using code space and the code space uh the localhost url was i think it's a bit different but once uh now everyone can access this api uh and give a content okay and email body here and you will get a spam on not spam like prediction will be either one or minus one okay so that's it okay so that's what is there in this notebook also you can follow this notebook and see how to create an api a code refactor and fetch the api so,358,2,2,rgr_aCg-338
35,one exercise for everyone is i am not too good in creating a good ui you can see that this is a very bad ui and if i even uh zoom out it will look even bad okay so what you can do is you can try to improve the ui okay and what you can add is you can add a heading here you can add a introduction here you can make the box like this you can make the button like this you can add the example spam email an example launch spam email on the left side and also what you can do is you can add another mod if you have like multiple machine learning models you can add different models on the top okay let's just spam classifier then image classifier whatever is there okay all of this on the top so you can change the ui you can modify the ui yourself so try to go through the entire notebook and uh create this um email spam class file model deployment also okay we all you already have the model try to create the deployment okay so what we have covered in this tutorial so i think we are towards the end of this tutorial we have covered how to uh use flask okay how to create a very basic website using flask then we have also seen how to set up a github and conda environment okay i've seen how to use html and css and python to uh then we have seen how to uh run a model locally how to integrate a model into our web page,358,2,2,rgr_aCg-338
36,and run it locally uh then we have seen how to deploy the web application to cloud use uh like push changes to github and then prepare it for uh like then uh publish to render then fix the errors in the render so we have not got any error uh while publishing but if you face any error you have to make the changes again and then again push it okay so those and then we have seen how to create an api and also saying how to fetch an api and do code refactoring so that's all the things that is covered in this notebook okay uh here are a few differences i have mentioned here and also i have mentioned how to do a vs code setup or if you want to run this code in vs code and also if you want to run this code in deplete i mentioned that i hope this was very um like very simple beginner friendly tutorial although we faced a few technique like technical difficulties in the middle but i hope this was a very simple deployment process now i'll take a few questions i think most of the questions are already replied but let's see okay uh thank you uh for the good comments um yeah uh hope this notebook is useful to you if you can refer it to this notebook anytime we'll make uh we'll add the link in the description okay web service is not working i am not sure why it is not working we can discuss this later you can just um post on probably you can post on,358,2,2,rgr_aCg-338
37,the youtube comment and we can see we can talk further okay okay there is a question okay where we can find the pickle models to test the service and write out yourself okay so the pickle models are there in this jupyter notebook you'll find it here okay the account vector as a model in the class five model you can try it out and also if you have any different model if you are if you have worked create a different model you can use that also it doesn't have to be the same image pattern plus five model and um okay so there are definitely this is not the i would not say this is the ultimate way of creating uh like deployment there are a few steps you might have to add authorizations also you might have to add uh like the models might be very big okay so you might have to add them in a database or create a docker okay and um like uh yeah you can do a lot of things so there is a scope of future work you can see you can implement authentication and user management okay if you want to rest it some feature you can uh scale the application for high traffic okay then you can um you can also so obviously improve the user interface we have not done a lot of in user interface site then um you can also see like implement real time updates for example like if someone is typing the email it's you can just say spam on or spam okay and a few more things okay,358,2,2,rgr_aCg-338
38,those are the things that you can do for future this was a very beginner friendly i tried to keep it very simple uh just for anyone who has created a model okay and uh just they can use the steps to deploy their model and showcase it in their probably linkedin or resume or anywhere okay this i try to keep it very beginner friendly but there are a lot of things that can be done to improve this uh from this okay cool i think that's everything um thanks for attending the session and i'll end it here i'll end the session here um okay thank you for attending and see you on a next video,153,2,2,rgr_aCg-338
0,hello everyone and welcome to my channel this is sohini from south bay california and i welcome you today so in today's video i'm super excited to release my first video on ml model deployment so for anybody who is interested in learning how machine learning models can be made you know useful to to to people it can be released to customers in order for them to try it out so deployment using cloud platforms that is what we will be covering in the video today if this is of interest to you please give this video a thumbs up and subscribe to this channel and hold on for the blog that i will be posting in the description box below in towards data science which will lead you through a very systematic tutorial that i have released in order for you to try out your own first ml deployment pipeline so watch out for the video so let's start with understanding what is machine learning models machine learning models typically are in order to understand patterns which exists in data and in order to apply the same patterns that it's learned from some amount of training data to future unseen data sets and the best way in which you can make your your trained model usable is if you deploy it deployment means that it is made available to a certain you know link web link so that other people then can you know log on to that particular website and you know put their own use case or put their own data set in order to see what the predictions are and in,358,0,0,IiZReWJ0b98
1,this case a lot of times it could be like an a b test scenario where people provide feedback if this model is any good or not now in realistic production kind of scenarios what happens is machine learning models are generally deployed as a continuous integration or a ci and a continuous deployment or a cd pipeline where multiple people are committing code to a particular github or maybe say gitlab or just a git repo and from that git repo a build is then created for this particular web app which is then deployed to the cloud platform so this is called the ci cd pipeline now in today's video what we will be looking at is let's say that you have a group of machine learning engineers and they have committed code to a github link and i will be providing you the github link that you can fork and use in order to understand how this whole process works so what we will be doing is we will be we will be going over this continuous deployment pipeline piece and we will be doing it on on gcp so if you want you can actually sign up for your own gcp uh you know account and there you can go through the systematic steps in order to install and launch your own first ml deployment pipeline and once you are there now you know what you need to do in a continuous framework in order to keep deploying code to a github account and then to use that code and deploy it as a web app we are going to learn about,358,0,0,IiZReWJ0b98
2,continuous ml deployment and i'm actually going to be using uh this particular um ripple that i have forked so ideally if you know this is the the main repo what you would like to do is you need to start here and you just hit fork if you hit the hit fork then it's going to create a fork for you so i have already um you know forked this particular ripple it's on my cloud i've actually made one small change so it is a little different and i'll show you uh what that difference is in a minute first things first you need to sign up for google cloud and for for google cloud again you get 300 if you just sign up you will have to start creating a new project first so you go up and you say hit new project and you know you give your details and you know you have to add a billing account to it like i mentioned you have 300 free to to begin with but as soon as you get you know your new project so this is the project my ml project this is where i i will be focusing so this has a billing that is already uh attached to it and here today i will be showing you working with two things first is google cloud platforms the cloud build how to build a continuous deployment in the cloud and how to see the service running in the cloud so these are the two consoles again i will be putting them in the description box below that is what we will,358,0,0,IiZReWJ0b98
3,be running okay so the first things first what we do is we will download this uh this particular uh you know uh folder we need to see if the app runs locally or not so we are going to cd to the to the app and again cd to the app files and here we are going to say python app dot py and as soon as this happens you will see that it says something is active at the local host so this is what you copy and if you paste it this says your first gcp web app runs congratulations right and so this is how you are you know getting a hold of the app and it is running now in order for you to figure out if your continuous if the health of this app is continuously fine or not we actually need to run this loop kit so what we will be doing we will be opening one more terminal inside this request underscore test and and now if i hit python loop underscore get dot py and this is asking if it's a local test yes it is a local test and you see this is actually running right and you say congratulations let's try loop post dot py is this local test yes and you see uh your your data was was output so this is the actually the outcome of your um of the of the ml model that you have so this is the prediction and you see this is telling that the app is actually running so at the local host level this is continuously,358,0,0,IiZReWJ0b98
4,running so you know that this app works and you can you have actually now uh checked it on the local system so now the job becomes that things are working in the in the local system we now need to run it on the global machine right now the two things that is important that uh that we understand that the two significant files that are that are going to help us is the cloud ml the yaml file and the docker file so let's look at the docker file first and i'll show you how you can actually generate a trigger point using either docker or the yml ideally yml is is more robust and then uh the you know docker file is is thereafter now what dockerfile does is it creates a containerized version of this app so that it can be uh you know uploaded as as one single entity because there are a lot of files right in this whole application so this is containerizing everything into one place and making it as an app so it's very uh very thin requirements uh it has and so that's the bare minimum format that it has and then comes the the yaml file so the yml file first it is it is giving you the steps so pull the container if it is already built in this case yes we have already dockerized it and then you are giving it a particular name and then what you are doing is is you are you are pushing this this docker uh you know with all the container tags and then you are calling,358,0,0,IiZReWJ0b98
5,it for a cloud run so let's uh go to this triggers uh the google cloud build and let's create a trigger and this is uh what let me call it first first ml pipeline okay and what we want to do is we want to look at push rates because pull requests there is nothing much you can do from it so that means whenever you're pulling from uh from a particular gate so here you really want to be looking at the push request because you're continuously deploying it to a particular uh you know application now the repository is you can either connect to a new repository if you say connect to a new repository it will let you authenticate to your github now i have already authenticated my my github so that's why it is already showing up so i already have the repository here and this is the ml model github and here of course the the branches will also show up so i write now i only have the master branch so that is all i want now if i give the automated configuration as auto detect so it is either going to focus on a yaml file or it is going to look for a docker file right ideally your yaml is always preferred over docker so i just hit create so now that this is this is run we need to do one more thing is in the settings we need to go and we need to enable the cloud run admin this all these by default you see they are all disabled so the only thing you,358,0,0,IiZReWJ0b98
6,need to enable is this cloud run admin if the cloud run admin is not enabled you will not be able to see the final outcome all right so make sure your cloud run admin is uh activated and once it is done all you need to do is hit run and this is going to run the trigger it will say that this branch has actually started and you can start seeing the the the building blocks as they happen so now it's it's done successful and you see this final url shows up this is the url corresponding to where your app has been deployed so it takes a few instance uh for you to see that the app is running or not you will see your first gcp application runs congratulations right and the same thing now you can actually figure out uh using your localhost as well python uh loop underscore get dot py is this a local test no now you're running it at uh you know at the local host so this is what you copy and you paste here and you see your first gcp application runs congratulations so successful the other thing you want to figure out is if the post runs as well or not so loop post dot py this is local now i need to put the address and see it it works so now you have officially uploaded uh or or built your uh your your web app and you have deployed it and you have done it as a pipeline sequence and now you can also check the history of the pipeline sequence,358,0,0,IiZReWJ0b98
7,you can check the history of triggers and you can you know run diagnostics on it from time to time to ensure your pipeline is running smoothly you,36,0,0,IiZReWJ0b98
0,foreign so let's start so we will be doing something called as deployment of machine learning models okay so before starting it that why we are doing deployment or why this is required let's try to see the content that we will cover in this particular session please ignore this ppt because this 50 was made on linux okay and i'm using back right now so the integrity problem is there but anyways the first thing that we will see is that why we need to deploy ml models okay what is the use case of deploying the machine learning models then after understanding that why we require to deploy the model the next thing that we will do is we will we will see what are the different different platforms that we have to deploy the model so premises versus iaas that stands for infrastructure as a service versus paas that is platform as a service and saas that is software and service so we will see what is the difference between the all three in a while okay then we will discuss a basic of what a life cycle of ml in production means okay so once we have the clarity that why we are doing the deployment and then we will use something called s plus so flask is nothing but it is a it is a python framework which helps you to make a website okay a web page okay that was a flask kick so the only prerequisite guys here is that you should know about machine learning algorithms and the second one is you should know about python that's it,358,0,0,bLJP-etT4Vs
1,nothing else this plus what i'm talking about right now i will be teaching you this from basics from zero okay so don't worry about flask so in the class what we require is that you should know about html page which i will teach you from scratch you should know about flask which i will teach you from the scratch okay so the first thing is we will try to build a small hello world app with class then we will try to create forms with class so forms is nothing but when you input something in the website how the data goes from a website to a backend that we will see here then we will try to run the ml model in flask and we will try to deploy the model on the three platforms that is gcp horiku and aws okay so now we have gcp aws now i will be not deploying on gcp okay and the reason is that to deploy on dcp you require a credit card and most of us will not have a credit card here so what i have done is so that's why i have included aws also so in aws you can use debit card also so why i am taking date aws so that we all can work on aws instead of gcp together but if you have learned about aws gcp is also on the same lines there is no difference not a single difference okay so that's why we will work on horiku and aws to reach here because we have to understand what is the meaning of deployment what is,358,0,0,bLJP-etT4Vs
2,the meaning of deployment so can anyone tell me what is the meaning of deployment please what do you understand when you hear the word deployment observe the problem is now for us or also man um you require a credit card okay so that's the reason so we have a constraint this that's why we'll be using only these two things for a time being okay now so what does it mean to deploy a machine learning model so what we have done till now is you have a jupiter notebook with you okay you you took a particular data correct and after taking that data what you did is you train the data right and after training the data what you guys do is you do a prediction but that prediction what you are doing it is for some use case right for example let's say housing housing price prediction now that jupiter notebook when someone wants to use it you have to send that file to a user to use it right yes or no everyone now let's say your mom or your dad has to predict the future price of the house by giving some details now you will tell them you will give the files first of all they will not understand what that file is all about when they will see the code they will not understand any piece of that thing in the code so do you think that is a better way of giving the models or files to for a prediction so what we have to do is so we have to make a website okay we,358,0,0,bLJP-etT4Vs
3,have to make a website so that what the end user will do is they will only try to input the data they will only try to input the data what is happening in the back end they don't know about it and they should not know about it because their main aim is to just get the future house prediction they have they are they are not interested at all that what is happening behind the scenes right so that's what you call as deployment okay making your model and delivering it for a production so that everyone in this world can use it efficiently with just one click that is what you mean by deployment so before starting the deployment let me open my onenote okay i will repeat again i think oh i am audible okay so basically what i was saying was that deployment is nothing but you put your model on the production now what is the meaning of production so basically making your model available to other systems on the web so that okay so that they can give the input or they can input the data and they can receive the prediction but right now right now what we are doing is we have a jupiter notebook with us and we write a program correct we write a program so once we write the program that is pre-processing and training and everything what we do is we do model dot predict we we give some data here and we give we get the paper we get we get some predicted values here but the point is if you try,358,0,0,bLJP-etT4Vs
4,to give the jupiter notebook to anyone else in the world will they will understand anything if they are from a non-boarding background of they are from other background they will understand anything yes or no no right so this this so what you will do here what you will do here is you will try to make a website you try to make a web application in such a way that you only tell the user that you put the inputs and click on the submit button and you will get the output so in that scenario they don't have to know anything about what is going behind the back end right that is what you mean by deployment so clear now what deployment means making your particular machine learning model available for the end users that is what simple simply deployment leads now now what is the life cycle of ml model can anyone tell me the definition steps that we have in machine learning model so i will tell you the starting point we collect the data that means collecting of data or data collection then what is the next step that we do after collecting the data yes you do something called as pre-processing then be something to eda that what exactly a three i said that exploited data analysis then then what we do now before that before that also we have some fish types right so feature selection we have to select some important features also right so suppose let's say if you have 15 columns then after that 15 columns which one are important you have to consider that,358,0,0,bLJP-etT4Vs
5,you try to do a trade split after doing the tensor split then you try to train the model okay once you try to change the model then you try to optimize the model by giving different different ways of doing it so changing the hyper parameter or a lot of ways optimizing the model and then you do the prediction of that so these are the all things that you have done till now everyone agrees on that okay so you have a csv file you do a pre-processing you try to train a model then you try to test it that is model dot uh predict and you try to test the model okay now now this is only the part one guys whatever you have done till here this is part one now what happens after prediction let's say you are satisfied with the accuracy then what happens is then comes the deployment part then after this you try to save the model you try to save the model so this is all together a new concept for you guys okay so saving the model which i will be teaching from basics that what you mean by saving the model okay and then after saving the model we will try to convert this into a flask app now guys this is the flask why we're using plus is because we know about python but there are a number of ways of doing it okay you can use any other web applications also in which you are familiar with but i will be going with flask because class is full pythonic and right now,358,0,0,bLJP-etT4Vs
6,everyone here is a python people so i will not confuse you guys with other things so plus and then we will deploy our aws slash foreign so this green line this green line what you can see it is the step for deployment which you guys have no idea about till now but at the end of the course you will have this idea about the greenland also so clear athira so where deployment comes into cycle now this is how it comes into cycle uh there are feature selection is nothing but let's say you have 12 features so let's say you have a name age and let's say some uh medical let's say diabetic values stuff like that now in this the name is not an important feature right because every person will have a unique name so this unique name is giving any output to the target variable so we try to remove this feature right so that is called as feature selection so we are selecting the important features that is called a switcher selection removing the unwanted features and only selecting the important features is called as feature selection did you get it now all right now before jumping into this part let's try to understand something about the deployment okay what are the different different methods that we have what are the different different servers that we have let's try to send that knowledge first okay so i will minimize this let's go to the ppt so this is a basically a life cycle of a machine learning model you have a data you data collected you train a,358,0,0,bLJP-etT4Vs
7,debug it and you deploy and test it so the first three shells that you can see planning and project setup data collection and labeling training and debugging this you do know last one the green box that is the part that we're gonna learn during okay now these are the different different services that we have so first is on-premises so in what we're doing on premises is let's say you are a startup okay let's say you have a startup and we have a team of eight members okay we try to make a machine learning model and we want to deploy it now we have multiple options right deployment means either you can use the service or you can use your own office space and build the service correct so that is what you mean by on premises so what you decide is no i will not take help of anyone all the data everything i will keep with myself so what you do is you set up everything in that office space that is your application your computer your data your middleware your operating system so middle so application is nothing but your application that is the app data is nothing but database or whatever you require to give to the uh give to the app to run runtime is for how many time that particular app will run okay so the application if you want it to run 24 7 24 7 if you want it to run for 10 hours then you have to give the runtime as 10 hours okay middleware middleware is nothing but when your front end,358,0,0,bLJP-etT4Vs
8,talks with the back end the between part is called as a middleware so i will not go into depth about this because this is of of networking topic okay which is not part of your thing right now then os then of course right if you have a laptop if you have a mac if you have decided that okay you have a laptop or a desktop so you have to give the os also right otherwise how it will work so the os also you have to decide then virtualization servers stories networking everything each and everything you guys have to do so when everything we have to do the good point is that everything is with us but what is a bad point here guys can you spot it out what is the problem that we will face in the future in on-premises small summary you have everything in your room and you have set up everything in your room when i say set up this green thing what you can see everything is set up by you individually so what is a bad point here what is the disadvantage here okay so do you guys know what virtualization is or what or networking is do you guys know about it yes or no two answers yes or no so the point here is you don't know about networking right so obviously you will hire someone for networking so if anything goes wrong with the networking they can handle that yes or no in a company in a company just think as a company guys think as a company so you will not,358,0,0,bLJP-etT4Vs
9,learn each and everything right you you have a application so of course for networking you will hire a people for stories you will hire a people for maintaining the servers you will hire the people correct so when you hire the people what increases the cost increases correct this is the first one then when you have so much things so you have to maintain these everything for that maintenance also got cost increases and that's the exact problem with the on-premises that when you decide that okay you will put something for your own self so when you're here for the first time it looks okay it's good but when you over a period of time you start facing the issue of maintenance a lot because if suppose your application gets a lot and lot of traffic then you have to increase the servers to increase the servers you have to put more man labor to put the more man labor more maintenance okay so the cost increases very very heavily okay and that is a problem with on premises everybody is clear about it okay so that's why the startups and everything know that they never set up the servers they always buy the servers from aws gcp or any other platforms because of this reason only because the maintenance of the server and costing of the server is very very high if you try to do on your own you will be not able to do is unless and until you are a unicorn company okay so what nowadays company do is they move to something called as i aas that is,358,0,0,bLJP-etT4Vs
10,infrastructure as a service for example gcp so i will go to gcp and i will tell them that okay i want to use your servers so what gcp will do is they will so again the same thing we have applications we have data we have runtime middleware and os so what gcp will do is that they will tell okay you deploy your applications but this green part is what you will manage that is application data runtime middleware and os so you have a control over this but all the other things servers storage networking virtualization this everything will be taken care by the gcp so the good part here is that you don't have to put or you don't have to look at this particular step if anything happens here it is a responsible of gcps responsible for it you are not responsible for it okay so you have to only take care of these particular part that's it that is what you mean by infrastructure as a service so infrastructure will be given to you you just use the service that's it so that is the difference between on-premises and iaas all the seven participants is clear till here the difference between the two run time is nothing but for how much time you want to run the application for example the runtime of amazon website is 24x7 correct the runtime of a ictc app is not 24 7 because from 12 to 12 10 they always switch off the website for maintenance so that is what you mean by runtimer theorem okay so guys difference between the two is clear,358,0,0,bLJP-etT4Vs
11,to you or not guys come on you have to response the answer can be either yes or no if no i will repeat it 100 times there is no problem i did a don't try to go in that depth okay because you don't know about these parts are the back end thing that we have so in short but what i will tell you is you have a backend you have a front end you talk between the two right so that is the middleware part so this is not part of your job this is a part of networking team so don't focus on this just try to understand what are the different different parts that we have the next thing that we have is platform as a service and the example of this platform as a service is foreign they will only give you the control for application and the data but rest everything will be controlled by hurricane even the os also so you don't have any option to select the os everything each and everything will be selected by hurricane so in a platform as a service you only have a control over your application that is a python file and the data that's it rest everything will be controlled by the holy group people this is what you mean by platform as a service so the control level in on-premises you have a control on everything on ias you have a control over these five things now in platform as a service you have a control over these two things okay does everything will be managed by the the,358,0,0,bLJP-etT4Vs
12,company from which you are buying the service is from aws gcp whatever you want to use then the last part that we have is software as a service okay for example zoom now so when you are using zoom do you guys have to worry about the application the data the server the storage the networking are you taking care of any of that when you use zoom as a user no right so for you it is a software as a service okay so a service that is given to you in which you don't have a control of anything you just use that as a software okay so all the softwares that you see for us for the end users it is software as a service like google drive it is software as a service dropbox software as a service google made software and service okay where you don't have any control you just use that software for your use that is what you mean by software as a service so these are the different different platforms that we have any questions please go ahead okay so i think there are no doubts till now so i will just move to the next slide so these are the different different examples that we have for different different platforms for example ies that is infosexual service you have aws you have micro azure you have google cloud engines then for pas that is platformer service engine and for sas all the all the things that you use right so cisco webex dropbox go to meeting so this everything comes into sas so these are,358,0,0,bLJP-etT4Vs
13,the different different examples that we have okay so what uh machine learning in production means so you have a historical data you try to train that particular model okay now you know this creation wala part okay this creation all apart you know which is the orange line now what you guys have to know is about a production wala part that is the blue line so what you do is you deploy the model and on the live data you try to predict the model so that the user can use that particular thing okay that is what you mean by ml in production so this orange part creation you know we will now try to learn how to do a production level tricks here now the first important thing is we have to see something called as saving the model so let's try to understand this concept first okay what a saving of the model means so what we do in the machine learning model we try to do the training and then after we try to get right into the prediction let's say let's say this whole code this whole code we use in the production okay training takes five minutes so have you noticed so suppose let's say if you try to train the model and if you try to creating the model the next day when you come you have to execute everything from the starting again have you noticed that in the jupiter notebook to do the prediction of some data so in the jupyter notebook you have written each and everything right when you close that notebook and,358,0,0,bLJP-etT4Vs
14,when you open that notebook you have to execute everything from again to do the prediction yes so let's say your training takes five minutes and your prediction takes one second so total time to do the prediction what it will be let's say you give the data okay you put this in a production your data is coming again the training is happening again the prediction is happening so at how much time the prediction will give to you to the user guys then i would like to move to coding okay if you're not responding anything okay so what i will do is i will directly move to coding now guys okay so basically the thing here is that we try to save the model okay now how we do is everyone please open your uh jupyter notebook no point of explaining because no one is interested i think so guys respond please respond otherwise i will also not feel any fun of teaching them if it is not understanding you can just say please repeat please repeat please repeat i will repeat it 10 000 times yeah so you can ask that right please repeat the question if you have not understood it it is for your benefit only guys i am going in so much depth because for your own benefit okay it's not for me it's for you so you should have taken advantage of this okay so you have done the machine learning projects right ml projects now in that ml projects what we do is we have a jupiter notebook and in this jupyter notebook first what we do,358,0,0,bLJP-etT4Vs
15,is we try to train the model and then after that after training the model you try to predict the model okay now let's say suppose this jupiter notebook you open the next day when you open the jupyter notebook the next day on the same data you again again you again try to train the model and you do the prediction yes or no yes so let's say the training of the model takes five minutes and the prediction takes one second so if you again execute the jupyter notebook so in how many in how much time it will execute for you 5.1 minutes so the point what i am saying here is if you type something on google and if it gives you the response after five minutes will you use google from the next time onwards so the same thing here also if you are putting your jupyter notebook entire code into production if you are putting the jupiter notebooks entire code into production then what will happen if some user is doing a prediction so it will always take 5.1 minutes to give the prediction yes or no because every time this piece of code is getting executed when you click on submit button so every time it will take 5.1 minutes to give you the output correct so the end user who is using your product he will he will use the product now he will not use a product right because he cannot wait for five minutes okay so basic idea is the basic idea is why we are training the model again and again why we are doing,358,0,0,bLJP-etT4Vs
16,that it is not required right so what i mean by saying that is let's try to start from basics so i will scroll down okay let's talk about the linear regression model okay so guys i am just teaching you the linear regression one more time just to give a glimpse about it so let's say you have x and you have y okay you have x and you have y x value is 2 y value is 4.,104,0,0,bLJP-etT4Vs
17,this is 3 6. 4 8. 5 this will be now what would be the y value for 5 10. what is the six what will be for six it will be 12. now have you noticed something what i am doing is when i wrote two of four okay your brain saw this for the first time when i see when i did three four six when i did three four six that is three into two six now your brain started fitting the model that okay this person is multiplying two when i did four of eight your brain got confirmed that okay it is if i give any value you have to multiply it by 2 and you will get the value of 5 yes or no so the first three data you took for a prediction yeah sorry the first three data you took for a training then you got okay the parameter is multiplied by two so now if i give any value i have to only just multiplied by 2. that is 10 and 12.,235,1,6,bLJP-etT4Vs
18,that is 10 and 12. correct so your brain got trained your brain got trade so see machine learning right what happens in linear regression is what is the formula for linear regression means foreign y is equal to m x plus c now in this mx plus c which one is the independent variable and which one is the variable which keeps on changing depending on the values so m x plus c what is m what is x and what is c x in the independent variable correct m is slow but in machine learning it is called as weight parameters and why c is bias or you can call it as a paid parameter okay so let's try to see that how this machine learning model works okay how we will get to know the value of these two so this is what we have to find out using linear regression so for the first time for iteration one was what will happen is y is equal to m x plus c this value of 2 will come in this location that is 2. what will be the value of m and c for the first time for the iteration what will be the value of mnc there should be some starting point right what will be the starting point foreign the regression model this value of m and c is random remember this value of m and c is random totally random okay so y is equal to let's say the random value of m is 0.1 into 2 plus the random value of c is let's say 1.,354,6,8,bLJP-etT4Vs
19,what will be the value of m and c for the first time for the iteration what will be the value of mnc there should be some starting point right what will be the starting point foreign the regression model this value of m and c is random remember this value of m and c is random totally random okay so y is equal to let's say the random value of m is 0.1 into 2 plus the random value of c is let's say 1. so what is the calculation that i get here so 0.1 into 2 is 0.2 plus 1 so your y is 1.2 this y is nothing but y predicted so you have a y predicted and for that particular y we have actual value that is 4.,172,8,9,bLJP-etT4Vs
20,so what is the next step that we do you have the predicted value you have the actual value what are the next thing that we try to do you try to find the error right so how to find the error you just do y minus y hat the modulus which is nothing but 2.8 so this is the loss that we have now once you get the loss what is the next thing that we have to do what is the job of machine learning model once you get the loss these are the basic questions which i'm asking which can be asked to you in an interview also and this is the first model that i am teaching you so you get a loss what is a job of a machine learning model now what we have to do is do this with this loss no this loss you have right so what you have to do with this loss you have to increase it or decrease this what is the job of this machine learning model so our job is to reduce the loss we have to bring this loss to zero at as much as possible so we have to minimize the loss so to minimize the laws which parameter that you have to play with here now to minimize the loss which two parameter you have to change mnc okay so these two parameters you have to change m and c but the question is whether i have to increase the value or decrease the value that's the question right because the changing of mnc can be in two ways,358,10,10,bLJP-etT4Vs
21,either you can increase the value or you can decrease the value now increasing the value will help us or decrease decreasing the value will help us how we will know that interpretation okay so whether to decrease this loss i have to increase or decrease the value of mnc so how do we decide that uh guys you have to start revising your topics okay if these basic things are not clear then there's no point of machine learning them okay so there's no point of just completing the course and just going forward you have to revise the topic you have to give you 100 here okay because these basic things you guys should know so there is something called as gradient descent okay so the weight updation formula anyone knows what is the weight updation formula w old sorry read new is equal to w old minus some learning rate derivative of loss with respect to derivative of wold okay this is a weight operation formula that we have and the gradient recent graph if you remember it is like this okay and this is the global minima that we have so your job is somehow your particular weight whatever your weight here is somehow it has to reach to the global minimum but the question is whether we have to increase or decrease it right so here comes the mathematics so if your point is here so slope is positive slope when the slope is positive anyone knows what will be the derivative as a whole it will be negative or positive so remember this when you have a positive slope the,358,10,10,bLJP-etT4Vs
22,derivative of that particular slope will be always positive it will be let it be any value i am not talking about the value i am talking about the sign anand can we use is not a question for every algorithm gradient descent is a method that is used to optimize the weights it is not so you have to use given descent so can we use is not a question it is gradient descent you it's used behind the scene which you don't know but it's used behind the scene okay so otherwise how you will know the weights here correct so positive negative wold okay this learning rate is some random value small random value so that you can fasten the speed of weight of optimization but the overall point is w odd minus plus value so the final value of wold will be increase or decreased the final value or the new value will be decreasing from wold or increasing from the voltage so you have w world minus plus is minus itself w whole minus v from the old weight you are subtracting something so your w old weight will be decreasing so that's why if your weight is here it will decrease your weight so that's how it gets to know whether i have to increase or decrease it okay so now it will go to iteration 2.,302,10,10,bLJP-etT4Vs
23,iteration two okay for iteration two it will now move to one one into two plus one the answer of y hat is three again you will do the same thing you will try to calculate the loss the loss is what loss is 1. so you have degrees the loss right then it will go till end of the iteration and at the end it will find out that the best value is 2 into x plus 0. after doing a lot and lot of iteration the best value of m is equal to 0 and c is equal to sorry m is equal to 2 and c is equal to 0. so now you put any value if i put 6 you miss you play six here what will be the value of pi as a predicted value if you put a new value 6 and you have a formula 2 into x plus 0 so what will be the output of y it will be 12 right so your model also learned that 2 is the important parameter here so if you take any value and multiply it by 2 the answer will be 12.,257,11,14,bLJP-etT4Vs
24,now you again train that model 100 times you will always on the same data if you are training that particular model with the same data at the end you will always get that y is equal to 2 into x plus 0 yes same data same order you again train it at the end you will get only this equation right thank you with the same data with the same model you run it 100 times you will get this equation at the end right okay if you run the model this hundred times you will always and always get the same formula okay now this formula if you are getting the same formula 100 times if you execute 100 times or 200 times then why we need to train the model again and again just try to save this particular formula somewhere in the model just try to save this formula right so we don't need to train this data again and again just try to save this model just try to save this model and you just move forward okay so next time when you are doing the prediction you just substitute the value here so let's say user is giving 100.,266,15,15,bLJP-etT4Vs
25,100 of x is nothing but 200 that's it how simple it is right and that's what we called as saving of the model okay now what we will do is what we will do is we will try to take one step further now once a further in the sense now we will not using the jupiter notebook because in the in the industry level we never use jupiter notebook we use some kind of ide that is integrated development environment okay so the famous one that we gonna use is something called as vs code that is visual code editor so everyone please download this visual code so visual pose editor okay i am giving you the link open that link and download for your respective laptop so for example it is mac os so download for mac os if it is windows download it for windows and if it is linux download for linux download download it and install it okay once it is done please let me know i will move further okay so if everyone has done this see unmute for participants okay i will do that what is the request then don't ask anything okay in the middle when i'm teaching something okay that's the only request after that teaching when asking it out you can ask me hundred dollars no problem but in middleton okay that's the only request okay i will mute all okay good so once you have done this okay so once you have done this so i will uh so you have to open the vs port okay so people who are in windows just,358,16,16,bLJP-etT4Vs
26,search here visual studio code so this is for mac for windows in the windows search just search for visual studio code and click on that and once you click on that this is what will open on your screen please repeat the derivative part in the so what happens is uh this is the formula that we have okay w nu is equal to w volt minus the learning rate derivative of loss with respect to derivative of old okay the weight voltage now let's say you're pointing somewhere here so when your point is somewhere here you have a slope derivative is more derivative is nothing but the slope right so the slope is going from low to high when the slope is going from low to high the slope is always positive so when the slope is always positive the derivative of this pole equation will be positive okay so when the derivative is positive so if you multiplied any small value with a positive value it will be the whole positive value yes and if it's the whole positive value if it is a whole positive value then w over minus of positive is what minus of positive is negative only right so at the final your weight will decrease so your weight will come down so yeah so this is this is the weight and this is the loss so weight decreases when it is here so that's exactly what is happening here okay okay okay so now let's try to understand how this visual studio code works now in this visual studio code on the left hand side you can,358,16,16,bLJP-etT4Vs
27,see some tools on the top you can see some tools now left hand side if you want to open a folder you can click on open and you can open any folder of your choice okay any folder for example i will come to uh tutored oh this is a new batch right so i will create a new patch deployment okay once you have created a folder then just click on open and once you click on open that particular folder will get open for you again i will repeat on the left hand side when you hover on this you can see something called as explorer click on that you will get a button called as open the folder when you click on open the folder this particular folder we get open so how to verify if the folder is open on the top you will see something called as your folder name deployment see this is a folder name so everyone do this so the beautiful part about this vs code is that you can control each and everything with this single thing you don't have to switch to any of your window explorer or mac explorer okay so what i mean by saying that is if you want to create any file under deployment you have to click on this plus sign okay click on this plus sign once you click on this new file sign it will ask you to name the file so what is the extension to name a python file guys dot py correct so what i will do here is test dot py okay once you,358,16,16,bLJP-etT4Vs
28,click on that test.py you will see that editor will open on the right hand side and a cursor will be blinking here so let's try to write a basic code okay that is print hello world let's try to see if this basic code is getting executed or not now one more important thing are you guys able to see this big white dot on the screen which you can see on my dot uh on my screen also this big white dot what you can see this indicates that your file is not saved okay so you have to save the file how to save the file you have to do control plus s control plus s will save the file control plus s will save the file for you okay now every time saving the file becomes hectic right because sometimes what can happen your computer can shut down directly or due to some reasons right so it is always advisable to always go to file and click on auto save go to file and click on auto save so what is auto save will do is it will try to save the file automatically what you write so once you have written it let's try to execute this file okay to execute this file what you have to do is on the top hand side you will see something called as terminal so click on that terminal and click on new terminal once you do that a terminal will open below of your vs code at the bottom our turn in terminal will get open okay so if you want to execute,358,16,16,bLJP-etT4Vs
29,this python file so i don't know whether you know this or not you have to write a command so python space followed by the file name that you want to execute so i want to execute test dot py right so you have to write here test dot p y and execute it and you will see the output in your terminal so your vs code if if you can see the output congrats your vs code and everything is working perfectly fine if you are getting any kind of error please let me know perfect excellent okay so we will be using this vs code from the next time onwards to do the all sort of coding so that you guys will understand that what happens or how do we code in the real time okay so what i will do is the saving of the model i will teach you in the next class but before that try to understand the logic that notebook no now forget about that notebook because that was only and only for the understanding purpose in the real time when you go to production level things know that notebook never works because we have to work with dot py file in the production and that is the reason that for initial purpose to understand it it's good but once you move to a company then you use the ids you cannot say that i know jupiter notebook and work on jupiter notebook no so this vs code or any other ids like pie chart or whatever you want to use you can use it but not jupiter notebook,358,16,16,bLJP-etT4Vs
30,so that's the reason behind it so that i can give you the exposure of all the other things also so that is the reason that i am not using jupyter note okay so guys i will stop the session now bye bye everyone take care enjoy your weekend,63,16,16,bLJP-etT4Vs
0,modern deployment is simply the engineering task of exposing an ml model to real use the term is often used quite synonymously with making model available via real-time apis building a model is generally not the end of the project in this model deployment tutorial you will learn about one of the web frameworks used by python confluence further you will understand how one can do machine learning model deployment using flask and finally how model deployment takes place using heroku so let's get ready to understand the real application of machine learning which is model deployment music if you haven't subscribed to our channel yet i want to request you to hit the subscribe button and turn on the notification bell so that you don't miss out on any new update or video releases from great learning if you enjoy this video show us some love and like this video knowledge increases by sharing so make sure you share this video with your friends and colleagues make sure to comment on the video for any query or suggestions and i will respond to your comments all right guys coming to introduction to web frameworks now web frameworks are these really powerful entities which we've been uh which we've been using the products off for a really long time i can assure you that now to give you an overview of where you might have been in a situation where you've used the product built out of a web framework let's just think of a situation where you know you may be swiping on your instagram you're taking a break from work you're relaxing and you're,358,0,0,ZRnTp4V7i_E
1,just trying to open the uh open the application instagram you're trying to swipe you're trying to refresh you're trying to watch release you're doing a lot of these things right now if you've ever been very observant with any of these social media sites like snapchat facebook or instagram or anything it's reactive to what you do right it will generate content or it will showcase content in an order based on your activity in the app based on what you like based on uh it will not show you things that you're not really into right if you love reading books and you keep searching books on instagram for some reason it will only start recommending books to you right it's not going to recommend some horror documentary for you to like see posts or facts and that no it doesn't happen it might happen in one or two cases where instagram is trying to check if you'd be interested in that but at the end of the day it uh you know instagram is an application it's not static it's not just one uh uh you know one page or one field that is showing the entire world impossible right even though it's the same content let's just say that there's a celebrity who posted something of course it's going to be the same for anyone watching uh through the instagram app anywhere across the world but at the end of the day how it is served to you how you actually get to see it on your phone and how the content how instagram basically decided to show you the content is,358,0,0,ZRnTp4V7i_E
2,going to be very very different right sometimes it might recommend it there are other times where someone might have shared it to you there's many many ways and you have to understand that in the back end for this to work for something to be dynamic for something to be actually analyzing and assessing what you are doing and then reciprocate to that and then add value to that right that's how these social media sites become really engaging we sort of feel very comfortable in social media because of this exact fact that whatever you are doing it feels like you're talking to a computer you're working on your phone there are many many intelligent ai based bots and there again technologies like web frameworks and lot more which are making your experience feel so personalized and feel so really well that you would love to do more of it right now think about the technical aspect of all of everything that i have just said it uh all the data that instagram has to show you or any other app for example it has to retrieve everything from servers which is stored around the world and eventually it have to query these servers for data it'll have to receive certain data to show you the data now you will go on to interact with the app you're texting a friend you're sending memes what not right so it has to take all these data back and update it back in the databases to see what your activity is what you're doing and what not right now this becomes really really really important for,358,0,0,ZRnTp4V7i_E
3,you to understand because implementing a concept such as these is what the challenge of web frameworks actually solved for us there was a time when we had a very split division between static websites and dynamic websites static websites regardless of what you would do it had its own set of code it would work only one single way for anyone in the entire world dynamic websites however are very different they can be reactive and they can focus on uh they can focus on customizing your own experience with respect to the brand the product e-commerce site whatever it is that you're using right you'll be surprised to know that uh you know even if you're not a shopper even if you don't consider yourself to be a person who shops on the internet uh you will be very curious to understand that there are almost everyone in the entire world out there who are trying to offer products or services that they use web frameworks are there right now uh the most important thing was this question right what if your instagram app or any other app could never do any sort of real-time processing or real-time data retrieval it would be really boring right maybe if i post something on my instagram today and you get to see tomorrow morning because the servers had to upgrade overnight or something like that it wouldn't make real sense i mean of course it has its own vibe and all of that but at the end of the day it really doesn't make sense right but what you have to also understand is that that,358,0,0,ZRnTp4V7i_E
4,approach where things are static right when things are static they work really quick and they are a lot lot simpler to not just build and create but to maintain and manage later down the line as well right it actually is very simple it actually is very fast to work with but the downside is that you lose a lot of personalization and you lose a lot of features a lot of almost real time or actually real-time features which again do not make sense especially if you're a person who likes to watch cricket or anything on your mobile phone and imagine that there's like a one-hour delay between the actual match and your phone do you think you'd still be interested and you'll be all hyped up not really right you'll definitely move to another application which might do things quicker than these ones so uh even though certain in certain cases web frameworks are associated to being actually slower but you have to understand that there is so much complexity that's going on in the background to retrieve multiple points of data across multiple sources and eventually bring it back retrieve it and it's not just it's bringing the data back to the client side and it's just holding it there right it has to serve the data to you in the right way so there has to be logic written to ensure that the data gets served to you not just served to you but served to you uh in a way that it's that makes it correct right it's not like it's just throwing random database queries at you and,358,0,0,ZRnTp4V7i_E
5,saying hey here's the photo that you wanted not really it doesn't work like that right now one really important thing that everyone has to understand about the world of web frameworks is that whenever you talk about web frameworks there is usually two big big divisions right uh every web application ecosystem in the entire world has two sites it's called as a client server architecture you might have heard of it where a lot of processing happens on the server side and only the information that's required by each of these individual clients basically what happens is that these clients keep pinging one common server right they can be 100 clients there can be thousand clients now for a fact you might know that there are millions of users right in fact i think facebook had a couple of billions of users or something so they cannot have one server for everyone they cannot have a billion server around the world and you cannot get your own customized server where all your data is stored not really right so a lot a mass of people connect to one particular server and you know with these big companies it is never one server they usually have one or two servers in every city with almost every countries around the world so it is diversified but not to a point where each of us get our own servers right that hasn't happened yet now an important thing you have to understand about client side and server side functionality is for the fact that it is very very very different a server is a very powerful machine,358,0,0,ZRnTp4V7i_E
6,which has the capability to not just serve one person it has the computing power it has the processing power and at the end of the day it has the capability to serve a lot more people right so what happens on the client side is very simple whenever there is any sort of interaction that the client does when i say a client that is basically you sitting uh you know wherever it is that you're watching this particular course right now you are a client because the video that you're watching right now is stored in a server and it is being fed to you it is being served to you like how food would be served in the restaurant right think about it that way so what happens is that the code gets executed on the user's browser and in your case the browser might be chrome firefox or whatever it is and it gives you an user interface right now wherever you're watching this course there is a user interface around you uh you would have got you would have come to our website to actually go on to watch this particular course right great learning academy similarly uh this thing all the data of great learning academy is impossible to be stored on your laptop my laptop or hundreds of laptops because we have a ton of content right now all that content is basically stored on our server and a lot of magic has to happen on the server side as well when you take a look at it your client just asks questions and it gets all the answers,358,0,0,ZRnTp4V7i_E
7,from the server now to ask questions and get answers there is one step in the middle which is missing which is basically processing right which is basically your uh whatever questions you ask there has to be some sort of thinking there has to be some sort of analytical application that is going on in the background uh in the back end to attach logic to your question and eventually serve you with a particular answer now all the data that the client requires can actually be stored on the server any and every data that is required to design the system that is required to build your application that is required to host your application and even to the capability of you testing your application everything that's needed for everything that i just said will be stored on the server now there are multiple softwares called middlewares wherein uh you know your client side will have a capability to talk to your server side very easily it's not as easy as connecting one wire and be done with it well in today's world of internet it actually is but at the end of the day there's a lot more complexity that goes on in the background but what you should know at this moment is whenever you're trying to use a web application you have to understand that whatever you are doing you're you're as a client and every single interaction that you do can actually be recorded and your behavior your analysis the products that you buy the way in which you buy every minute thing can be in detail assessed because all,358,0,0,ZRnTp4V7i_E
8,your data can get stored on the servers as well and of course server is not just trying to save your data right it's not trying to steal your data or anything as such it's just trying to do it to probably help you better make better recommendations or anything be it on amazon be it on netflix be it anywhere uh across right so it becomes very very important for you to know if that's the case then what is the definition what is the actual definition of what a web framework would be well a web framework is basically a collection of various tools guys we consider it to be an architecture because at the end of the day it provides you a ton of tools it's not just one tool that you go about using it's not just one technique that you're using here it's a variety of technique variety of libraries and there's so many things that you're given right think of it like a bag of goodies uh your bag of gifts for a developer uh that you can actually use to develop your application right now everything that you would require to build and maintain even extremely large online applications it's actually all the core functionality is provided by these web frameworks all you have to do is you have to build your own logic on top of it add your own flavor to whatever product that it is that you're offering and eventually get it out there right now you might be asking a question at this moment of time saying hey you know if everyone is just building,358,0,0,ZRnTp4V7i_E
9,their own products they can just do it in their own ways why do you even require a framework to go about doing it one really important thing as a developer as a programmer or as a web developer uh you have to understand that writing a web application is not really simple it is an extremely complex task now usually there's a lot of code whenever the term complex is thrown around in the world of information technology right so what happens here is that the advantage that web frameworks provide to you is they're like these helpers which will make sure that you write a small amount of code you writing this much code will have this big of an impact overall right that is again one of the really big reasons of why you would want to consider it if you are a programmer and if you're trying to solve an application if one guy can do it in 100 lines with the use of a web framework the other guy has to write 2 500 lines of python code which guy would you prefer to be a simple question right so code reusability you write the code once and you use it multiple multiple times anywhere you require and this kind of an approach not just makes your task as a programmer or as a developer faster but for the actual program itself that it is running since the code becomes small and it knows that it has to reuse certain parts of it think of efficiency and think of how fast the website or the web app will be able to,358,0,0,ZRnTp4V7i_E
10,run now because it doesn't have to do a lot of processing right i'll give you a simple example this turns me down a lot but let me know your opinion whenever you try to open a web application whenever you go on to try to uh you know open something let's say such as netflix or prime or anything just giving you a hypothetical example if these websites right they load lightning fast let's just be honest with each other it's never that they are the bottleneck right if something is slow 90 percent of the time we say hey our internet might be busted or something like that but when you think about it if these websites were really slow let's just say i open this website right now and it probably takes even somewhere around 30 35 seconds to open right it's not really a lot 30 seconds is not a lot of time but at the end of the day if you actually understand and analyze it we are so used to having really fast internet and really fast technologies around us where waiting for 30 seconds to open a website that that website will lose the customers business right because there's someone else where you type in the name hit enter boom their website is ready for you to see even a matter of 30 seconds now my entire sentence lasted 30 seconds here and you probably didn't think that 30 seconds is a lot of time but loading times uh if they are shorter they lead to increase in revenues worth in millions of dollars and if your loading time,358,0,0,ZRnTp4V7i_E
11,becomes high or something is something is fishy something is not right that will just tank your entire company's wealth it has happened in many cases you can actually google about it there are a lot of companies who have lost a ton of business billions of dollars worth of business just because of the fact that uh you know there's a person there's a client who went to a competitor because something was slow something was not right with their uh web web application or whatever it is right it is that critical so code reusability becomes a very very important part of it now an important part that you have to understand about web frameworks is that it does involve a lot of coding right now when you have to create an entire backend when you have to create an entire server and ensures that it provides everything that is needed for the application to run you require some sort of a server side language it's called scripting language or whatever it is where you have to sit and code the server you have to teach the server what it has to do and how it has to be able to serve these clients right now one advantage that you get with the frameworks is that python is again one of the world's most popular programming languages that we have today and it is providing you complete support to be used with respect to frameworks such as django and framework such as flask for this exact purpose so you're already taking a very complex process and simplifying it using web frameworks now i'm telling,358,0,0,ZRnTp4V7i_E
12,you hey you can further simplify the usage of this web framework with an easy to use programming language such as python to help you completely get up and get started with whatever application that it is that you're designing i don't think anyone would say no to that right because that at the end of the day is the entire essence of how you can simplify something really complex and yet at the same time create a beautiful application or a beautiful solution to a problem using that right fantastic guys so that was the introduction to web frameworks in this next section we're going to take a look at the introduction to flask now flask as a library on its own is a beautiful framework to work with the best part about flask now i know that some of you all are thinking oh so flask can do a lot so it must be a very heavy uh framework to learn it must be really complex it must be very hard and all of that well to your surprise i can actually tell you that flask is a very lightweight framework that has the capability to uh in fact it provides a wide variety of tools and techniques through which you can use to build any application that you want now some of you all might be thinking oh can you only build a really small application so how is it well you can build an application as small as you want or an application as large as you want as long as you have the hardware to support everything that you're building flask,358,0,0,ZRnTp4V7i_E
13,will always be there to ensure that you can do something faster better the best part is you can do all of it in an easier manner right so uh how is flask so lightweight a web framework has to be very heavy and dense because it consists of so many things the advantage that flask has is that its entire core functionality becomes light because of the fact that it depends on two other libraries to help it work right it's not like flask is working standalone flask asks the help of two other external libraries it's called ginger to or ginger to however you want to call it and it uses a template and there's something else called as the worx you wsgi toolkit to function as you can see it requires the power of a templating tool and it requires the power of a tool kit to actually complete its own uh architecture its own cycle of usage and eventually through these we make a lot of sense and we uh you know at the end of the day have a lot of capabilities to build tasks easily right now i know i know for a fact that all of you guys are saying like okay so what does wsgi works you ginger too like you've never heard of it uh probably and you might be very very confused at this moment to see these new fancy letters and terms well let me tell you that wsgi basically stands for a web server gateway interface now what it is used as is basically a standard it is used as a standard for the,358,0,0,ZRnTp4V7i_E
14,creation between an interface that exists between your actual server and your web application now again let me quickly take you back to that instagram example every time you open instagram do you see a bunch of code do you see a lot of servers do you see a lot of databases not really you actually see a web application and all that data it is getting everything is happening in the background right whatever you see is what the app wants you to see as an user interface but uh there has to be an interface which brings data from the server to your application and that is what wsgi is actually responsible for right fantastic now the next key term that you would have not heard is works you works you basically is a toolkit that is used to implement the concept of request and response and many many other utility functions as well now this becomes a foundational concept because let me tell you this when you open instagram and let's just say you're trying to stalk your best friend or someone you have to enter their name and hit search so basically what you're saying is hey instagram i request you to basically show me the details of this person you send a request to the server saying hey i need the details of this person and what does the server say if you are authorized if you if you follow that person if that person has a private profile whatever it is if everything is all right the server will say hey thank you for your request here is my response,358,0,0,ZRnTp4V7i_E
15,now here is this person's profile sit and stop till you are happy something like that right so when you have the capability to actually tell the server that i'm requesting something and to get something from the server as a response back and many many other utility tasks works you is very very critical in that as a library right and then we have ginger too ginger to basically is a templating engine that's built for python now whenever you say a templating engine think about this right uh whenever you hit refresh on a web page whenever you move to another uh web page in a website or whatever tasks that you do on youtube or wherever it is you will see that every time you try to do something most of the elements there will have to be re-rendered right it's very simple go to youtube right now keep hitting refresh multiple times the entire interface of youtube will not refresh right it's only those videos which are refreshing everything else is remaining constant because you're not trying to refresh the entire architecture of youtube that you're just trying to refresh the videos that youtube has to show to you so this is a bit of static and there's a bit of dynamic flavor that's present in the web application there and all of these usually happen whenever you make use of templating engines and ginger 2 is beautiful at doing that especially if you are working with flask right so i hope with this slide you're very clear about these very three terms which might seem really funky uh to a beginner,358,0,0,ZRnTp4V7i_E
16,because trust me when back in the day when i had to learn these terms in advance when i saw this for the first time as well even i was wondering well whoa what is wsgi why have i never heard of it right well you don't have to face any of that right now because you guys know what all these are so the next question one might be asking is saying okay so for what tasks would you go on to use flask there are many many tasks when you see building a web application you know you can work on supporting secure cookies your cookies don't have to be you know unsecured anymore you have to if you understand the importance of cookies you will know how important this first point is now whenever there is any sort of situation wherein there is a requirement for fast debugging you create an application and you have to take it live before you take it live what do you do it goes through rigorous amount of testing right now testing is a process on its own and debugging is another headache of its own our web framework is considered very good if it can provide you fantastic methods to not only just build your application but at the same time provide you ways of how you can debug this application right let's be very honest here even the most uh uh you know really talented and really amazing programmers among us we make a small amount of mistakes every now and then right every time you run a code it it is going to throw,358,0,0,ZRnTp4V7i_E
17,you some error it's going to be a very silly error or a very complex error but you do get that error and you have to solve it you have to fix it how is the web development framework helping you solve that well flask seems to be doing it at a really good pace third point says ginger to templating we already took a look at ginger to templating and we understand what it does in terms of rendering these dynamic web pages for us and whenever you have a client or whenever you have a requirement that says hey the web application i want to build i want to take a modular approach to building this tool it's not like there is an all hands collaboration everyone is just working on it happily and one day you have not no product and three days later you have an entire product ready no it doesn't happen like that right anywhere across the world across any domains what happens is that you have a very structured way of taking it really slow breaking a huge complex problem down into what into really smaller chunks of uh you know chunks of problems and trying to attack them trying to solve those and eventually once you start solving all these tiny problems it will lead to a very big solution when you have solved the overall problem that was at hand right that's a modular approach you take to designing your web application i made it sound very simple at this moment of time but understand that when you are designing an app yourself you are going to,358,0,0,ZRnTp4V7i_E
18,require uh support from a web framework that provides this modular approach and it's a very very very important point that you guys have to understand there are concepts where you can have an api an application programming interface basically wherein uh you know you can work with a website and you can give a lot of functionality to your customers your people to your prospect who are coming to your website right so think about a situation wherein there are new people who are coming onto your website and let's just say you're an e-commerce website or something like that what is the first thing that you would suggest of course you show your website show all the products the next thing that you would do is get you'd get them to create an account for themselves right wherein they can track all the orders that they have made you can take a very detailed look at your own self your own profile that is basically giving you importance in that particular product you can implement all of that easily with respect to flask and uh you know whenever uh you provide the concept of logging in and logging out users will have to register right it's not like anyone else can log into anyone else's account it wouldn't make any sense if that was the case so for logging in and logging out you're gonna have to be registered and to be registered how you get your user id you get your password uh if there's a profile section you can go on to set a lot of things in your profile and you,358,0,0,ZRnTp4V7i_E
19,can do a lot of things basically right that capability where does where do you think it comes from it comes from all of these uh parts where you're constructing an api for a web application right and wherever you think about providing any sort of additional functionality for registered users now if i just go to a website as amazon right now the web app is still being served to me but maybe if i log in there are different things that will be shown now if i just go to their website if i open it in incognito mode there's not a lot of things that it's going to basically uh you will not be allowed to do a lot right i don't think you can probably buy a product without create an account i'm not really sure on that but there are certain advantages of having a registration right so be it in terms of you as a customer using it or be it in terms of a person who has actually provided you the web application trust me it makes it easier for both the parties that's involved right fantastic now uh there are certain prerequisites to learning and using flask and there are two really really important prerequisites the first really important prerequisite sits with the fact that you have to have working knowledge of python right now flask is a web framework that is popularly used in a programming language like python so if you are brilliant at flask but yet if you cannot implement it in python that would that would be slightly problematic right so you're gonna have,358,0,0,ZRnTp4V7i_E
20,to have some sort of basic understanding in python and then you get to start working on libraries so uh for this course do not worry if you do not have any previous experiences with python or flask it is actually sort of okay because at the end of the day we're not really seeing a lot of code or i'm not walking you through a web application at the moment so the goal here is to just to get you guys comfortable with flask right now take a look at the second uh prerequisite the second prerequisite says you require hands-on experience with html yes this is definitely a requirement now whenever you're working with web applications whenever you're working with websites html is 99 of the time the foundation that the entire website gets to be built on the entire skeleton for the website so it really becomes important that you understand this because again down the line uh through the course you will see that this is actually a very very important requirement out there right so guys with this we have completed our introduction to web frameworks we know a good bit about them and we have had an introduction to flask and we understand a good amount of flask as well coming to a section where we will be discussing how you can install flask it's actually very very simple guys i have only one slide to show you guys to install flask because it's really that simple first step is you have to have python in your machine right so make sure that you get the latest version of python,358,0,0,ZRnTp4V7i_E
21,installed for whatever operating system that you have you you should know at this moment that python almost supports every single operating almost every single operating system that's out there so install the latest version which is required for you and then you're going to require something called as an integrated development environment it stands for an ide in short uh you know something like sublime text something like pie chart visual studio code google collab whatever it is right you can have an environment where you write python code and once you've set that up you have to create something called as a virtual environment in your project here's where things gets interesting the way that flask works right uh and the way that python talks to flask and your overall uh thing uh has to be involved you're not just using flask and python here you'll be using a browser you would have had a server you'll be using the server client architecture that i just told you if you do not create a small virtual environment for the code to work in any changes that you might actually make will interfere with the operating system itself yes uh that's the reason we create a virtual environment where any changes that you make is bound only to those uh that particular environment it doesn't affect or hurt anything in your real operating system and i think it's very important for all of you all to work with that right so you have to create a virtual environment to basically install flask it's one simple command pip install plus you already know this moment of,358,0,0,ZRnTp4V7i_E
22,time if you use python that pip stands for uh pip installs packages or it's a package manager that comes bundled with many operating systems that's out there and one single command will basically help you set up and install this entire web application framework and to see and to check if you have actually basically installed it correctly and all of that here's a small piece of code you can find it in the official documentation as well uh you have to just run that piece of code and if it works fine without giving you an error it you have flask ready right now this simple application all we're trying to do is we're importing the library we're creating a small app we have a root which says uh which consists of a function right we're not trying to create multiple web pages we have just one page which shows us hello world so every time this is created and it is running it's a constant thing it's a server that has to run in the background right and you can look at this here at the bottom it says hey i am running the server on this particular uh this particular web web page this particular ip address right all you have to do is it's basically self reference 127.001 is basically local reference that it provides and if you just click on that it'll take you to a web page where it is where it prints hello world or whatever it is that you're trying to return with your hello world function right so one simple piece of code again three or,358,0,0,ZRnTp4V7i_E
23,four lines piece of code extremely simple and if you have your flask set up really correctly all all it takes is you to run this piece of code get your uh server setup and eventually uh after you've run your setup you have to understand that it's a server client architecture right when you open this website you become the client but on the back end the server should be still running to show you that hello world right so this is a constant thing that keeps running and keeps running and keeps running and when you decide saying that hey okay i'm done seeing hello world on my webpage and you want to stop your server you just get to stop this piece of code and it will work right so guys again as i just told you two simple slides of how you can install flask and get away with that now we'll come to a quick section where we'll try to understand a little bit about flask one important question you have to always think about flask or any other web development framework is how easy is it for you as a developer to go about performing or tasks such as debugging debugging is very very important as we have already discussed at this moment of time and whenever you have executed a sample piece of code right the development server that you are basically working on it will not work in debug mode it will be working in production mode it will be actually thinking that you have your app ready and ready to function when you are saying hey this,358,0,0,ZRnTp4V7i_E
24,is not ready for launch i am trying to debug it you actually activate something called debug mode and activating debug mode does a lot of things for you the most important thing is it enables automatic reloading now whenever you do any sort of changes to the code as soon as you come to your client set you open the web page it it it is ready for you to see you don't have to rerun the code you don't have to do a lot of different things you don't have to keep refreshing it 100 times for every single change that you do that has to be really advantageous right automatically load you change something in the code open your client boom changes are shown this is huge hugely advantageous you know that huge if i could just say if i could put it in caps on the screen trust me i would now what it also does is activate something called as the python debugger and as you already might know python debugger is very critical when you're working with python because it will give you a chance to take a look at what is happening there's a concept called variable explorer where you can understand what happens to what value stored in what address for what variable at what point of time that's such a complex sentence that i just said but the level of depth you can go to check what your variables are doing is only possible because of the fact that you have the python debugger there right now how do you get into the debugging mode that will,358,0,0,ZRnTp4V7i_E
25,be a very important question you're asking and again if you are running your server right now and if you're just happily printing hello world all you have to do is stop the server run this piece of code uh in python flask underscore env equal to development flask run all you're trying to say is hey we are going to the debug mode right now and uh you can you know bring all the advantages of debugging that flask provides along with the feature of automatic reloading as well that is debugging in flask the next thing is using html templates now uh that that hello world thing that you would have printed and if you haven't done it i really recommend that you pause this course install flask and it takes five ten minutes at max for you guys to actually go on to print that hello world to make sure you guys are set up right now when you have a simple application such as that which is displaying only one simple message you're not really using any sort of html attributes out there but do you see a hello world every time you open instagram not really right you see a lot of complex stuff and all of these complex stuff that's being served to you basically uses the concept of html to display the information in many cases as well there's a helper function that flask provides it's called render underscore template and this function just talks to the entire ginger to uh template engine and it brings forth this capability for you to display any information that you want uh,358,0,0,ZRnTp4V7i_E
26,and serve it in the form of an html file now the advantage the biggest advantage that happens whenever you're doing that is for the fact that you can write html code and you can save it in dot html file and you can use this dot html file to not just write the skeleton of your web application but you can write a bit of functionality you can add maybe one or two bits of logic there as well that is how important html is to flask and alongside debugging and alongside html you can already see that flask is actually punching way above its weight in terms of the capabilities that it is providing to you right and then the next thing is setting up a database now ladies and gentlemen uh beat flask b django beat any web application development in the entire world out there your data has to be stored somewhere it's very easy to say that hey you just throw your data in a server what do you mean by that your server is not only providing compute capabilities but it is also giving methodologies to store data in a structured manner so a database will give you the capability to store almost all kinds of data that's being used by the application you can have something called a structured query language you can have these sql and no sql databases through which you can integrate it maybe something like sqlite or something and eventually interact with the database easily but you require any data at any moment of time you get it right that's how simple it should be,358,0,0,ZRnTp4V7i_E
27,and one more important thing which goes unsaid especially if you're a beginner is you have to understand in your database the data gets stored as rows and columns it's not the database does not look like instagram right from the database you pick up data you transform the data you convert the data from one format to another and then it gets to look like an instagram picture you get it so converting that rows and columns worth of data to make sure that it looks like what it is supposed to look is a conversion task that has to happen that involves logic and processing and there as well your server plays a critical role right so that was a little bit of a hint for you guys to understand to take a deep dive into understanding certain important capabilities such as debugging database handling and whatnot with respect to flask now we come to a very very interesting section where we can compare somewhere around four or five points of why flask or maybe why django can be better in many many ways right now let me be honest with you guys i have used both flask and django for a really long time and at this moment of time i can tell you for a fact that flask does some things really really well and django does some things again in a very different way but beautifully well right so it's like comparing almost apples and oranges but if apples and oranges were to taste the exact same way but look different i think it would be a very good comparison to,358,0,0,ZRnTp4V7i_E
28,wrap up our entire comparison between uh flask and zango right well of course that's not the case so let's quickly take a look at a small yet very simple comparison between flask and django the first point that we're going to be discussing is handling admin related tasks in the back end if you are a person if you are learning this course you you want to use flask somewhere down the line you want to make sure you're you're the admin and you're trying to build something right so what sort of capabilities is flask providing you here where it may or may not be disadvantages to that django well in this particular case flask does not actually provide any sort of direct admin related features but django does provide certain features to help you get started really quick now do not jump ship at this moment of time thinking oh django is providing admin facilities let me just move there directly flask does not do what giango does inherently but there is addable functionality to flask which will eventually get you to do something very very similar to this just that you will have to take a bit of extra time and you will have to spend some time not just creating the logic for that but ensuring that logic talks to your already existing application that is it that one step gives uh django a little bit of an advantage in terms of flask at least in this point then coming to the template uh that uh uh you know that both of these frameworks use well of course as we already,358,0,0,ZRnTp4V7i_E
29,told you the templating engine for flask is ginger two right it is based uh ginger two is actually in itself is a library the templating engine is based on django's templating engine django comes built in with its own template engine and to make sure that flask is faster and more effective than django what people do they separated out the functionality and they use ginger too as a separate library rather than making flask the heavyweight library that it is not right flask is a very lightweight library as we discussed and its lightness comes from the fact that we are actually using uh these other libraries in support of that is this a good thing is a bad thing well that's not a state for comparison because at the end of the day doing this makes flask really faster but if you're a person who's learning from scratch where you have a template engine ready to go in many cases django can be seen as advantages as well that okay so coming to the third point the third point talks about visual debugging now visual debugging becomes very very important for you to not just look at a couple of lines of code aimlessly to try to figure out what went wrong but if you have any sort of capabilities where it's highlighting where something went wrong where it is giving you some sort of a visual cue to tell you that something is either right or wrong right it this functionality trust me can be implemented both in flask and django just for the fact that in flask it does not have,358,0,0,ZRnTp4V7i_E
30,any native support as flask itself it does not provide any support for that but django does provide it uh provide support for visual debugging is this a downside no it can still be implemented in a very different way which still works and sometimes it seems to be work better than what django can provide because again you might have already seen that a lot of third-party accessories to your cars and bikes may give it a more flashy appearance and may make it look a bit better that's the same case with respect to flask here as well coming to the fourth point which is development styles now development styles are really really interesting for you guys to understand and analyze because here what the advantage of flask does is flask gives you a wide variety of tools and techniques to develop app it's not saying that you have to follow only one way of to follow only one track use only one tool use only one method to work not really flask is giving you the biggest advantage in terms of you know workable freedom to create your application this is where django has a downside django's workflow is actually considered to be very monolithic it's only one way of how you can approach things attack things and solve the problems in some ways if you are looking for that kind of a simplicity and if you're looking for that kind of uh non-complexity you know another word for simplicity uh if you're trying to work on that path yes django is going to be a better flavor for you to go take,358,0,0,ZRnTp4V7i_E
31,a look at but if you want that extra freedom trust me when you're a web application developer this is exactly what you require you any tool and technique that's given to you it's almost like you accept it as gifts because at the end of the day you will use it somewhere down the line it will help you and without it you'll be wondering down the line saying oh what would i what would i have done if it worked for that that thought is very profound and flask can trumps django in that particular point there coming to development styles development styles is where again this is uh when you take it as a comparison point there's no winner in this point because flask again is built for a very different purpose flask is built for rapid prototyping and rapid development what i mean by rapid prototyping is that you have a concept in your mind you have an idea in your mind and you want to take that concept and idea write and create a product as soon as possible you want to make sure that you are prototype your working idea of your project it went from your idea to something in python the process of taking your idea into a ready-made product makes very easy in terms of flask because it is giving you every push that it can from the back end to make sure you have these capabilities that make it that makes it very easy right but then let's take a look at the story of django here because django was not built for rapid prototyping django,358,0,0,ZRnTp4V7i_E
32,was not built to take your idea and rush it into a product and of course it's not the final product you'll be working on it but here django was built to take it easy it was built to make sure that you have a structured slow one way of how you can work with your uh you know your web application requirements but sometimes now now i know there can be some of you all who watching this course who are hardcore django users and you might be saying oh not really we have multiple tools and techniques and methodologies to use exactly my point here just like how flask uh overcomes certain disadvantages it has in terms of django django also does uh overcome certain disadvantages it has in terms of what fl flask can provide as well right so every single time it's a fight of these uh web frameworks and each of the time that uh the winner basically is us as developers because we get better products that we can use flask makes it more and more and more easier to work with and django sees that and even they bring more functionality more tools and they make it even more easier to work with right so who's winning at the end of the day it's we as developers who are finding the most fun ways to work with these tools and make our tasks easy right fantastic guys with that we have completed a small comparison with our uh django versus flask section we take a look at the applications of flask so coming to the section where we are,358,0,0,ZRnTp4V7i_E
33,discussing the applications of class again uh this is a section where i definitely don't want to get myself uh uh where i don't want to get carried away because at the end of the day there are so many applications there are so many uh situations scenarios everything that we can talk about but one thing you want to understand is that i have tried to condense all of these into one or two slides to showcase to you saying hey everyone does use uh flask to a good extent right now you might have heard of this social media website even though it might not be as popular as the other side trust me it's extremely popular it's called pinterest right pinterest is where you go to check out these photos these spins as they call it and whatnot and it's one of the one of the big one of the really big social media sites out there in today's world and one of the interesting facts is that the creator of pinterest actually shared saying that hey this was actually built using the flask framework we use flask to build a good amount of pinterest and that that is really amazing right you can open up a new tab right now and open pinterest and you would be fascinated with how beautiful that social media is how beautiful that app is and it gives you a lot of creative freedom to even analyze and understand what might have gone into building that tool right if that was interesting take a look at the second point right uh when the u.s elections happened the,358,0,0,ZRnTp4V7i_E
34,presidential elections in the year 2012 happened barack obama's entire 2012 campaign was actually uh the website that he had for his campaigning it used a lot it used a lot of flask as its main web development framework right so right from you tagging and seeing a couple of memes on social media all the way to a presidential candidate who is you know fighting for office at the end of the day is using the same web development framework so it should show you the massive jump around wherein you can go from domain to domain to domain even in healthcare if you have any healthcare apps on your phone open it up i bet there will be uh something which uses all these web development frameworks we are discussing right fantastic now to all of our international viewers uh if you're in the united states of america i don't know where else lyft is very popular but lyft is a very very popular ride sharing app uh just like how we have multiple apps here in india as well the fact is that these right sharing apps also make use of a lot of frameworks to work but mainly all these frameworks still have to talk to your web development framework that's out there which is basically flask right so beat pinterest beat barack obama's campaign or beat lyft or the thousands of other clients that there are who use a flask you have to understand that these are some of the companies that make use of this really powerful framework right companies such as red hat netflix samsung patreon uber airbnb nginx,358,0,0,ZRnTp4V7i_E
35,nginx is basically a load balancing uh software that's very popular you have teradata your mozilla firefox ladies and gentlemen even as a client uh side website mozilla will also make use of uh you know uh free flash to a good extent what you see on your screen right now is trillions and trillions of dollars worth of business we're not talking millions we're not talking billions right now in the tea league we're in the trillionaire league here and if all of these guys say hey we use flask and we love using flask to a good degree and that it is helping our business grow to a good extent you can take it from the trillionaires that for sure that this app uh this development framework that we are trying to discuss is definitely adding a lot of positive value to all the domains and companies it's a part of right very important for you to know and then once we go on to discuss the applications of flask we have to definitely come to a section where we'll take a look at some of the advantages and disadvantages of the framework as well right now in this section we can quickly take a look at a couple of advantages and definitely you have to understand that the advantages always outweigh the disadvantages but there are one or two disadvantages which is very important for you to know we're going to be discussing that as well first advantage of course the first point has to be rapid prototyping because rapid prototyping is what everywhere in the world today are after and flask is,358,0,0,ZRnTp4V7i_E
36,providing that on a silver platter why do you think people will say no to that right and then after rapid prototyping the second point is that overall code base is comparatively simpler and smaller if you are a programmer if you're a developer you should be smiling at this point of time because uh knowing that you can have a same application a really powerful application with really less code should make you really happy and it doesn't mean that if your code is less it gets complex not really that's because it you are using the magic of python to work with right python never gets really complex to work with at least it has never for me or a couple of thousands of other developers who share the same opinion but yes it's important the next advantage is that you can build highly scalable complex projects now it is very very easy for you to think and assume to say hey i can build simple projects fantastic i can probably build complex projects but how scalable is it how do i take a small product that i built using flask and how do i enlarge it to a point where it's scalable today maybe your website is serving 10 customers tomorrow your website might be serving 10 000 customers every single hour right there you're gonna have to require the capability of you have to have scalability or else when you have 10 000 customers a day your entire thing is going to crash and you have to sit and write another application again flask will give you the advantage of you know,358,0,0,ZRnTp4V7i_E
37,removing all that headache away now the next biggest thing is for the fact that database integration is actually very simple and very straightforward in the terms of flask because again there is so much data that's required for the app itself to work and then there's user data that comes in then there's the data for the server itself to work then there's data for the server to talk to your client there has to be certain way to assert that the client is the actual client so there's certain information uh with respect to authorization data right there's a million types of data that has to be stored in these databases and uh making your database talk to your server and your client is what the challenge that exists even though it sounds like a very simple challenge sometimes it can get complex but in the case of flask this entire process is very simple and straightforward for your usage right fantastic there are a couple of more advantages that we have to discuss first of all flask since it uses other external libraries to work really fully and to give its extensibility it as a library has a very small core and yet at the same time it is so powerful and it's so intense to work with right and then uh another approach that you might be seeing in today's world is something called minimalism as in smaller the better lesser the better well uh developers at flask actually have used this approach in creating the tool itself for us for you to create better apps because at the end of the,358,0,0,ZRnTp4V7i_E
38,day if you are given like a million tools million techniques and a ton of different things you will rather be confused and you will be ambiguous to understand and figure out what to be used in all of that but if i say hey this is exactly what you would need to build something like that straightforward you go there you pick it up you build your app good to go right that approach is very much uh seen here and then when you take a look at a lot of documentation that's available any framework let me tell you if you're into data science if you're into data analysis you're into big data you're into any domain that uses a bit of python out there all of these libraries most of them are open source they're free to use amazing to work with at the same time they are backed by a very very very big community of talented really talented extremely talented developers and experts right so all these experts they write down documents to make sure that you can go on to use the tools easily they can give you tutorials they can give you tips they can give you techniques they can give you a lot of workarounds they can give you a ton of different insights about the frameworks because those are the guys who have written the damn framework right so at this moment of time uh i have intense respect to all these open source contributors all these communities out there and everyone who's making this uh uh you know a very very interesting web development framework for,358,0,0,ZRnTp4V7i_E
39,all of us now uh again i am definitely taking uh definitely taking these uh couple of seconds here to credit all of these community members and you will understand once you start working with these applications right there will come a point in time where you will start appreciating all the hard work they do because their hard work translates to you being easy at your own work to creating the application right now at the end of the day it's not really all bells and whistles every advantage comes with a minute disadvantage and we definitely have one or two disadvantages that you have to take a look at now learning flask is a very uh it's a very good process it's an amazing process you get to learn a lot in a short amount of time but the advantage uh there sort of peaks out because at the end of the day when you have to work on a really complex project you have to have a lot of previous experiences working on simpler projects to actually set up an environment that's required for a larger project to work with right now it's not like anyone can go about working with a really complex project with just one or two days of training never happens right so this is slightly disadvantageous because uh you might feel like flask becomes very complex to use but it not really is very complex it's just that the domain itself is providing that level of complexity and challenge that you have to solve right and then when you're working on complex projects maintaining your complex projects will,358,0,0,ZRnTp4V7i_E
40,actually get complex regardless of any library that you use unfortunately in this case maintenance becomes slightly a task if you're working with flask as well and compared to django when you take a look at it flask does not provide any sort of login mechanism built in any sort of authentication where you can create yourself uh you know where you can register create an account log in log out do anything right it doesn't come built in you don't have anything that can give you all of that if you want login functionality you have to sit and write of course there's multiple templates available out there but you'll have to do something yourself it doesn't come built in with flask you can still get the same functionality or you can get better functionality out there but in this case you just have to work a bit for it right that again can be seen as a small disadvantage and then when you take a look at the last point it says migration of apps can be very difficult yes migration of apps can sometimes be very difficult because in uh in this case where you might have used flask to work with your app maybe down the line you say okay maybe i have exhausted all the capabilities that flask can give me now i want to jump ship to something else then it becomes very difficult for you to move your entire application from flask to something else like django it's not a copy paste it's not a drag and drop it's months and months and months of hard work i'm very,358,0,0,ZRnTp4V7i_E
41,sure right so that uh migration can be seen as very difficult of course i don't blame flask for this because whenever there's a web development framework out there no one expects uh their clients to be leaving and ensuring that you know they can they can leave right they every single product every single developer out there ensures that you they do everything they focus their attention on keeping you into their product into their system into their ecosystem right so migration is difficult because migration is you walking away from flask and that can slightly be surrounding yes it can be better to improve a lot of things with respect to customers but the way it is is the way it is right well with this guys we have discussed a ton of different things with respect to flask uh you know you've come to the end of this course at this moment quickly summarize everything we have learned we started this course by taking an introduction to understanding what web frameworks are we went right to the absolute basics we took a look at web frameworks eventually after understanding what web frameworks are why are they important where have you used i gave you an ample amount of examples to help you understand that we dived right into the heart of understanding what flask is why do we require it where it is used what it is good at what is not good at and we had a section where we had to deep dive into flask to take a look at it we showed you how you can go about installing flask,358,0,0,ZRnTp4V7i_E
42,and working with it even verifying the installation as well after uh that particular section we ensure to take a look at understanding many many things with respect to how you can how easy it is to integrate a database with respect to flask how is debugging done and we also compared flask with respect to another king another giant in the world of uh web development which is django right so you saw five or six points where uh at the end i definitely say the fight was very much uh you know it is it stopped in a there was no clear winner because again you can implement each other's functionality using third party tools one tool can do something better than the other so again comparing apples and oranges doesn't make sense but that kind of a clarity is definitely required for you guys as beginners as starters in the domain right now once we had the comparison we dive right into the heart to take a look at the various applications that are there out of last you saw that flask is being used by companies who are absolute absolute giants in the world today and after that we also ensure to take a look at a section where we discuss the advantages and disadvantages of flask as well right so that is a section which will showcase to you all the good that flask brings and one or two disadvantages definitely something that you should know it's very important for you to understand the disadvantages because you will get to know any point of time in the future if flask will,358,0,0,ZRnTp4V7i_E
43,be the right tool for you or not at that moment of time right so guys all in all we definitely covered a lot of uh stuff about flask which will definitely give you a very good introduction very good foundation for you to build on your flask journey right so guys thank you so much for watching my name is anirudh rao and i wish you all have a very nice day ahead so as all of you know uh model deployment is the application of a model which you're making it right your job actually doesn't end when you have built the model building the model is one part of the entire data modeling exercise wherein you have pre-processed the data you have looked at the data you have tried extracting out the insights which are available on the training data set and based on those insights you have built a model but the entire exercise really doesn't end by just building the model the key is for deploying the model and looking at how your model really performs on an unseen data now uh what really is done as far as the model deployment is concerned what we require is a graphical user interface right now this graphical user interface could be uh developed either using a platform like java or net or any other platform which an organization already has or it could be built by using these simple html pages what i'll be showing you today are the uh is the html way of deployment wherein i have built some basic html pages so my idea was not not to create,358,0,0,ZRnTp4V7i_E
44,some fancy stuff as far as those html pages are concerned in fact i have not used too much of a style sheets as well in case if you all of you are aware about how the html pages are being made so the user inputs are taken through the graphical user interface which is either built on the platform which is already available in the organization as i said either a java or a dot net or if the organization has deployed an enterprise-wide application like the sap then there are some inbuilt packages which are available which allows you to create those user interface so ideally speaking the key requirement for a model to be deployed is get those user interface right why we require these user interfaces because three through this interface is what we will take the user input what are these user inputs these user inputs are related to the features or the variables which were available in the training data set and using those features we have built them on okay so what you would have seen till now is there is a train there is a test which you created and we keep saying that test has to be exactly the same as a train so using a test data set we actually validate the model which happened now this graphical user interface will allow you to test the model on an unseen data set why we call it as an unseen data set because this is the data which a user would be entering it now as an example let's assume you have built a application to find,358,0,0,ZRnTp4V7i_E
45,the uh whether somebody would default on the loan or not if i enter into the bank and i fill in the application saying that i want to apply for a loan then while i am filling up the application there are certain user inputs which i am providing these user inputs are taken by the bank and they are given as an input to the model which will tell me what is the probability that i as a user will default on the loan okay so because i am the new user who is entering into the bank i'm the new user who is filling up the application form that's why my data is known as an unseen data set whereas the test data on which you test the model is something which is already given it to you it's a historical data instance is already okay so this is the fundamental difference now there are some other app ways of also getting the user interface or getting the user information which is like an auto generated text suggestion box facebook auto tag i'm sure all of you would be aware about it when by default facebook allows you to mark the picture which you have liked it so there is an auto tag which is by default which is enabled the idea was collect this auto tag or collect this information and use this as an input for predicting it or for giving a suggestion to a user saying you have tagged a photo of this kind so either you have a user interface available to collect the user input for an unseen data,358,0,0,ZRnTp4V7i_E
46,set or there are certain applications which are giving you some default options through which the data gets connected the idea is to collect the data okay now what are the key features of a model deployment architecture first is the modularity now why we say modularity is a key for a model deployment and specifically the pre-processing steps is because certain times the requirement is not just limited to doing a prediction from the data which is available and i'm sure you would have seen it in the modules sometimes we just want to extract the inside to the way right so sometimes even the eda or the pre-processing which we have done on the data set is important so maybe the pre-processing which has been done on the brain data is something that you want to see on an unseen data set right this is one and for one example wherein we say that modularity of the way the model has been created is very important reproducibility which means my model deployment architecture which i am creating should be backward and forward compatible people from the software background would be able to appreciate the fact that what is a backward and forward compatibility if there are some changes which have happened in the platform which we are using from a deployment perspective then that platform should be compatible to the application which were deployed on the previous one right scalability now this is uh this is probably a key requirement whenever we have the application being deployed on a at an enterprise level and to ensure that the application which is deployed is scalable,358,0,0,ZRnTp4V7i_E
47,and it allows multiple users to be collecting the input or multiple user input to be collected through the interface people have moved on to a deployment on a cloud and what we also would be looking at is the cloud deployment but what we'll be using is a demo version extensibility which means if there are certain additional tasks which needs to be done why from where do we have these additional tasks as far as the model development is concerned so if let's assume if there is a new data set which has come in and this new data set talks about certain features which were not collected earlier and we are rebuilding the model taking those features into an account then what we need to ensure that the model which we have built and deployed should be extensible it's not that we have to probably get into an exercise of doing a deployment from a scratch okay the ability to test the variation between different model versions so uh what i will not be able to show you that right now is in terms of how do we compare different models which have been deployed but i'll show you on the cloud platform that how this version control is managed and whenever we have different models being available every model has a light right so let's assume there is a model which was deployed in an organization for three months and maybe the model is not giving the required output sometimes we just go back and look at the previous version which was deployed now that previous version could be related to certain,358,0,0,ZRnTp4V7i_E
48,different pre-processing steps which were done or a different kind of features which are available so the ability for the model deployment architecture to be able to manage these versions and provide an option to the organization to be able to test the model on different versions okay so if you really look at this in a complete way then modularity reproducibility scalability extensibility and testing if all of you are from the software deployment background you'll appreciate that most of these key features these are the key requirement of any software which you deploy and when we are actually deploying and model it is in in a very broad way we can say it's like a deployment of a software right so that's why we are able to see that there is a strong relationship between the architecture requirement what we see here for a model deployment and when we compare it back to any software developer automation this is eliminating the manual starts the case study as i said this is a case study uh which i would be using it this is to predict the salary just two minutes we'll spend on what the data set is who is the specialized agency of the un which is concerned with the world population health based upon the various parameters who allocates budget for various areas to conduct various camping initiatives to improve health care annual salary is an important variable which is considered to decide budget to be allocated okay now this is the data which we have it i quickly move on to the model which was created i will not spend time,358,0,0,ZRnTp4V7i_E
49,in terms of discussing how the model was created so what i have done is for this data set which has the variables like age work class sampling weight education education number of years marital status occupation relationship race sex capital gain loss arch native country salary i have created an html page to collect the user input for all the variables which are independent so in this data set all these variables are independent and based on these variables we are expected to predict the salary salary is available as a binary value which is less than equal to 50 k or greater than 50 the objective of this data set is based on these independent variables you are expected to predict whether the salary would be less than equal to 50k or it will be greater than 50k so all these variables will work as an independent variable this is a dependent variable as the dependent variable class is binary we are using logistic as one of the things now assuming i have built the model so the model is ready with me now how do i go ahead and do the deployment using a flask as an environment what is flask flask is a web framework that provides library to build lightweight web application input this is based on wsdi toolkit what is wgs stands for a web server gateway interface which is a standard for python web application development and it is using jinja 2 template engine what is ninja is a web template engine to render the dynamic web pages right what is the dynamic web page which allows you to,358,0,0,ZRnTp4V7i_E
50,take the input and based on the input the page gets rendered what is rendering rendering means that the page gets processed flask requires 2.7 or higher version now what are the steps for doing a deployment the first step is you have to create a pickle of the model what is a pickle now if you remember in our last couple of flies back i showed you that for a model to be deployed we create a executable jar file and that jar file we deploy in an environment similar to that executable jar file what i am creating here is a pickle in a very broad way you can assume pickling is a process where in the model which you have created you are converting that into an object why we are trying to do that we are trying to do that so that i can use this file for the deploy okay the file which you create has an extension as dot pkl the library or the package which you require in python for creating a pickle is in pickle right so what i have done here is i can show you the code we have imported the library pickle and i am opening a file right i am opening a blank file in a write mode which i am calling it as adult underscore flash dot pkl and then in this file which i have created i have dumped the model right this is the model which i have created which is using a logistic when i this is the file which i have created right so the model is dumped into,358,0,0,ZRnTp4V7i_E
51,this file now to find out whether the dumped file or the pkl file which i have created is working fine or not i have loaded this pkn file and did a test on the test data set to find out whether the model which i created is giving me the same result through a ptl file and through the mod okay i'll just repeat this step to check it so the first thing which we do from a modern deployment perspective is create a pickle file what is the pickle file a file which has an extension is dot ptl which is very similar to an object file which you created how do i create a pickle file we have to import the library pickle if you already have not installed it i am sure you know it you have to save it install the file name create a blank file in a right mode ensure that the extension of the file is dot pkl and then the model which you have created dump that model into the file which you have created once you have dumped the model please check whether the pkl file is giving you the same result as the model once you're sure about it then what you have is an adult underscore flash dot ptl file available which is nothing but a model ready for it okay so this is what is step number one what is the step number two you have to create a user interface file for getting the user inputs or for looking into the unseen data set right now as i told you there are,358,0,0,ZRnTp4V7i_E
52,multiple ways in which you can create you can have a java application being developed you can use a dotnet environment you can use any other application uh front end which is available for the enterprise wide application like sap as an erp or you can use an html editor to create the html pages now these there are a lot of options which are available for creating an html pages you can use the css for doing the stylesheets and on i have created very simple html basics using notepad as an editor okay so this is the file which i have created as an html file i've used notepad as an editor the objective was just to ensure that i'm able to get the user input that is the reason i have just created a very simple file now this is the way the html file looks like people who are comfortable with the html they would be able to look into it otherwise i'll just explain it to you you have broad divisions of an html5 the first is a header the second is the user input which you are trying to take it right now these options are available for a drop down kind of a list so there are multiple ways in which you can take an input you can allow user to enter the input or you can allow user to select from the available image another file which i have created although it was not mandatory is a result file okay i can have the prediction result being displayed on the same html file but i have intentionally created,358,0,0,ZRnTp4V7i_E
53,a separate file as a result file just to show you that you can have a separate file as a result file because normally in an organization you do not get to see the result on the same thing right otherwise there was no restriction i could just add this part of the code which is an html code onto the same input file which i have created okay so step number two is create a user interface here in i've said create an html page because i am using the html page as a user interface right okay step number three create a file with an extension dot pipe using python editor like jupyter node now what i have created here is a file which is known as app.py this is the main file which is being looked by the flux okay let's look at it what the file has these are two standard libraries which i have seen numpy pandas i am using a flask right this is a class as a package which i have loaded using a pip once it is loaded i am using it so i am using a flask and i am using a request to render template now what is request render template these two are used by the html pages right i'm sure you would have seen i am using the word rendering rendering actually means processing the html page which you have the first line which you write here is the app right so app is equal to flask this is here you are initializing the flask okay the second part of the text is i'll,358,0,0,ZRnTp4V7i_E
54,probably explain it through here these are the libraries you have it this is where you initialize the flask the next part is you define from where you are getting the user input right here i am getting the input through a file which i have created as an html which is edit.html fourth block of the code is fourth block of the code is talking about the model which you have created okay and here i am using adult flask dot pk this is the file which i have created using apical library and this is what i define it which we call it as a prediction function which means those user input which i am getting from where they will be processed the next part which i am giving is a output page okay now if this was not a separate page i would have given the block or the portion which was available in the first html file but because i have created a separate page as a result and that's what i'm mentioning okay this is the main function of a dot pi file and this is from where the program really starts okay this is the first function which gets executed by the class now let's just go back and look at this one more time so these are the libraries this is where i'm initializing the class this is where i'm giving an input which is a dot html file this is the dot ptl file of the model this is the output part of it and this is the main function which will get executed whenever i will initiate,358,0,0,ZRnTp4V7i_E
55,the or whenever i will execute this app okay so three step process for doing a deployment let's just quickly go back and look at it one more time step one create a picture file step 2 create a user interface step 3 create a file by the extension dot py and this is what the format of the file should be okay now you have to be very careful when you are creating a structure what i have done here is i have created a folder by the name flask i will just show it to you okay this is the folder which i have by the name flask under that i have created another directory by the name templates right which has these two html files the main folder which i have is a flask under that i have another subfolder by the name templates which has adult html and result html at the same level when i where i have created a templates i have these two files being stored edit underscore flash dot pkl and app dot paper okay what is this is the model file which i have it and this is the dot py file which i created you just go back and look at it i have adult class dot pkl and have dot this is mandatory that you should have a structure like this now to execute this file what we'll do is we'll go on to the anaconda prompt here in i am at the anaconda prompt i will go to the library i go to the directory which i have created so i am saying cdc,358,0,0,ZRnTp4V7i_E
56,colon gl class i reach the flask once i am added directory from where i have to execute the file i will say python app dot py okay once i say python app dot py it this is a lazy loading lazy loading means in the software industry uh or in the web pages whenever you see a lazy loading means these are not loaded automatically you whenever you request for it it loads it so this command will take another couple of two three minutes and then it will show you a url which is this url this is a url for running the app on a local host what is the local host which means running it on your machine so once you do python app dot py you copy this url paste it here and what you get is a form now this is the input html form which i have created let me just enter some values so let's assume age is let me enter my age as 40 years working classes these are the drop-down options which are available let me say local education is 12th standard marital status is married occupation is let's say a craft repair relationship is other relative race gender as a female capital gain let's say you might enter some value triple line capital loss hours per week 40 native country let's say canada and i say summit now what it is doing it it is executing the model doing the processing and it take me to the second page which is the result page when it will show me an output of a prediction which,358,0,0,ZRnTp4V7i_E
57,says income less than 50 this is how i deploy a model using class and i'm able to get a url which i'm able to run on my local machine now i'm sure you would have figured out that this is a url which i have created which i am able to run only on my local machine so if i have to give up give this out to multiple users if i have to give this out to multiple people then it is not facebook right unless i share my desktop and things like that but practically it is not visible so what we will do now is so this class as a deployment is good enough and i'm actually i've created some a model and i want to show this as a demo or i want to just test this as a model how does how will it look like when it will actually get deployed flask is a good environment to work on now these are different ways in which the model could be deployed the way you need to read the table is the the column really represents the different ways in which the model could be deployed each row represents that for this each type of a model deployment what are we using for a training perspective how does a prediction happens how the prediction results are delivered what is the latency for prediction and what is the system management right so each row represents the values or the outcome of each model deployment against these factors the first one which we call it as a rest api and what we'll,358,0,0,ZRnTp4V7i_E
58,just see in the next slide is what is exactly is arrest application programming interface is what we use for doing a deployment of a model now in this case what is assumed is that you already have made the model and this model has been made based on the data set which is available as historical data and that's why we're calling it as a batch okay which means the train data which was used to build the model was available in a patch what we do as a prediction is on the fly which means the moment i enter the input i am able to get the output as a predicted value and this is what we will see today when i enter the input using the html user interface we'll be able to see the output as a predicted output the prediction results delivery this this again happens through an rest api latency for prediction which means how how much delay is there from the time when i take the user input to the time when i get the output back is so so system management difficulty is also so so when we move on to different patterns then i'll probably explain you why we say the managed system management difficulties pattern two of our model deployment is when you have the data set being used for training is a batch prediction also happens in a patch which means it's not that the user interface or the user inputs i have collected and i'm able to get the output immediately in this case there is a database which is created which means i,358,0,0,ZRnTp4V7i_E
59,keep entering the input so let's just go back to the loan application example let's assume there are 50 customers who have worked in today when they wanted to apply for a loan right so there are 50 applications which a bank has for a ticket bank will keep entering the details of the the values or the parameters that are available in the application and these features or these variables are stored in a db now what will happen is maybe the the batch frequency which is defined maybe at the end of a day maybe the next day maybe in middle of the day all these input records which are available in a shared tp and why we call it the share db because it's the same data set uh structure which has been used for training the model is where we are actually keeping the new record which come in and through this db we this db is the new record which have been entered in the dbr given as an input to the model which is available and the output which you get is again stored in the db so that's why we call it as a patch okay i'll just repeat it one more time so you have the data set which is available as a historical data the normal data set or the csv file which you have seen it for a prediction or for the new entries which are happening they are again stored in kind of a dot csv file right they are again stored as a data set now these records which have been stored as the,358,0,0,ZRnTp4V7i_E
60,new records which are yet to be predicted are given as an input to the training set and what you get as an output is the prediction value these predicted values are again stored back in the data set which you can go and refer it so let's assume there are 50 applications which are coming these 50 or a batch of these 50 application inputs would be given to the model the output would be again stored back in the data set or in the database or in a csv file and which will say record number one so for anjuna the loan is rejected for approved something like that right so that's why we call it as a batch this happens through the share db latency for prediction is very high because this is a badge processing right so it's not that you are able to immediately see the result based on the frequency you define it for a batch execution is what it takes system management difficulty is easy because you have everything available in a offline kind of an environment right you have the model which is available as an offline you have the data set which is getting created from a prediction perspective is also kind of an offline so it's the easy one pattern three is a streaming okay and what is the streaming in this case what we are trying to do is we are actually making a change in the model when a new data set comes in so let's assume and this is this is the streaming is a platform which is used by all the e-commerce,358,0,0,ZRnTp4V7i_E
61,or a marketing kind of an industry what they really do is let's assume there is a model which was created on the data set which was available till yesterday right the new records which come in the new records will predict based on the model which was created as of yesterday but these new records will immediately become part of the train data set which means now the value which i have predicted right is what will be used as a given value and even the model building is getting changed every time right so you have the training being done as a streaming the prediction being done as a streaming latency obviously is very low it's a very difficult environment to manage because normally what we have is we have a tray of the architecture or the hardware which is available on which the model has been built we have a separate hardware or a separate a set of environment which is available on which we deploy the model now herein because the new record which have come in as a stream is again being used for training the model you have to be absolutely clear in terms of how the environment has been managed okay so this is the most difficult now if you go back i said when you are doing an interface through an api the system management difficulty so so because in this case the hardware which is being used there is no separate uh if i can use a word a demarcation of saying that this is the separate environment which has been available from a training perspective or,358,0,0,ZRnTp4V7i_E
62,from a model building and this is a separate environment from a prediction perspective pattern four which is more used for my mobile applications again the training is streaming the prediction happen on the fly this is using an api on the mobile latency is low and the system management typically okay so what is the rest this is a representation level state transfer api which is a an api as i said is an application programming interface which is used to integrate the user input which you are taking with the available enterprise application on demand analysis by gui which is a drag and drop prediction interface which allows user to get prediction when they need them okay this is whenever you want it it's like giving a in input now and that's what we'll do we'll enter the values on html pages whenever we enter the values we just press summit and we'll get the protection out scoring code export which is what mod model is available in an executable jar file or as a java source code and can be deployed anywhere right something on the similar lines is what we'll be using on a flask environment so the key requirement for a model to be able to execute without the necessary support of the libraries what the one which we have used in the python is to ensure that i bundle the libraries in such a way that machine is able to understand right and this is what we call it as an object file so once you have created something which a machine can understand then you don't require those supporting,358,0,0,ZRnTp4V7i_E
63,libraries because they are bundled along with the object which has standalone scrolling engine which is a separate staging and a protection environment so that the model can be tested and implemented in a stable isolated way some more deployment options which you have is a cloud deployment in a big data environment cloud deployment as i said it has become almost a necessity now looking at the scalability which you require from uh for an organization and the way the hardware is changing people they do not really want to invest on this as an asset and hence they buy the cloud they have the space available in the cloud and nowadays the industry has also provided your platform which is available as a service okay so when i am talking about and that's what i'll uh probably touch when we move on to the next one so when i'm talking about heroku as a platform this is nothing but a platform which is available as a service on a cloud environment okay now uh when i was talking about the clouds there are multiple options which are available for an organization to get into a cloud deployment one is you have your own private cloud which is available right private cloud means this is a cloud which has the security restricted to an organization and on that private cloud you can have a platform which is available right and then you deploy it the second option is as i said the organizations have already created a platform so heroku is such kind of applied so heroku as a platform is available as a pass,358,0,0,ZRnTp4V7i_E
64,which is platform as a service p double a s so what you need to do is this is a free environment you can create your own login and you can use this environment to do a model deployment there are paid versions of heroku which is also available the free version has certain restrictions in terms of number of uh users who can provide a user interface or number of users who can access the application number of applications that you can deploy but if you are looking at a demo kind of an application if you are looking for a small project to be deployed you can use these platforms right so from a cloud deployment you have an option of buying a cloud creating your own environment or you have an option of using the service as a platform which is available on the cloud and that's what we use as a help okay so two things which will do one is we will look at a flask deployment and we will look at a heroku diploma the case study as i said this is a case study uh which i would be using it this is to predict the salary just two minutes we'll spend on what the data set is who is a specialized agency of the un which is concerned with the world population health based upon the various parameters who allocates budget for various areas to conduct various camping initiatives to improve healthcare annual salary is an important variable which is considered to decide budget to be allocated okay now this is the data which we have it i'll,358,0,0,ZRnTp4V7i_E
65,quickly move on to the model which was created i will not spend time in terms of discussing how the model was created this is a problem statement you had and this is a session which probably you would see it or you would have seen it in the logistics okay so what i have done is for this data set which has the variables like age work class sampling weight education education number of years marital status occupation relationship race sex capital gain loss working arts native country salary i have created an html page to collect the user input for all the variables which are independent so in this data set all these variables are independent and based on these variables we are expected to predict the salary salary is available as a binary value which is less than equal to 50 k or greater than 50 okay i'll just repeat one more time this is a data set which is available in your logistic module as well the objective of this data set is based on these independent variables you are expected to predict whether the salary would be less than equal to 50 k or it will be greater than 50 k so all these variables will work as an independent variable this is a dependent variable as the dependent variable class is binary we are using logistic as one of the okay the application of a model which you're making right your job actually doesn't end when you have built the model building the model is one part of the entire data modeling exercise wherein you have pre-processed the data you,358,0,0,ZRnTp4V7i_E
66,have looked at the data you have tried extracting out the insights which are available on the training data set and based on those insights you have built a model but the entire exercise really doesn't end at by just building the model the key is for deploying the model and looking at how your model really performs on an unseen data now uh what really is done as far as the model deployment is concerned what we require is a graphical user interface right now this graphical user interface could be developed either using a platform like java or net or any other platform which an organization already has or it could be built by using these simple html pages what i will be showing you today are the uh is the html way of deployment wherein i have built some basic html pages so my idea was not not to create some fancy stuff as far as those html pages are concerned in fact i have not used too much of a style sheets as well in case if you all of you are aware about how the html pages are being made so the user inputs are taken through the graphical user interface which is either built on the platform which is already available in the organization as i said either a java or a.net or if the organization has deployed an enterprise-wide application like the sap then there are some inbuilt packages which are available which allows you to create those user interfaces so ideally speaking the key requirement for a model to be deployed is get those user interface right why,358,0,0,ZRnTp4V7i_E
67,we require these user interfaces because three through this interface is what we will take the user input what are these user inputs these user inputs are related to the features or the variables which were available in the training data set and using those features we have built them on okay so what you would have seen till now is there is a train there is a test which you created and we keep saying that test has to be exactly the same as a train so using a test data set we actually validate the model which happened now this graphical user interface will allow you to test the model on an unseen dataset why we call it as an unseen data set because this is the data which a user would be entering it now as an example let's assume you have built a application to find the whether somebody would default on the loan on if i enter into the bank and i fill in the application saying that i want to apply for a loan then while i am filling up the application there are certain user inputs which i am providing these user inputs are taken by the bank and they are given as an input to the model which will tell me what is the probability that i as a user will default on the loan okay so because i am the new user who is entering into the bank i'm the new user who is filling up the application form that's why my data is known as an unseen data set whereas the test data on which you,358,0,0,ZRnTp4V7i_E
68,test the model is something which is already given it to you it's a historical data instance is already okay so this is the fundamental difference now there are some other app ways of also getting the user interface or getting the user information which is like an auto generated text suggestion box facebook auto tag i'm sure all of you would be aware about it when by default facebook allows you to mark the picture which you have liked it so there is an auto tag which is by default which is enabled the idea was collect this auto tag or collect this information and use this as an input for predicting it or for giving a suggestion to a user saying you have tagged a photo of this kind so either you have a user interface available to collect the user input for an unseen data set or there are certain applications which are giving you some default options through which the data gets collected the idea is to collect the data now what are the key features of a model deployment architecture first is a modularity now why we say modularity is a key for a model deployment and specifically the preprocessing steps is because certain times the requirement is not just limited to doing a prediction from the data which is available and i'm sure you would have seen it in the modules sometimes we just want to extract the inside to the way right so sometimes even the eda or the pre-processing which we have done on the data set is important so maybe the pre-processing which has been done on,358,0,0,ZRnTp4V7i_E
69,the brain data is something which you want to see on an unseen data set right this is one and for one example wherein we say that modularity of the way the model has been created is very important reproducibility which means my model deployment architecture which i am creating should be backward and forward compatible people from the software background would be able to appreciate the fact that what is a backward and forward compatibility if there are some changes which have happened in the platform which we are using for one deployment perspective then that platform should be compatible to the application which were deployed on the previous one right now this is uh this is probably a key requirement whenever we have the application being deployed on a at an enterprise level and to ensure that the application which is deployed is scalable and it allows multiple users to be uh collecting the input or multiple user input to be collected through the interface people have moved on to a deployment on a cloud and what we also would be looking at is the cloud deployment but what we'll be using is a demo version extensibility which means if there are certain additional tasks which needs to be done why from where do we have these additional tasks as far as the model development is concerned so if let's assume if there is a new data set which is coming and this new data set talks about certain features which were not collected earlier and we are rebuilding the model taking those features into an account then what we need to ensure it,358,0,0,ZRnTp4V7i_E
70,that the model which we have built and deployed should be extensive it's not that we have to probably get into an exercise of doing a deployment from a scratch okay the ability to test the variation between different model versions so uh what i will not be able to show you right now is in terms of how do we compare different models which have been deployed but i'll show you on the cloud platform that how this version control is managed and whenever we have different models being available every model has a light right so let's assume there is a model which was deployed in an organization for three months and maybe the model is not giving the required output sometimes we just go back and look at the previous version which was deployed now that previous version could be related to certain different pre-processing steps which were done or a different kind of features which are available so the ability for the model deployment architecture to be able to manage these versions and provide an option to the organization to be able to test the model on different versions isn't it most of these key features which i have just spoken about these are the key requirement of any software which you deploy and when we are actually deploying and model it is in a very broad way we can say it's like a deployment of a software right so that's why we are able to see that there is a strong relationship between the architecture requirement what we see here for a model deployment and when we compare it back to,358,0,0,ZRnTp4V7i_E
71,any software development automation this is eliminating the manual starts so what is the rest this is a representation level state transfer api which is a an api as i said is an application programming interface which is used to integrate the user input which you are taking with the available enterprise application on demand analysis by gui which is a drag and drop prediction interface which allows user to get prediction when they need them okay this is whenever you want it it's like giving a in input now and that's what we'll do we'll enter the values on html pages whenever we enter the values we just press submit and we'll get the protection out scoring code export the model is available in an executable jar file or as a java source code and can be deployed anywhere right something on the similar lines is what we'll be using on a flask and so the key requirement for a model to be able to execute without the necessary support of the libraries what the one which we have used in the python is to ensure that i bundle the libraries in such a way that machine is able to understand right and this is what we call it as an object file so once you have created something which a machine can understand then you don't require those supporting libraries because they are bundled along with the object which has standalone scoring engine which is a separate staging and a protection environment so that the model can be tested and implemented in a stable isolated way some more deployment options which you have is,358,0,0,ZRnTp4V7i_E
72,a cloud deployment in a big data environment cloud deployment as i said it has become almost a necessity now looking at the scalability which you require from uh for an organization the way the hardware is changing people they do not really want to invest on this as an asset and hence they buy the cloud they have the space available in the cloud and nowadays the industry has also provided your platform which is available as a server okay so when i'm talking about heroku as a platform this is nothing but a platform which is available as a service on a when i was talking about the clouds there are multiple options which are available for an organization to get into a cloud deployment one is you have your own private cloud which is available right private cloud means this is a cloud which has the security restricted to an organization and on that private cloud you can have a platform which is available right and you deploy the second option is as i said the organizations have already created a platform so heroku is such kind of applied so heroku as a platform is available as a pass which is platform as a service p double a s so what you need to do is this is a free environment you can create your own login and you can use this environment to do a model deployment there are paid versions of heroku which is also available the free version has certain restrictions in terms of number of uh users who can provide a user interface or number of users who can,358,0,0,ZRnTp4V7i_E
73,access the application number of applications that you can deploy but if you are looking at a demo kind of an application if you are looking for a small project to be deployed you can use these platforms right so from a cloud deployment you have an option of buying a cloud creating your own environment or you have an option of using the service as a platform which is available on the cloud and that's what we use as a help okay so two things which we'll do one is we will look at a flask deployment and we will look at a heroku deployment the second part which is the hero now what we try doing it is we will try to deploy this in a cloud we will try to deploy this on a cloud platform which is heroku as i said heroku is a platform as a service which enables to deploy build run and operate the application entirely on a cloud now how does an heroku platform looks like this is what the heroku is okay the first thing which you have to do is let me just show you uh the first part of it here in i've already logged in so if i just say heroku login this is the first page which you get it right because i'm already logged in that's right giving you this as an option otherwise it will show you log in here okay create a login in heroku platform now this is a free platform which is available right you don't have to pay anything as i said this is a simple application,358,0,0,ZRnTp4V7i_E
74,if you want to deploy you really don't have to pay anything you can create a login which is free of cost the first thing which will do it once you've created a login you have to create a new application right for creating a new application it's very simple you could just follow the instruction which in adobe is giving you what i have done here is i have created an application which i am calling it as an address salary okay first time when you create this application this is a blank application now this application can be deployed in two different ways you can create a repository on github okay now i'll just show you why i'm saying you can use this as a repository let me just go back here once you are into an application you selected you have an option here as a deploy okay now once you get into a deploy these are the three options it will show you which means i can deploy to salary prediction application which is blank for the first time when i created it using a github now what will be what will github do i would have created a folder like this on a github right what is a use of a github github is an environment which is available if there are multiple developers who are working on it and all of them can refer to the code which is available in a github and they can work on that code they can enter the code right so what hello allows you is if you have an application which has been,358,0,0,ZRnTp4V7i_E
75,created by through multiple users to multiple developers and assuming the application has been deployed on a github because github is a very common platform which is being used by all the developers so helpful by default assume that you have a model or you have the pkl file along with the remaining user interface is available on a github so you you can actually create a application through the github or heroku has a client level interface right which is a cli which is available through the cli you can actually deploy the application which is very similar to deploying it through a github right or you can use a cli application now when you want to use a cli application heroku requires to download a package which is a git what is git and before i end the session i'll show you certain links also from where you can get some additional information what is good it is nothing but a version control and software together right so here by default requires a gate package so what you need to do is you need to download the git from google so it's simple you have to just go there search i have a windows environment so i'll say git for windows and i'll get to see an executable file right once you have created uh once you have downloaded the kit and executed it now you can use in heroku cli environment right i'll show you this cli environment right now it's a will exactly follow the same path the difficult part is the cli because hearing you have to write the commands through,358,0,0,ZRnTp4V7i_E
76,the git sub those commands automatically gets exported so the first thing create a login second create a new application for a new application just follow the platform instructions third is you download the git and use the heroku cli to deploy the application now what this requires is to create two different files the first file is a proc file and the there is another file which i'll just show you is a configuration file okay so heroku platform deployment requires you to create two additional files one is a profile and the second is a configuration file what is the proc file proc file requires you to provide this as a command gunicon is a web user interface right the way you have a flask which was a wsdi which is a web server's web server gateway interface similarly gunicon is a wsei environment for hero okay so you have to say webgunicon and then you have to provide these two arguments at colonel what is this app this is the name of the dot pi file so remember what i have created the file name was app.pi so hence i have provided the name as app here the second is through which you have initiated the class so if you go back here this is the name you have given when you've initiated the class so hence my proc file will read app colon please remember this proc file which you will create should necessarily have p as a capital and should not have any extension it should not have any extension even if you have created the file using a notepad remove,358,0,0,ZRnTp4V7i_E
77,the dot txt extension for the file okay third step is you create a configuration file which is known as a requirements dot php this file necessarily should have an extension and dot php r is in small case now what this file contains is the exact version of the libraries through which you have created the model okay what a as i said heroku is a platform right for your model to run when we went on to the flask right when i went on to the anaconda prompt this anaconda prompt already had a python one right this already had the packages which i required which is a numpy pandas or flask or booney phone or whatever this already has those packages available when i'm using heroku helico is a platform which is available for deploying any kind of an application so how will heroku know that what i am giving it to him as a dot pkn file as a python file which has certain built-in packages which are available so whatever you have used in the model creation or using the app file all these libraries you will have to provide it either as a double equal to sign or as a greater than sign right so when i say flask double equal to 1.1.1 which means load the flaws which is 1.1.1 right so this configuration file or the requirement file will carry the version of the library package which you have it how do you know the version you can go on to the python and you can figure out the version which you have it right so this is,358,0,0,ZRnTp4V7i_E
78,the third requirement you have to create a requirements.php file both these files which is the prop file and the requirement.txt should be created in the same directory where you have the main application so remember i had the main application in the flash directory so i have these two files created there then in the step 4 what i will do is i will go on to the anaconda prawn and i'll go on to the directory which i needed which is a class directory and execute these commands okay i'll just show you these commands which are available on a heroku platform because we are doing a installation using a cli these are the commands that you have right first you have to do a heroku login i'm already logged in then you have to say heroku git clone minus a this is the name of the application so adult salary prediction is the name of the application so that's why in my login it is by default showing you this is a command so what i am doing is i am creating a clone of this application so what this command does it it actually creates a directory by this name which will work as a clone then i move on to this directory right once i move on to this directory i'll say git add dot add dot means get i told you is a version control software git add dot will add all these files which i had in a library before that an fpy profile requirement.txt right and then you say comment comment means these are the new files which,358,0,0,ZRnTp4V7i_E
79,you should consider for the application once you say comment it will give you a url which is the name of the application and heroku app.com this url even you can run it right on your machine okay so once i go on and type this url i'll exactly get the same so the difference between this and the earlier one which i showed you was the earlier one was locally available this was this was deployed on a local host it was locally available on my machine this is a url which anybody can use it and once you enter the input here so let me add the values here 40 let's call this as 12 married let us leave this here as it is triple 9 double a 66 okay so exactly the process is same it is now executing the model and rendering the input which i have given it and will give me the result back on to the result file which area fan of so this is how you deploy a model using a heroku platform let's just go back one more time create a login create a new application if you want to deploy it using a cli download again create a file by the name proc file which will have this as an entry create a requirement.txt file which will have name of all the libraries and the packages which have used it when i created the model or it's this is basically setting the python environment which by default when you were executing on an anaconda you had it then you execute these commands and you will have,358,0,0,ZRnTp4V7i_E
80,a url which can be used by anybody these are some references or links which you can use which will give you a little more information please note that these are not the only links which are available but these are the ones which i thought was good for you to get started on all these uh terms or the environment or the platform which have used like unicorn flowers okay so this is how you deploy a mod i hope this was useful if you haven't subscribed to our channel yet i want to request you to hit the subscribe button and turn on the notification bell so that you don't miss out on any new update or video releases from great learning if you enjoy this video show us some love and like this video knowledge increases by sharing so make sure you share this video with your friends and colleagues make sure to comment on the video for any query or suggestions and i will respond to your comments,223,0,0,ZRnTp4V7i_E
0,music it was so everybody so in the previous video we have discussed about deployment of an lp model using docker and the installation of all that flask docker gitlab jenkins and configuring it to make our requirement satisfied dependencies so after that video we have discussed about how to write flask api to deploy our model and we have a run our flask api into the browser and we have successfully find out which to it is a negative of which to it is positive okay so after that now we want to ship our flask so that we want to deploy it into the docker image and run to form a docker container so for that we want to write some a docker file okay so the first thing is to write up docker file so how to write it to just click just open some text document and to docker eyes to make an image so what a docker generally give the docker will give you every type of dependency or environment in which you can just put your code into that the docker container and it will just run it that if the container run it will trough function similarly that what you run on your local machine the only difference is that that that particular image you can just give to anyone okay so you can just that anyone can run its api which need a dependency of flask or pandas and numpy and your person are don't have to download or install anything on their local system simply they need to copy be a paste into the cat that container,358,0,0,rb_DkKAZzyA
1,and they can run okay so first of all docker we need to make an image or we need to make environment how to make that environment with the help of dockerfile so first we will just write dockerfile hit enter and there is no extension so dockerfile doesn't have an extension open with any notepad so my favorite is notepad plus plus though you can use anything as an editor of your text so first of all we will use an base image so docker file generally first have a base image so if you just type a docker hub so the docker hub we will find there are so many images or pre-built images already available so if you just type ubuntu so to run your application you need some operating system like environment okay so docker will give you open toe as an image as a base image that you can import or write in your docker file and in that ubuntu install all the dependencies which you needed to run your code and after that just copy your all that folder all the code into that ubuntu operating system and when you run that particular image will come will become container and that you can access it okay so that particular after that become a container it will be very much small in size only that space that it needed that ubuntu plus all the dependencies plus your file so that image that we can also see it ok so i'm going to use alpine so alpine is a flavor of for linux so generally docker people used to this alpine image,358,0,0,rb_DkKAZzyA
2,so why they use because it's a very small in size 5 mb it will give you only the linux environment and rest you need to install it ok so it's not a much bulky so that's why you could become it will run as well as it will be very small in size so first just right from and what we will use we will just search that because alpine is so much basic that if you install paint as an umpire so paint danny and numpy need something called dot viel file and dot g's plus plus file so it will just run that file so it will take much time if you just directly import alpine and copy and install painter so it will it will be much more slower so we already so some people already have installed some of the latest machine learning libraries on the top of alpine that you can directly import okay so the one name is fro all v led okay so slash that is spelling is alpine and after that python and machine learning here you can see so this is exactly 180 mb of size so this one we will write in our docker file ok so just copy it ok so just copy it and from just paste it ok so which one will use the latest one the tag that we will use so depending upon which version you want to use that is written here so the latest latest only one of that 6 to 3 mb of sighs and after decompressing you will get a 180 mb of size on the,358,0,0,rb_DkKAZzyA
3,top of it will run pip install because he's already have python installed ok so that is that advantage of having the docker because the image that environment you will already get in this alpine on the top of it python and several machine in libraries which are the libraries that let's see so we will get machine lab like a parent like numpy pen disciple secular so so far so good okay so upgrade and we'll just upgrade prep and after that we will use working directory slash app okay so in this in the inside this container we will make an application folder in this application folder we will everything will copy in that application folder okay whatever run in only in that particular working directory now after that we'll copy everything so dot means everything will copy inside this app okay after that we will just run pip install - our requirements dot txt okay so we need to write some requirement files also so let's open some text and here you will write require mints okay so if the spelling is correct okay so just open it and just type simply which modules you need to install by pip okay so i will install flask that i need i will need an lt k library just close it so these two thing i need you can just also give some version name also like for example equal to equal to and 1.1.1 and analytic a library would you really want to use latest one so that's why i am not going to give any type of version here so if you have,358,0,0,rb_DkKAZzyA
4,"some compatibility issue you can give the version name also okay so after that copy it after running that pip all the requirements in the top of it i will run some analytic a module under the slip library i will want to install pu and katie it will have stopped word and it will give you porter stemmer and stemming algorithm that you want to vectorize your particular text file okay so for that how you can install it you will just type python - m l tk dot downloader pu n kt and after that if you install everything whenever you run it will run on some port if you remember it then they have to expose it to 404,000 and after that whenever we run the container so it will run on entry point okay so just search on internet what is actually entry point so entry point actually when you run that container so in the shell here that previously we have run our code if we just type python it will open for me python shell okay so here you can just type anything by app dot py or the name of your app or anything that it will want to run okay here you can just open the cmd and just try to find if we i type see a python so it will open for me this python so in this i will just open my actual py file which i wanted to open so it will type cmd and after that app dot py okay so that's it so in this way you can just write your docker",358,0,0,rb_DkKAZzyA
5,file so after that we will this docker file here we have written our docker file this docker file we need to copy it in our virtual environment okay so how i can copy it i've already uploaded into my git repository so just go github.com slash 1 1 2 3 so if you just open it you will just find that there is something called deployment of an lp model so that particular thing i will copy so here you can see that i have also given the glossary of nlp with python for machine learning so here you can get all that libraries and what it actually named lyle amortization root cause and lp l tk what is actually is all about it's also written go and clone it and that's requirement or txt file is there flass and l tk and the docker file is available so just open your virtual environment first so here we will open our virtualbox so just open it ubuntu so we'll wait until that are open to what should box will get open now as you can see our operating system get started so now i will just open the putty and here i have already saved my session previously so just open it yes here we go so just open some desktop or some resources so just go to some document like okay so document and here it doesn't have anything so we will clone our repository here just copy paste and just write in git clone and it will clone the repository for you so there is something contained like deployment of an lp model,358,0,0,rb_DkKAZzyA
6,go inside it and here you have these all files available ok so what we will do will first just check how much like first check pseudo super user and so that we don't have to write every time so we are there's a root user we will use so just take docker image ls so it will give you this images that i have so previously i have created the same docker file and build it so now i'll give it some different name so i'll try to first remove it so for that docker remove and that image id so here just us talking remove image id so after that okay so docker image remove so this is the command and this actually some containers running that is not allowing us to remove it so for that we will use container remove that already have been stopped five six ie so after that you must remove so i had removed it so just again check that image so there is nothing called nlp so now we can build our own image okay so just type docker build so what docker bill will do it will build an image for you that will have all the dependency already installed in it alright so i will give a tag so - t and the name of your nlp model so just nlp or just give it a python and give the tag version one and after that where that docker file is present in the same directory dot press enter and it will go through their docker file and it will try to install all the,358,0,0,rb_DkKAZzyA
7,"dependencies which is it won't so i have already down already downloaded this image previously so here i can see this one okay so that's why you don't have to download it will take it from the cache okay so that process become much more faster in your case it will go and directly connected with the docker hub and will download this image and after that further process will get carryout and after that if you copy the app and in that install that requirement collecting flask and collecting ltk it will install on it so it will take some time till it install every type of dependencies needed to run a flask and n lt k now after that it will just take some time and you can just see how much it's the download process is actually the same so here it found that we need to download some from analytical dot downloader and in that funked library pound module and it has successfully build image okay now we can just check docker image ls so here you have and by python that have been created nine seconds ago of size of 275 mb alright so just we have created it but there is some if you just run it for that we have docker run hyphen p that is the port that we will give so the port is the outside we will try to access this for 4000 and inside expos is 4,000 okay so outside let me give 5,000 it become make much more clear and much more sense to you that outside we can give any type of port",358,0,0,rb_DkKAZzyA
8,"available to you and inside there is 4,000 which is actually the port that is exposing from inside the container okay so after that we will run it at some background so for this is detach mode so - d and after that nlp python version 1 and you can just also can give the image id also as in place of this so we'll just name it as my my nlp ok just run it and it is running at the background to check just type docker ps so something is running so so if you go at your browser and type your that 100 168 or 43 or 200 is ip of my virtual machine and just i 5,000 you'll get that this is something here is running so just type hello or hi previously we have just check so it will give this positive comment if you you've know something i just positive negative comment okay so it's working okay so if you just just check that the previously what we have code that written it is executed perfectly but the one thing that is a minor change that you have to do is app got py if you just look at the app dot p y through vii through viii meritor you find out that under the hood under the name main function we have provided host is equal to 0 dot 0 dot 0 dot 0 because in the docker if you just don't give this it will try to run the localhost that can only cybil inside this container okay it won't be accessible outside of your network okay or",358,0,0,rb_DkKAZzyA
9,inside your local machine where your that local host is running so that running inside the container so it can accessible only inside the container that's why we previously we have run it with the help of one 27.0 or 0.21 that is the ip address of local host that can only run on side the your computer to run it anywhere else in your connecting network you have to give this or some given port okay some given any other ip address okay so this minor change that you have to do and after just come over for at whim and carry out the same process of making an image and after that running it so after checking that this is container id the container is actually running and it's if you go to any other place any other browser connected with your network you can on a five thousand port with the ip address you can just access your application okay so that's how we need to we can create this particular application with now there is one thing that i want to let you know that if you want to just go inside that your docker container we just have to run this command docker execute that is execute that is run already for running container open that interactive shell for that we have to press - it hi for interactive t4 tty shell okay so just run the container id or container name so it's my container name is my nlp and after that just type slash bin slash sh press enter now you are in the container you have plus press,358,0,0,rb_DkKAZzyA
10,ls so here the present working directory is slash app okay you can go also the are outside you can play ls here you have now inside you can just see you have app folder go to app and here you have all the files that is that we have copied into that a particular app okay so come out for come out press ctrl d for that is the command to come out of the container and you can also the check the locks what is running inside the container so for that you can just check called docker locks and the my and lp it will show you these are the locks that have been running because we have executed some of the commands here so it will show you some running we can also just type hey hi there so if we just predict it's a positive comment so right now if we can just again check so here something have been included here 1 2 3 4 5 now hey 1 2 3 4 5 6 7 ok so something have been executed right now because we have executed something so that's how we can check the locks that's how we can make image and we can run the container we can see the container we can name it we can run it we can set the port so that's how the docker walks after that we will make up total ci cd pipeline that it will fetch the image from the get lab it will just build it it will dock rise up container and everything we can see on the,358,0,0,rb_DkKAZzyA
11,jenkins that we'll do in the next video that is given inside in the link in the description so everything relatable video previous video also given in the link in the description go and check it out also that link of this kit of repository thank you so much guys if you liked that video subscribe and share with your friends who want to learn the ci cd pipeline and that main tools and technology which people are using in the industries if your student go and subscribe my channel lots of things that there i'm uploading afterwards thank you so much guys bye bye,136,0,0,rb_DkKAZzyA
0,hi everyone in this video you will learn how to dockerize a machine learning application and this tutorial consists of two parts in the first part we build our flask app and apply the machine learning model and in the second part we dockerize everything and for this i actually got help from francesco who is a docker captain and a real expert in this field hello everyone and thanks for the introduction i'm francesco docker captain and i like to help developers understand continuation concepts you can find me on twitter and youtube in the links below in the second part we will take care to containerize the application that patrick is about to create let's start so the second part is available on his channel and i will put the link in the description below and now let's get started alright so let's start from stretch by developing our machine learning model and our web app and for this i have one empty directory that i called app and then i have another one where that i called mldf so this is where we do our machine learning development and yeah here we don't start completely from scratch so i already have the data here and a notebook where we train and come up with our model so this is not the focus of this video but you will learn how we transfer our notebook to a production ready web app so let's do this and yeah let's go into the ml def folder and if you are working with machine learning stuff then the chances are pretty high that you use a conda,358,0,0,S--SD4QbGps
1,environment so i want to use the same in here so we start from scratch by setting up a conda environment so i say connor create minus n and then i give it a name and i also like to specify the python version i won so here i want python 3.9 so hit enter and you don't need to use a conda environment but i recommend that you use at least one kind of virtual and management so you could also just use the build in virtual and this works just fine but in this case i want to demonstrate how to do this with a conda environment so now we activate it and the great thing later will be that when we have our docker container then we no longer have to worry about this local setup here so then everything should work in our docker container but for now we have to set up this locally so yeah now we have this activated and now we can install all the packages we want so the first thing i install is two pointer and ipi kernel for the two fighter notebook support then i also want to install the a kernel for this environment so i say ipython kernel install minus minus user minus minus name and for the kernel name i give it the name as the environment so also nlp and now i need to install all the packages i need to develop the model so in this case i'm going to use scikit-learn but the same approach works also with tensorflow models or pytorch models so in this example i want,358,0,0,S--SD4QbGps
2,to keep it simple but again the approach is the very same for any kind of machine learning or deep learning app so now we have this and in this example i need one more package and this is the nltk package to work with text and now we should have everything we need so now we can start our notebook by saying two fighter notebook and this fires up the server and here we see the data and this is the notebook i prepared so this is the notebook and i will upload this to github and put the link in the description and now we go over this only very briefly so what we're doing here is we work on a project to classify twitter tweets and say if they are positive or negative so the first thing i want to do is change the kernel and make sure that we use the kernel from the environment that we just created so we select this nlp and then let's run the first cell and see if all the imports work and here we get a error module not found so i forgot to install this in the environment so let's go back to a kernel to the terminal and then activate this in a new window conda activate nlp and then we say conda install pandas and hit enter alright so now we have panda so now let's go back and run this again and now the imports work so yeah and then now let's go over this briefly so here we load the data and again i will put the link for the,358,0,0,S--SD4QbGps
3,data set in the description below so you can download this as well and then we do some text pre-processing so we here we define emojis and stop words and then i created one function pre-processed that gets the raw text data and then here for example i remove your l patterns the user patterns and then i remove stop words and i apply the slammatizer and the lemmatizer is a popular technique in nlp that groups together inflected forms for example better and good so we instantiate this here and this is available in the nltk package and when you run this the first time you might get a error and a note that you need to execute this command so we have to say import nltk and then download this package and this package so this is important because we have to remember this for later because we have to do the same commands in our docker container but yeah this is the pre-processing so we define this function then we call this with the pr and get the pre-processed text and here i do a train test splits and then i set up all the different models or pipeline steps i need so the first thing here is to use a vectorizer and so i set this up and call vectorizer fit and then vectorize a transform and then i try out different classification models as last step so here i have this helper function to evaluate the model and yeah here i create different models for example this bernoulli naive bayes and then call model fit and model evaluate and yeah once,358,0,0,S--SD4QbGps
4,we've done this and decided for the final best model now the next thing to do is to save all the different steps and all the train models and then load this in our application but in this case i want to show you one best practice that you can use with the sk learn package and for this we use the sklearn pipeline and the way it works is that we define a pipeline object where we put in all the different steps so the first step is the vectorizer and the second step is the final classification model and then we have everything together so now i have to only call one fit command and then one predict command and don't have to call the vectorizer and the bernoulli model separately so yeah i recommend that you set up a pipeline like this and then here i evaluate it again to see if this still gets the same results and now we have to only save one trained pipeline so after having set pipeline dot fit we now save this pipeline and for this we use the pickle module and then we load this again and here i call model evaluate to check if this is still the same and yeah this approach is the same for every machine learning pipeline so if you use tensorflow or pytorch then also at some point you want to save either all the steps separately or maybe you could save the whole pipeline in one step and then later in your application we call a model load again so yeah this is how we save and load,358,0,0,S--SD4QbGps
5,the model and then here at the very end i have one function to predict the text so here we call this pre-process function so this step is still separately because this is not a model that we can put in our pipeline but then here we only call model predict so this will apply the vectorizer and the final classification model and then it returns the label so here i test this with some example tweets so now let's run all the cells and hope that this works all right so the code run through and now we see the final output so here we have the classification i hit twitter this is class zero which means negative may the force be with you this is one which means positive and then here again we have one negative tweet so yeah our classification works and now we have all the code we need in the notebook and now the next step is to transfer the code that we need from here to our application so let's go back to our folder and if we go to the ml dev folder then here we should see our pickled file so this is the pipeline so the first thing i want to do is i want to copy this over to the app and now let's go back to the terminal and here we can quit the server so we no longer need our notebook and now let's go back and into the app folder and here i fire up my code editor so in here the first thing i want to do is i create another,358,0,0,S--SD4QbGps
6,folder and i call this api and then in here again a new folder and i want to call this models and then i move the saved model into this folder so now we have this and then in the api folder i want to create two files so i want to have one app dot pi and and another one and we call this utilities dot pi so yeah so now we have the after pi the utility stop pi and the models with the saved model and now the first thing to do is to bring over the code from the notebook to the utilities file so let's put this next to each other and now let's bring over all the code that we need so we need a few imports so we need import ray and pickle then we need this lemmatizer so let's copy and paste this and then if we go down so we don't need this data set but we need the emojis and the stop words for the preprocessing so again let's copy this and i will make this smaller for now so yeah here we copied this then we need this pre-processing and the lemmatizer so let's copy and paste this whole function and then again here we don't need this um train test split stuff and we also don't need the stuff where we set up the different models and fit them and evaluate them so the only thing we need at the end we also don't need this pipeline stuff we only need this where we load the pickled file again so let's copy and paste,358,0,0,S--SD4QbGps
7,this as well and here we have to be careful so this is now in the models folder so we say models slash pipeline dot pickle file and then let's go down further so yeah we could also use this predict function and maybe this with if name equals equals main to test the code so now we should have all the codes that we need and now let me reformat this a little bit so i want to put the lemmatizer up here so that we see it right away then i also want to have this where i load the model at the top so that i see this right away as well and then let's create one final function and i will call this define predict pipe line and this only gets the final text and then here we can say return predict because this function still gets the model so down here it gets the model and the text so here we say predict and then we put in the load at pi and the text and then this is basically the only function we have to call and then down here here let's say predict pipe line and then it only gets the text and then i want to i can close the notebook and make this larger again and now let's open the terminal in here so in here i want to run this and make sure that this works so now let's make this larger again and the first thing in the terminal i want to do is make sure that i'm in the correct environment so like you,358,0,0,S--SD4QbGps
8,can see here this is a different one that was activated by default so i deactivate this and say conda activate nlp and now we first let's go into the api folder and then we can run python utilities dot pi and yeah this works so here we should now see the same output that we've seen in our notebook so this works as well now we have all the code for our model classification that we need here and the last thing to do is now to implement the flask app so for this we use flask and of course we have to install this and here i'm simply going to install this in our conda environment but i install it with pips so i can say pip install flask and this works just fine all right so now we have flask so now let's import the stuff we need so we say from flask we import flask then i also want to sonify and i also want request and then we say from music utilities utilities we want to import the predict pipeline function this is all we need from there then we set up our app so we say app is our flask app with the underscore name convention and then we need to create one function and we call this let's call this predict function and for now we say pass and we have to decorate this of course so here we say at and then app dots and now this is new since since flask 2.0 we can say app.post so for a post request and then we define the url,358,0,0,S--SD4QbGps
9,endpoint so we say this should be at slash predict and here what we want to do is we want to basically send a text to this endpoint as json data and here we get the text and then call the predict pipeline so the first thing we do is we say data equals request dot json and then since we want to deploy this to production i also want to do a couple of error checking here so i wrap this in a try except block and then say the sample equals the data with the key let's call this text so now the user should put in a json file with the key text and otherwise we get a key error so we say accept and then we catch a key error and here we can say we want to return jsonify and then this should be a dictionary where we put in the key error and then as a message we could maybe say no text was sent so yeah otherwise it works and then we have the sample and this is a raw string right now so we want to put this in a list so we need a list with at least one element so we say sample equals a list with this item and then we call the predictions pipeline so we say predictions equals predict pipeline and this only gets the one sample now and then again we could and do some more error checking with this but yeah i leave this up to you but now for the very end i want to do one more error checking,358,0,0,S--SD4QbGps
10,so here i want to say result equals and then again i want to chase sonify this and then i put in the first predictions so this should again have only one element so i can say predictions and then zero and if this does not work then this will or this could maybe throw a type error and i show you why in a second so we catch a type error as e and then here again we can return a error as json object and here i want to make the exception message as a string and otherwise always say this is also the result and then at the very end we return the result so yeah this is basically all we need for our predict endpoint and now again i want to say if underscore name equals equals underscore main then i want to test my flask app and i want to say app dot run and here i give it um host the host object equals as a string 0.0 and while i'm debugging this and trying this i can simply still say debug equals true this uses the internal flask development server so this is fine because later we use a production ready server and not this one so now let's test this again by going to our terminal and then this case we run python app.pi and this should start the flask server so yeah this is up and running and now we can go to a separate terminal and then use a curl command to send a post request to this endpoint so let me copy and paste this,358,0,0,S--SD4QbGps
11,in here so we say curl and then a post request and we need to send this json data with it so here we have the key text and then we use this tweet and then here we have this endpoint at part 5000 and then slash predict so let's run this and see what happens and we get a response back so the endpoint work but we get an error object of type in 64 is not chase and serializable and so what happened here is that in our application if we go back and have a look at here so here we catch this type error and this is exactly what happened because now if we go back to the predict pipeline and in the predict function so here this prediction is a numpy data type and not a python int so we have to convert this to a integer and in fact i want to do this a little bit different so instead of putting a tuple in here i want to create a dictionary so this will look nice in json format so let's create a dictionary from this so this needs a key so here we put in the key text and then the text then we put in the key let's call this spread for prediction and then here we want to put in the label and yeah this is the label to prediction or other way right the other way around prediction to label so yeah so now we have this so let's save this and run this and this is still in debug mode so if we go,358,0,0,S--SD4QbGps
12,back then this should be reloaded so now let's do the curl command again and now it says connection reviews so i think i accidentally quit it this because here i have a syntax error so if we go back then of course here this should be before the colon so now it should work so now let's run our app again and see if this is up and running so yeah now our app is running again so now let's send a call command again and yeah now it works so now we get the correct prediction back where we see the label the prediction and the original text so now let's also test this with the negative one so here we send the text i hate twitter and then we get the negative labels so yeah this worked so now our app is running and now we want to dockerize this so before we do this i want to do one more thing so in our api folder i create one file requirements dot txt and this is where i put in all the packages that i need so um what we can do here is if we go back to the terminal and quit our application so since we used a conda environment we can say conda list and this will list all the packages that we have here but here we basically only need to grab the ones that i installed in the beginning so conda installs a bunch more so we need scikit-learn and we also see the version here and i highly recommend that you put in the version in,358,0,0,S--SD4QbGps
13,the requirements txt file as well and pin this because if we have a difference between the versions where we trained our model and then later when we load this and do it in our app if there is a difference then our app or our model could break or produce different results so we want to have the same version for scikit-learn and i recommend the same if you use tensorflow or pytorch so we pin the version here and then we also need the nltk so let's search for this so here we have nltk and then let's also copy and paste this and pin the version to 3.6.6 and then we also need flask so let's search for flask and copy and paste this and yeah for flask it doesn't really matter but i recommend that this should be at least 2.0 because then we can use this app.post command so let's pin this to at least 2.0 so we can say greater equals 2.0.0 and then this is all that we need and i also want to write a comment here so i copy and paste this so this is a command that we need to import nltk and download the packages for nltk so i put this here as a comment as well and then yeah francesco can see this and now we have everything we need for our web app so we have this one directory with the api and here we have our flask up and running and yeah as a next step we want to dockerize this and as i said in the beginning you can find this,358,0,0,S--SD4QbGps
14,on francesco's channel so i hope you enjoyed this tutorial and then i hope to see you next time bye,26,0,0,S--SD4QbGps
0,so today we will see how to create a url or deploy your flask application simply and get a url which you can use as an api in any application you want so there are certain steps which we will follow and if you're new to this channel do like share and subscribe to my channel and if you are clueless about this video so this video is a second part of the previous video which i created where we deploy and create a machine machine learning model we export it as a pickle file and then we create a url using flask to create a simple api system for our machine learning model now in this video i will deploy this model so we get a deployed url link which we can use to create api calls to instead of running it on our local environment so few steps which we will require here is first we'll have to deploy this to uh uh store this in a g repository so we will need a get repository for this then we will also need a music requirements do txt file which will have all the dependencies our flask application require so the current dependencies which we require are flask py lear and the pickle libraries and their individual dependencies which we will install by pyon after all this we will create a virtual environment first so we only take in those libraries which we require now if you don't know how to create a virtual environment i will just show you i will first increase the appearance yeah so to create a virtual environment we,358,0,0,LBlvuUaIg58
1,will use a library called as vv which simplifies process of creating virtual environment in python to do this all we need to do is type python space colon m space b env vv and the name of the virtual environment you can keep it anything you want i will just keep it v andv for simplicity if i do this it will create a virtual environment for me and it will uh create a virtual environment for me as you can see a virtual environment is getting formed here which will include things like uh the include folder li scripts to activate this virtual environment after this gets installed we have to go to vnv scripts and start the activate function in the vnv it will be available after it gets downloaded so as you can see our vmv is installed now to activate the virtual environment all we will do is we will write v env scripts and the activate command in the scripts folder doing this our virtual environment will get activated you'll be able to see that all the files are getting uh it will be stored in virtual environment now if i write my python main.py now it will give me an error that this module does not exist this is because in our virtual environment these libraries are not installed to install these libraries i will just go here and write flask pip then we are using psyit learn so we will wr pyit lear pip after we go there we will see what command is used to install these libraries and we will install them now pyit learn is,358,0,0,LBlvuUaIg58
2,using pip install pyit learn i will copy this code then flask i think is uh pip install flask yeah so what i will do is i will wr p install psychic learn space and flask using this our psychic learn and flas libraries will get installed in our virtual environment after which we will be able to run our main.,79,0,0,LBlvuUaIg58
3,pi code again so as you can see it is installing the related dependencies which they require as well now we will wait till these get installed as you can see our libraries gets installed and after this if we run the python main.py again we will be able to see these files and our program we should hopefully run as you can see our program gets clearly running and if i go to this side i'll get the hello va after this before we deploy it onto github we need to uh convert this vnv and add it to git ignore dogit ignore is the file name using this the files which are inside the v env folder will get ignored while pushing it to the g repository so i will add v env then we need to add the libraries and their version number in the requirements. txt file now requirements. txt file is a gway for the online hosting platform which will see all the libraries required and install them now simple way to write all your required libraries in this is to use a command called pip freeze pip freeze the greater than colon and write the file name which is requirements.txt you can name it anything but a general convention is to write requirements.,284,1,3,LBlvuUaIg58
4,pxt and if i enter it will write and update the files which are required for our program so as you can see our requirements.txt file gets updated after this we'll have to store this in a git reposit to do that using vs code i can directly go to this source control and initialize a repository i will directly publish it to github and i will store it in a private repository let's say ml deployment trial okay i will publish this so as you can see all this file gets published on my get repository after it it gives me a notification that is deployed i will go on my g repository and you can see that all these files are here so now to create and deploy this we will use a platform called as render i'll just go here render so it's a cloud application hosting for developers it's url is the dashboard of this section and after you go to the dashboard you will have to log in here i will log in once again i am signing it with my google account as you can see i have tried deploying various applications and they very successful after you go to the dashboard of render you need to create click on this new button and click on a web service so a web service will be able to deploy a flask application so we will select the build and deploy from a g repository then we will connect it with a repository so i will just go here just give it some access after this uh you can select the repository,358,4,4,LBlvuUaIg58
5,you want to give the permission to i have just created a repository called ml deployment deployment tri like this and i will give it some access after doing this it will directly take me back to the render where i'll be able to see my newly created git reposit after this i will click on the connect music button i will give it some unique name let's say i give it ml deployment trial new i have already used this before so to create something region keep it as it is given the branch is the g branch which you are using which is main in our case so it will keep it at as main through directory now this specifies which directory to look into in our g repository we have just created in the uh source of the repository so we will just keep it as it is the runtime is python 3 now build command here is python install requirements.txt now if you have named your txt file as something else you will have to change the name here as i have kept it mine as requirements.txt i will keep it as it is now start command now start command is basically the application how will it run on the gicon server now this is basically your name of this file which is containing your flask application as i have kept the name of my file as main.py i will write gicon main app if you have named your file as anything else let's say app then you'll have to write gicon app colon app so this will tell the server that,358,4,4,LBlvuUaIg58
6,my main.py file is my flask application then instance i will create a free instance and if you have any environmental variables you can add them here and i will just write create a web service it will take some time to to deploy and you'll be able to see all the logs which are there here once it is deployed you'll be able to see that it is successfully deployed so let's see how if there are any errors in deployment you will be able to see those errors here and then you can revert back and solve those errors and solve some bugs which you have so as you are seeing this is installing the requirements.txt libraries and after this it is uploading the bill it will take some time to create this build and it is taking more time for me as my cpu is occupied deploying deploying yeah it is successfully deployed it is showing me build successful so if you guys are sticking till here then i would appreciate if you like this video and subscribe to my channel so i can create more such contents for you and if i go onto this link right here it will give me the it should give me the the hello worldall which is the output of our you know our source url so guys we have successfully deployed and uh deployed a flask application on render so if you enjoyed this video thank you for watching do share this video so it can reach all your friends and all the people who might need this video thank you for watching do comment,358,4,4,LBlvuUaIg58
7,down below what you would like me to create next and what are the things which i can music improve,26,4,4,LBlvuUaIg58
0,"hey everyone, welcome to this tutorial where i'm going to show you how to use fastapi. fastapi is a web framework designed specifically for building api apps with python. it's extremely popular and comes with pretty nice features and high performance right out of the box, at least as far as python web frameworks go. here are some of the benefits of using fastapi. it's really easy to learn, as you'll see. it's also fast to develop with because it comes with some really useful abstractions. and finally, it's async by default, so the performance is also pretty good. this makes it a solid choice for building any kind of python backend. in this video, we're going to learn how to install fastapi and use it to build our first app. we'll also check out some of its in-built features, like the interactive documentation. let's get started. to install fastapi, open your terminal and run the following commands. you'll need to install both fastapi and uvicorn. uvicorn is going to be the server that we use to test and run our fastapi applications. once you finish installing it, create a new directory for your project. open this directory in your code editor and create a file called main.py. in your main.py file, import fastapi and then use it to create a new app. and this is how we define a path in fastapi. this is an app decorator and it defines a path for the http get method. and the path is going to be this . so that's going to be our root directory. so that when somebody visits this, this function is going to be called.",361,0,21,iWS9ogMPOI0
1,"so that when somebody visits this, this function is going to be called. and then we can return this hello world object here. to run your server, go back to the terminal and then use uvicorn. your command should look like this. type uvicorn and then the name of your file, which should be main , and then the name of your app . and then you can use this --reload flag to make the server automatically refresh anytime you make changes to the file. you should see something like this. and if you click to this url here, you should be able to see the api route. so here we're at the home route and you can see the hello world object being returned. so now we have a really basic app working. let's figure out how we can add routes to our application. routes are going to be this url when you enter a different thing. for example, you want to see items or you want to see a user. that is a route. so let's see how we can add that next. in fastapi, routes are used to define the different urls that your app should respond to. you can create routes to handle different interactions. so let's say we wanted to build a to-do list application. we'll need different routes to add or view the to-do items on the list. let's go back to our app and start by creating an empty list of items. so this is going to be our to-do items. next, i'm going to create a new endpoint for our app. and this one is going to be called create item.",362,21,43,iWS9ogMPOI0
2,"and this one is going to be called create item. and users can access this endpoint by sending an http post request to this items path. and it's going to accept item as an input. so this is going to be a query parameter. so once it receives this item, it's just going to add it to this items list. and we can actually make it return the item or we can make it return the whole items list as well. just so that we can see the results of different things we add to the list. to test this, open up a new terminal and then send this curl request directly to our url. we're going to pass in the item by using a query parameter like this at the end of the url. so once you send it, it should return the current list of items. and you can see here, we've just added an apple to that list. but we can modify this and add other items too. so now we've added two items and this endpoint works for adding new items to the list. to view a specific item on the list, we're going to create a new endpoint using the get decorator again. the path for this endpoint is going to be items item_id inside the curly brackets. this is a way to say that if we go to a path, like for example, items 1 or items 2, then we can actually use that variable and use it to query this items list.",336,43,58,iWS9ogMPOI0
3,"this is a way to say that if we go to a path, like for example, items 1 or items 2, then we can actually use that variable and use it to query this items list. now be careful because every time you make a change, the server will reload and this items array will be reset back to this empty array. so before you test this get items endpoint, make sure you create an item first so that there is actually something there. so i've just refreshed my server and i've added this apple and this orange item again to my server. and now i can use this get request with this 0 index to get the first item. so if i do that, i get apple . and if i run it again and change this index, i should get the orange . that's because whatever you put here as this index will become this itemid variable, or whatever the name is in your function, and then you can use that as a parameter. if you specify the type hint here, then fastapi is smart enough to convert the type for you. now what happens if we try to get an item that doesn't exist? so let's go back to the app and try items number three. and we get an internal server error. this isn't really ideal because in this specific case we know exactly what went wrong. but if our users see this internal server error, it's not a very helpful error message.",334,58,71,iWS9ogMPOI0
4,"but if our users see this internal server error, it's not a very helpful error message. so the next thing we're going to look at is how to raise very useful error messages, so that when you're developing the app you can debug it and figure out what went wrong. fastapi makes it really easy to raise specific errors to handle whatever situation you're dealing with. in this case we tried to find an item that doesn't exist in our server, and we just got back an internal server error, which isn't really helpful. for situations like this, there's a universal set of http response codes that you can use and everybody will understand. if an item doesn't exist, that's usually a client error, because we're saying that the client's looking for a specific item that the server doesn't know about. so let's click into this client error response, and you scroll down you'll see this 404 not found. so this 404 code is probably the way we'd want to respond in this situation. to do that, scroll to the top of your app and import this http exception from fastapi. and then down in your handler, we'll modify this to have a condition checking whether or not the item exists, and if it does we'll return it. otherwise we'll raise this http exception. we'll put this 404 status code in, which means that item was not found, and you can even use this detail parameter to give more information about why it wasn't found.",331,71,82,iWS9ogMPOI0
5,"we'll put this 404 status code in, which means that item was not found, and you can even use this detail parameter to give more information about why it wasn't found. now if i go back to my terminal and then run the same request again, you'll see that our error has updated to something much more useful. next, let's dive a little deeper into how we can send information to fastapi. we're going to look at how you can use request and path parameters. now back in our project, we actually already have an example of a path parameter, which is this create items here, because this item appears as a query string in the url path. but this is probably not the right way to do it, so we're going to turn this into a json payload later. but for now, let's create a new endpoint, which i'm going to call list_items . this one's going to use a query parameter as well, and this time it's going to be an integer. fastapi is smart enough to convert the type of the parameter for us, so even though when we send a url request, they're all technically strings, type in this as an integer, then it's going to convert this into an integer. and this particular function is going to take this limit query parameter that we specify and use that to return some number of items from the list. so if we put in limit 3, it's going to return the first 3 items. or if we leave it default 10, then it's going to return 10 items from the list. let's go ahead and test it out.",365,82,94,iWS9ogMPOI0
6,"let's go ahead and test it out. i've just added 10 items to my list on the server, and now i'm going to use this request on the items endpoint. this is the same endpoint as this one, but because i'm using a different request, it is going to hit this method in the server instead of this first one. so let's go ahead and use that. and then with limit 3, i'm getting the first 3 items back. and if i leave the limits blank or don't specify it, then it's going to use the default value and return all 10 apples. and the default value, of course, is defined in the function header. now, if i want to build a to-do list application, then my items might actually be a more complex data structure than just the string. luckily, fastapi also supports pydantic models, which allow you to structure your data and also provide additional validation. this will make things like testing, documentation, and code completion in your ide a lot easier. to get started, first import basemodel from pydantic. then extend this basemodel to create an item class. and because these items are supposed to be items in a to-do list, it's going to have two attributes, a text, which is a string, and is_done, which is a boolean. now, we can update our app to use this item model. so for instance, when we create this item, instead of taking a string, we can now pass this item model. and here, when we get an item, instead of returning a string, we can also return an item.",352,94,109,iWS9ogMPOI0
7,"and here, when we get an item, instead of returning a string, we can also return an item. now, if we try to use the same curl request to create the item, it's not going to work because we have specified the item through the query parameter. but when you use a modeled object like this item here as part of the argument, it's going to expect that to be in the json payload of the request. to make it work, we have to send this item data as a json payload instead. so this is how you can do it in curl. and we no longer have the query parameter at the end of the url either. so when you run that, you should see that it now works. and in our response, we no longer get a single string, we actually get an object that conforms to our item model we've defined here. so by default, our is_done value is false. and because we didn't specify that here, that's exactly what we got. now, if i go back to my model, and i want to make one of these fields required, for example, this text here, i can simply delete this default value. and now when i go back to my terminal, and if i try to send it something that doesn't have a text value, for example, i call this title, instead, it's going to fail because now it's validating the request model for me. so this is another nice advantage of using pydantic with fastapi is that you can really easily just create a validation for your server as well.",356,109,121,iWS9ogMPOI0
8,"so this is another nice advantage of using pydantic with fastapi is that you can really easily just create a validation for your server as well. so far, we've looked at how to model the request data and the input payload to fastapi. let's look at how we can model the response as well. this is actually super easy, because all you need to do is just use the same base model from pydantic for your response. so for example, let's go here where we list the items or where we get the item. all you have to do is add a new argument to the decorator called response model. and then just put the item class that you'd like returned from this response. similarly, if you want to put the response model for a list of items, you could do it like this. so now this is just another way to tell our server and our interfaces that the response from this endpoint would be conforming to this model here. and this is really useful if you want to build a front-end client that interacts with fastapi. by doing this, it makes it really easy to work with front-end frameworks like react or nextjs, because now you have a defined response structure that you can rely on. now i'm going to show you one of my favorite features of working with fastapi, especially if you're just starting out. this is the interactive documentation feature. whenever you start a fastapi server, you get a documentation page for free that you can actually interact with and use to test your api.",351,121,134,iWS9ogMPOI0
9,"whenever you start a fastapi server, you get a documentation page for free that you can actually interact with and use to test your api. so far, we've been doing all our testing in the terminal, but it can be pretty hard to type out this command over and over again or to make modifications to it. if you go back to your local fastapi server and then add this docs to the end of your url, you'll get taken to this swagger ui page where you get to see all of your endpoints. you get to see which http method they accept. and if you click into them, you can also look at the type of parameters they take. you can even test them. so for example, here, let's test our post request. click try it out, and then just update this request body with whatever you want. then when you hit execute , it's going to give you the curl command you need to run to test this out in your terminal, but it's also actually going to send the request and you can see the response body here. so this is really useful because you can see how your api is configured and you can also just test it easily without having to fiddle around with commands in the terminal. and if you type in redoc instead as the path, you get this other set of documentation and i think it's pretty much the same thing, so use whichever one you prefer.",331,134,144,iWS9ogMPOI0
10,"and if you type in redoc instead as the path, you get this other set of documentation and i think it's pretty much the same thing, so use whichever one you prefer. and there's a very small link here, but this one's also actually quite useful, but if you click it, you get this json file, which is basically everything you need to know about your fastapi server. so here it's got all the paths, all the different schemas of each path, and all the different responses that can be returned. so this is really useful if you want to just export this json and build documentation or build a front-end client, maybe in javascript or something, that can interact with your server. although fastapi is quite popular and easy to use right now, how does it compare to an industry standard framework like flask, which has been around for much longer and has much wider adoption? well, fastapi is async by default, so it can handle a lot more concurrent requests right out of the box, which i think is nice. and as you've seen, it's really easy to use. the way you define routes, define response models, validate data, and throw http exceptions is as simple as it could be. so with flask, i don't have a side-by-side comparison at the moment, but if you go trying to do the exact same thing, it's just a little bit more fiddly. currently though, flask does have higher adoption and it is an open source project with community support, so you know that using fast at least it's reliable and you can build on it and there's a lot of resources.",365,144,153,iWS9ogMPOI0
11,"currently though, flask does have higher adoption and it is an open source project with community support, so you know that using fast at least it's reliable and you can build on it and there's a lot of resources. fastapi is still new, although it's gaining popularity quite quickly. i think some of these points compare to django as well, except that django is a heavyweight framework, so if your use case is going to be a very lightweight backend, then fastapi is probably the choice to pick in that case. hopefully this video has helped you to get started with fastapi. if you're wondering where to go next, you could look into databases, integrating things like sql with your fastapi server, or if you want to build an app for users, have a user system, or secure certain endpoints, you could look at authentication mechanisms such as jwt tokens. and if you've built your fastapi app and you want to learn how to deploy to a server so that it's live and anyone in the world can use it, then check out my video here where i show you how to deploy a fastapi application on aws. otherwise, i hope you found this useful and and thank you for watching.",274,153,159,iWS9ogMPOI0
0,what is going on guys welcome back in this video today we're going to learn how to easily turn our machine learning model into a usable api so let us get right into music it all right so we're going to learn how to turn our machine learning models into apis using fast api in python today now the reason behind this is that often times when you train a machine learning model and it performs well it makes good prediction you don't want to just hand that model to the user maybe because you want to sell it or because the user doesn't know how to work with the model not everyone is a computer scientist or programmer not everyone knows how to use the model and load a data set and so on so usually what you do is you make the model part of an application so there's a feature or there's an endpoint in your application that uses the model or in the simplest way you just provide an api interface to the model so api already the stands for interface so saying api interface is redundant but you can provide an interface in the form of an api to your model and one very easy way to do that is using fast api this is what we're going to do in this video today uh what we're going to do specifically is we're going to train a simple machine learning model to classify handwritten digits and then we're going to make that model accessible uh using an api so this is going to be a very simple example here for this,358,0,0,5PgqzVG9SCk
1,we're going to open up the command line and we're going to install four packages using pip so pip or pip 3 install and we're going to install first of all of course fast api then also uicorn to run the application then also pillow to work with images and also scikit-learn to do the machine learning part once you have all of this installed we can go ahead and create a python file to train the model so let's call this train model.py and here we're just going to build our model this is not going to be any part of the api yet this is just going to uh load the data set and train a simple random forest classifier on the data set so we're going to say here from sk learn.,175,0,0,5PgqzVG9SCk
2,model selection we're going to import the train test split so that we can have a train and a test set so that we can also evaluate the performance of our model and at the top we also want to import pickle for serialization because of course we want to ort our model so that we can use it in the api so the first thing we do is we say x and y is going to be equal to fetch open ml and the data set that we're going to use is the mnist data set mnist 784 i think that's 28 28 because of the pixels so 784 pixels um we're going to say version equals 1 and we're going to say return x and y true so that we get it as a tle x and y here all right so that is our data and now we can split that into xt trin and x test into y train and y test that is going to be the result of the train test split applied onto x and y with a test size of 0.2 so we have 20 test data 80 uh train data and then our classifier will be a random force classifier and i'm going to provide here n jobs is going to be equal to -1 so that we can use all cpu cores for the training to speed it up because otherwise it's going to take quite some uh time and then we're going to do clf doore x test and uh y test actually first of all i forgot to do the fitting so clf fit,358,3,3,5PgqzVG9SCk
3,x train and y train and then we're going to evaluate this on x test and y test and finally after this we're going to say with open um mistore model pickle in writing bytes mode as f we're going to say pickle.,55,3,3,5PgqzVG9SCk
4,dump clf into uh into into f there you go so the file stream f and the classifier clf so we can run this now this is going to train the model to do the handwritten digit classification this is not anything new on this channel we have multiple videos here or i have multiple videos here where i show you how to do that i have i think two or three videos where i show how to do specifically handwritten digit recognition and we have many more machine learning tutorials this is just a simple part now we're going to take this model once it's trained and exported and we're going to turn it into an api using fast api and then we're going to also build a simple html page that is going going to work standalone of course uh in a real application you would have a front end or something we're just going to use a simple html file to access the api but you could also do it in a command line if you want want to so i'm going to run now or i'm going to create now a python file that i'm going to call main py and here i'm going to import the following things first of all io which is a core python package second of all pickle which is also a core python package then numpy as np i think numpy should be already installed as part of pyed learn if not just install numpy as well then we're going to import uh pillow so p.,345,4,4,5PgqzVG9SCk
5,uh image actually no image ops sorry and then also from pill we're going to import image or actually let's just go ahead and do do it like this um import pill image and import pill image ops and then we're going to say from fast api import fast api file and upload file now fast api works with type hinting so we need to specify what kind of file types uh what kind of parameter types our parameters half data types parameters half uh and then we're going to say from fast api.,122,5,5,5PgqzVG9SCk
6,middleware doc uh c rs which stands for cross origin uh resource sharing and this is now only important for the demonstration in this video because i'm going to send a request to the server that is running from an html file and it's not going to allow it because it doesn't have the proper origin but to circumvent all of this i'm just going to allow all the origins all the methods all the headers and so on which is why we need the course middleware all right so these are the imports then we're going to load our trained model which is the mist model we just trained so mist model pickle file reading bytes is f and then we're going to say that the model is equal to pickle load and we're going to load the content of f all right so then let's create our api we're going to say app equals fast api very similar to flask here then we're going to say app add middleware and here we're going to now add the cs middleware and we're going to say allow underscore origins is going to be equal to a list containing an asterisk then i'm going to copy this and we're going to change this here to uh credentials first of all is going to be equal to true i don't even know if i need all of them i just brute force it here so methods is going to be equal to asterisk as well and then finally we had uh headers as well there you go so this is just so i can send requests using uh,358,6,6,5PgqzVG9SCk
7,the html file now the actual endpoint the actual api uh function here will be the following we're going to say app so at app like a decorator post and we're going to have the uh url pattern here predict image and the function here is going to be an asynchronous function so async define the function predictor image and this is going to take a file as a parameter which is going to have the type upload file and we're going to say this equal to file and then three dots here so this is just a type hinting this is important for um for fast api now in python itself type hinting is not important other than for readability and for other tools but in in the case of fast api the typ hinting is actually relevant and necessary so contents that we get here from this file is going to be equal to await file.,205,6,6,5PgqzVG9SCk
8,read so we're going to rate a wait for the file to be read this is then going to be our content and now we're going to extract the image from that content we're going to say the pillow image is going to be equal to image. openen and now we're going to get the by stream io. bytes iio cont like this and we're going to convert this to grayscale so to an l in my case the images that i'm going to pass are going to be uh already grayscale of course you need to say pill. image not just image um and i think does this work why is it music underlined uh actually like this there you go yeah like this so we open the by stream of contents and then we convert that to grayscale that's what we're doing here now since the way that uh psychic learn processes the images and the way uh pillow processes the images is slightly different we need to also invert it so we need to turn black into white and white into black and so on so we're going to say pillow image is going to be equal to pill image ops um do invert at least if you want to have the digits like this otherwise an easy way if you don't want to invert is you can just uh draw them white on black this is also possible so image ops and then we're going to do invert the pillow image then we're going to say pillow image is equal to pillow image.,347,7,10,5PgqzVG9SCk
9,resize in the case you don't pass a 28 28 pixels image which i'm not going to do i'm going to pass the proper size already uh you want to resize this and you want to do that with antialiasing so pillow image anti-alias why doesn't this work anti-alias shouldn't this work i think it should let's see if it produces any problems when i run this um then we're going to say the image array is going to be equal to numpy array of the image so of the pillow image and we're going to reshape this like this and then we're going to feed our image into the model and get a prediction so we're going to say model predict image array and then we're going to return the following dictionary the key is going to be prediction and the content or the value is going to be the integer of prediction zero like this all right so this is now our uh api application before we can use it though we need to somehow uh write an application or a website or an html page that uses it of course you can also use it with curl but we're going to create an index html file which is going to be our front end here um in an actual application you would have a separate front end application that calls the api or you would have some tool that calls the api but we're just going to keep it simple and create an index html file here and um what we're going to do here let's call this image classifier or actually digit,358,11,11,5PgqzVG9SCk
10,classifier uh what we're going to do is we're going to have a uh form or actually we don't need a form even we can do everything with javascript or we have to do everything with javascript we're going to say input type text is going to be uh an input or actually not text sorry this is a file obviously where uploading an image not text input type file will have the id uh image input and we're going to accept all kinds of images so image slash and we don't want to limit ourselves to a specific type here so we're going to just go with an asterisk and then we want to have a simple button and this button is going to call a function a javascript function that we need to write to upload uh and send the image and to also process the response for the given image so i'm going to say button on click is going to be equal to upload image which is a function um and the button will have the text upload or let's call it classify like this and then below this button we want to have an element with the id prediction result and by default it's going to be empty but when we press the button when we call this function it's going to fill it up with information provided that the request succeeds so we're going to now write the javascript section here we're going to say script typee um text javascript and we're going to define our asynchronous function here async function upload image now we need to use cy bracket,358,11,11,5PgqzVG9SCk
11,since we're not in python i'm going to say that the input element that we're interested in is document.,24,11,11,5PgqzVG9SCk
12,files zero so if we have a file here then of course we're going to uh process it otherwise what we're checking here is if there's not such a file we're going to just do it simple and call an alert please select a file to upload and we're going to return otherwise we're just going to say constant file is equal to input files zero so we're going to get that file which is existent and we're going to say form data is going to be equal to new form data and we're going to say form data append pend file file so we're sending the file to uh we're creating a form but we're doing it in the background asynchronously so we're not going to actually post and then reload the page or be redirected we're going to do everything here in the background which is why we create form data we append the file to the form data and then we're going to try to send the request to our api so we're going to do try and we're going to say const response is equal to a wait fetch and we're going to now access in our case since this is running on local host http colon1 1271 so local host on port 8000 which is the default fast api port and the endo was predict image which is this one here so that is the url and what we want to do here now is we want to specify the method uh method post and the content the body is going to be the form data so this is what we're,358,13,13,5PgqzVG9SCk
13,actually passing what we're actually posting to the endpoint that is what we do now what we do is we say the result is going to be equal to await response.,40,13,13,5PgqzVG9SCk
14,prediction so since this is a json object it's going to have the field prediction uh we're going to get the prediction and display it here in uh in the paragraph right so in any case of an error so if some error happens here we're going to catch it and we're going to just log it to the console as an error message so we're going to say error and then error like this and then we're going to say alert failed for some reason doesn't really matter and that is now our logic in the front end the most important part actually is just getting some image data and posting it to the back end so this is actually the whole magic this is just sending this to the back end everything else is getting the data and showing the result here so that is the the magic here sending the data to the actual endpoint so i can run this now and if i didn't make any mistakes of course i cannot just run it like this we need to open up a terminal uh can can i open a terminal here there you go current now i can say uvicorn let me just see the proper command here uvicorn main and then also the name of our app main is the name of the file and app is the name of the app this is what we created here the fast api app and then d- reload to don't have to constantly um restart the application when we make changes and then i want to open up my index html file in,358,16,16,5PgqzVG9SCk
15,the browser so there you go we have our index html file here now i can browse for a file and i can open for example digit one and classify it and you can see it has the prediction five and you can see it is actually a five so digit two should be an eight there you go eight and digit three should be a three there you go three so yeah we turned our uh we turned our machine learning model into an api that we can use in the browser of course you can make this much more professional by using uh a proper front end and a proper api with different functionalities and so on but this is how simple it is to just take a pickle file the exported version of your model and to put it into an api because actually the whole api is this and you don't even need that if you don't want to just uh play around with it in fact this is not recommended in an actual application i just did it so that i can send the request but the whole magic is this here and loading the model that's it and once you have the all you can easily do that and then you just have to write some front-end code to actually use it so that's it for today's video i hope you enjoyed it and hope you learned something if so let me know by hitting a like button and leaving a comment in the comment section down below and of course don't forget to subscribe to this channel and hit,358,16,16,5PgqzVG9SCk
16,the notification bell to not miss a single future video for free other than that thank you much for watching see you in the next video and bye,36,16,16,5PgqzVG9SCk
0,hi everyone i'm patrick and in this tutorial we learn how to deploy machine learning models with fast api and docker and then have a production ready app so you can use this template to deploy the container everywhere you want in this video we go ahead and deploy to heroku because there's a free tier and you can follow along also this approach should work in the same way for any machine learning or deep learning framework you want to use so if it's psychic learn or tensorflow or pytorch the approach is the same and it's also not too difficult so let's get started let's quickly test the final app so in this video we built a language detection model and as you can see here we have the live url at heroku app.com predict and we send a post request with this text and if we click on send we get this response with languages english if i for example switch this with the german version and click on send then the language should be german so this is correct and fun fact on the side at assembly ai we also have a automatic language detection feature in our api so this is actually a real world project so let's see if we also can develop an accurate model and deploy this so first let's build and train the model in a notebook so here i'm in a google collab and the focus of the video is not how we develop this specific model but rather how we go from notebook to production app in a moment so let's go over this very,358,0,0,h5wLuVDr0oc
1,quickly the data set is available on keggle also this notebook and all the rest of the code is available on github i put the link in the description so here we have our imports then we load the data set and analyze this so we have lots of different texts and the corresponding language this is our x and y then the first step we apply is a label encoder so this transforms the labels y so basically what it does is that for each of these texts these classes it assigns a number starting from zero and whenever we have a preprocessing step that transforms our data we have to remember this and later also use this in our code so for example this is another pre-processing step here we apply regular expressions to remove these special characters so we also have to use this in the code later then we have the typical train test split and now we build our model so it consists of two parts in this case the first one is a count vectorizer here we fit this and then transform this and then we have the second step which is a naive bayes model and then we can use model predict with the test data and then here we have some metrics and for example print the accuracy so we can see this is 98 so pretty good now one best practice we can apply here if it's possible is to combine all steps into only one step this is much less error prone and then we only have to save one model instead of two here so,358,0,0,h5wLuVDr0oc
2,with sklearn we can do this with the pipeline with tensorflow for example we often have a sequential model where we can put in all the layers in this sequential model so with the pipeline we can now combine the vectorizer and the naive bayes and then we can fit the pipeline and then we can use this pipeline to predict the test set so now we only have to apply this one step and if we compare the accuracy then we see this is the exact same result so now when we are done with training the model we have to save and download this and with sk learn we can do this with pickle dump with tensorflow and pie charge there's also a very simple api to save your model one thing i recommend to do here is to also save a model version so you can keep track of the current version you have so we use this syntax major minor and patch version so this is our first miner model version and one other thing i want to mention here is that now in this case this will be one pickle file so we can actually go ahead now and click on download here but if you use tensorflow or pytorch then often it will save this in one whole directory and we cannot download a whole directory so there's no download button so one trick you can apply is to sip a folder with this command so exclamation mark sip and then you give it the name of the zip file and the name of the folder and then you can,358,0,0,h5wLuVDr0oc
3,download the zip file so now we have this and um finally let's test this one more time so here we have the text and if we run this then we see this is italian and we can also see that y is only this number eight here so here we actually apply the label encoder classes to get the actual um language so italian so i printed this in the top so here are the label encoder classes so we also have to get this for our code and yeah now we're done so now we can use this model and build our app now let's create the fast api app and for this in the root directory let's create an inner directory app and in here we will put all the code so we have one main.pi file this is where we will put the fast api endpoints and i go over this in a moment and then we have a inner model directory and here i start the downloaded pickle file the trained model and then another file that i called model.pine basically this is a helper file that does the model prediction so here i hard coded the model version then i also use path and the path of the current file to get the base directory and this is because the folder structure inside the docker container can be a little bit different and i want to make sure that we can find this pickle file here so then we can open this and make sure that this version corresponds to the file name and then we can say pickle load,358,0,0,h5wLuVDr0oc
4,and loaded our model then here we have the classes so this is from the label encoder the classes in the same order and then we only need one helper function in this case predict pipeline that gets the text so this is a string and then here we do all the same pre-processing steps that we did in the notebook and then we can say model predict we because we now have only one step with the pipeline object so then we get the prediction and this is a number so then we have to access the index of the classes and then we can return the language so this is the model.pi and now let's go to the main.pi so here we import fast api and base model from pedentic i show you what this is doing in a moment then we also import predict pipeline and the version as model version and be careful to start your path with app so app.model.model for this file and then it also finds it in the docker container then we create the app and in fast api it's super simple to create your endpoint so it's very similar to flask we define a function and then decorate this with app.get or app.post so i often like to have a endpoint for a health check so here we simply return health check okay and here also the model version for additional information for example you can also return the api version here and then we have one predict endpoint this is a post endpoint and in here we simply call predict pipeline and put in the text,358,0,0,h5wLuVDr0oc
5,and then we return the language and this is a dictionary and now to make sure that we pass the correct data types to this api when we send the data so the input we want to have should be a string and fast api works with tie pins so for this we can define a class text in that inherits from base model and this should only have one field this is a text this should be a string and now when we send the data and this is not a string then fast api can detect this for us automatically and then raise an error or show an error in the api and this is super cool a super cool feature of fast api so this is why we use this base model and in a similar way we do this for the output so for the response we say prediction out which inherits from base model and here this should be one field language and this should also be a string and then in the code here we can access payload text and we have to make sure that we have one field language and this is basically all we need for this simple api and now we can dockerize it now to dockerize this of course you have to have docker installed on your machine and then in the root directory we need to create a file that is called docker file and then it's also a good practice to have a dot docker ignore this is similar to a dot git ignore here i simply copy paste this from github you,358,0,0,h5wLuVDr0oc
6,find this in the description and this ignores certain files inside the container and now for the docker file we can go to the official fast api docs there's one section fast api in containers so i recommend reading through this because there are different ways of doing it one way of doing it is to use the official docker image with g unicorn uv corn you can also find this here on github so it says docker image with uv corn managed by g unicorn for high performance fast api web apps and in order to use this we can copy this code so this goes in the docker file and this is basically all we need to do and this uses this base image then it copies the requirements txt inside the container then it runs pip install requirements and then it copies the app directory also in the container and that's why also we have to have this folder named app and then we need one file that is called requirements.txt and here we put in all the dependencies so in our case since we already used the base image it already includes fast api uv corn and g unicorn so the only missing dependency in our project is scikit-learn for example if you use tensorflow then you can put in tensorflow here or pytorch and for example it's also important to mention or worth mentioning that you can use tensorflow cpu oftentimes because you don't need the full version and then your container will be much smaller so then it's also good practice to pin the version so for this we can,358,0,0,h5wLuVDr0oc
7,go back to the notebook and then in here we can for example say import sk learn and then sk learn dot underscore version this should give you the current version and then we can copy this so it's 1 0 2 so it follows the same pattern major minor patch so let's go back and say equal equal and then this and now this is all that we need so now we can build the image now to build the docker image we can open a terminal and then i actually noticed one small change that we have to do so as you can see in the docker file we have the app requirements txt so basically an app directory is the new root directory and we have an inner app directory so we have to say copy from app to app slash app and now save this and now we can say docker build and then minus t and give it a name here i call this language detection app and then a dot for the current folder and then hit enter and now this will build the docker image and now this was already done so if you do this for the first time then this might take a few seconds or minutes so now we have this and now we can run the container by saying docker run and then we map the port from the docker container to our machine by using 8080 so this is port 80 and then the name language detection app so now this is running and starting and now you should see that the uv converters,358,0,0,h5wLuVDr0oc
8,are starting and it's listening to zero zero zero port 80 so basically this is our localhost port 80 so now we can find this and then go to this route and for the base route this is a get request so this is working health check okay and model version is zero one zero so now we can for example use postmen to send a post request or what's really cool with fast api is that we get automatic documentation by using slash docs and here we can see all the endpoints so we have the home and we have slash predict and then we also can try this out and as you can see it it knows that we need this schema with a text field and then the string and this is because if we go back here we defined this as a base model with text so the text and it needs to be a string so that's why it knows that we need these fields and then we can click on try it out and let's say hello how are you and then execute and then we get the response directly here and can see language english and you also get the curl command if you want to try it from curl so let's try it with a german sentence so let's say hello we get as diem question mark and then execute this and then language is german so this is working so now as last step let's deploy this to heroku and now we can deploy the container everywhere we want in our case we do it on heroku,358,0,0,h5wLuVDr0oc
9,so let's actually stop our local container and clear this and now um the first thing we want is a git repository so we can say git init and of course you have to have git installed on your machine this will initialize an empty git repository then i also want to add a dot get ignore so here we also ignore some files and folders we don't need so i only ignore the virtual environment and this file and now you could continue in this terminal i actually want to switch to my normal one so here we now say get at dot so everything so we can check get status um that all these files have been added and now we can say git commit and give it a message so let's say initial commit and now we can um start creating our heroku project so for this of course we need to have a heroku account and the heroku command line interface installed and now we can do everything from the command line so first we say heroku login and now this should open the browser and then here you should be able to put in your credentials so i think i already did this and this should be stored so yeah so now we can go back and see we are now logged in and now before we can continue we actually need one more file also in the root directory and this is called heroku.yaml and for this i can recommend a documentation site on the official heroku dev center building docker images with heroku yamo so basically we only need,358,0,0,h5wLuVDr0oc
10,this part so let's copy and paste this in here then of course we have to add this to git again so we say git at heroku yamo and then we say git commit and add heroku yamal as message and now we can create a new heroku project so we say heroku create and then we have to give it a name so let's use language detection app and then it needs to have a unique name that is available so let's try one two and see if this works all right so this worked so now this will be the url that we can use to access our api so now we can say heroku and then git remote and then we set the remote for this project so this was called um language detection app one two and now this will create a remote and now we can say um heroku stack set container because this is using a docker container and now we only have to push this with git so now we can say git push heroku main but as we can notice we are still on the master branch so we have to say or have to change the name of the branch to main by saying git branch dash m main now we are on the main and now we can say git push heroku main and now it will push everything to heroku and start the app so this might take a while all right and this worked so deployment is done so now we can grab the url we've seen above so this one is where our,358,0,0,h5wLuVDr0oc
11,api is now live so this time i want to test this with postman so we can check the home url with a get request and send this and then we see health check okay and model version and now we send a post request with this data to slash predict and now let's see language is german so now let's check a different um text so now let's say ciao and now let's send this and we get italian so this works so this is our app deployed at this end point and this is all that i wanted to show you i hope you really enjoyed this tutorial if you have any questions let me know in the comments below and then i hope to see you in the next video bye,174,0,0,h5wLuVDr0oc
0,"in this video, i'll teach you fast api by working through a real project. i'll go over everything from the absolute basics to some more advanced concepts like setting up authentication, logging in various users, connecting to a database, and all of the components that you actually need if you want to build a real production-grade application. now, that said, this video is not designed for absolute beginners. while i will teach you everything from scratch as it relates to apis and kind of web app development, i'm going to assume that you have some experience in python. that said, let's quickly have a look at the finished project and then we'll get into some of the theory and start writing some code. so, the project we'll be building here is a simple photo and video sharing application. think of it like the really early days of instagram except it won't have nearly as many features. now, the way this works is you can sign in. so, i've set up a really simple user interface here in something called streamllet. we'll talk a little bit about that later in the video, but this isn't going to be focused on building the interface. it's more going to be focused on the back end and the logic and the well api with fast api. so, you can see that i was able to sign in here and then immediately i'm brought to a feed where i can see some different photos. i can see the date they were posted and the user that posted them. and then i also have some videos as well. now, you have the ability to upload something.",364,0,14,SR5NYCdzKkc
1,"now, you have the ability to upload something. so, for example, if you were to come here and let's just pick maybe a random photo that we have here. okay, let's just go like this and call this thumbnail. and then we share it. it's just going to take a second. and then this will be uploaded to our feed. so right now it's just uploading the video. and if we go back to the feed here, we should see the photo app. and you can see the photo shows up by us and that it's on today's date when i'm recording this video. okay, so that's the application. i know it seems pretty basic, but i promise this is going to teach you a ton of concepts that you need to understand as it relates to fast api. and i think the most important thing is all of the authentication and authorization which most people skip in these beginner type tutorials. so with that said, let's hop over to this myro board that i put together because i want to start going through some theory that's really important to understand before we can even start building apis. and by the way, as we get later into the video and we're going to start setting up the images and videos, that is notoriously pretty difficult to do. in order to do that effectively, we're going to use today's sponsor, image kit. don't worry, they are free to use and play with. you don't need to pay for them and they just make this process significantly easier. so big shout out to them, but more on that later.",360,14,31,SR5NYCdzKkc
2,"so big shout out to them, but more on that later. okay, so let's get into the video. now, we're going to use fast api, right? this is fast application programming interface. that's what api stands for. now, this is essentially a back-end framework. what that means is that this is going to be running on some type of server and it is going to be essentially controlling data. all an api really does for us is it facilitates the access and control of data. in our case, it's going to be image or video posts, right? or different user accounts. but before we can get into all of that, we need to start understanding some kind of core concepts of web apps in general. so, let's start by talking about urls and endpoints so that we can get the terminology out of the way. now, this whole thing right here and this one as well is a url. you've seen this, you know, millions of times before, especially when you've browsed to a website. and i want to just go over the components of the url so that we understand what they are. now the first is the domain. okay, the domain is essentially the website, you know, the space of the url. so training.devlaunch us. this is our domain. techwithim.net. this is the domain. the domain will typically end in like us or.com or.net or.ca or something along those lines. now after the domain, you have what's referred to as the path or sometimes called the endpoint. now, this is the particular route or kind of the page or resource that you're going to be accessing from this domain.",365,31,54,SR5NYCdzKkc
3,"now, this is the particular route or kind of the page or resource that you're going to be accessing from this domain. so, for example, we have training.devaunch.us tim. so, i'm going to the tim page right here. courses python. so, i'm going to the python courses page right. and for a typical website, these make a lot of sense. but for our apis, we're going to have to design these ourselves to kind of control the access and the route or the endpoint to particular resources. so when we look at the project that we're going to build here where we're sharing, you know, videos or photos, we might have an endpoint that is, you know, our aapi.com photo, right? and then we can access a particular photo. you'll see what i mean in a second, but let's keep going. now, the next point is the query parameter. now, the query parameter is some extra bit of information that is typically used to filter the page or to get some more specific type of data. it always will come after a question mark. you'll see some kind of path or endpoint, a question mark and then one or multiple query parameters. in this case, we have a parameter video equal to 1 2 3. okay. and then if we come here, we have utm source is equal to youtube and page is equal to two. so we have two parameters and you can have as many parameters as you like. you just have to have these amperands that separates them. okay? so just understand that these are the core components of a url and an endpoint.",360,54,74,SR5NYCdzKkc
4,"so just understand that these are the core components of a url and an endpoint. now let's keep going here and talk about the request and the response structure. so whenever we visit some type of website, we refer to that website as the client or the front end. okay, so us as a user, we go to our computer, we type, you know, some website. okay, and then we are now on the client or the front end. okay. and the front end is this kind of visual interface that we're able to interact with, that we're able to use. in the case of our post application or our photo application, we can see the different post, right? we can make a post, we can sign in, we can sign out. now, the way that that actually works behind the scenes is that this user interface is communicating with some type of api. api stands for application programming interface. the api you can essentially see as kind of a secure layer. it's running on a different device, some kind of server, so some computer essentially sitting in some location and it's facilitating all of the access to our data. so if we want to sign into our account, we send a request from the front end or the client to this api and this api returns some response. okay, this is the flow that i'm trying to get you to understand. you go to some website, you do something. if this thing involves some access to data, right?",336,74,90,SR5NYCdzKkc
5,"if this thing involves some access to data, right? especially if it's maybe some confidential data or something, what needs to happen is a request will be sent to some backend, this kind of secure location. it will essentially check can this user do the thing that they want to do and then if they can, it will send this response back doing that thing. for example, deleting a post. we would send a request to the backend. the backend would say, okay, yep, you know, i'm going to be able to delete this post. and then it would return the response saying, hey, this post was deleted. i want to upload a photo. i send a request to my backend. i say, i want to upload some photo. it returns some response and says, yes, your photo was uploaded successfully. so, all of the heavy lifting, all of the secure operations, everything related to data essentially takes place on this backend or this api. and that's what i'm going to be showing you how to build in this video. now, when we send a request, so we have this front end, right? this client, it sends some data to the back end. and the main parts of a request are the following. okay, we have a type of the request which we're going to talk about in a minute or the method. we have a path. the path is what we looked at here, right? so this path or this endpoint and oops, i didn't mean to save that. and we have a body. this is optional, but this includes additional data that we want to send along with the request.",366,90,111,SR5NYCdzKkc
6,"this is optional, but this includes additional data that we want to send along with the request. so for example, like the image that we want to upload or the caption or the name of the post or something like that. and then we have headers. this is typically other additional information that has to do with things like authentication. so in the header of our request, we would include something that indicates that we are, you know, this user. okay, we are signed in as, you know, tim attechwithtim.net. the header would kind of indicate that when we send that to the back end. i know this seems a little bit vague. just bear with me. we're going to make all of this crystal uh clear, sorry, when we actually get into the api example. we then have the response. okay, so the request is the thing that we send from the front end to the back end essentially saying hey we want to do something and the response is what comes back from the back end to our front end or to our client. remember the front end is the thing that you as the user actually see and then the backend is the thing that we as a developer typically write that facilitates all of the communication and the data. so from the response we include a status code. you may have seen something like you know 404 before which means not found. that's an example of a status code. so our backend will send a status code to the front end indicating what happened with the request.",352,111,127,SR5NYCdzKkc
7,"so our backend will send a status code to the front end indicating what happened with the request. common status codes are something like 200 for example, which means successful. i'm going to show you a few more in a second and you get the idea. then we have a body. same as we have the body in the request, we can send some additional data back to the front end that the front end might need. and then we have headers. same thing. headers will be related to typically security, authentication, the type of data, some weird things like that. if i scroll down here, i'm just going to show you a few status codes and a few of the request types which are important to understand. actually, let's start with the request types. so, here is an example of a very simple api. this is a books api and this is the books route or path or endpoint or whatever you want to call it. now, you can see that we have something called a get endpoint. what this means is that we are retrieving some data from this resource essentially from books. we then have delete. this is the type of method you would use when you want to delete something. we have post. this is the type of method you use when you want to create something. and you have put. this is the type of uh method you would use when you want to update something. so remember how i mentioned when we send a request, we specify some type or some method and that indicates what the front end wants to do.",361,127,147,SR5NYCdzKkc
8,"so remember how i mentioned when we send a request, we specify some type or some method and that indicates what the front end wants to do. so, if the front end wants to get some data about some books, it sends a get request. if it wants to delete a book, it sends a delete request. if it wants to create a new book, it sends a post request. and you can send all of these requests to the same route or the same endpoint. okay? and based on the type of the request, you can do something different. so, i hope that makes sense. but these are the common methods or types of requests that you can send. then we go over to http status codes. now, you don't need to memorize all of this, but these are just some examples of status codes that you may see. so, you send this request, right? you say, hey, i want to get a book, for example. all right? then, what the uh back end or the api is going to do is it's going to retrieve that data for you. it's going to send it to you and along with that, it's going to give you a status code. so, for example, it may say 200, which means okay. may say 2011 because it created something. you know, you get all these different ones like these redirection. you have errors like bad request, unauthorized, payment required, a bunch of stuff that you can look at. you know, internal server error. don't need to memorize them. i'm just showing you that there's a bunch of status codes that are commonly used in web development. okay.",370,147,170,SR5NYCdzKkc
9,"okay. now, let's just have a look at an example request and response. and i promise we'll get into the api. and again, this will all crystallize, but this is just going to really help you understand how we design the apis. okay, so user wants to update a post that they made. so, the type is patch. this is actually the exact same thing as put. so if you ever see patch, it's the same as put, which just means you want to update something. if i wanted to create something, i would use post, right? but i don't want to do that. i want to update. so i'm using patch. okay, so the type or the method of my request is patch. the path is api post. and then this is the id of the post. so i'm saying, okay, i want to go to my api. i want to modify a post. this is the id of the post that i want to modify. and then the body would be this. this is the data that i'm actually sending where i'm saying, hey, this is the updated title. and this is the new caption that i want to use or the description or whatever for my post. then the headers includes this. this is essentially my authorization token indicating, hey, i'm authorized to be able to perform this operation because of this thing right here, which we'll talk about later on. so we take all of this, we send this to our backend and then this is the request, okay, that we create. now from the backend we get a response back.",357,170,194,SR5NYCdzKkc
10,"now from the backend we get a response back. so we send a request, we get a response back and the response looks like this. we have status code 204 which stands for updated and then we have some body and this body says hey this is the title, this is the description, this is the post id, this is when it was updated, this is when it was created and it gives us that information back to the front end so we can display it to the user. we then have some headers and in this case we say hey the type of you know data that we're returning back here is application json which is essentially just the format of this data which we can talk about later. okay i know that's a lot of information but that is the kind of flow of an api and hopefully this is going to help us understand how we create apis in a minute when we start coding them out. now last thing that we'll talk about a little bit later i'll just quickly show it to you is the authentication. so, when it comes to using an authenticated api, um it's a little bit more complex than simply sending requests and getting responses. essentially, what you need to do is get something called a jwt token. this jwt token looks something like this where you send this along with every single request to indicate, hey, i'm authorized to perform this type of operation. essentially, you're identifying yourself and what user you are so that the api knows you can do this thing or you can't do this thing.",362,194,203,SR5NYCdzKkc
11,"essentially, you're identifying yourself and what user you are so that the api knows you can do this thing or you can't do this thing. you'll we'll look at that later. i don't want to confuse you at this point. but that is kind of the primer on web app development and apis. again, think of an api as essentially this kind of back-end server that sits there that facilitates all of the operations that have to deal with data. creating, reading, updating, deleting data. that's effectively what an api almost always deals with. and it's doing that so it can do it in a secure way and then return data to some front end where the user can view it, display it, you know, mess with it, etc. so with that said, let's get onto the computer and let's start actually writing some code in fast api. all right, so i'm inside of my code editor here and for this video i'm going to be using pycharm. now you can use any code editor or ide that you want, but i typically do recommend pycharm, especially for python projects because, well, it is pycharm and it supports python the best. in fact, i do actually have a long-term partnership with pycharm. so, if you want to use it for free, you can click the link in the description, try it out, and see if you like it. it definitely is a great editor, and again, what i recommend for pretty much any heavy python projects. okay, so what i've done inside of pycharm is i've just opened up a new folder. you can see i've got a folder here called fast api tutorial.",369,203,218,SR5NYCdzKkc
12,"you can see i've got a folder here called fast api tutorial. the way i did that is i essentially just went to open and i just opened a folder on my desktop. okay? and you can again use any editor that you want. um, just make sure you open a folder. now, from here, what i'm going to do is i'm going to open up my terminal and i'm going to start setting up my fast api project. now, in order to do that in python, you need to use something called a package manager. there's two notable package managers in python. the first is pip. the second is uv. now, i'm going to suggest that you use uv because this is significantly more modern and it just works a lot better. uh, but if you don't want to use uv, you can replace the commands i'm going to show you with pip. okay, so uv is something that you need to install. if you don't have it installed, i will leave a video on screen that explains how to do so. but once you have uv installed on your computer, what you can do from this open folder is you can type uv innit and then dot. what this is going to do is create a new uv project for you where you're able to isolate all of the dependencies for this particular project in this kind of one folder. okay, so we're going to type uvanit dot. what that's going to do for you is it's going to create a few files inside of your folder.",349,218,235,SR5NYCdzKkc
13,"what that's going to do for you is it's going to create a few files inside of your folder. you're going to see a main.py file, a piprotoml, and a few other files that you don't really need to worry about too much. now, this piprotoml file is going to include all of the dependencies for your project. and it's what we're going to start modifying now by installing some different dependencies that we need for this project. so, if you want to work with fast api, in order to do that, you need to install it. so, what we're going to do is we're going to type uvad and then we're going to start by just adding fast api. okay? so, we're going to type uvad fast api. when we do that, if we go back into pi project autotoml, you'll see this dependency has automatically been added for us. okay, now that we have it installed, we'll be able to actually use it inside of our python code. now, as well as fast api, what we're going to do is we're going to type uvad and we're going to install python-env. now, this is something that we need to use to manage environment variables because in a minute we're going to have some environment variables for handling our images and videos. so, we're going to go ahead and install this. now, there's a few other things that we need to install as well, so just bear with me. we're going to type uvad and we're going to install fast api- users and then inside of square brackets, we're going to type sql alchemy like this.",360,235,249,SR5NYCdzKkc
14,"we're going to type uvad and we're going to install fast api- users and then inside of square brackets, we're going to type sql alchemy like this. okay, so make sure it's spelled exactly like this. uv add fast api- users sql alchemy. this is what we're going to use when we start handling the authentication and the authorization later on in our project. so, we're going to go ahead and press enter. and then same thing, it should get added to our dependencies. okay, there's a few other ones that we're going to need here. so, we're going to type uv add and then we're going to add the image kit. so, we're going to say image kit like this. uh, and sorry, it's going to be image kit io. now, this is the package we're going to use to handle our images and videos again, which we'll look at later on. we're also going to say uv and we're going to add uvicorn and then standard. unicorn is a web server in python that allows us to serve our fast api application. you'll see how that works in one minute. so, we're going to add unicorn. and i promise we are almost done. just a few more that we need to add. and then, sorry, we're going to add one more here, which is going to be a io sq light. like that. we're going to use this for interacting with our database. we may potentially need some more later, but for now, i think this should be fine. and that should handle all the dependencies that we need for this project. okay.",357,249,271,SR5NYCdzKkc
15,"okay. okay, so we're going to close that and we're just going to quickly go here and make a new environment variable file. now to do that, we're going to go new file and we're going to call this file env. this is the file that you create when you want to store sensitive credentials, tokens or keys that your application is going to rely on. in our case, we need to access a key for image kit which is going to allow us to handle the image and video uploads which i want to do now. okay, so inside of this file, there are three variables that we need to define. the first is going to be imagekit_private_key. all right. the next is going to be the imagekit_public_key. and then the last is going to be the imagekit url. okay. so, we're going to put equal signs for all of these. and we're just going to quickly grab these three values. so, we don't need to come back to this until much later in the video. now, you may be wondering, what the heck are we doing, tim? we haven't even started writing the api. i promise we're going to get there. i just want to get through everything, do all of the setup, and make sure that it's all ready to go. so we can just focus on coding and i'm not moving around too much. right now what we're doing is we're creating this essentially kind of secret environment variable file. this is something that's going to hold some values that we need for uploading the images and videos.",353,271,291,SR5NYCdzKkc
16,"this is something that's going to hold some values that we need for uploading the images and videos. like i mentioned, we're going to use image kit to do this. so what i need to do is get some keys and values from imagekit. so i'm quickly just going to open up the imagekit website. i'm going to leave a link to this in the description. what we're going to do is we're just going to make a new account on here. again, it is free to use this. you do not need to pay for it. and essentially what this does is give you all kinds of amazing tools for handling your images and videos, which is typically a huge pain, but they have all kinds of things like image and video optimization, formatting, cropping. it's very interesting. so anyways, what we're going to do is make a new account. i've already just made a new one. so again, i'll leave that link in the description. and from here, what we're going to do is go on to the developer options. from developer options, we're going to look for our public key, our private key, uh, and then we're going to get our url, which is up here. so, i'm going to copy my public key and then i'm going to put my public key right here. i then am going to copy my private key, which is something that you do not want to share with other people. and before it will allow you to do this, you need do need to set a password for your account.",351,291,308,SR5NYCdzKkc
17,"and before it will allow you to do this, you need do need to set a password for your account. so, if you press this, i'm just going to blur my email, but i'll press the profile page and i'll just quickly set a password. okay, now the password is set. so, we'll go back to developer options. we'll go private key and i'm just going to copy this after i use my password. so, let's use that and copy this. okay, so now that i've copied it, i'm going to go back to pycharm. i'm going to paste it. again, don't share this with other people. i will delete it afterwards. and then lastly, we're going to grab this url endpoint. okay, which should be right up here. and we're going to paste this inside. and now we have the keys that we need. okay, so we're going to close the environment variable file. and now what we're going to do is start setting up kind of the scaffolding for our project. so i'm going to make a new folder. and this folder is going to be called src. this is typically best practice when you're writing a fast api application. you create this source or actually let's change it to be an app directory where you actually have all of the code for your application and then you have this main py file which is kind of what triggers the application to run. so what we're going to do inside of this app folder is we're going to make a new file and we're going to call this app.",352,308,328,SR5NYCdzKkc
18,"so what we're going to do inside of this app folder is we're going to make a new file and we're going to call this app. py and this is where we're going to start actually writing our fast api app and start getting into some python code. all right, so let's start writing a api. we've gotten to the point where everything is set up. what we're going to do from this uh file right here. so from here we're going to type from fast api import fast api with this capitalization and we're going to say app is equal to fast api with a set of parenthesis. now this is the fast api application that we just created. and what we'll need to do now is start setting up the different paths or endpoints that we want to have accessible on our api. now remember for our api, we're setting this up essentially to handle data, to be able to accept some type of request from our front end or our client and to return some type of data. so we can create data, delete data, read data, update data, right? we need to decide because we're designing this api. so i'm going to start by just writing some simple dummy endpoints just to test and see how this works. then we'll get into endpoints that actually make more sense. so the way that you make an endpoint in fast api is you type app which is the name of this variable right here that we defined dot and then you specify the method for this particular endpoint.",351,328,341,SR5NYCdzKkc
19,"so the way that you make an endpoint in fast api is you type app which is the name of this variable right here that we defined dot and then you specify the method for this particular endpoint. so it can be get, post, put, delete depending on what you want this to do. now the most basic type which is common is to use app.get. now when you do this what you're going to do is you're going to specify the endpoint. so we're going to say slash and then something like you know hello dashworld. okay, so this is the path or the endpoint. now, i also forgot i need to put an at symbol here because this needs to be a decorator in fast api. a decorator is something with the at before it. and what you do beneath this is you define a function. the function should typically be named something similar to this endpoint or path, but it doesn't need to be. you put a set of parenthesis, and then inside of here, you can return some data. so, let's just quickly return the autocompleted data where it says message hello world. okay, so we have atapp.get hello world. what we're saying is, hey, when you go to our api and you go to hello world, this function is going to be called and then we're going to return this data. now, the data that we always return from our endpoints is either going to be a paidantic object, which we'll talk about later. i know that might not make a lot of sense, or it's going to be a python dictionary.",360,341,356,SR5NYCdzKkc
20,"i know that might not make a lot of sense, or it's going to be a python dictionary. the python dictionary looks like this, right? you have some key associated with some value. and the reason why we return python dictionaries is because when we create apis, we work with something called json. now, json stands for javascript object notation. it is the format essentially for dealing with data across the web. and you can essentially think of json the exact same as you would think as a python dictionary. okay, it's not exactly the same. there's a few minor differences, but in our case, we can assume that anything that is a valid python dictionary will be a valid json object. again, keep in mind there's some caveats there, but generally that is the case, especially with simple data. okay, so now we've got this application, right? we've defined this endpoint, but what we need to do is run it. now, there's many different ways to run the api, but the way that i'm going to suggest we do it is by going into this main.py py file here, deleting everything inside of here, and then importing this app file and running it using something called unicorn. so, what we're going to do is we're going to say import uicorn like this. we're then going to say if_ame is equal to_main then we're going to say unicorn.run. we're going to put app colon app. i know this seems weird. i'll explain what it is in one second.",335,356,373,SR5NYCdzKkc
21,"i'll explain what it is in one second. we're going to say host is equal to 0.0.0.0 and we're going to say the port uh is equal to 8,000 and we're going to say reload is equal to true. okay. now, what are we doing here? well, first what we're saying is all right, i want to use this web server called uicorn, which we've already installed, right, with uv, and i want to run a web server. now, for the web server, i want to run an api on it. the api that i want to run is app.app. so that's inside of app. so the app folder here, the app file and then i want to run the api which is inside of the variable app. so let's say i were to change this and i called this hi. okay, then i would change this to be hi. all right, so just keep that in mind. that's how i'm getting these variables essentially. so we have the name of the folder is app, right? the name of the python application is app and then this here is app and we have app.app. probably should have picked a better name for that, but it's okay. hopefully, you get the idea. now, when i say host, this is specifying the domain essentially that i want to run this server on. because we're running this locally on our own computer, when i specify 0.0.0.0, that just means run it on any available domain. so, it's going to run on what's called local host, which is just our own host, so only we can access it.",356,373,392,SR5NYCdzKkc
22,"so, it's going to run on what's called local host, which is just our own host, so only we can access it. as well as our private ip address, meaning anyone else on the network would be able to access this as well if they knew the private ip address of this machine. now, there is ways to run this publicly, so anyone can access it. not going to get into that in this video, but essentially the way that you're going to be able to access this application is you're going to go to whatever the ip address of this machine is. if we're on the same machine, it's going to be localhost. we're going to go to port 8000, and then we can access this resource right here, which is hello-orld. so for this what we can do is run this main.py file. to do that we simply type uv run main.py. go ahead and press enter. and it says that there's some issue. this is because i'm currently running this app on another um uh what is it? editor. so let me just shut the other app down and rerun it. and then we should be good to go. you can see that it's running now. again that issue you wouldn't have run into. it's cuz i had a demo application running in a different um editor that i have open.",304,392,408,SR5NYCdzKkc
23,"it's cuz i had a demo application running in a different um editor that i have open. you'll see what's happened here is it now says unicorn running on and then it shows you the url or the domain where this is running right and then it kind of goes through this thing saying hey you know started the reloader process and by me specifying reload equals true anytime i save or make a change to this file like if i do something i don't know hello here and then i save this you'll notice that the file or sorry the server will shut down and restart with the changes that i made. so, it's really useful for when we're debugging and building something because it just shuts down and restarts anytime you make a change. now, if i want to actually be able to view my application, what i can do is go to this url. so, you can just click it and open it up. now, it's saying this 0.0.0 isn't working. so, what we can do is change this to be 127.0.0.1 or localhost port 8000. and when we do that, it should give us this uh thing here saying detail not found. that is totally fine. that's exactly exactly what we're expecting. okay, so our api is now running and it's time to talk about the coolest feature of fast api, which is the docs endpoint. so here, what you can do whenever you have a fast api application is you can go to doccks.",336,408,419,SR5NYCdzKkc
24,"so here, what you can do whenever you have a fast api application is you can go to doccks. okay, when you do that, it's going to bring you to a page that looks like this, which actually specifies all of the endpoints and the configuration that you've set up for your api. so from here, if i open this up, we'll be able to actually test out our api by pressing this try out button. this is going to send a sample request to this endpoint and then tell us what the response would have been. so what i can do is press try it out. i can press execute and then you see what it does is it sends a request to this url and then it tells me that i got this as my response saying message hello world and then it also told me the code of this was 200 which means success. okay, 200 successful response. so there you go. we just sent a request, right? we tested it out. it's all working. this is the thing i love about fast api is that you can actually do this. you can directly go here and test out all of your endpoints by simply going to the slashdocs endpoint. this will become more useful later on, but always check this out. it's very, very useful. now, there's also another endpoint uh called redoc. this is kind of a newer version of that docs endpoint. it works the exact same way. we can test this out if we want um you know test the api etc etc uh and kind of see how this works.",361,419,437,SR5NYCdzKkc
25,"we can test this out if we want um you know test the api etc etc uh and kind of see how this works. i am not going to uh dive into this too much right now but the point is there's this other endpoint called redoc which you should be aware of but the one that i prefer to use is called doccks. now just another quick thing if we wanted to we also could just directly go to slashhello-world. if we do that you see it will give us message hello world because we set up a get endpoint and by default whenever you go to a url in your browser you send what's called a get request right and because we sent the get request we got the response back and the browser is able to actually render it and show it for us. but generally for all of the other endpoint requests we're going to be looking at, we're going to have to use this docs page or another tool to test the api. so we've now written kind of this dummy get endpoint. however, it doesn't really help us accomplish our project goal, right, of creating, you know, posts. so what i want to do now is i want to start adjusting the endpoint to actually make sense to our project. it will change over time, but we're going to slowly kind of build towards what's called a crud application where we have create, read, update, and delete functionality. so, what i'm going to do is i'm going to delete this and i'm going to start setting up some stuff for handling posts.",361,437,446,SR5NYCdzKkc
26,"so, what i'm going to do is i'm going to delete this and i'm going to start setting up some stuff for handling posts. so, what i want to do is i want to set up my application so i can essentially retrieve and create new user posts. for now, we're going to start with text posts, but then later we'll get into the images. so, i'm going to make a new dictionary and i'm going to call this my text posts is equal to and we're just going to have an empty dictionary like this. okay. now, what we're going to do is we're going to make an endpoint and we're going to say at app.get get and this is going to be slash posts. okay. and for the function, we're going to say define get all posts like that. now, what this is going to do is just return all of the posts that we have. so, we're simply just going to return text post like that. super simple. if we go back here now to this and we refresh, you'll see that we have uh why is it still showing that? okay, let me just restart my server because for some reason sometimes this messes up. so, we'll just restart it. okay. and i don't know what was going on there. i had some weird issue. but anyways, i got this now back to the docs page. and you can see we now have slashposts. and if i just try this out and execute, you see it just gives me an empty response. okay. that's what we're expecting because currently we don't have any posts.",361,446,467,SR5NYCdzKkc
27,"that's what we're expecting because currently we don't have any posts. however, if we put a post in here, then we would be able to retrieve it. so, that's a good endpoint. but what i'm going to do now is i'm going to start making some posts. so, i'm going to have some id like one. okay. and i'm going to have this associated, if i can type properly, with some post. so for my post, i'm going to have another dictionary. i'm going to have title, you know, new post and content, you know, cool test post. okay. and now i want to make an endpoint that allows me to retrieve one individual post. so first of all, let's just go back actually and let's quickly test and go refresh. okay. and let's try this out and execute. and you can see that we get the one post showing up. but maybe i want to be able to kind of filter and get just an individual post. so i'm going to make a new endpoint. i'm going to say at app.get and i'm going to type post slash and then inside of parenthesis i'm going to type what's known as a path parameter. so this is a dynamic value that we can actually change and adjust in order to get an individual post. so we're doing id, right? and then what i'm going to do here is say define get_ost and i'm going to say id is of type int.",326,467,487,SR5NYCdzKkc
28,"and then what i'm going to do here is say define get_ost and i'm going to say id is of type int. now what this is doing is it's going to directly map this id parameter to the id value that i have inside of this path parameter inside of curly braces and give it as a parameter to my function so i can use it inside of here. so now what i can do is i can say return text post okay do get and then i can get id okay so now what i'm able to do is if i go here and i refresh we should see that we have another endpoint you can see it says get post with an id we can pass the id so let's pass id of one and we execute this um it's not giving us anything and i see the problem that is because id is a string when it should be a number. so if we change this to a number now and we go refresh and then try out go with one and execute. you can see now that we get the individual post rather than a list of all of the posts. okay. so just showing you this is how you create what's called a path parameter. now if we want to return an error here if the post doesn't exist, what i need to do is i need to import this http exception. now from this function i can do something like if id not in text posts then i can raise an http exception.",349,487,495,SR5NYCdzKkc
29,"now from this function i can do something like if id not in text posts then i can raise an http exception. i can specify a status code in this case something like 404 and i can say the detail post not found. when i do that that's going to indicate okay we've had a 404 error you know not found and then post.found and that's how you can return an error. so now if we go back here and we refresh and we try to access a post with id like four or something and execute, you'll see that it says detail 404 and then it gave us a 404 error code. okay, so what i just did quickly is i just had chatbt generate a bunch of test posts because i'm going to use these um throughout the rest of this kind of section here to demo a few more things. all right, so what we've done is we've added a bunch of text posts. we've added what's called a path parameter. we had a normal kind of query endpoint here. and the next thing that i want to do is show you how we can use what's called query parameters inside of our functions. so up until this point it's been pretty straightforward, right? we just can call ost. we can call for a particular id. now i want to make it work so that we can do some kind of more advanced filtering using query parameters and then we'll continue from there.",332,495,507,SR5NYCdzKkc
30,"now i want to make it work so that we can do some kind of more advanced filtering using query parameters and then we'll continue from there. so i'm actually going to go inside of this function right here and i'm going to start adding some optional query parameters that we can pass to this that will allow us to kind of filter some of the content. so a query parameter remember is the thing that comes after the question mark. so something like maybe you know length equals 10 or something right whatever. um so that we can actually filter kind of the number of posts maybe that we're receiving. so what i'm going to do is i'm going to add a query parameter called limit. i'm going to say limit colon int is equal to and by default it's going to be none. now what i've just done is i've just specified that i now have the ability to pass a query parameter called limit to this post endpoint. and i can then check if this query parameter exists. if it does, i can use it. if it doesn't, i don't have to. so my idea with the limit parameter is that maybe i don't want to receive all 10 posts. maybe i only want to receive the first three. well, i can specify that with the limit. so let me show you how this works. i can do something like if limit, then what i'm going to do is say return text post up to the limit.",338,507,522,SR5NYCdzKkc
31,"i can do something like if limit, then what i'm going to do is say return text post up to the limit. okay, now this is not bulletproof because if the limit is larger than the number of posts, that will give us an error. but for now, that's fine. and we'll just use that and then otherwise we'll just return the text post. so now if i go back to my page here and i refresh you're going to see that if i look at post we have the ability to add this limit query parameter. so what i can do is go try it out for limit i can pass maybe three for example and then execute and what it gave us an error. okay that's weird. so the reason we got that error sorry is because uh we're trying to apply a list operation on a dictionary. there's multiple ways we can fix this, but for now, what i'm going to do just to make it easy is i'm going to say list of text posts dot and this is going to be uh values like that. so, we'll just convert the values into a list and then return them. so, let's go back here and let's go refresh. and then we'll go here. you can see we have a limit again. let's go maybe five and execute. and now you see we get a list with five items. if we change this to three, we get only three items. and if we don't have any limit at all and we execute, we get all of the items showing up.",354,522,538,SR5NYCdzKkc
32,"and if we don't have any limit at all and we execute, we get all of the items showing up. okay, so just showing you that's how you add a query parameter. if you want to do that, you simply specify it in the uh parameters here. you can make it optional, which means you can have something like is equal to none or you can make it mandatory by removing this and now you have to pass the query parameter. if you don't um then you could potentially get some errors inside of the function. you can pass multiple parameters like maybe you know content length or something or whatever and then you can specify something like int can make it string. the reason why you need to specify the type here is so that it can be auto documented and validated by fast api. the way the fast api works is that it automatically validates all of the data input that's coming into the function and out of the function for you. so when i specify that this is an int, if i try to send something other than an int to this function, it's actually going to raise an error for me, okay? other than a number. so this documentation is actually very very good inside of fast api. and that's why you use what's called a python type hint inside of the parameters and you specify like what does the function return, what does it accept so that it can be really well documented and it can have what's called this data validation.",347,538,549,SR5NYCdzKkc
33,"and that's why you use what's called a python type hint inside of the parameters and you specify like what does the function return, what does it accept so that it can be really well documented and it can have what's called this data validation. okay, so at this point we've looked at query parameters, we've looked at path parameters, we've looked at the get endpoint extensively. now i want to look at the post endpoint and creating new data. so what i'm going to do is type at app.post and i'm going to do slashpost. okay. now from here what i want to do is be able to create a post. so i'm going to make a function called create_ost. what we're going to take for creating a post is actually something different than for getting a post. so, like i said in fast api, it has automatic data validation, which means it's going to check the data that's being sent into the function to make sure it's accurate. now, up until this point, we looked at the query parameters and the path parameters, but there's another way that we can send data to our api, and it's by using something called the request body. okay, the body is kind of like more hidden information. it's not directly inside of the url. it's in the field called body. and the way that we accept that type of data is by creating something called a schema in fast api. so what i'm going to do is i'm going to make a new file here. and i'm going to call this schemas. py. okay.",354,549,566,SR5NYCdzKkc
34,"okay. now inside of here, what we're going to do is we're going to define the type of data that we want to accept in our various endpoints. so in order to do this, we're going to say import or we're going to say sorry from piantic import the base model and we're going to define a python class. so we're going to say class post create. okay. and this is going to inherit from the base model. and then we're going to specify in here the fields that we want to uh accept essentially for a post. so for a post we're going to accept a title and we're going to accept some content. okay, so we have title and content for our post. now, the way that this works is that you inherit from this base model, which is kind of this special object in python that has some special features. and what we're able to do now is use this as a type to kind of receive body data inside of our functions. again, i know it seems a little bit confusing. this is referred to as something called a schema. very common inside of fast api to use this and it's common that you put it in a separate file called schemas. so from app.py, pi we're going to say from app do schemas import and then we're going to import the schema that we just wrote which is the postcreate schema. okay. now what we can do is for our create post we can say post. so let's do this post colon post create.",349,566,583,SR5NYCdzKkc
35,"so let's do this post colon post create. now when we do this because we're using a pidantic model by default fast api knows that we're receiving request body. okay. so not receiving the uh query parameter receiving the body. so now what we can do is we can use this post data directly inside of the function to create a new post. so we can do something like let's do the following. um okay we're going to say text post and then max of text post.keys plus one because we need to find what the next id essentially should be. and then we're going to say is equal to and we're not going to do post.dict. what we're going to do is we're going to make a dictionary and we're going to say the title is post.title and the content is post dot content. now, because in my schema i've defined that my title is a string and my content is a string, fast api will make sure that these are indeed strings before it allows me to call this function. if they aren't strings, it's actually going to automatically raise an error for me and tell me that i have a bad request, which means when i try to access the title or access the content here, i know that they're going to be valid. so again, fast api automatically validates the data that comes into the api based on the types that you set, which is extremely useful. so now we've created this post endpoint. it's just going to create this new post.",345,583,596,SR5NYCdzKkc
36,"it's just going to create this new post. but what we really should do is we should return the new post that was created. so to do that, actually, let's just do this. we can say post is equal to like this or maybe just new post is equal to this and then we can say this is equal to new post and we can return the new post. so now we have a post endpoint to create a new post. so what we can do is we can save should automatically reload. so now if we come here and we go try it out and we change this to like you know cool post and new post or something and we go execute you can see it gives us the data back. and then if we go back to posts, uh looks like the limit is required this time. so let's go like 12 and execute. and you can see we get the new post and our cool test post is showing up. awesome. so all of that is working. we now know how to accept request body, right? so this different type of data and to create data. now to delete data, it would be pretty straightforward. you go app.delete and then same thing and you would kind of continue along these lines. and there's more stuff that you could do related to that. i'm not going to show that this second. and we'll show it more when we actually get into kind of the finalized project.",336,596,614,SR5NYCdzKkc
37,"and we'll show it more when we actually get into kind of the finalized project. all right, so we're almost going to move on to databases, but i just want to cover one or two small more things about fast api that you should be aware of. now, the first is going to be the output type. so, when we created the post here, right, we're just returning new post. and if we go back to our kind of documentation here, let's just go to docs and have a look at it. you'll see that it doesn't show us like what type of data is going to be returned. okay, it gives us kind of an example, but this isn't exactly, you know, what we're looking for. it's not the best documentation in the world. so, what we can do to enhance the documentation and actually give us um some more validation in our code, which is better, is we can specify the type of data that's going to be returned from these functions. so, for example, for my create post, i can actually put this arrow here, and i can specify that i'm going to be returning this postcreate type. now that's going to look exactly like this, right? so it's the same schema that we had here. now if we wanted to return something else, we can do class post return or post response for example can be the same thing but just so the name makes more sense. and then we can change this to post response. and we can import this from here post response.",349,614,628,SR5NYCdzKkc
38,"and we can import this from here post response. so now we're indicating okay whatever we're returning from this function is going to be of this type. now what this is going to do if i go back to my documentation here and i refresh is it's now going to indicate to us if we look here the type that's actually going to be returned to us. you can see it gives us the value here. example value is going to have title and content being returned on the successful response because we've specified that type. now what it also does is it means we can only return data from this function. now that is of this type. if i try to return something else like just a normal dictionary, if i were to actually execute this endpoint, it would give me an error and say, hey, this doesn't match this schema. there must be some kind of problem. okay, so it adds a layer of protection for us to make sure that we're only returning what we want to return. and actually, in fact, let me show that to you. if i change this to like an empty dictionary and then i go to my front end and i refresh here and i just go like try it out and i execute, you'll see it actually gives me an error and it says, hey, you know, missing this data type. we're missing title. we're missing content, which we should have in the response. so, we need to go back to new post and then it'll work from now on. okay.",352,628,643,SR5NYCdzKkc
39,"okay. so, if we want to type the rest of our data, we can do that as well. so, for example, when we're getting a post, we should also say that this will be the post response. and then for getting all of the posts, uh we're not going to type this one right now because the response types are actually a little bit different even though it's not intended. anyways, you get the idea. you can do this, okay, which is another useful thing to do inside of fast api. and if you wanted to have like a list of posts, you do something like list, okay, and then post response like that. awesome. so now that we have that, what i want to do is start connecting us to databases because right now you'll notice if i refresh this application, all of a sudden my data is going to disappear. only the data that i have written inside of my code will stay persistent. now that's because right now all of our data is just inside of a dictionary that's stored in memory. so when our code refreshes, it all gets cleared and kind of reinitialized with this new dictionary. now that's not ideal. so what we're going to start setting up is database connection. so, we're going to go to our app folder and we're going to make a new file called db.py. now, inside of all of these python api libraries, you typically have something called a om.",327,643,658,SR5NYCdzKkc
40,"now, inside of all of these python api libraries, you typically have something called a om. an om stands for an object relational mapping and it's something that allows you to prevent writing sql code or nosql queries and instead to write python like code to be able to define data, retrieve data, create data, etc. okay. now, the om that we're going to use here is something called sql alchemy. um, i believe we already installed it. if we didn't, then we can install it again in a second. but the point is this will allow us to really easily work with our data. so bear with me here because this is a little bit of code that we do need to set up. not all of it's going to make, you know, 100 sense right now, but i promise it will later on as we keep going. okay, so let's start with some of our imports. so the first thing we're going to do is we're going to say from collections.abc import the async generator. we're then going to say import uyu id which is something we can use to generate a unique identifier. we're then going to say from sql alchemy import create engine. we're then going to say from sql alchemy. okay. and this is not what i want. we're going to say import the column. so let's spell column correctly. we're going to import string. we're going to import text, datetime, and foreign key, which we'll use later on. uh, and did i spell foreign correctly? no, i did not. so, let's spell foreign key correctly. okay, we're then going to say from sql alchemy.",364,658,681,SR5NYCdzKkc
41,"okay, we're then going to say from sql alchemy. okay, this is going to be dialects. let's spell that correctly. postgres sql import uu id. we're then going to say from sql alchemy.exe ext.async io import the async session and the async session maker as well as the create async uh engine. so let's bring in the async session maker. again, i know this stuff seems confusing. it's just a little bit of setup and then you never need to do it again. and then we're going to bring in from sql alchemy.org import the declarative base, but this is actually going to be a little bit different. it's going to be declarative base like that and the relationship. okay. now, by the way, all of this stuff i do not have memorized. you don't need to memorize it. it's simply something that we need to be able to set up the database. once the database is set up, then it's a lot easier for us to work with this. so, what we're going to do is we're going to define a uh variable and we're going to say the database url is equal to sqlite. okay, so sqlite plus a io sq light. okay, colon three slashes dot slash test. db. now, what this is going to do is it's going to allow us to connect to a local database file on our own computer called test.db, which is in the current directory. and sorry, this needs to be a plus here. so, let's fix that.",334,681,702,SR5NYCdzKkc
42,"so, let's fix that. that we want to use sqlite plus aios sqlite, which is essentially an asynchronous version of sqlite, which is a really simple database that we can run locally. later. if you wanted to connect to a production database, you simply need to change this url to a url string for like a remote database or a different type of database and then everything in your code would stay exactly the same. you would just work in a different database. now, what we're about to do here is we're going to define what's called our data models and then we're going to create the database which will automatically create the data models for us. now, essentially the data models is the type of data that we want to store. and if you've ever worked with databases before, you'll know that when you want to store data, you need to specify the structure of that data. at least when you're working with a sql database, which is what we're doing right here. so, we need to specify like, okay, what do we want to store for a post, for example. well, we need to know the user that posted it. we have to have an id. we want to know maybe the caption of the post, the url of the file. that's the kind of stuff that i'm talking about here. okay. so, what we're going to do is we're going to create the data model for storing a post. we're going to make it a little bit more complex now and we're going to start working towards actually being able to store videos and photos for our api.",365,702,718,SR5NYCdzKkc
43,"we're going to make it a little bit more complex now and we're going to start working towards actually being able to store videos and photos for our api. so what we're going to do is we're going to say class post and this is going to inherit from the declarative base. now this is important. you need to inherit from the declarative base so that it knows uh that we are making this a data model and this is something that we're going to store in our database. okay. now we need to specify the table name. so we're going to say underscore table named underscore underscore is equal to post for example. and then we're going to start specifying all of the fields that we want to have or all of the columns that we want to have in our data model. so we're going to say id is equal to column. and then we're going to type uuid. we're going to say as_uid is equal to true. we're going to say primary key is equal to true. and we're going to say the default is uuid. uuid4. and let me just fix the spelling here. okay. now let me quickly explain what we're doing here. essentially what we're saying is we want to have this id column. now for every single uh entity that you have in a database, you need to have some id. now the id is typically what's referred to as the primary key. the primary key is something that must be unique. so in this case, every uni that we're that sorry, every id that we have needs to be unique.",361,718,739,SR5NYCdzKkc
44,"so in this case, every uni that we're that sorry, every id that we have needs to be unique. that's why we've marked it as primary key. and because we've specified this uu id thing, what this means is that we're automatically going to generate a random unique id for it every time we insert one into the database. so every time we create a new post, a new id will randomly be generated that is guaranteed to be unique, which is the primary key or the way that we will look up this entity. okay. now, the next thing that we're going to have is we're going to have a caption. the caption is going to be some column and this is going to be text. now, you'll notice that the way that we specify the data that we want to have is we say, okay, we're going to have some name, some field name, so like caption id, we say it's going to be a column. if we're storing data, if we're storing a relationship, we would set it to a relationship or a foreign key or something different. and then you specify what you want in that column. in this case, i want to store text, right? so, i say, okay, caption, it's going to be text. then i'm going to have a url. this is going to be the url for the uh what do you call it? photo or video. and this is going to be column of string. and we're going to say nullable is equal to false, which means this can't be null. it has to have a value.",358,739,756,SR5NYCdzKkc
45,"it has to have a value. we're then going to say the file type is equal to column. same thing, string nullable is false. we're then going to have the file name. it'll be the exact same thing. so string nullable equals false. we'll then have a created at and we'll say column date time. and rather than nullable, we're just going to say default is equal to datetime dot utc now. and then we're going to import datetime. so we're going to import uh datetime like that. not now. import date time like that. and now this should work. okay. so that's all that we need for the post. later we will adjust this slightly to link it to an individual user, but for now because we don't have users in our app, we're just going to have kind of a simple post. so the way that this will work now is that whenever we want to create a post, we'll need to pass a caption, url, file type, file name, and created at or actually create at will be automatically created for us. and then we'll be able to make this new post and store it in our database. if we had other data models, we would define them inside of here or in other files and import them inside. but for now, this is how we're going to do it. so next, we need to actually create the database. so we're going to say our engine is equal to this create async engine with a database url.",338,756,777,SR5NYCdzKkc
46,"so we're going to say our engine is equal to this create async engine with a database url. we're then going to say the async session_maker is equal to the async session maker and we're going to pass our engine and then we're going to say expire on commit is equal to false. don't worry too much about that. essentially what we need to do right now is we need to create this database engine which can look at all of these models and automatically create the database for us. we're then going to say async define create db tables. what this is going to do is create the database for us as well as create the tables. so we're going to say async with engine.be. okay. and then this is going to be as connection. and what we're going to do is say await con. okay. the connection. so, run_sync and then we are going to say the declarative base. okay, dot metadata.createall. now, what this is going to do is it's going to find all of the classes that inherit from the declarative base and it's going to create them inside of the database. that's all that it's doing. we're then going to have another function. we're going to say async define get async session. okay, this is going to return an async generator with an async session and none. and then we're going to say async with and i know this is confusing. bear with me. we're going to say the async session maker as session and we're going to yield the session. okay, again bear with me. this is actually almost done at this point.",362,777,800,SR5NYCdzKkc
47,"this is actually almost done at this point. what we're doing is we're creating all the databases and the tables, right? what this does is it starts the database engine and then it creates all the tables and creates the database. then we have this get async session. what this is going to do is it is going to get a session which will allow us to actually access the database and write and read from it asynchronously. okay, so we're doing this using async. if you don't understand async in python, don't worry too much about it. i will explain the the kind of important stuff later. so that's all we need from this particular file. later we will add a few other things to store our users for the application, but for now this is fine. okay, so there we go. we've now written the database stuff. what we're going to do now is we're going to go into our app. py file and we're going to start importing some of the things that we need so we can actually use the database. so we're going to say from app db import the post the create dbn tables and the get async session. okay. now what we need to do is we need to set something up so that as soon as the application runs we create the database automatically if it's not already created. now, in order to do that, again, this is going to look a little bit complex. don't worry, just set up once and then you're good. we're going to say from sql alchemy.exd.async.io and then we're going to import the async session.",362,800,819,SR5NYCdzKkc
48,"we're going to say from sql alchemy.exd.async.io and then we're going to import the async session. then we also need to import something called the async context manager. so, we're going to say from context lib, this is built into python, import the async context manager. we're then going to say at async context manager. and we're going to say async define lifespan. we're going to take in our app which is an instance of fast api. and what we're going to do is we're going to say await and we're going to say create db and tables. and then we're going to yield. don't worry too much about this. it's kind of advanced python syntax you don't really need to understand. and then all we're going to do here is when we say app is equal to fast api, we're going to say lifespan is equal to lifespan. what this is going to do is it's going to automatically run this function as soon as the app is started. it's going to create the database and the tables for us and just make sure that this is essentially handled correctly and cleanly exit once the when sorry the application stops. now these other imports we'll use later on. uh for now we'll just leave them in. okay. so now if we just want to do kind of a sample test here, we can shut down our server and rerun it and it should automatically create the database for us. however, it's giving me an issue saying datetime.uutc utc isn't a function. let me quickly fix that. so, we're have to actually change this to say from daytime import daytime.",364,819,838,SR5NYCdzKkc
49,"so, we're have to actually change this to say from daytime import daytime. so, just change that import here in the db file. and now, if you come back here and we shut this down and we restart it and we got another error. let me quickly fix that. okay, so another silly error here. we're going to go inside of our db.py file. what we're going to do quickly is say class base and this is going to inherit from the declarative base. and then we're just going to say pass. and we're just going to change all instances of declarative base down here to say base. kind of a silly error. again, a little bit complex. and then same thing when we go here, just change this to say base. and that should fix the problem for us. essentially, we just cannot directly inherit from declarative base. uh we need to do this kind of other base class first. a little bit weird, but that will allow us to set up the database. i know the database setup seems a little bit confusing, but once it's set up again, you don't need to change it. so let me just shut this down and then restart it. okay, so the server is running. we didn't get any errors. and the way we can test if this is working is if this test db file was created. it looks like it was for me, which means the database connection is set. and now we don't really need to touch much in this database file. and we can just start using the database to actually create new posts. so in fact, let's start doing that.",369,838,862,SR5NYCdzKkc
50,"so in fact, let's start doing that. let's actually scrap pretty much everything that we have so far. i know it seems like why do we get rid of it, but trust me, we're just going to write this in a better way now. and what we're going to do is we're going to start writing the endpoints for uploading or creating a new post and doing the image and video upload. so, as i mentioned, we're going to start doing the file upload. now, to do that, we're going to have to import a few more things. and again, just bear with me. i'm going to explain it step by step as we go through that. and then we'll start adding the users, a bunch of other stuff as well. so, what we're going to do is from our fast api import at the top, actually, we're going to import file. we're going to import upload file the uh what is it form and the depends. okay. now we're going to make a new endpoint and we're going to say at app dot and this is going to be post because we're going to be uploading a file and this is going to be slashupload. now this is going to be to essentially make a new post. so when we make a new post, we're going to upload either a video or a photo and then some caption for that video or photo and then it will be posted. so in order to do this, we're going to say async define upload file.",343,862,877,SR5NYCdzKkc
51,"so in order to do this, we're going to say async define upload file. we're using async because we're going to have some async operations in here where we essentially wait for something to occur. fast api is asynchronous by default. so you can always use the async keyword for these functions. now what we're going to do is we're going to say file and this is going to be upload file is equal to file with three dots inside. what this means is that we're going to be able to receive a file object to this endpoint. okay. we're then going to say the caption is a string which is equal to some form data. so rather than actually just accepting a request body, we're going to accept something called the request form. there's different types of data you can send to the endpoint. for example, you can send a file, you can send a form, you can send a request body, you can send query parameters. this is just another type. so, we're going to accept the caption from some form data. okay. we're then going to say our session is an async session, which is equal to depends, and this is going to be get async session. now, this is going to look a little bit weird, but in fast api, we have something called dependency injection. this is essentially a dependency injection. and it'll light up in a second here when we start using it where what's going to happen is we're automatically going to get the asynchronous session by calling this function and pass it as the variable session inside of this function.",361,877,894,SR5NYCdzKkc
52,"and it'll light up in a second here when we start using it where what's going to happen is we're automatically going to get the asynchronous session by calling this function and pass it as the variable session inside of this function. this is how it works when you want to essentially trigger another function to run as a dependency for this function. so we're saying we want to get the asynchronous se session sorry for our database so we can use the database inside of this function. in order to use it in this function. that's how you do it. you write this kind of dependency injection where you say this function depends on this right here. it will run the function. it will get the database object. it will pass it to us and then we'll be able to use it directly inside of here. okay. so now inside of here, what we're going to do is we're going to take the file and we're going to upload it. that's going to take us a second to be able to do because we need to use image kit. so before i do that, i'm going to show you how we can create a new post with some kind of fake post data and then how we can do actually the upload for that. so to make a new post, we can say post is equal to post and we can specify things like the caption is equal to the caption. we can say the url is equal to you know dummy url and later we'll fill that in.",351,894,908,SR5NYCdzKkc
53,we can say the url is equal to you know dummy url and later we'll fill that in. we can say the file what is this file type is equal to for now we can just go photo then we can do a comma and we can say the file name is equal to dummy name okay so let's go dummy url dummy name again there's a few other things we can add later but for now this is fine now in order for us to actually add this to the database the way that we do that is we say session do add okay and we simply add this post here that we created. then we say await session. so the way that you add something is you create a new post or create a new object of whatever it is that you want to create. you add this to the database session and then you commit the session. it's important that you commit this because committing it will actually save it. adding it to the session is like staging it saying hey this is ready to be added but it won't actually fully be written into the database unless you commit the session. now another thing you can do after this is you can say wait session.refresh and you can refresh a particular object and what this will do now is it will go and look in the database and populate any entries here that were automatically created when it was added to the database.,334,908,915,SR5NYCdzKkc
54,"now another thing you can do after this is you can say wait session.refresh and you can refresh a particular object and what this will do now is it will go and look in the database and populate any entries here that were automatically created when it was added to the database. so for example we have this created at and this id when we specify the post we didn't specify an id or a created at that gets automatically created when we commit this in the session. so when we refresh the post it essentially gets hydrated with that extra data where we now get the id and the created at as a part of this post which we can now return to our user. so what we can do here is we can just say return and then we can return the post. we don't need to do that but we can if we want, right? so we can say return post uh and then we will get that data. so we can test this but before we're going to know if it was actually added to the database, we need to write an endpoint that's going to allow us to view the different posts. so we're going to say at app.get and we're going to call this the feed. so we're going to say slashfeed. we're then going to say async define get feed. and what we're going to do here is we're going to say the session, this is an async session is equal to depends on get async session.",343,915,925,SR5NYCdzKkc
55,"and what we're going to do here is we're going to say the session, this is an async session is equal to depends on get async session. now the reason we're doing that is because we need to access the database here in order to get all of our posts. so we need to bring this in as the dependency injection. now quickly i do need to import something. so i'm going to say from sql alchemy import select because this is going to allow us to select different posts. so now what we're going to do is we're going to say result is equal to await session. this is our database.execute. okay. and this is how you can execute a query. and we're going to say select and we want to select on the post um what do you call it object or post kind of table. and we want to say order_by. and we're going to say post dot. okay. like this created dot descending. okay, so querying here is a little bit weird, but what we're able to do is say, okay, i want to select posts, right? so, i want to start looking through posts and then you can add these various filters like i want to order it by all of the posts that are created at and the descending, right? so, i want to go in the order in which they were created. then i could also do something like, you know, dot filter, right? or like all these other things. and you can filter by specific criteria.",343,925,944,SR5NYCdzKkc
56,"and you can filter by specific criteria. i'm not going to go through all of it, but essentially if you just look up like sql, alchemy, fast api online, you'll see all of the different ways that you can query different data. if you want to just get all of the posts and you don't care about doing any kind of filtering, you can just say select post. that will give all of them to you. okay? and then if you want to check various fields or relationships, you can do that. we'll look at that a little bit later on. okay. so now we have our result. this is going to be all of our posts. now what we're going to do is say post is equal to and we're just going to go row okay zero for row in result doall. now what the resultall is going to do is just give us all of the results from this particular query. the reason why we're doing this is because i want to convert this into a list. essentially we need to step through all of the results and take them and pass them into this in order for us to actually access them all at once. it's due to the way that fast api returns the results here. it returns in what's called a cursor object where you're stepping through the database. so what i'm doing is i'm looping through all of the values and then just pulling them into individual values that i store inside of here. okay.",339,944,961,SR5NYCdzKkc
57,"okay. then what i'm going to do is i'm going to say my posts data is equal to a list and i'm going to say for post in posts and i'm going to start creating kind of a more uh comprehensive post object that i can return to my front end. that's going to include some data that we need about our post. so, we're going to say post data.append. okay. and then inside of here, we're going to have an object. for the object, we're going to say id is equal to a string of the post do id. so, i'm going to convert it from a uu id object to a string. i'm then going to say the caption is the post.caption. i'm going to say the url, if we can spell this correctly, is the post dot url. i'm going to say the file type is the post.file type. and then i'm going to say the file name is the post.file name. and i'm going to say created at is the post.created atiso format, which is going to give me a time stamp in a format i can actually read. then i'm simply going to go down here and i'm going to say return post and i'm just going to return all of my post data. okay. uh and that should be post data like that. so that is going to complete this kind of get feed function which will give me all of my posts. this should allow me to create a post when i upload a file. again, for now the file um we haven't specified but we'll do that in a second.",361,961,979,SR5NYCdzKkc
58,"again, for now the file um we haven't specified but we'll do that in a second. so now let's give it a test and see if this works. so, let's rerun our backend. i'm just going to shut this down and restart it. let's go here. let's refresh. we see we have an upload and see it says multiart form data. so, i'm going to try it out. and now it allows me to choose a file. so, let me pick some file here. okay. so, i just uploaded some files, some pay slips from a while ago. let's go caption hello world. and let's execute. and let's see what we get. and it gives us the response body. okay. photo, dummy url, date, hello world, and the id. so now if we want to see if it's actually in the feed, what we can do is go to get feed, try it out, and execute. and you can see that we get the post. i think i press this twice. so we get two posts showing up here, but it's reading those from the database. and the important thing is that if i were to shut this application down and restart it, these would still be in the database because, well, they're here persistently, right? and they're stored in this file. if we want to delete them, we can just delete this database. okay, so now we have the ability to upload or create a new post and to kind of view the post. now we need to start actually handling the image and video upload. and to do that, we're going to use image kit.",360,979,1006,SR5NYCdzKkc
59,"and to do that, we're going to use image kit. so what we're going to do is make a new file here and we're going to call this image kit or not image kit, images. py. okay, so from here we're going to go images. py and we're going to start importing a few things. we're going to say fromv import load.env. we're going to say from imagekit. okay, io import if we can spell this image kit. and remember, we imported this or installed this at the beginning. we're going to import os. and then we're just going to call this load.env function. we're then going to say image kit is equal to image kit. and we are going to essentially specify all of the variables that we defined here in ourv file. so to do this, we're actually just going to use this autocomplete. we're going to say our private key is equal to os.get envage kit private key public key os.get env public key url endpoint. os.get envage kit url endpoint. okay, so the same variables that we have here. and actually, let's just make sure they're spelled correctly because this one is a little bit different. so let's fix this to just be imagekit url. okay. now, what are we doing here? well, this load.env env function will look for the presence of this enenv variable and essentially load it for us. okay, so it will load these values in so we can now access them. now to access those variables we use this os.got get env function which will be able to find the presence of these environment variables and load them into our code.",364,1006,1029,SR5NYCdzKkc
60,"now to access those variables we use this os.got get env function which will be able to find the presence of these environment variables and load them into our code. very important that we're doing this on the back end not on the front end. and i want to explain to you how we now kind of upload content using image kit which was which is what we're about to do. so let's go into app.py py and let's import image kit so that we can start using it. so what we're going to do is say from app dot images import and we're going to import image kit and then we're going to say from and this is going to be imagekit ioles.upload file request options import the upload file request options because we're going to use this in a second to specify how we upload the file. now before i start kind of diving into the code here, i want to go to the image kit documentation and start kind of explaining to you how this works. all right. so if you remember at the beginning of the video, we would have created an image kit account. the account looks something like this where you have this dashboard. now image kit can automatically host all of the images for us and it can handle uploading, deleting them, cropping them, modifying them, and more importantly just being our storage. so we don't need to worry about storing images and videos which can be a huge pain. now, you do have the ability with image kit to connect this to external storage.",353,1029,1040,SR5NYCdzKkc
61,"now, you do have the ability with image kit to connect this to external storage. so, for example, if you go to external storage here, you can add a new one and you can can connect it to like an s3 bucket if you're familiar with what that is or other locations where you can essentially have image kit managing this external bucket. you don't need to do that. uh, but you can. in our case, we're going to use image kit as our dam. so, it's automatically going to handle all of the asset management for us. eventually, when we start uploading stuff, we'll see in our media library that it will show up here. we'll be able to click into the various files. we can view them here. we can view the information. we can edit the tags, get embeds, urls, all of this kind of stuff. just makes it very easy to manage the images. now, in terms of using image kit, uh what we can do is use a single api. funny enough, we're building an api and we're going to use an image kit api to upload the images. once the images are uploaded, we can then just change some query parameters in the url for the image and we can specify the width, the height, if we want to add text on top of it, if we want it black or white, if we want to crop it, whatever. as you can kind of see, it's showing you right here, which makes this extremely useful. it also has performance optimization, a lot of other features, but let's go into the docs.",360,1040,1056,SR5NYCdzKkc
62,"it also has performance optimization, a lot of other features, but let's go into the docs. so, i'm going to go docs like this. let's go to the python documentation. you can see it supports a ton of different frameworks. i'm going to explain to you how we upload it. now, you can use this with a lot as you can see, javascript, react, angular, all of this kind of stuff. in our case, we're doing this from python. and so, because we're doing that, we imported or we installed the image kit io library and then we initialized it like this. now, we're going to be doing what's called a backend or server upload. so, essentially, the user is going to send some image to our backend and then our backend is going to upload this to image kit. now, it's important that we do it this way so that we can securely control what images we upload or which ones we don't and have them stored on our server. whereas, if you were to upload this directly from a front end or a client like a javascript application or something, um, that's not as secure. okay? and you don't have as much control because you're essentially exposing different tokens for image kit on your front end, which you may not want to do. so, it shows us right here some kind of code snippets of how you can do the upload. you have some url to an image. in our case, it's going to be some data which i'll show you. and we can do imagekit.upload file.",349,1056,1073,SR5NYCdzKkc
63,"and we can do imagekit.upload file. pass the different piece of information and then it just gets uploaded and it's going to return to us essentially an object that looks like this where we have the url for the image, the name, tags, all of this kind of stuff that we can then use and store. okay. and then if we want to generate a url, we can do something like this. again, we're not going to talk too much about everything here, but that's kind of how we do the upload. hopefully this makes a little bit of sense, but the point is user will send a file to our api. our api will then upload it to image kit. we'll grab the url, save that in our database, and then serve that to our user on the front end. so, we're going to go and we're going to import a few other things that we're going to need to perform this upload. so, what i'm going to do is i'm going to say import sh util. i'm going to import os. i'm going to import uuid and i'm going to import temp file. now the process is going to be that when the user sends us this file, we're going to create a temporary file which is a copy of this file. we're then going to upload that copy and then essentially remove or delete that copy from the machine because we'll no longer need it. so what we're going to do is create a variable and we're going to say our temp file path is equal to none.",352,1073,1087,SR5NYCdzKkc
64,"so what we're going to do is create a variable and we're going to say our temp file path is equal to none. we're then going to say in a try block with okay temp file.named named temporary file. we're going to say delete is equal to false. and we're going to say the suffix is equal to os.pathsplit text. and we're going to say this is going to be file.file name. and then this is going to be at index one. okay, this is a little bit weird. again, just bear with me here. we're going to say this is as temp file and we're going to say this is as temp file. what this is going to do is essentially make a named temporary file that ends in whatever the file name is that was uploaded here. then inside of here, since we have this temporary file, we're going to say the temp file path is equal to the temp file.name. and we're going to say sh util.copy file object. and this is going to be filefile and then the temporary file. and let me just correct myself here. what we're doing is when we create this temporary file object, we're using or we're having the end of the temporary file have the same extension as the file that was uploaded here. so if they upload a jpeg, for example, we create a temporary jpeg. if they upload a mp4, we create a temporary mp4. okay? so what we do is we create the temporary file. we then copy the contents of this file into the temporary file. and then that kind of completes this width statement.",362,1087,1107,SR5NYCdzKkc
65,"and then that kind of completes this width statement. then we're going to start doing the upload. so we're going to say our upload result is equal to imagekit.upload_file. what we're going to do is we're going to say file is equal to and we're going to open the temporary file path in this rb mode which stands for read bytes. we're then going to say our file name is equal to file.file name. and we're going to say the options is equal to and this is going to be the upload file request options. and in here we're going to say use unique file name. this is going to be equal to true. and we're going to say the tags is equal to and inside of a list we're going to say backend upload. so we know that we uploaded this from our api. now this is all we need. like that's literally how easy it is to use image getet. we just write this one line where we essentially open the file. we upload it to image kit and then it's going to give us a result that contains all of the metadata that we need. so we're going to say if upload result dot this is going to be response.http status code is equal to 200. that means that this was successful. then what we're going to do is all of this where we essentially create the post. okay. now we just need to structure this a little bit better because it's a bit difficult to read. but we have this try block, right? so, i'm just going to put an except block here.",360,1107,1127,SR5NYCdzKkc
66,"so, i'm just going to put an except block here. so, we're going to say except exception as e. and then we can put a pass right here for now. and then we're going to say finally. and in the finally block, we're just going to make sure we clean up the temporary file. so, we're going to say if temp file path okay or or sorry, not or and os.path.exist the temp file path. then we're just going to say os.unlink. and we're going to unlink the temporary file path. and then we're going to say file.file.close. okay. so, this is just going to clean up our file objects. so, at the end of this function, we're all good and everything's cleaned up. okay. and then for the exception, let's just quickly handle this. we're just going to say raise http exception status code 500. and we'll just put whatever the string error was so that we're able to have a look at it. okay. so, now at least the try accept block is done. and we just need to handle this part. so, what we're saying is all right, you know, we've now created the temporary file. we've uploaded it. we're going to check the response and we're going to make sure that this was successful. now, if it was successful, what we're going to do is we're going to create this post. but for the url this time, we're going to change it to be the upload result dot url.",328,1127,1150,SR5NYCdzKkc
67,"but for the url this time, we're going to change it to be the upload result dot url. for the file type, we're going to say this is video if file.content content type dot starts with video slash otherwise we're going to put the type as image and for the file name this is simply going to be the upload result okay dot and this is name so now we're actually using the content from imagekit and we actually will have the url for the image hopefully that makes sense but that's pretty much what we need to do for the upload and i think that should actually be good. so, what we'll do now is let's bring open our terminal again. let's just restart this and let's go back here and let's test this from our docs. okay, so let's go upload. all right, we're going to go try it out. we're going to put a file. so, we need to upload an image. so, let me just upload one of these images here. let's go hello world and let's send this. and it says module nt path has no attribute split text. okay, so i need to remove one of the t's here. so it's split text in one word. so that was my problem there. let's go back and refresh and just try this again. so we're going to go try it out. upload again. just upload an image or a video. needs to be some media. say hello and then execute this. it's going to take a second because it does need to do the upload.",351,1150,1170,SR5NYCdzKkc
68,"it's going to take a second because it does need to do the upload. and then we got some issues saying nontime object has no attribute http status code. so we probably just spelled something wrong. so let's go here and fix this. yeah. so we have upload result. this needs to be response metadata. okay. so let's spell metadata correctly. and now let's test it again. apologies guys. this is just a part of programming. refresh. try it out. choose a file again. some media. okay. hello world. and let's see. and here we go. we get the response body. and now what i want to check is the url. so it has a url here. so let's copy this and let's paste this in our browser. and we should see that we get the image now and it is uploaded. and importantly, if we go to the media library, we should be able to refresh here. and we should see that now the images appear happening twice cuz we uploaded it twice even though we got an error the last time. and you can see this shows up. and by the way, this is kenny, one of my co-founders from dev launch and someone that we were filming a testimonial video with. anyways, that's kind of the random image that you guys are seeing. okay, so it's working. we can see it's uploaded here in imagekit. it's working now from the back end. what what i want to do next now is test out this feed endpoint and see if this gives me all of the content. so, let's execute. and you can see there we go. so, we have this new post, right?",371,1170,1206,SR5NYCdzKkc
69,"so, we have this new post, right? we have the image kit url. and now i just want to talk about kind of the advantage of using image kit and what we can do with these urls that is super cool, which i'm going to show you in one second. so, i'm going to grab one of these urls, which i would recommend that you do. i'm going to show you how we can modify modify it, sorry, by literally just changing some parameters in this url. okay. so, i have the url in this one tab here. and then i also just pulled up the image kit documentation. and what you're able to see, let me just make this a little bit bigger, is that we can have these transformation parameters directly in the url to modify the image. so, notice i have, you know, image kit demo. and then we can do these transformations of like the width and the height directly on this url. let me show you and then i'm going to show you a bunch of other ones that we can do. so what we do is we go in between the file name and the uh what do you call it? um project id for image kit and we just put this in directly. and when we do that you see that i just modified the width and the height directly by cropping it by literally just putting those parameters. if i put like 500, see now that it goes up. if i put width of like i don't know what is this 700.",351,1206,1222,SR5NYCdzKkc
70,"if i put width of like i don't know what is this 700. see, now we get an image that looks a little bit more complete, right? and we can just add it directly like that. now, we also can pass this as a query parameter if that's easier for us to do. and you can see it shows some examples of kind of resizing the image. now, if we go here to image transformations, you'll see there's just so many that we can do here, right? like we have an ai transformation, which is currently in beta, which is kind of cool. we can add overlays directly on top of the image. so we can have like some local image we want to put on top of this image and directly embed it. we can do effects and enchantment or enhancements, sorry. so we can have like econtrast. so so let's actually copy this and see if we can get it. so we'll do tr and then h-300 e contrast. okay. and make sure there's no space. and then if we run that, you can see that it gives us some more contrast. we can sharpen it. so if we want to pass eharpen, let's change that to e d-sharpen. and you can see it now sharpens the image a little bit. it's a little bit difficult to see probably with my screen recording software, but it is doing that. and then we have video transformations. these are cool because we can have, for example, like thumbnails for videos. so if we upload a video, we can try to get a thumbnail from a specific portion of it.",362,1222,1244,SR5NYCdzKkc
71,"so if we upload a video, we can try to get a thumbnail from a specific portion of it. so for example, we can use this ik thumbnail.jpeg. if we just put that at the end of the path, it'll just give us the first frame as a thumbnail. we can get a thumbnail from a specific time five. so we're doing like 5 seconds in. that's how we're getting the thumbnail. we can do transformations on the thumbnails. we can trim the videos so we only get a certain amount of length from it. and then of course the most important thing in my opinion is the optimization. so you can do image optimization. so you can automatically compress the images without losing quality and load them significantly faster. you also can do video optimization and there's like so many different things. so if you do care a lot about image and video, definitely check out these docs that i will leave in the description. and one with video specifically is like changing the quality of the video so you're not loading, you know, massive 4k videos or something. um, and even if you set it to like 90 of the quality, it's, you know, three times smaller, which is significant. so anyways, that's the images. they are, you know, working. now i want to continue with the api. so we have the ability to kind of get a feed to uh, what do you call it? upload a file. now let's write the ability to delete a post. and then what i want to do is move on and talk about authentication.",356,1244,1265,SR5NYCdzKkc
72,"and then what i want to do is move on and talk about authentication. and so we actually have different users kind of signing in and only the user who made a post can delete a post and like you need to be signed in to be able to make a post. that's pretty important, right? so let's make another endpoint here. let's call this at app.delete. for deleting, we're going to take in ost and this is going to be a path parameter of our post id. what we're then going to do is say async define delete_ost. we're then going to say postc id and this is going to be a string. we're then going to say the session is async session and that depends on the get async session. we're then going to have a try. for the try, we're going to say the postc_uyu id is equal to uyuid.uyuid and then this is going to be the postc id. we're then going to say result is equal to await session dot execute and we're going to say select post dot where okay and we're going to say where the post do id is equal equal just two equals to the post underscore uyu id. now the reason why we need to convert the post id to a uu id is because this will be a string by default and we need it to be this uu id object. when we do the comparison they match. okay. so then after this we're going to say the post is equal to result and we're going to say dotscalers okay with a set of parenthesis and then dot first.",364,1265,1280,SR5NYCdzKkc
73,"so then after this we're going to say the post is equal to result and we're going to say dotscalers okay with a set of parenthesis and then dot first. what this is going to do is just return the exact result rather than giving us this kind of object that we need to loop through. so that's what scalers does even though i know it sounds a little bit confusing. we're going to say if we do not have any post then we're going to raise an http exception and we're going to say the post is not found with status code 404. okay. and then otherwise what we're going to do is just delete the post. so we're going to say await session.de post and then we're going to say await session do commit like that. and when we commit this it will delete the post. we're then going to return and we'll just say success is true. and then we can have some message like post deleted successfully. and then we're just going to have an except in case there's an error. so we'll say accept exception as e. and then what we're going to do is just raise an http exception saying hey there's some error. this is the error. okay. and that should be all that we need to do for deleting a post. so we can save that and let's now give it a test. so, let's go to here. let's grab a post id. so, maybe one of these old ones. and let's refresh and let's go to posts. let's pass in the post id and execute this. and it said 404 post not found. um, okay.",369,1280,1303,SR5NYCdzKkc
74,"um, okay. so, that's maybe an issue. let's go feed. and let's try to get the post. and okay, it looks like it did delete that post maybe because it said 404, but then it deleted it. so, let's copy this. let's paste the other one and let's do it. and there we go. okay, now it says success. true. post deleted successfully. okay, so that looks like it's working. and then if we go back to the feed, we can execute. and now we only have one post inside of here, which is a valid post. cool. so deleting is working. upload is is working. getting is working. now what i want to do is i want to start handling the user authentication. this is all great, but it only kind of matters if we can like sign in. and for example, if i make a post, you know, you shouldn't be able to delete it, right? we have to have kind of rules and authentication and that kind of flow uh for our endpoint. so, let's go ahead and start doing that. to do that, i'm going to go over to app.py. i'm going to make a new file, call this users. py. now, this is where we're quickly going to talk about jwt tokens. so, let me hop over to the kind of i don't know whiteboard and explain that to you. okay, so we're going to talk about authentication, right? this is arguably the most complicated part of most web applications. and for this app, we're going to use something called jwt tokens or jwt o. jwt stands for json web tokens. they are a very common format for web authentication.",370,1303,1335,SR5NYCdzKkc
75,"they are a very common format for web authentication. and the way that they work is that they essentially validate or authenticate a user by the user including this token in all of the requests that they send to the server. so this little diagram that i have for you explains it. let's go through it. so essentially the way this works is you have some user let's call her sally right and she logs into the application. now before she can log in she needs to make an account but to make an account you don't need to do anything fancy. so let's imagine you know she makes some account she has some credentials. okay now she signs into the uh server. the way that she does that right is she goes from her computer. so she's on some website and she sends her login details to this o endpoint. so maybe that's her username and her password. right now, this is our api or kind of our server. and what it does is it checks the user credentials and it says, okay, are these valid or are they invalid? they should be valid. so, when they're valid, what's going to happen is it's going to generate a signed jwt token. now, this is a special token that just looks like a random string of characters that essentially identifies sally. it tells us, okay, this token belongs to sally. so, if i see this token, it means that sally is the one who sent this request. that's effectively what that means. sally signs in. she then gets some token back. and this token is sally's token.",360,1335,1356,SR5NYCdzKkc
76,"and this token is sally's token. anyone that has this token is sally in the eyes of our api. okay. so, what sally does is she now stores this token or really her computer stores it in the browser. she's not going to do it manually. and now for the rest of all of the requests that she uses with our um api, she sends this token along with the request. so, she has the request plus this jwt token. we verify the token is correct and then we say, okay, who is this? oh, it's sally. okay, great. so, it's sally. so, we can now go do this thing because sally's allowed to do that thing. that's essentially how this works. this is an oversimplified explanation. the point is user signs in, they get some token, they store this token, they then send that with every request as they um kind of hit our api and start interacting with it. and that identifies them to us as that user. and there you go. right? that that's how it works. okay. so that's what we're going to implement. now to do that, we're going to use something called fast api users, which is a module that we installed that just makes this process a lot easier. so in our users file, we're going to say import uyuid. we're then going to say from typing import optional. and we have a lot of other stuff to import as well. we're going to say from fast api import depends and request.",336,1356,1381,SR5NYCdzKkc
77,"we're going to say from fast api import depends and request. and then what we're going to do is say from fast api users import the base user manager the fast api users and the uu id id mixin as well as models. okay, now this is giving us an error because i need to make this fast api users plural. and now we're good. then we're going to say from fast api or fast api users.authentication. okay. and we're going to import the authentication backend if we can spell this correctly. we're also going to import the bearer transport as well as the jwt strategy. now with this you can use various types of token strategies. in this case we're going to use jwt. don't worry, there's some other ones as well, but we're not going to look at those right now. we're then going to say from fast api users.database import the sql alchemy user database. and we're going to say from app db import user and get user db, which are two functions that we're going to go and write now. okay, so because we're now going to have users in our app, we are going to have to make some changes. so, we're going to go to database.py pi and we're going to start writing a user model that we can essentially use to store uh users. okay. so to do this we're going to say from fast api users db import the sql alchemy and this is user database. also going to import the sql alchemy user table uu id. okay this is complicated that sounds crazy long but these are just what we need to import.",366,1381,1399,SR5NYCdzKkc
78,"okay this is complicated that sounds crazy long but these are just what we need to import. and then what we're going to do is beneath the base we're going to say class user and this is going to inherit from the sql alchemy base user table uyu id and base. we're then going to say relationship. okay. and this is going to be post and then we're going to say this back populates the user. essentially what we're doing here is we're creating this um table, this user table, so that we can have a relationship between our posts and between our users. we want to know what post was created by what user and what posts exist for what user. so we're going to say post is equal to this. and now on this user, we'll be able to find all of their posts. okay, so that's why we're saying back populates user. so what i'm going to do now on this post um database model is i'm going to create the relationship content to the user so that from a post we know what user posted it or what user created it and from a user we know what post that they have. so on my post i'm going to do this. i'm going to say my user id is equal to column and then this is going to be uh what we want here uyu id. we're going to say as uyu id is true. we're gonna say foreign key and this is going to be user do id and we're going to say nullable equal to false. okay.",354,1399,1414,SR5NYCdzKkc
79,"okay. now a foreign key is essentially a reference to another table. so in our case what we're saying is for this post we want to have a foreign key which references the id of a user. that's what we're doing. so we know the user id that posted this post that made this post. now, as well as that, we're going to create a relationship and we're going to say user is equal to relationship and then we're going to have user and we're going to say back populates posts. now, these relationships automatically link these objects together and allow us to use the post and the dot user attribute on both post and user and the foreign key allows us to have this link so that we know the user's id. now what we've created here is what's known as a one to many relationship where one user can have many posts. there's different types of relationships that we can define in our database models. this is not a sql course so i'm not going to get into that you know too in depth but the relationships to be aware of are one to many, many to one and one to one. most frequently you are using one to many which means you have one user with many potential posts. all right. now, if we wanted to flip the relationship around where one post had many users, for example, we would simply change the foreign key to exist on the user table rather than on the post table. so, typically the child, so in this case like one user has many posts.",356,1414,1427,SR5NYCdzKkc
80,"so, typically the child, so in this case like one user has many posts. so, the post would be considered a kind of a child of the user in terms of the relationship hierarchy is the one that contains the foreign key. again, this is something you need to look at more if you're designing databases. and again, this is not a sql course. the point is we need to make this relationship. so now users are linked to posts. now that we have that, we've created the user, it's inheriting from base as well as from sql alchemy. we can go back to users and we can start using this. and actually, sorry, there's one more function i forgot i need to write here. so let's go down to the bottom and we're going to say async define get user db. we're going to say session. and this is going to be the async session equals depends. okay. okay. and let's spell depends correctly. and depends is a capital of course. we're going to say depends on get async session. and then what we're going to do here is we're going to say yield sql alchemy user database session and user. now why is this giving me an error? i guess i didn't import it. so let's go from fast api. so from fast api import depends with a capital d. and i think that should be good. now, essentially what we're doing is we're just writing a function that's going to get us the user database table. um, so that's effectively what this is doing. again, don't worry too much about it. the database stuff is a little bit confusing.",366,1427,1453,SR5NYCdzKkc
81,"the database stuff is a little bit confusing. the point is it will just give us the database table that's associated with the users, which we're going to have to use here. now, inside of this users py file, okay, so from here, we're going to create some variable called secret. this should be equal to some random string. um, you can actually generate this with a specific function on your computer. point is, you don't want to share this secret string because this is actually what's used to sign your jwt tokens, which identifies them as unique to this particular app. if someone were to have access to the secret key, they would be able to decode your jwt tokens, which you don't want. so, this is something that you want to keep secret, hence the name secret. in my case, i'm just making it something random. so now what i'm going to do is i'm going to say class user manager and this is going to be uu id id mixin. so we're going to inherit from that. we're also going to inherit from the base user manager. and then we're going to have the user and the uu idu id. okay. again, i know some of this stuff seems confusing. this is directly out of the fast api users documentation. you just set up one time and then you're good to go. you don't need to memorize it. just bear with me. so now what we're going to do is we're going to have the reset password token secret and the verification tok token secret both being equal to the same thing which is a secret.",362,1453,1472,SR5NYCdzKkc
82,"so now what we're going to do is we're going to have the reset password token secret and the verification tok token secret both being equal to the same thing which is a secret. can make it different if you want but that's all that we need. now we actually don't need to do anything else inside of this class but i just want to show you that we can write various functions here so that we can handle things that happen when a user registers when they forget their password or when they are requesting to for example verify their token. so what you can do is you can hook into all of these common user operations that are going to automatically be written for you by fast api users. so we can do something like async define and then you can see there's a function like on after register and this is a function that's automatically handled for us and what we can do is we can just say you know print you know user has registered and there you go right so after the user register boom this print function will run and if we wanted to do something specific we could do that inside of here we could have another one like async define and then on after forgot password right and then boom we can do something on after request verify and then we can do something. i'm just showing you some examples if you can hook into these. if you look into the fast api users docs and you can control what's happening based on certain operations, it will be automatically handled for you.",364,1472,1478,SR5NYCdzKkc
83,"if you look into the fast api users docs and you can control what's happening based on certain operations, it will be automatically handled for you. so now what we're going to do is we're going to write a quick function. we're going to say async define get user manager and we're going to say the user db is equal to sql alchemy user database and this is equal to depends get user db. okay, we're then going to yield the user manager with the user db. so essentially we're taking our database user kind of injecting that inside of here. and now we have this user manager class which will allow us to manage the users in fast api. we then are going to have the bearer_transport which is equal to the bearer transport and then we're going to have our token url be equal to o jwt lo. so when someone wants to log in they go to this endpoint and then they can pass their credentials and they can log in as a user. we're then going to say define get_jwt strategy. and this is going to return jwt strategy. we're going to pass our secret and we're going to pass our lifetime seconds. now, the lifetime seconds is how long you want a jwt token to be valid for before the user needs to sign in again. i believe this is 3600 seconds, which is going to be um what is that 10 minutes or 1 hour or something? yeah, i think yeah. so, sorry, this is 1 hour. so, 60 minutes is 3600 seconds.",349,1478,1493,SR5NYCdzKkc
84,"so, 60 minutes is 3600 seconds. so, you can change how long you want the token to be alive for essentially. and if you want to invalidate the token after a certain period of time, by default, jwt tokens have some lifespan. the longer you make them, the more convenient it is for your users, but also the less secure because that means like you know someone could come onto your computer for example and they would um you know be able to just start using the application because the jwt token is still alive. okay, now we have the jwt strategy. we're going to define the off backend which is going to be equal to the authentication backend. we're going to say the name is equal to jwt. the transport is the bearer transport. and we're going to say get strategy is get jwt strategy. okay. we're then going to say fast api users is equal to fast api users user uuid.uid get user manager and o backend. so when we define fast api users, what we do is we specify this is the user model that we're using and this is how we get the user manager. this is the backend that we're using which is jwt tokens. then we say the current active user is equal to fast api users.curren user active equals true. what this is going to do is when we call this current active user function, it's going to automatically give us the current active user by going and checking the user's jwt token. again, i know this stuff is confusing. you're not going to understand a lot of the stuff that's being written.",364,1493,1509,SR5NYCdzKkc
85,"you're not going to understand a lot of the stuff that's being written. the point is this framework when you do the little bit of setup that we're doing here will automatically handle all of the authentication for you. we just need to kind of hook it up and do this setup. once we do this setup, all the jwt off will be handled automatically and all you need to do is just use this which you're going to see. so now we've written everything that we need for users. py. so what we're going to do is go back to app.py and we're going to start actually using this because now we need to essentially connect different endpoints to this um what do you call it? jwt kind of backend which you're going to see. so now that we've done that, we're going to import this. so we're going to say from app dot users and we're going to import the oback backend. okay, we're going to import the current active user and we're going to import fast api users. now what we're going to do is for our app, we're essentially going to connect the different o endpoints that we need to our fast api users endpoint. so you'll see what i mean in a second, but we're going to say the following. we're going to say at app.include include routouter and we're going to include the fast api users.get offers routouter with the offc_backend and we're going to say the prefix for this is equal to slash off slashjwt and we can say the tags is equal to off.",353,1509,1522,SR5NYCdzKkc
86,"we're going to say at app.include include routouter and we're going to include the fast api users.get offers routouter with the offc_backend and we're going to say the prefix for this is equal to slash off slashjwt and we can say the tags is equal to off. now, what we're doing here is we're saying, okay, in my app, i want to include all of the endpoints that are automatically provided by fast api users. so, i'm just going to say app.include router, and i'm going to connect it to all of the endpoints here. and i'm going to prefix this with jwt, which means i go to jwt plus all of the endpoints that are automatically included by my fast api users um kind of module that i brought in here. okay, so things like resetting your password, uh, you know, after you forget the password, what happens? all of those endpoints are automatically written and handled here for you, and we just include them into our app. you'll see what i mean in a second. essentially, we're just including some routes that are automatically written inside of fast api users in our app. okay, now we're going to keep going because there's some more routes that we need to include. so, we're going to say app.include router, and this is going to be fast api users.getregister routes. this time we're going to prefix it with slash off and the tags will be off.",318,1522,1532,SR5NYCdzKkc
87,"this time we're going to prefix it with slash off and the tags will be off. however, for the register routes we need to pass in some schemas which we're going to write in a second which is user read and user create and then we're going to go write those in schema. so let's do that really quickly and then come back and i can explain what we need. so for user read what we need to do is we need to say from fast api users import schemas. i then need to import uu id. so i'm now going to come and i'm going to say class user read and this is going to be from schemas.base user and this is going to be uu id dou id. okay. and then this is simply going to say pass. i'm then going to say class user create. this is going to be schemas.base userreate. and then i'm going to say pass. and i'm going to say class user update is going to be schemas.base base user update and then pass. now, these are schemas that automatically come from fast api users, but similarly to some of the schemas we had before, we need to just create our own kind of dummy um schema that inherits from it that we could then override if we want to. so, that's exactly what we're doing. now, we're going to go back to app and we're going to import the schemas that we need. so, from apps schemas, we're going to bring in the user read the user create. okay. and the user update which we'll have in a second. all right.",362,1532,1550,SR5NYCdzKkc
88,"all right. and now this should be good to go. okay. so that's that for the registration routes. so again, same thing. we're now going to have the ability to register automatically be created for us. i'm going to show you this in the documentation in a second from fast api users. now if we wanted to bring in other routes, we can do that. so for example, we can say at appincclude routouter and then we can say fast api users.get get resert reset password router. right? and now if we do that, we now are have the ability to reset the password. now let's keep going. let's say app.incclude router and we can do the get verify router. now this one we actually need because it will allow us to verify user. so we bring that in and then notice automatically we bring in user read. and then lastly we can have get users router. so i'm going to say app.incclude include router get users router and then we can include the user read and the user update. okay, so let's quickly just reload this and i'm just going to shut this down and restart it uh so that we can test and make sure it's working.",271,1550,1567,SR5NYCdzKkc
89,"okay, so let's quickly just reload this and i'm just going to shut this down and restart it uh so that we can test and make sure it's working. so now if i go here you can see that i have all these new routes that are popping up right like login register forgot password reset password request verify token verify users me users me users id user id user id right all of this kind of stuff that i'm able to now utilize because i'm using fast api users again i'm not going to go through literally every single thing here but i will show you the basics so what i want to do now is i want to register a new account so what i can do to register is i can go try it out and i'm just going to do a new new account here. so i'll go tim at techwithtim.net. and then for the password, we'll go one, two, three, four, five, six, seven, eight. okay, we'll just make is active true. and then we won't set anything for the super user is verified. uh you can change these so you don't pass these when you register later on, but for now in kind of debug mode, that's what we're doing. so actually, let me just change this like timwithtim.net. let's execute this. and then it's going to now give me my id, my email, and then all of this information. and now what i can do is i can sign in with this user. so what i'm going to do is i'm going to go to authorize.",354,1567,1578,SR5NYCdzKkc
90,"so what i'm going to do is i'm going to go to authorize. i'm going to pass my email and my super secure password and press on authorize. and now it's going to essentially sign me in. now that i'm signed in, it's going to give me this token. and what will happen is whenever i send any requests, it's going to use this token. so now i can go to for example users me and i can just send the request here and you're going to see that it tells me hey i am tim one attewithim.net and notice that automatically this super long token was included in my request because that's how this works. so if you sign in here with authorize then by default this token that identifies you is going to be sent in all of the future requests as you're using this documentation and you're going to be able to verify if you're user or if you're signed in or if you're not signed in etc. so now all the user stuff is working. however, we need to associate users with the posts and we need to make it so that i can't actually call these endpoints like upload feed or post unless i'm signed in. so let's do that. so i'm going to go to main.py. and the way that i can make some of these routes what's called protected is i can add a dependency that forces the route to get the current active user. and if the current active user doesn't exist, it won't let me call this endpoint.",347,1578,1590,SR5NYCdzKkc
91,"and if the current active user doesn't exist, it won't let me call this endpoint. so for example to be able to actually make an upload well i need to have a user right like i need to be signed in as a user. so what i can do is i can just change the function to be like this. so i can just say user colon and then i can say user and i can say this is equal to depends and this is current active user like that. for user i can just import that from my database. so let's just import user like so. and now what will happen is if i try to call this function and i'm not signed in and i cannot get the current active user this function will not work. so literally all you have to do now is if you want to protect these routes you just add this dependency. so for example for the feed same thing we add the user we say okay you know it's just a get current active user and now it won't work unless we have the user. now same thing for the post we're going to get the current active user. so let's do that. and we're going to have to change some of the content of these functions now so that it actually saves the user that is doing this operation. so let's go back to the db and for our post we're now associating with users. okay, so that's all good. but for our app when we are uploading images, we now need to associate them with that user.",360,1590,1604,SR5NYCdzKkc
92,"but for our app when we are uploading images, we now need to associate them with that user. so for our upload result here, let's take user id and let's make this equal to the user do id. so now we're storing the particular user for every single post. okay, so that's good. that's all we need to do there. then for deleting the post, we need to only allow the user to do this if they're signed in as the correct user. so once we check for the post down here, we're going to say if the post dot user id does not equal the user do id, then we need to raise http exception. so let's do this. okay. and we're going to say status code 403, which means unauthorized. you don't have permission to delete this post. so now if i try to create a post with an account that didn't make this post, or sorry, i try to delete a post with an account that didn't make it, it's not going to allow me to do that. okay, so now we've just added the authorization check and i think that is pretty much good. now we're going to go to get feed. and for get feed, we're going to make some enhancements here so that we actually know which user made these posts in the feed. so first things first, what i'm going to do is i'm going to include the user id in my response here. so i'm going to say user id is equal to the string of post dot user id.",348,1604,1620,SR5NYCdzKkc
93,"so i'm going to say user id is equal to the string of post dot user id. and i'm also going to include if we are the owner of this post so that the front end knows that. so we're going to say is owner and we're going to say post dot user id is equal to user id. so now we're checking all right is the user id of this post equal to our id. if it is, that means we made it. if it's not, well, we didn't make it. okay. and then i think that should pretty much be good. but there's one last thing actually that we will do. okay. so, for every single one of our posts, i also want to include some basic information about the user. and in this case, that's going to be their email. and what i'm going to do is i'm going to get the user's email. so, i'm going to say email. and then i'm going to say this is post do user do email. and i think that will work. uh, but i'm not 100 sure. let's see. okay. okay, so now we kind of have protected our endpoint so that only authorized users are able to use this. and that should pretty much wrap up the api, but of course we want to test this. and then i'm just going to have a little simple front end that i'll show you how to use so we can actually see it visually. all right, so let's authorize ourselves by signing in with that account. so we're going to say tim1 techwithtim.net. okay, if we can spell this correctly.",364,1620,1644,SR5NYCdzKkc
94,"okay, if we can spell this correctly. all right, then we're going to have 1 2 3 4 5 6 7 8 and authorize. okay. so now let's go. okay, that's fine. from here, let's go to the feed and let's try to get the feed. okay. and when we get the feed, gives us internal server error. let's see what the problem is there. okay. and of course, the error came from this where i'm getting the post do user. um that is not working properly just due to how we're loading the user. so i'm going to do another approach here. this is a little bit more complicated, but will allow us to get the emails. so i'm just going to say result is equal to a weight. this is going to be session.execute and i'm going to say select user. then what i'm going to do is say users is equal to row zero for row in result doall. i then i'm just going to create a list of all of my users. this is not the most efficient way to do this but for this example it's fine. so we're going to say user dictionary is equal to and this is going to be u do id and this is going to be you do email for you in users. okay. now what we're able to do is we're going to be able to get the user's email like this. so to get the email we can simply say user_dict and then this is going to be do get and it will be post do user id otherwise we'll just put unknown. okay.",360,1644,1667,SR5NYCdzKkc
95,"okay. so now this if we save it should reload. okay. and now if we go back to the feed let's try it again and you see now that we get the post and it is working and saying we are not the owner of this post because we were not signed in as that user. so now if i take this post id right and if i try to delete this post you're going to see that it doesn't work. so let's go here because i'm not signed in as the correct user. so let's go here and try to delete. execute. and you see it says you don't have permission to delete the post. perfect. okay. so that wraps up the api. the api is working. it's fully functioning. the next thing that i'll quickly show you is how we can get a nice little user interface here. so we can actually play around with the ui. now, i'm not going to write the user interface from scratch. if you want all of the code for this, it will be linked in the description down below. you can simply copy it and paste it inside of here. what i'm going to do is just make a new file. and i'm just going to call this frontend. py. i'm just going to copy a bunch of code that i've written previously, which is the front end, and paste it in here. okay. now, this code uses something called streamlit, which is something that we need to install. so, what we're going to do is we're going to keep our back end running.",354,1667,1692,SR5NYCdzKkc
96,"so, what we're going to do is we're going to keep our back end running. we're going to go into a new terminal, and we're going to type uvad streamllet. now, that's going to add streamlit to our dependencies, which is going to allow us to now run this front end. so, again, the all of this code that you see will be available from the link in the description. you can literally just copy it into a file called front end. and this is going to handle essentially interacting with the api that we just spent all of that time writing. i will go over how it works in a second, but let's just quickly test it. so to do this, i'm going to type essentially uv run streamllet run main dot or not main. this is front end. py. when i do that, it will start running streamllet. it should just open it in your browser. and then you have a user interface. what you can do is you can type in an account like tim attechwithim.net. you can type in the password. when you do that, you can either sign up and make a new account or you can just log in. and if you press log in, it should bring you now, let's wait a second, to the feed. um, failed to get user info. okay, interesting. so, we'll fix that in a second. uh, but it should bring you to the feed once i fix this problem. okay, and you can see here that it now loads the feed. we have my username, the date, the image, and then we can delete this.",361,1692,1714,SR5NYCdzKkc
97,"we have my username, the date, the image, and then we can delete this. and we can also expand it directly from here. now, i want to talk about kind of how this works. so, i'll walk through the code a little bit, but again, just copy the code from the link in description. it's not really valuable for me to write this out with you because this is not a tutorial on streamlit. now, first things first, the jwt token is something that we store in our session. that just means that we're storing it. so, we're constantly using it anytime we send a request to the back end. okay. the way that you send jwt token requests is you include bearer plus the jwt token. and then when you do that, you're able to actually send an authenticated request. you saw this in the examples when we were looking at the documentation from fast api, but i'm just doing it directly kind of manually here in python. again, i'll show you how the request kind of looks in a second. so here is the login page, right? so when i want to log in, what i'm doing is i'm just passing a username and a password and i'm passing it to this url. so o jwt login, right? passing my data and then i'm able to log in. now when i do that, i get some token data and i store that access token in my state. so i'm able to use it as my jwt token to sign in.",340,1714,1732,SR5NYCdzKkc
98,"so i'm able to use it as my jwt token to sign in. i then get my user info by passing a request to user me so i can make sure that i'm signed in successfully. and i store that again in my session state. same thing for registering. i send a request to allregister and then after i register i send another request to get my jwt token um so that i'm good to go. okay. now we have the upload page. on the upload page what i do is i have files right? so i have a file which is the uploaded file that i select which i'm going to show you in a second and i have a caption. i then include that in the request to upload, right? as well as my headers which include my jwt token that's then able to upload the image and then show it on the feed. now, the cool part here is when we start talking about actually displaying the images with image kit because it makes it super easy for us to view the images and to actually perform transformations on them. so, let me show you with the ui. if i go here and i go to upload. okay, so let's go to upload. let's change this here and let's upload here this one with maniv and me which is a testimony and i'm going to say hello world dev launch worked okay because this helped him land a job. let's go share and then it's going to upload this file for me to my backend api.",349,1732,1748,SR5NYCdzKkc
99,"let's go share and then it's going to upload this file for me to my backend api. and if i go back to the feed now just takes a second to load. we'll see the image appears here. and one thing you'll notice if i make these larger is that we have this caption that i'm kind of putting directly on top of the images by using image kit where i say hello world dev launch work. now it's quite small. i'm going to show you how to make it larger. but the way that i do that is from my ui before i load the image. so if we go down here, let's do this. you can see that i have these file types. so i check if it's an image, which is all we've done right now, but i'll show you videos in a second. then what we'll do is we will essentially transform the url to use the transformation that puts the caption directly on top of it that looks like this. so this is essentially the text that you use. if you want to add a caption, if i want to make it bigger, i can say like font size 100 and then it will make it much larger and then it directly transforms the image from me from this one url and passes it back very quickly. okay, so that's how i'm kind of um transforming this. now, i want to show you uploading a video, which we'll do in one second, um, and then displaying the caption for that video. but that's kind of how um, i'm doing that transformation with imagekit and displaying the images.",366,1748,1763,SR5NYCdzKkc
100,"but that's kind of how um, i'm doing that transformation with imagekit and displaying the images. so, let's go back. and now, if i refresh this, uh, let's sign in again. so, 1 2 3 4 5 6 7 8 and go log in. then, let's wait a second here. okay. and we'll go to the feed and okay, get rid of that. and you can see now it's quite a bit larger cuz i made the font size 100. if i made it like 1,000, obviously it would be even much bigger. and you can see the text shows up on the image. now, if i want to upload a video, i can totally do that as well. so, let's go files and let me find a video. okay, so i just found this screen recording that's not that big. so, i'm just going to upload this screen recording. so, you can see right there. going to go ahead and press on share. because it is a video, it will take a little bit longer to upload. so, we'll just wait for that to complete. and then, as soon as it's uploaded, okay, so let's go back to the feed here. we should be able to see it directly from image kit. you see it's going to take one second to load here and then there you go. we get the video and we can watch it, right? and it just shows up directly here. now, same thing with when we transform the images, we can transform the video. so, let me show you a few examples of how we actually do that. let's go here.",360,1763,1788,SR5NYCdzKkc
101,"let's go here. and what i'm going to do is i'm just going to put in some transformation parameters. right now, you can see that i'm just putting it quality at max and width 300. but i can adjust this to, for example, crop the video. so, for example, if i just take this transformation parameter, i'll just show you what this does, but essentially it crops the video and then adds a uh blurred background and puts it in vertical format for me. in this case, it's already vertical, but if it wasn't vertical, it would do that. so, if i go back to my was it example here ui, let's just go back to the feed. so, it reloads. okay. and actually, you can see that this did work now. so, it resized to be quite small. and you can see now that we have kind of this video like in the middle and then we have this blurred background which is exactly what i got the transformation to do. um the quality is quite low obviously because of how i adjusted it and the size that i made it but you can see that it is working and sorry if i just scared you there with that audio. um that scared me as well but the point is it is loading. um cool. so i think that's pretty much it guys. i mean that covers everything that i wanted to show you. we made a application that's able to post photos and videos, is able to handle user o connected to a database, has all kinds of different more advanced features that you typically don't see in more beginner tutorials.",367,1788,1805,SR5NYCdzKkc
102,"we made a application that's able to post photos and videos, is able to handle user o connected to a database, has all kinds of different more advanced features that you typically don't see in more beginner tutorials. and while i know this was a really long video and a lot of code, i wanted to be super detailed and covered everything in as much depth as i possibly can. if you guys enjoyed this video, make sure you leave a like, subscribe to the channel, and i will see you in the next one. music",124,1805,1808,SR5NYCdzKkc
0,"welcome back aliens. my name is ain reddi and let's talk about fast api. so when you work with python, you can do multiple things with python. you can use python to build consolebased application. you can use it for gui building. you can use it for web framework. you can use it for ai and of course for different things. we got different libraries, different frameworks. but now we are talking about fast api which is a web framework for python. and this is not the first option. we have multiple frameworks available for python. we got django, flask and now we got fast api. in fact, we already have a django series which was created 5 to 6 years ago and now i should have gone for flask and then fast api. in fact, fast api was never in the option. i wanted to explore flask but then i looked at fast api and thought i thought hey that's simple uh easy to go with and it works. and the most important thing is it has this amazing features which we're going to talk about now. so let's head towards the homepage of fast api. now before we talk about fast api, i want to show you the project which we're going to build to start with. okay. and maybe later on we'll update this project mode. we'll add more features based on what fast api has to offer. okay. now this is a simple page because i wanted to explore c operation and this is what we are going to build. now hold on.",347,0,23,Lu8lXXlstvM
1,now hold on. using python and using fast api we can build the back end and you are looking at the front end. now the front end is actually in react and no we are not going to learn react in this course. this imagine this page or this front end is already created. now to make it work you have to build a backend for it. okay example if you talk about this things whatever you are doing here this data is coming from the back end. so the database which we're going to use is posgress but you can use whatever you want. the back end we are going to use python and the framework is fast api and the front end is in react. so we are going to skip the react part. you will get the code as it is the entire react application. you just have to build a back end and run it will work. i mean it should work uh based on your configuration. now what we're trying to build here is this is a simple inventory application. nothing very fancy and i don't want to launch my own startup for this uh simple application.,262,23,36,Lu8lXXlstvM
2,"nothing very fancy and i don't want to launch my own startup for this uh simple application. right now in this we got a products here and which in which you can see we got list of products and then uh i have an option of sorting this of course this is of react part nothing to do with the back end but what is there in the back end this data is coming from the back end and you can edit this data you can see if you click on edit it is changing here again the part of front end but the actual saving or updating of data is happening in the back end uh i can add a new product here i can just simply say refresh okay now this is something this is the first time i'm looking at this bug not a bug but then how will you go to add now okay i should have done that before okay so refresh the page there's something i need to update in the front end as well anyway the point is let's say if you if you got this front end i want to add the id which is let's say five and i want to say let's say asus laptop description budget laptop price is let's say 999 uh and then quantity is let's say we got 12 quantity and add and you can see it has got added which is which is here now this is the entire front end. you can search for the product. example, if you say a laptop, it will show you the laptops, right?",357,36,39,Lu8lXXlstvM
3,"example, if you say a laptop, it will show you the laptops, right? again, this is a front- end functionality of searching. but then uh when you search something, okay, this is filtering. we should not be using that. maybe i should be doing that with the help of id. so if i say three, you can say there's three. so this is basically can be done through the back end as well. uh what else you can do? you can delete the items. so this is the back end feature. so if i click on delete pen because i don't want cheap products in the inventory. i'm just kidding. pen is very important. uh and you can see it is got it got deleted. okay, simple stuff. now i want to show you the back end for this. and this is the entire code repository which i'm going to share with you. so there's a front end folder which you can see here and the back end is this files. uh not the perfectly optimized code for the production but we are not doing for the production now. we are basically learning a framework right. so this should make sense. and now let's head towards the homepage of fast api. and this is it. this is fast api and they are claiming it's a fast api framework. it's high performance, easy to learn, fast to code and ready for production. uh out of this i have tested for easy to learn, fast to code and ready for production. i have not tested for the high performance. but if they are claiming it and a lot of people are using it, so it should be okay.",371,39,66,Lu8lXXlstvM
4,"but if they are claiming it and a lot of people are using it, so it should be okay. now if you want to read documentation, you can read it from here and that's the homepage. you can just look at for the documentations here. and fast api is modern fast performance. again, i have not tested it, but it's very easy to work with and you will see that when when we do that. okay, so very very high performance on par with nodejs and go. okay, that's what they are promising. uh faster code we have seen this few bugs and okay other thing looks good. now this is what fast api is promising. so fast api is basically a web framework and we are going to do this step by step. so the first thing we need to do is in fact there are some prerequisites. uh you need to know python. of course we are not going to teach you python in this course. so you should know python. you need to basically understand how basically a web framework works. and in in fact i'm going to give you a glimpse uh what happens when you communicate from a client side or whenever you access a particular website how it works. but having that knowledge prior to it will be helpful. but don't worry i will talk about it as well. and uh that's it. you don't you need just two things. uh in terms of softwares to do this you need to have python installed in your machine and a editor now or id to be specific.",353,66,86,Lu8lXXlstvM
5,"uh in terms of softwares to do this you need to have python installed in your machine and a editor now or id to be specific. now you can use uh ide like pycharm for python or you can use vs code or any other id which you like. we are going to use vs code here because for pycharm uh it will be bulky and not everyone will be having it and people are getting used to vs code. so let's use vs code in this course and maybe in between i will show you how do we run this in pycharm. okay, let's that's a separate thing but initially we'll start with vs code and there are certain things which we have to install and stuff. yeah, initially there will be learning curve but once you get used to it it's very easy. okay, cool. so that's about the introduction for fast api. apart from this uh make sure whenever you watch the upcoming videos don't just watch the videos practice them because practicing is very important otherwise it's of no use.",239,86,94,Lu8lXXlstvM
6,"apart from this uh make sure whenever you watch the upcoming videos don't just watch the videos practice them because practicing is very important otherwise it's of no use. and uh go through the documentation you know in case if i miss something just go through the documentation and you'll be good in fact i wanted to add one disclaimer the ui which you are seeing here is it's created by ai okay so if you find any glitch here you can blame the ai if you love it i did it i i did those prompts with the ai so before we start coding with fast api let's do the most important step without which you can't continue and that is the setup of course set in your machine you need to have those things so let's go step by step. the first thing you have to check is open your terminal. now based on which os you're using and i mean we call terminal everywhere now. so cmd terminal whatever you prefer open that and check do you have python installed. so how do you check? it's very simple. you say python - version and if you see a version there that means python is there. now i have tested with two different versions 3.12 and 3.13 it works. if you have a previous version, i would recommend you to upgrade. otherwise, if you have a low version, try it out. if something is not working, just upgrade. okay, that's one.",328,94,106,Lu8lXXlstvM
7,"okay, that's one. next, if you want to use the front end, see whatever front end we saw in the previous video, it's not compulsion that you can run that application with that front end. it's optional. we have another way of accessing those pages. we can use postman like your rest client. uh we can also use insomnia. you can use swagger. and by default, fast api gives you docs in the format of swagger. so you can use that but front end looks good. you will get that satisfaction. yeah, you have done something. that's the only reason we have that front end. but in case if you want to run front end, uh you need to make sure that you have nodejs installed and i have tested with version 22. make sure that you have upper version than this. uh i have not updated my node from a long time. in case if you don't have these two softwares and i'm assuming you have python because this is the advanced part of python. so you should you need to have that. i will not show you python. let's get towards nodejs and this is this is where you can click on download and there should be option of this. this is the latest version you can go with lts version. you can also go with the current version and that should work. now once you have uh these two let's see what else we need. so you need these two. done. verified. next. now you need an id. so in my machine i got vs code and i already have vs code open in one of the screen. let me just drag it here.",373,106,134,Lu8lXXlstvM
8,"let me just drag it here. this is how it will look like. i will just increase the font size. and this is where we'll be doing all the work. so make sure that you have python, you got nodejs. again, nodejs optional if you don't want to use the ui. uh then you need vs code or pycharm or whatever id you prefer. uh next in your vs code you need to have python extension installed and again i'm assuming that you have worked on python before it's just that if you have not worked on python on vs code uh you need to make sure that you have this extension. so micro python from microsoft okay next before we do any more configuration in vs code now this section or this small part is only for those people who want to run the ui. now in that case instead of taking a lot of trouble what you can do is in fact i would recommend you to do do this. so on github you have this particular folder called front end in the project. okay. so this is our project and in this we got multiple branches. if you can see we got product with ui, product with db, product put, delete and post. so this is our first repository which we're going to start with. again i'm going to type everything except the front end part. but if you jump to the last branch with products with ui, this is what you need to download. okay. uh, of course you will get everything. but this is where this is important for us. now, how do you download? it's very simple.",367,134,155,Lu8lXXlstvM
9,"it's very simple. you can simply click here and click on download. now, why i would i'm recommending download instead of this uh cloning is when you clone you will get the entire project. okay? and when you open, you will get again you will get all these files. when you download what will happen is you will get everything a zip file. unzip it and then delete the extra folders or the extra files here. you are only concerned about front end. okay? just keep it there. again, when you clone it, you can also delete all the other files. but it is easier to download from the file explorer or the whatever you call these days now. what it is called? yeah, file explorer. so you can delete from there. so download it, unzip it and get this folder. this folder is important if you want to run the ui and i'm going to do the same thing. so i'm just going to download this and i should have shown the steps you know simple. so open unzip done. this is where you got your folder. uh open this. you got only one branch which is fast api demo products with ui and out of this you need just this one folder. apart from this you don't need any any of the things. you can just simply copy this and let's create a folder for us. i will do that in the c drive and let me make a new folder and i will say fast api demo recording because i'm i'm using it for recording.",348,155,179,Lu8lXXlstvM
10,"i will do that in the c drive and let me make a new folder and i will say fast api demo recording because i'm i'm using it for recording. so i will use that because after some time after a few weeks i will look at this project and i i will not realize why i have created this project. so let me just create that folder in the c drive and put your front end there. simple stuff. u now once you have these steps ready what you can do is go to your vs code and open the folder. so c drive fast api - demo- recording and open this folder not the front end open the fast api demo recording folder and yeah you got it just say trust and you are in now once you got this project this is where your front end is and you can run the front end as it is uh in fact i should be showing that now as well so let me do that so i will just go back here go to your terminal and in the terminal move to front end. so, make sure that you're in the front end folder. and if you have never worked on react before, this is a react project. let me show you some certain things. so, this is the main package.json. this this is where you handle the dependencies. you write the script, when to run, when not to run, how to stop, how to debug and all the stuff. now, to run this basically, you can simply say npm install. now, we're not running it.",360,179,192,Lu8lXXlstvM
11,"now, we're not running it. we are installing the dependencies. so whatever dependencies needed for this project because see if you open this package of json one of the dependency is react itself. so react is a library. so you have to get that. now once you get this uh dependencies you have to say npm installed so that it will download all the dependencies. and how do you know if you have dependencies downloaded or not? in the front end folder you will you will see one more folder called nodes modules. and you can you can only see that when you have your dependencies downloaded. so we we can't see that now when once i click on npm install it will download the dependencies for you and you can see it is doing it fast and this is the folder i'm talking about and as you can see we got this uh downloaded and those things are in this node modules and now it's time to run the node or the react project. so i will say npm start. now how do you know it is start? if you come down there's a script called start. this is where you have to enter and this will start the project. okay, it says something is already running because i think my older project is still running. i will just close it now. no, i don't want to use another port. let's do that again. and now it should start. okay, so it is opening now. and this is your project. now it says fail to fetch is because the back end is not working.",357,192,213,Lu8lXXlstvM
12,"now it says fail to fetch is because the back end is not working. uh that pop-up came and went because i've set the timer for 5 seconds. but no products and nothing will work. okay, even if you add products, it will not work because the back end is not connected. in fact, you you can also check because this is what you're going to do later. uh you can go to console and you can see it will say something is blocked or the server okay server is not server is still running on the other project. my bad. now let's try refresh. yeah, this is what i was expecting. i was not expecting cause error. so it says error connecting or connection refused. there's nothing on the back end. cool. so let's make this work. but what other setup we need? so let's go back here and close this. this is the front end part. the next thing we need in this particular setup is the fast api itself. so what you will do is you'll open a new terminal here and let's install the fast api. in fact, we need two things. one, the fast api itself because that's a framework we're going to use. the second thing we need here is a web server because we are building a web application. rest api is also a web application. and this web application runs on a server. you need to you need to get that. so if you have worked on java before, if you want to have a web server there, we got tomcat. for node, for react or nodejs, we got node itself.",362,213,239,Lu8lXXlstvM
13,"for node, for react or nodejs, we got node itself. uh for php, we got zam server. so for every particular language, we got some servers. here we have to get the web server which is called uion. i know unique name instead of unicorn that went for uon. good. um so what i will do is i will get the uicon but before that i will in in fact whenever you want to install some packages we use something called pip right but we have a problem here if i install let's say if i install fast api here so i will say pip install uh fast api is that simple just write a command it will install the problem is this will install fast api for all my python projects so maybe i'm working on ai as in some other project or maybe i'm working on uh consolebased application in some project. the moment i say pip install fast api, it will install fast api globally for all the projects. we don't want it. we want fast api only for this project. now how do you isolate? so basically in different uh languages they have different ways of doing it. in python normally what you do is if you want to have some packages specific to one project you create something called a environment your own environment or maybe we can call it as a virtual environment. so here we want to create a virtual environment. so how do we do it? it's very simple. you say python - m and then you specify what you want to create. i want to create a virtual environment.",361,239,256,Lu8lXXlstvM
14,"i want to create a virtual environment. so v env that's what it's called virtual environment. and you have to give a name to it. you can give any name. maybe you can say fast api project if you want. uh i will simply say my environment because i want to keep it unique. uh so my environment. okay. now the moment you do that it will create an environment which will take some time. okay. so you can see it is still not doing anything because things are happening behind the scene and this is what you got. you need to get this folder. if you don't have this folder something is wrong with the command or the the command here. and in this you got all your environment set up ready. and by default you will get these two but we want more. so what we you will do is you will say pip. okay. now you have to activate it first. now how do we activate? so to activate based on what you're using. if you're using powershell or a command prompt, you need to use different script. so this is these are the scripts. for powershell, this is a script. for command prompt, this is a script. so for powershell, just copy the path. i hope this will work. just going for the absolute path. let me try. yeah, it worked. so instead of typing, normally you just have to type this much. but absolute path works. then why to worry? in fact, you should be copying the relative path. yeah, this is what i was expecting. but no problem. we are able to run this script and that's important.",369,256,291,Lu8lXXlstvM
15,"we are able to run this script and that's important. now, how do you know it is successful? it has created the environment. if you see before your path, you should be getting this your environment name. so, your uh environment is activated. now, how do you deactivate? it's very simple. you simply say deactivate with the right spelling. and now you're out. so, this is how you do it. so, if you want to get in, activate. if you want to go out, deactivate. now let me show you what's the difference of i mean what how it helps. so if i say pip list it will show all the packages installed this in this machine and if you scroll down this it's a huge list for different project we got different libraries i just keep adding them and now okay if i activate again and now let's check you should be able to see only one right but then i want two more so the what are two more so you have to say pip install you need to get fast api and we need one more which is web server which ubicon. so you will say not unicorn ubicon and done. when you say run this code, i mean run this command not code, it will download those things for you which will take some time and yeah so you can see we got all the packages here. so we have dependencies dependencies right? so there are some dependencies where which dependencies are dependent on uh so it will get that as well.",345,291,308,Lu8lXXlstvM
16,"so there are some dependencies where which dependencies are dependent on uh so it will get that as well. in fact if you want to check you can do that uh pay list and it will give you everything now. so you got fast api, you got ubicon and we are happy. that's the setup you need and i'm tired doing the setup here. so these two things and fast api ubicon you need. then you can start with the code. now it's time to start coding. but again we will not be writing a lot of code in this particular video because we have to also understand some theory part before we go ahead. but we'll start with coding and then we'll get into theory. so what i'm going to do is we got these two things here that perfectly works. let me create a python file and let's name this as main. py. so what exactly i'm trying to do here? so let's say whenever a user visits the website of of course we are not buying any domain here but we'll run this project in the local. okay. and the way you can access that is by saying localhost colon. so instead of 3,000 let's go for 8,000. what are these things here?",280,308,325,Lu8lXXlstvM
17,"what are these things here? now if you are new to web development this local host see whenever you talk about any website or any web application you have to deploy it somewhere right so maybe you're deploying on some server maybe provided by amazon so what you do is you deploy your application there and it is running on the server now if if you want to access it you have to use the ip address because every server will have their own ip address and since we don't want to remember the ip address instead of that we use domain names right so whenever you go put to your website like when you go to nodejs this is their website domain if you go to fast this is a website domain in the same way if you want to access particular website you have to provide a domain name now since we don't have a domain i'm running on my own machine so i can use the ip address now since we want the ip address of the same machine i can use local host next in your machine you can run multiple services multiple server services and how how will you differentiate between these services and that's where we can use something called a port number. example, when i was running a react, react was running on port number 30,000. the fast api will be running on uicon server and by default the port number will be 8,000. so when i say enter, it says the site cannot be reached. why is because our server is not running.",352,325,330,Lu8lXXlstvM
18,"why is because our server is not running. so what we can do is we can run the server. but before we run the server, we need to return something, right? so let's say the server is running and you want to show some output here. so when you go to this homepage it should return let's say welcome to the list track something like that. now in order to achieve that what you can do is you can create a function because in python when you want to achieve something you create a function but again it is giving you s some suggestions. i don't want to use the co-pilot suggestions. so i will just nose it for some time now and okay so let's create a function. so i will say defaf and the job of this particular function is to greet the user. so i can say greet. okay, simple function and i'm going to print welcome to the list go track. okay, simple stuff, nothing fancy. i want to print this. okay, now there are two problems here and i will tell you those problems in some time. so i will just save this. now question arise, how will you run this? now of course you can write this is your python file. you can simply say python main python space main. py and this will this should be able to print. it's just that we are uh running it but we are not calling it. okay. so we have to call the grade function. okay. so let's call this and run. and you can see it is printing on the console.",358,330,354,Lu8lXXlstvM
19,"and you can see it is printing on the console. but we don't want to print on the console, right? we want to print on the web page. now in that case, i don't have to call it here. it should be called through the web page. so whenever i refresh this page or go to this homepage, it should call that particular function and it should return this welcome to the list track. so that means we we're not supposed to print it. we are supposed to return it to whoever is calling it. so printing is not an idea. now we have to return this. okay, that solves one problem. and once we save it, let's go back. run. uh, it will still not work. reason the server is not running. okay, even if you have the code, the server is not running. now, how to run the server? of course, you will not be using python because python is for the console based. this is where we have to use a web server. now, one of the web server we have already installed is ubicon if you remember. so, we can use ubicon here and we can run it. so, i can simply say ubicon. then we have to provide the file name and then we have to specify hyphen - reload. okay. now let me try to run this and let's see what you get. okay. it says it is trying to will the changes in the directory because reload means the moment you say save it will just reload. uh uicon running on server this. okay. you can see the port the address here.",362,354,383,Lu8lXXlstvM
20,"you can see the port the address here. now 127.0.0 zero is your own machine and then 8,000 is a port number but here somewhere here it says error loading the asgi app which is your web server import string main must be in a format okay so it needs a format if you can see this is the format it is expecting here so it's module and attribute so we are specifying the module here which is main this is your module because that's what you're creating here but something is missing some attribute is missing uh okay let's go step by step. now what is missing is we are using a web framework which is uh fast api and nowhere we have mentioned that we're using fast api. so we need to create the object of fast api and let's say the object name is app equal to and fast api we can simply use fast api here and create the object for fast api. but then this is not happy. it says uh fast api is not defined. okay, why it's not defined is because it is defined. it's just that we have to import it. so i will say import fast api. um no i have to import fast api but this will be coming from fast api library. yeah. so from fast api import fast api. okay. and then you can create the object. so this fast api is a library. this is your class. i hope we can see the documentation. yes. so you can see this is a class. cool. uh and then we can create the object.",357,383,403,Lu8lXXlstvM
21,"uh and then we can create the object. and now since we have the object here when i run this now so i will just i can say ctrl c to stop the server and come back to the main screen. and now when i run this i have to pass main colon app. so you have to also mention the object name. cool. now it should work. enter the server is running. congratulations. we can have a round of applause. something is working after a very long time. and now it is running on the server. that's cool. so when i go to this particular page, in fact the same page we are doing here. and now if i hit this again, oh okay, we got some output. server is running but it says not found. okay. so there were two problems before. first the server was not running. second we were not getting whatever we want to print. uh but at least one problem is solved. the server is running but we're not able to print. so let's solve that. and before we understand that, let's move towards something called rest api. see, before we go move into rest api, we got a client and we got a server. now, what's the job of a server is to give services, right? now, this can be a file server, this can be a web server, this can be some other server. now, when you say it's a file server, the job here is to give files to whoever who is asking for this file.",344,403,429,Lu8lXXlstvM
22,"now, when you say it's a file server, the job here is to give files to whoever who is asking for this file. so, let's say if there's a client a and says, hey, i want that file, it will go to the request goes to the server. file server says, okay, let me search for the file and yeah, i got the file. this is your file. that's how the file server works. and it uses its own protocol to do that. there's something called ftp or sftp if you want to secure it. uh on the other hand, for the web server, what's the job of a web server? to give the data to the user. now, normally this data can be in the format of html or it can be format of json. now, why html is because in the real world, we all use web pages, right? so when you go to google, when you go to any website like amazon, you when you go there, you see the amazon page, right? you see all the products. now you are able to see that beautiful pages because of the html and css and javascript. now all these things coming to the client side from the server. so when a client says hey give me amazon.com the amazon.com says okay these are the files take it and view on the browser. so that works right. but the problem is when you create this type of application because see one application needs multiple things. there are multiple layers here. user normally uses front end but the data is stored in the database. so we got the front end and we got database.",369,429,449,Lu8lXXlstvM
23,"so we got the front end and we got database. so there are two apart two things apart. now user is concerned about the data. so of course user wants a data that is coming from the database. so don't you think the front end should be able to interact with the back end directly or with the database directly? uh we can't do that. first of all there are multiple issues. one is is not secure directly accessing the database. second uh you will get data in normal text we want something good and the third problem is to access the database we have to use sql if it is if it is a relational database and that's why what you do is in fact there are fourth problem also sometime you don't want data directly what you want is processed data something you have worked on something you have applied filter on or maybe combining multiple things your calculations and some data in this case what you do is you add one component in between which is back end. so we got a front end, we got a datab database and now we have the back end in between. now back end will be returned in any language. it can be python, java, javascript doesn't matter, right? so that's a back end which we build. so the request goes from the front end to the back end. then back end does the processing of it and applying logic to it, fetching data from database if required and then sending the response to the client or the front end. now this can be in two format.",360,449,464,Lu8lXXlstvM
24,now this can be in two format. one from the back end side. you can create those beautiful pages and send it to the client side or you can send the front end without the data just the layout just the layout and then you can populate those data later example let's say if your internet is off and you are so let's say on your phone you don't have the internet you don't have your mobile connection as well but still when you open instagram you can see the layout there right what is missing is the data you can't see the image you can't see the text but layout is there is because the layout is fixed. now what is coming from the server is only the data. now in which format you will get this data? the format is json. way back we used to use xml but it is bulky and lot of issues. now json is a perfect format where you can get the data in a structure format and it will be less bulky to understand and you will see some json data if you have not seen that before. so this is the communication. now we started with rest and now it's time to talk about rest. now when you send this data the data has to be in a proper format and that's where we got this term called rest which is representational state transfer. see whenever a client sends a request for the data you are sending the state. state means current data or current state of the data.,348,464,476,Lu8lXXlstvM
25,"state means current data or current state of the data. so you're sending the state from the server side to the client side or vice versa. you're sending from data from client side to server side. so you're sending a state. next you need to send the state in a representational format. it should be easy to represent and you are transferring it and that's why we got this term which is representational state transfer rest so in order for a client to reach to the server of course server simply not accept all the request uh what if you are building a website for the e-commerce and this and the client says hey i want to look for job here the server can't do that now because server says okay i was good with uh products but now you're asking for a job i can't do that so as a server it has to open some endpoints. okay, these are the endpoints. you you can you can b browse the products, you can buy the product, you can do the payments. so those are the endpoints which we have and to build this endpoints we we use something called apis which is application programming interface. so in order to access the server we have to access their apis. so if you're a server and if you want client to access you you have to build those apis. okay.",310,476,487,Lu8lXXlstvM
26,"okay. so if you want this server or if you want this client to get the data from you by the way this is json okay so if you want your client to get data uh from the server so client has to request and the server has to accept it now question arise how will you accept it now if you go towards something called crud which is create read update and delete these are the crud operations right now for this operations we got this amazing protocol for the website which is http or web applications. so whenever we use internet we always use http. so if you check here not here let's say if you go to this page http yes there's also s there for security but the general protocol is http. now http http says i will give you certain methods to work with. now this methods will help you for the operations. so you got get for reading, you got post for create, you got put for update and you got delete for delete. in fact, there are more methods which we can use but these are standard uh i mean mostly used. okay. now after all this theory let's come back here and let's say greet. now whenever send someone is sending the request to the server we have to understand that for the homepage i want to return this and to do that we have to use decorators and we have to say app dot. now this will have because app is fast api itself. it has something called methods. now there are multiple methods here.",356,487,500,Lu8lXXlstvM
27,now there are multiple methods here. uh we can use get we can use post somewhere here there should be post put patch uh there should be delete as well somewhere up yeah delete so we got all these methods i'm going to use get here because we are getting data so whenever you want to read something from the server we say get and we have to also specify the root the path okay so i'm accessing the homepage here so if you see i'm trying to access the homepage and to do that i can say slash again what are the other options we'll see later but we i have this option here which is slash and i think it should work now. so if you do this the moment you save you can see it is reloading the server. that's what we wanted. go back to the homepage and refresh. you got it. you can see that it says welcome to the list track. and finally after talking so long we are able to get something on the screen. and the code is so simple right? i mean look at the things which we have done. just do the setup once you get all the libraries and this much of code and your server is up and running with welcome to the code track. so this is fine where you are requesting for the homepage and you are written a text. but then if you remember we started with a application where we want to work with the products. a product will have an id. it will have a name uh description the price the quantity.,362,500,514,Lu8lXXlstvM
28,"it will have a name uh description the price the quantity. now how we are going to do that? of course, i can just change this url. so, instead of having homepage, i can say uh slash products. now, that's how you do it for the rest api. now, whenever you visit a page, let's say if you go to google and if you search for teresco, of course, you will get all these details here. but look at the address bar. so, it is google.com because that's the homepage. then slash search because now we have a different path, right? so, we are searching and then we are passing a query. so you put a question mark and then you say the the variable q equal to the list. so this is what we are searching right. ignore the other parts. this is just for the other things. so we can just ignore this. this is the important stuff. so search that's the path and then question mark q equal to list. now this is how you basically do the query. now when you visit a page and if you want to pass the query as well, this is the obvious way. but then if you want to search something as a resource because all the products online or in the database in the back end they are the resources. so you are requesting for a resource. so instead of saying uh question mark q there should be a different way uh something like this.",332,514,535,Lu8lXXlstvM
29,"so instead of saying uh question mark q there should be a different way uh something like this. so let's say if i want to search about products and i will go with the google.com this will not work but i just want to show you how it works how it looks basically. so it is google.com proucts. now it will it should give me all the products from the database. okay that's my expectation. then if you want a specific product, let's say if you want a product id with one, product id with two, then you will say slash one, this will give me the first product, this will give me second product and then so on. what if you want to uh add a new product? now this is where the things will change. now if you want to add a product, how will you do it? so will you say add here? now that is not how you do it. so basically what we what happens is for the fetching you know when you want to read you will use the same link and for posting as well when you want to add a new product you will use the same url and now you will say okay how will a browser differentiate between when when i want to fetch data and when i want to save data and this is where the http methods comes into picture which we have talked about before. so when you want to fetch data that's when we are going to use get. so the same url or the same uri but the method will be get.",360,535,548,Lu8lXXlstvM
30,so the same url or the same uri but the method will be get. if you want to submit something if you want to post you will be using the same url but different method name. okay remember that point because we are going to do that in this particular project. so now i want to fetch the products right now before you even fetch the products you need to have the products. now how we are going to do that? uh since it's already opened here. so the libraries are already opened. i don't want to uh open again. so if you see we have also added pye now we have not added manually when you ask for uh uicon you got this by default and this is what we are going to use now. cool. so what it basically does so pilent is basically for data validation. okay. and also if you want to convert your data from the server side into json format to send it to the client we are going to use this. again this will make much more sense once we do the coding for it.,249,548,561,Lu8lXXlstvM
31,"again this will make much more sense once we do the coding for it. the first thing we're going to do here is now since i want to work with the products, what i will do is i will just go back here and say app dot uh again a get request but this time instead of saying slash okay i have to snooze my copilot and then here it will be products because that's what i'm asking for and i want to say get okay and i'm expecting all the products not just one product and here i can define a function and all these functions are by default async so you don't have to mention async and then i will say get all products if you can be better with the uh name of the methods, it will be helpful to read. and then this is where you will return the products. at this point, i'm just going to return all products just to see if things are working out or not. so i will just save this. so when you save it should reload and then let's go back here and instead of hitting the homepage and i'm just hitting the home page just to check if everything is working. i will say products enter. and you can see it says all products. awesome, right? but then we don't want text as all products. we want the actual products and not from the database because we have not done the setup for the database yet. so we'll do that later. so let's say we we have a inmemory list. we want to work with it.",361,561,574,Lu8lXXlstvM
32,we want to work with it. so there should be a list somewhere here. and let me just create in fact you know we are done with this. i can remove this or maybe just keep it there. why not? uh let's me let me just create a list here somewhere here. but then how will i create the list of products because of course this is a list of products but then we don't have a products there. there are different ways of doing it. of course i can just create uh a dictionary here by specifying all the values. that's one way of doing it. otherwise if you go with the object oriented way which is a better way here. what i can do is i can create a models or maybe i i can create a product file but models make sense because these things are called models which holds data or which is used to represent any data. so example if you want to represent all the users of a system. so that's one model. so you can create a user model and this will represent the users because a model will have a name and it will have some properties. example here let me create a class and this will be product and this will have some attributes. so i will say id of type int. then we can have name of type strr. then i can have also have a description here of type str. then we can have a type of price which can be float. and then last thing we need is quantity of type int. okay. so these are the things you need.,366,574,596,Lu8lXXlstvM
33,"so these are the things you need. so basically you can represent the data in this format. so you can create one product which will have all these properties, right? and then you can just go back here and create those objects, right? so i can use the constructor of it. i can say product. but then to use the constructor, i don't have it here and i can just ask for the constructor. so this is the constructor. i hope this will work. i have not tried this before for this application because we were using pentech. but let me try. so i will say id is equal to one or maybe i can just pass the value name. i will say phone. okay, it says product is not defined. uh what i will do is i will just import this. so from models import uh product. okay, now there's no issue and i can add description. so name is int is done. i mean the id is done. uh name is done.",226,596,615,Lu8lXXlstvM
34,description i'll say budget phone and then let's say the price is 99 and the quantity let's let's say we have 10 of it so at this point i just have one product or maybe i can add one more just by giving a comma here so two laptop gaming laptop 9 and let's say we got six quantity of this so we got this two products and now instead of returning all the products here i can return the products itself right so what it will do now is when you request for the products it should return this list and let's see if this works okay i'm not sure let me try i will say enter oh we got it so i can just repetitify this and you can see we got the data in this format so this thing works so basically we can just create this thing and you can return it from here now this is one way right and this is working nothing wrong with it the problem is in future maybe you want to do data validation so let's say when you are send this when you want to send this from the client side to the server maybe maybe a client want to add a new product and when you say you want to pass the id the client is sending an id let's say 10 that works what if the client sends the id as minus one or minus 10 that doesn't that an issue and this will accept it right this will not say any problem there because it's an integer uh what if you pass,358,616,616,Lu8lXXlstvM
35,a price which is negative of course price should not be negative what if you pass the description of only one character uh that doesn't make sense why description will be of one character there should be at least two words even if you're lazy okay so how do you do that validation and that's where we got this particular library called pyntic which will help you for the data validation so we have already seen that now how do you make this as a pyntic so it's very simple you come back in the models and here you can just import you can say from py tech because that's what you want to use we just need one thing from that which is a base model so i can say import base model of course you can take it lot of things from there so you can import all these things from it and we are going to use some of them later like field but here okay why i'm deleting that part i will just import parentic import uh base model this is what you need and then you basically inherit because if you want to achieve that feature you will simply do inheritance and you got it you got your base model now you don't even need to create this constructor by yourself that is done taken care of and things you need to change is instead of specifying this syntax it should be with the variables because that makes much more sense right why do you pass in this format so id equal to and then the list goes on now instead,358,616,616,Lu8lXXlstvM
36,of typing in fact this should be name so instead of typing all the things i i just i do have the data ready with me i'll just copy paste that oh wait instead of saying copy paste i should say code reuse so paste and we got it.,63,616,616,Lu8lXXlstvM
37,"so you can see we got ids. we got four ids because we got four products. and then we got phone, laptop, pen, table which is a name. and then we got description. we got the price and we got the quantity. now with this changes, let me just go back here and refresh. you should be able to see all this. now if you're not able to see in this format, you just have to uh download this extension called pretty print from the chrome and you will see in the proper format. now this is one way. okay. but what if you don't want to use here you want some good ui to test it out? uh you can use something called swagger. now in other frameworks basically you have to add a swagger. with fast api you get this by default. so what you do is you use the same url but not products. remove that. you say localhost colon 8000 and then you say slash docs enter and you got your docs. this is swagger. okay.",234,617,635,Lu8lXXlstvM
38,okay. and now if you click on greet because we have created two endpoints if you remember we got the endpoint if you go back to main we got this endpoint which is for the homepage and we got the endpoint which is products and here we got the first now how do you use it if you're not if you are using this for the first time click here uh there will be an option of try it out click here and then there will be option of execute but before executing make sure that if it is asking for any parameter or not at in this at this point we don't have any parameters you can click execute and this will respond here. so it says welcome to the disco track. uh scroll down. we got another one which is all products. expand this. scroll. and you can say try it out. execute. and it will give you all the products here. right? also it returns if you observe it returns a code 200. again we have not talked about that here but we'll see. remember this point the code 200.,250,635,648,Lu8lXXlstvM
39,"remember this point the code 200. uh just to give you a hint there are multiple codes available if something goes wrong on the uh with these resources uh or the file is not found or something you will get 404 you might have seen that everywhere right so that's a part of the status code okay so this is how you basically fetch all the data now it's time to fetch one product because now we are getting all we don't want it let's say if i pass an id one we want that in this case how will you do it should be simple right let me create a new function and this time i will say get product by id. okay, so it should accept an id and based on that it will it should return the product. now at this point i'm just going to pass a variable here. let's say id and this will be of type int. this is the variable i'm going to work with and based on the id should return and of course there should be a mapping as well.",245,648,653,Lu8lXXlstvM
40,"this is the variable i'm going to work with and based on the id should return and of course there should be a mapping as well. so let's say at app.get get and then you have to specify the url and of course that should be product or maybe you can say products but let's say product slash u now so let's say if you simply say product and by default i'm assuming the id is one how will you do it so one of the easy ways if you know that the id will be always in sequence and there's no missing point uh you can simply say uh you can use the feature of list where you can fetch a particular element based on the id something like this. i can say return uh product and then i can specify let's say if i'm assuming that's a first element i can simply return by zero. that should work right and let's say we are not even using that. so we can remove this till this point and at this point and let's work with it. and now let me hit this url product not from the swagger but from here localhost col 8000 product not products product enter and this is returning the first product. okay, that's cool. but every time it will return the first product. and the second problem is what if you want to fetch the product id two and two itself is a first product. then this will work. and this will not work. that means uh because it will always return zero.",349,653,664,Lu8lXXlstvM
41,"that means uh because it will always return zero. and even if you say one, what's the guarantee that the second product has the id two? no guarantee, right? even if you want to fetch the id with two, in that case you have to pass two. okay? so what you will do is you will map it with two so that you will get two and now i can just say one because that's that is two okay now that's a problem right i got two here and i want to use that two here how will i use this two here that's that's crazy now we can what we can do is we can hardcode so for two it should return uh with id two we can do some coding for that and for three we should do do some coding for that and for four we should do some coding for that uh what if you have 100 products you have to get 100 methods. that means we have to make this two dynamic. in this case, if you want to make it dynamic, just put it in curly brackets and use a variable name. let's say id. so you can say product id. but don't mention id. mention the number and that number will come here because this is a dynamic path and you can use that path here. so you can say id type int. so whatever name you are going to use here, use the same name and you are okay. and now i can use this id. so i can simply say id minus one. that should work right. let's try.",361,664,681,Lu8lXXlstvM
42,"let's try. so go back here and now product slash. if you say one, you will get the first product. if you say two, you got the second product. and i will also do p5 here. and if i say four, you will get fourth product. but if you say six, it will say internal server error. and there will be error here as well. so because we are going out of list. okay. okay. and that's why you have to handle the exceptions. but i'm not going to do that. i'm just want to make it simple. uh this is working. but then the problem is what if you don't have a product like this. maybe you have product five and six here. but still this will not work. so even if you say six enter, it will say internal sub error because here we are sending the request 6 - 1 which is five and we don't have the sixth element. okay, this is not the right logic. so let me just write the right let me just write the correct logic here. so we have to search the products for the id. so i'm just going to iterate product in products and then here i'll check if the id provided by the product or one of the product. if that id matches with this id in that case return the product. if it is matching return the product otherwise you can return product not found. okay. uh that should work. but there is an error here. there should be double equal to no problem. now cool. let me go back here. and now let's fetch for six. it is working. perfect.",369,681,714,Lu8lXXlstvM
43,"perfect. if you search for four, there's no product as four. so it says product not found. just to verify there's no id four here. okay. and what if you want to do it for two? it is working. so this is how you fetch a single product. okay. that's the logic. okay. so that's for fetching all the records, one record. but now what if you want to add a record? so maybe we have four. or i want to add more. in that case what you will do is uh let's create a function and this will be add product and now you can guess i don't have to tell you now uh you guessed it right we have to say app dot now we can't use get because we are submitting data we are creating a resource in that case you have to say post and then the url the url will be same as previous which is product we don't have to modify that how will you differentiate between a single product or this this slash and this one is because of the get and post. now how will you add a product? so we have a in my list here. i can simply say the products in this i want to add. so product dot we have a method called append list. we can use that and we can append it with oh we can append it with what we are sending data from there. we have to also accept that data here. now in which format you will get the data? you will get the data in the format of product itself.",361,714,737,Lu8lXXlstvM
44,"you will get the data in the format of product itself. so you will get the product of type product. so just get that and add it here. okay. so what we're doing here is we are creating a method called add a product of type post. so you can see we have we have this we have this url which is product and then from the client side you have to send the product details that will go here and then you are appending. but the question is how will you send this data? okay. so let me just save this. so now we have to also return say return product whatever we are getting just return that. okay. but how will you make this work? this is this should be a bit tricky. let's try. first of all, we can't do that from the browser directly. we need a react application or i can use swagger because for post you need a form. okay, we don't have a form here. but swagger says don't worry, i will take care of it. just refresh the swagger. now i don't want to greet. i just want all the products. i will just keep it open. uh and i will keep open the ad product. okay. in fact, we have okay, let's just test add product. and if you expand add a product, it will give you some details. the of course you can click on try it out. but it will give you a schema. okay, default schema where you can add the elements or add the values. and let's see. so i will just click on try it out. and you can edit this values now.",373,737,768,Lu8lXXlstvM
45,"and you can edit this values now. so let's say this is eight. and i'm going to add a new product. let's say watch from i will say titan watch. and the price is let's say 80. and the quantity we have is let's say 50 because we have it. okay. so this is the data which you have to add and this has to be in a json format. so when you click on execute this data goes to the server and you can see this is the request it is sending using curl behind the scene and the url. okay. so there is some problem. it is sending request for product. oh i forgot to add a slash here. one slash and this is what happens. okay. uh let me just copy this so i don't i don't have to type it again. uh execute. so do i need to refresh this add product. we'll say try it out. paste. execute. and yeah, this time it worked. you can see there's no error. this is the url and you got the data here and this is 200. but how do we check if this get got added to the list? let's go up. let's see all the products. try it out. execute. this is what we have done hardcoded. and this is the new data. okay, things are working out. so this is how you fetch all the record. this is how you fetch one record. and this is how you work with uh posting the new record. oh, this is working. and now it's time to focus on put and delete.",356,768,804,Lu8lXXlstvM
46,"and now it's time to focus on put and delete. so basically, if you want to update the product or you want to delete a product, how will you do it? so that's what we're going to see now. now before i do that i just want to remove this extra spaces from here and let's look at the code. so basically we got this main. py where we got all the product in the list. we are still not working with database and we basically have this methods like get and post. now in this get we are fetching all the products and in this get we are fetching a product by its id and then here we are basically adding a new product. okay. uh so now let's try to update it. but before we do that, in fact, there are a lot of things which have to do before we do that because if you can see my shirt color has changed. it's a new day and i have to do that steps once again to run this. so, first of all, i will run this server. before that, i have to change my environment. so, the virtual environment it is using now. oh, it is creating a new environment. no, no, don't create a new environment. i'm using a wrong thing. i i i have to activate it. so, activation done. and now it's time to run this. and also i will disable my co-pilot or snooze it for some time. and let's run this. and this will load the server. no problem with the server loading. and we have to also run the front end.",364,804,829,Lu8lXXlstvM
47,"and we have to also run the front end. make sure that you don't run this in the environment. so you have to deactivate this so that you will come out of the environment. and now you can run the front end. and you have to switch to front end and say npm start. now the front end started as well. uh but again we the front end is not fully configured yet and that's why you can see it is failed to fetch. so there's no need to run the front end. i don't know why i run that muscle memory maybe. uh but let me just go with this. so there's a problem with the front end. that's fine. okay. but but the back end is running and the way you can test it you can just go to localhost col 88 uh 8,000 and then slash here we have to say docs which will load your uh swagger in which you can test all the things and we need to first test it out if it is working so that we can continue. so it's still working nothing is wrong yet. okay so let's go back here and now it's time for the update. now if you want to update a product how will you do it? first of all, you need a function. and we'll name this as a update product function. and then this will uh also accept the product itself. but then with product, we have to also pass the id. so which particular product you want to update, right? so you can ask for the id. so i will say id of type int.",365,829,852,Lu8lXXlstvM
48,"so i will say id of type int. and then you will also ask for the product because that's what you're going to update. now if you want to update a particular field of a product, you can also use patch. but we'll not be doing that here. we'll be only using put and this is where you will update and we also do the mapping for this. this should be app dot now instead of using get or post we are going to use put and here we'll say the same url which is product. cool right and now i can basically write the logic for updating our product. so it says there's an error on line number 36 and there is an error. so you can see it is expecting a block but i'm going to write a code for updating. now when you want to update you want to update the product from the products list with this particular product right and uh which one to update now you have to basically iterate between these products to find which id it is matching with 1 2 or five and six and then you can do it. so in that case i can say use a for loop. i'm going to start with i uh in range or maybe i can also use product but then i can't replace it. so let's go with the range itself because i want the uh index of it. so i will say uh for i in range and i will go till the last value and for that i have to say i mean second last value. so length of products.",364,852,866,Lu8lXXlstvM
49,"so length of products. now once you have this range it will start from zero. it will end at three because we only got four products here. right? and of course in future if you add new products this will go to the second largest value. so if the value if the length is 10 it will go till 9. okay then you will basically check for each product. so you can check products of zero or sorry products of i and then you can check the id if it is matching. okay, if it is matching that means i have to use if. if it is matching with the id whatever you have passed in that case for that location that means products of i for that location replace it with the product which you got from the client. looks good. and if this is successful you will return product added successfully. again this should be a json data but i'm sending a plain text but that's fine. it it works. and what if you don't find any products? of course, this will come out from there and then you will say return no product found or you can say product not found whatever works. so this is the update code and i want to verify if this is working. so the moment i saved it this got reloaded. i can go back to swagger now or the docs refresh this and first of all let's get all the products from here and we'll update one of it. so execute and you got all the products. let's say i want to update uh this table here.",360,866,886,Lu8lXXlstvM
50,"let's say i want to update uh this table here. so i will just copy this and the id is six. remember that go back to put because this is what you want to do update product and you will update. so you will say try it out type here six and replace this schema with your schema. in fact you can uh change the values from here but i will just paste it. then i will keep the idea same. i will change it to table desk and i wouldn't table for your computer. okay. and then price since we have added this description the price should go up. that's what we do, right? and let's say the quantity remains 20. okay. so we have changed name price description but not id and quantity. let me execute. let's see what goes wrong. nothing went wrong here. uh it says product added successfully. that's great. that means if you go back now here and if you try to execute this once again and look at the product. so we got table test changed uh description changed price change. okay, that's how update works. but now let's work with delete. so let's go back here and now you help me. it's okay. now the question is how will you help me when you're watching this video? think just think about it. what will be the logic? i will get it.",310,886,913,Lu8lXXlstvM
51,"i will get it. telemetry uh so i will say app dot it should be delete the method okay and then you have to also pass the uri we'll go with the same uri and as i mentioned before all these uris are same what is changing is the method here okay so let's define a method so we'll say define delete or def delete product okay now question arise based on what value you want to delete maybe a user will send the entire product and then you can delete it or uh id is enough right because every product will be different based on its id so you can simply pass an id here let's do that so except the id so we'll say id colon int and based on the id you will delete it but again you have to iterate right because you have to match with id it's matching with so we'll say for in fact the same code which we have written before for range and this will be length of products and then you can check if products of i. in fact, you have to match the id. if it is matching with the id which you mentioned from the client side, in that case, delete the products of i. as simple as that. and if this works, you can return product deleted. and what if you don't find the product? so you have to some mention that somewhere. so we say return same thing product not found. so once we are writing no product product found and once we are we're writing uh product not found.",356,913,922,Lu8lXXlstvM
52,so once we are writing no product product found and once we are we're writing uh product not found. uh there should be some consistency right and i'm not doing it. okay. so we got this products here right now i want to delete let's say uh six itself or maybe in between let's say i delete the second one two. so the id is two. let's go back to uh okay i think we have to reload this put and delete. delete is here. and we are going to delete two. so i will just type two here and execute. nothing much. and it says product deleted. that's great. let's go back to get all products. try it out. execute. and here we go. you can see we don't have to anymore. and that's how you update and delete. but i want to show you the code once again. and this is it. so this is update product where you are getting the id getting the product the new details which you want to add and then you are iterating and then you are basically checking for each id if it is matching or not. the moment you find the match you will update the product from the list and that's how you update for delete. same code it's just that instead of updating we are deleting it. cool right? uh but then remember everything what we have done now we are doing with inmemory list.,321,922,946,Lu8lXXlstvM
53,"uh but then remember everything what we have done now we are doing with inmemory list. so this is a list but in the real world we don't use list right we use a permanent storage maybe like posgress or mysql or any type of database it can be rdbms it can be nosql databases or somewhere and now it's time to save the data in the database so whatever option we were doing like creating updating uh deleting and reading this should be done through the database now database you can use anything which you want maybe a csv file you can use uh nosql databases or you can use rd dbms and here we are going to use the r dbms. now you have multiple options to work with. you can use mysql, you can use postgress, you can use oracle, whatever you prefer. now i'm going to use postgress but because it's one of the best advanced open-source uh tool that's what they say. so if you go to postgress, this is what they say the world most advanced opensource relational database. but of course there are other options as well. whichever you prefer will work. now question arise how will we are going to do this? now whenever you work with fast api one of the recommended way is to go with sql alchemy. but why we need to use sql alchemy here. now sql alchemy is your python sql toolkit. that's good. the other options as well. but it also provides you something called a object relational mapper. now why this is important? see if you go back to our code which we have done.",364,946,962,Lu8lXXlstvM
54,"see if you go back to our code which we have done. we basically have a class right? a class like this where you have a product and then you got certain fields there. now let's say if you want to create a database and if you want to create a table. now if i ask you hey you got this particular class here and i want you to create a table for it. now in your mind actually you can do the mapping right? you can say okay i got a class which is product. let me create a table called product. i got this fields like id, name, description, price, quantity. i can create the table with those columns like id, name, description, price and quantity. as simple as that, right? so, and then you got the table name, you got column names, but you also got the column type because in the class also when you define those properties, you mentioned, hey, that's an id which is integer. so, here also we can say integer. now, based on what dbms you're working with, uh the name will change from numbers to integer. so, here we can have string or wcad or text and list goes on. so, the type defined in the class will be defined for the table as well. but you will say okay mapping is done but what about the data? what about the row data? so each row will represents one object. so let's say we have created four products right that was in the list. now each product here is the object of a product class.",356,962,982,Lu8lXXlstvM
55,now each product here is the object of a product class. now that object goes to database and becomes a row. so what we are trying to do here is we are trying to connect the object and the relations which is tables and we are doing a mapping for it and that's what we call or rm. now you will say what's the benefit of it? you know this looks fancy but why do we use it? see when you work with databases like let's say posgress or mysql and if you have a language like python and whenever you want to save data basically what you will do is you will take the object and this object will have the values. now you will write the sql query. so let's say if you want to create a new record in the database. so you will write the sql query which is insert. so you will say insert into product. then you will say values and in bracket you will mention the values. right? now from where you will get this values from the object then manually you have to pick up the value from the object put it there. pick up the value put it there. so you have to create that query as a developer. so that's for the insert.,289,982,997,Lu8lXXlstvM
56,"so that's for the insert. what about let's say if you want to fetch then you will write the query which is select star or based on what columns you want to fetch based on how many rows you want to fetch you will write the query in that format what if you want to update for everything you got queries and you have to write those queries now you will say okay i know how to work with queries but the question is do you want to really write that in the python code what if someone else can do it for you so they will say let me give you a tool use this and let me take care of creating all the queries for you. you just say if you want to insert, you just say add. if you just want if you want to fetch maybe you can say get whatever method is. so basically you can use those methods or functions and this will do the things for you. cool, right? and that's why we are going to use sql alchemy. then the question is how do you get this in your project? now if you head back to your project here, okay, let me stop the server because we are going to make some changes. now if i check pip list, nowhere you will find sql alchemy. you will get pyic by default but nowhere we got sql alchemy right. we need to get that. apart from it we also need to get the driver for posgris. now based on what dbms you're working with. so if you're working with mysql you need a mysql driver.",366,997,1011,Lu8lXXlstvM
57,"so if you're working with mysql you need a mysql driver. if you're working with posgris you need a postgris driver. and we need those two things in this project. sql alchemy and the driver for postgress. so let's install it. so i will say pip install. so we need sql alchemy and we also need the driver which is psy cop g2. i don't know why they went for this weird name uh because this is pg makes sense posgress and uh psycho. if anyone knows this let me know in the comments. i've not not explored why they have this weird name. uh 4c for mysql it's a very simple mysql connector for python perfectly makes sense right but i don't know why they went for this name but this is a driver for posgress and this is your sql alchemy and you have to also make sure and before you work with posgris you have to also make sure that you have posgris in your machine if you don't have it click on download it will download the postgris for you based on which os you're working with i'm using windows and these are the version i think in my machine i got 17 or 16 doesn't matter because the last update was in 2022. so that is 17.6. uh they are releasing the beta 18 beta is released. that's great. uh if you want to use beta, you can try it out. but if you're new to the development world, stick to the stable versions because if something goes wrong, you will always blame the system because it's in beta. maybe it's your mistake.",361,1011,1027,Lu8lXXlstvM
58,"maybe it's your mistake. now when once you get experience, once you have worked on multiple project and if you if something goes wrong, you can still blame it. but at least you will know are you really responsible for it anyway. so you will get this download. now make sure that when you get this you also get pg admin. now if you don't have pg admin in your machine uh or if you don't get this by default search for pg admin somewhere and you should be able to get it. i think you will get that by default. pg admin. there should be a tick mark. now how the pg admin looks like. so this is how the pg admin i'm just opening it now looks like bit heavy to start with in fact earlier pg admin 3 was very heavy four they have changed the interface to make it lightweight but since it's connected with the posgress service which is running behind the scene uh it takes time so by time it is opening let me just go back to our code and let's install this so this should take some time now since i already had this in my system it will be faster yeah, it it was fast. and now if i check pip list, there should be sql alchemy and there should be the driver. perfect. now once you got these two things, we can get started and work with the database. okay, but i i also want to show you the pg admin. this is how it looks like. uh i should zoom it a bit. okay.",357,1027,1044,Lu8lXXlstvM
59,okay. so this is pg admin and by default i think you have to add the server by specifying the username password. username by default will be posgrisis. the password you have to set when you're installing it. and i forgot what is my password for this particular pg admin. but we'll figure it out because i use different machines for uh project and for recording and i always mess up the names. okay. so we'll use this. we'll try to save our data in the postgris database and if you're using mysql the only thing you have to change is uh in fact you have to change two things one is a driver and second some lines in the database connection that's it and it will work now once we have our libraries ready so we got sql alchemy and we also got the postgris driver it's time to start with the configuration now why do we need configuration so let's say if i go back to main now this is where we are and let's say if i now say in this get all products instead of returning this particular list which we are doing here i want to connect to database. so of course before we do this there should be some database connection. so i will write some statements here. okay again i have to disable or snooze my copilot. okay i will write in a comment. i need uh database connection because without database connection how will you connect to database? and once you are connected then you want to fire some query right uh which will do. so here we have to write some query.,365,1044,1059,Lu8lXXlstvM
60,so here we have to write some query. so first we have to create database connection and then we can write the query. now we know that in fact in the previous video we have talked about we are going to use sql alchemy. so i want to create this database connection which talks to the sql alchemy. and to do that of course i can do all the configuration in this file but it's better to have in a separate file. and this i will say database py or you can say config. py whatever works for you. so i will say database. py. and this is where you have to do do the configuration. so basically what you need the first thing you need here is something called session local. now using the session local we have to first of all create this object here so that we can use that object in our main file. so database connection. so we'll be using session local to access the data database. but how do you create object of session local? now first of all what is session? so every time you connect to something it's a session. so if you connect to a server that's one session. if you connect to a database that's one session. okay. and you you need to create a session before you do this operations. now to create a session we got something called a session maker. so you'll be using that and this will create the object of session local. but then from where you are going to get this session maker also uh for session local you can name it anything.,362,1059,1082,Lu8lXXlstvM
61,"but then from where you are going to get this session maker also uh for session local you can name it anything. if you want to use it as only session that works uh this is just a variable. now to get the session maker you need to import that from sql alchemy and that too from sql alchemy because we are going to use orm here and i'm going to import session maker. okay, this is an inbuilt part. we just have to use it. so you can see session maker is a class and we just have to call the constructor. it will create the object for us. but then this session maker will take some parameters. the first parameter it will take is the for the commit. so whenever you do database transactions normally we have to do the commit for it. but by default it will be auto commit. we have to disable it. so we'll say auto commit equal to false. and you can get these values uh from the session maker. so if you scroll down there should be this variables which you have to assign. okay there's one constructor which is engine we'll be passing that as well and there should be another constructor which will take all these values. so we can see we have something called auto flush and there should be something with the commit. we'll do auto flush as well but i'm looking for the commit. where is it? i should have searched for it. but sometimes trolling works because you come across certain new things. uh but still i will now search for auto commit. yeah.",361,1082,1104,Lu8lXXlstvM
62,"yeah. so this is auto commit and i think by default is false. so that's we can we don't have to set that auto flush is true. so we have to disable this but we'll disable both. okay. so we'll come back here and this is false and we'll also do auto flush as false not true. and then the third part is you have to set the engine. now remember remember when we looked at that there is also engine which you have to set the value for. and we can do that with the help of bind. bind equal to engine. now what is this engine here? now engine here helps you to connect with the database because you have to also specify the which database you're working with because nowhere we have mentioned that. now so for your python application yes you're connecting with the database but which one we have no idea. it can be mysql, postgress, oracle. so you have to mention that also for the particular dbms you have to also set the username and password. so we can do that with the help of engine. so you can say create engine and this is where you have to pass all the values. so in this you have to pass which dbms you're working with, what is the link to connect to it, username and password and what is your database name and we can put that here and the ideal way of doing that is something called a db url. so i will create a db url here and let's create a url which is poss localhost col 5432.",358,1104,1122,Lu8lXXlstvM
63,"so i will create a db url here and let's create a url which is poss localhost col 5432. as a port number and then you have to mention your database. now this particular url is pretty common uh whenever you work with rdbms. if you're working with let's say mysql, you will say mysql colost 5430 is no. so for mysql, the port number changes. i i think i forgot the port number for mysql. i think it's 3306. and if you you're using oracle, you have to put the port number for oracle which starts with one and ends with seven. i forgot the number but you got the point right? uh this will change and this will change but also i want to specify the username and password i know where i'm doing that. so there are multiple ways of doing this because if you click on create engine okay i'm not able to go inside it. okay it's because it's not able to detect it. so i will import that as well from sql alchemy and import create engine. okay. now i should be able to click it. so you can see it is asking you for the url and also you can pass certain other parameters. okay. but i'm not going to pass those things from here. i mean separately the username password. i'm going to pass in the url itself and yes you can do that. so the the basic syntax is you mention posgris sql col slash and then after two slash you mention your username. for postgris the default username is posgress itself and then you give a colon and then you specify the password.",367,1122,1143,Lu8lXXlstvM
64,"for postgris the default username is posgress itself and then you give a colon and then you specify the password. i'm just guessing the password for this particular machine is 1 2 3 4 5 6 7 8 for for posgress. let's see if it is working or not. and then you say add the rate. so username password at the rate localhost colon the port number database. this is the url which you have to work with. and the same url now you just have to pass it here. so db url and your job is done. by doing this you are now able to connect with the database hopefully. hopefully. okay let's keep it as session if that works. so we got session uh session maker and these things. now once you have your database py file ready, it's time to go for the next step. the next step is very simple. go back to main and here instead of database connection, we can create the object for session which we have done that before or maybe i will just write db and i can say session. but now if you see this is not recognizing because it's a part of database. i just have to import this. so come back here and say from database import session we can get the access to it and i think there's no problem. you can see there's no problem here. and now with the help of db you can fire the query as simple as that. okay. once you get the hold of db you can fire the query.",349,1143,1164,Lu8lXXlstvM
65,"once you get the hold of db you can fire the query. so i can say db dot there's option of query in which you have to pass the query. uh but then what exactly the query i'm going to pass here? am i going to write the actual sql query? of course not because we are using sql alchemy. so we i want to use the om feature of that. i just have to work with the python objects and it should create the query for me. so if i want to say save it, it should create the insert query. if i say fetch, it should create a select query. the sql alchemy should do that. but the question arises nowhere we have done the mapping for the database and the class. the class is there. okay. the class is there but this class is pilentic. okay. and when you try to connect with the sql alchemy, it will say okay something is not matching. it's not my syntax. so you have to basically create another product class for the sql alchemy for the database. so the database schema will be created based on that particular class of product and plus you can mention the column types. you can mention what column you're working with column name, column type and if you want to add some filters to it or the special access. let's say if you want to specify a particular column as a primary key. so you can you should be able to do that here. i mean not here because this is a parentic based model. you can't basically add hey this is a primary key.",365,1164,1187,Lu8lXXlstvM
66,"you can't basically add hey this is a primary key. you have to do that in this separate class. and also there's no way here you can change the table name because it's a pyic again. so you need a class of product for the pyic for the data validation and you you need a separate class of product for the sql alchemy. so let's create the class for sql alchemy to use. uh so let's get back here. i don't know why how this got open. let's go get back here and create database models py. now this models is for the pyntic. this is for the database for the sql command. okay. and i'm so lazy. i'll just copy this because this will be same. i mean not exactly same similar. and uh this will change. now this was base model from parentic. instead of using pyntic base model, i want to use base from sql alchemy. now how sql alchemy will know that this product is supposed to be linked with table or database is because of this base. now what is this base? there's not a inbuilt keyword. uh but you have to use something called base equal to declarative base and this thing you have to import. so this function will create the base for you which you can use to inhite your class. but you need to import this. so from sql alchemy dot extension dot from declarative it will import declarative base. okay. certain things you have to remember. so declarative base comes from this and once you get this object you can basically or once you get this class you can pass it here.",366,1187,1213,Lu8lXXlstvM
67,"so declarative base comes from this and once you get this object you can basically or once you get this class you can pass it here. and if you go to base declarative base i'm just checking the documentation. okay coming back here. so once you got this you now this class is ready to be mapped to the tables. but then this will not work. we have to do some changes here. so instead of specifying type we have to actually assign the type to it. so you say id equal to and now we have to specify something called column in which you have to specify all different values. is it a integer or a string or is it a primary key or not or do you want to do indexing or not? so those thing you can do with the help of column and that's why it is different from the model class the bilentic model but you need to import this. so from sql alchemy we'll import we need column. now we'll also be using the type. so let's import that as well which is integer. apart from integer we are going to use two more types which is string and float. now if you remember in the models in fact here itself. so we got string and float and int. so we'll import those things here and this particular class which is column. so here let's pass in the constructor. the first thing you have to pass is it's of type integer. then you have to specify do we have to use primary key. this is a primary key. yes, true. t should be capital.",364,1213,1235,Lu8lXXlstvM
68,"t should be capital. and do you want to do indexing as well for faster processing? i will say yes. on the primary key there should be. so this is how basically you create the primary key just by mentioning primary equal to true. and for the other columns, we don't have to mention everything else. just column and the type for it. this will be of type string. and you have a choice of adding the indentation for this as well or indexing for this as well. so we'll be using equal to column of type string, column of type i think it was float and column of type integer. so these things we need here. and also you can basically create or you can add a table name here. so you can say table name which uh your sql alam may be used for storing. so you can have the same name product or if you want to have something else you can add it here. uh i'll get i will go with the same name product. and i think that's it. this looks cool. now once we have defined our model i'm expecting that this should reflect in my database. okay. uh let's see. first of all, we are not running the front end and back end. both are in the stop state. is it? no, front end is still running. the back end is stopped. i can just run the back end. okay, back end is running. and now i can go back to my database. we are using post posgress and this is pg admin. and if you have not logged in here, you can basically click here.",367,1235,1264,Lu8lXXlstvM
69,"and if you have not logged in here, you can basically click here. it will ask you for your password. just just enter that and then scroll down to your database. so the database we are using is cisco. and make sure you have database because sql alchemy is not going to create database. it is going to use database and create tables. so create that table database by yourself. you can right click here and say create database. simple stuff. once you get this database here, expand and go to schemas. this is where you will find the schemas. and in schemas, scroll down, go to tables. and this is where you will see the tables. but even if i refresh, the tables are not there. we have done the mapping, right? table why tables are not there because nowhere we have mentioning that hey sql alchemy you have to create a table we have to mention that somewhere so where do we do it let's go back to main and let's specify somewhere that hey you are responsible to create the tables and the way you can do that is by using the database models okay do we have to import database models yes so so import database models okay and once you have database models you can use your base now base will say okay i this my job here was to do the inheritance part and i' i've done that. apart from it, you can also get the meta data of that particular class product. now how do you get the metadata? you just use the metadata and based on the metadata.",358,1264,1282,Lu8lXXlstvM
70,"you just use the metadata and based on the metadata. so in this metadata you'll be having a table name, you'll be having the column names, you'll be having a type of it and based on it you will say create all. so whichever class inheriting base just create a table and then you will say binding will be with the engine. there are multiple ways you can do the binding you can do with the help of connections and stuff but we are using engine which we have created here uh in the database. this is the engine but this is giving error because we are not importing engine. so from database we have to also import engine. okay. now this line will create the tables for you. but we are seeing error here. oh, looks like this should be small. i don't know why i've written capital i there. okay, looks good. no problem. now the server is running. but i want to verify that by going to the database. let's refresh the table. and voila, you can see we got a table. in this table, we got the columns. in this column, we got id, name. in fact, the best way to see this is by uh right clicking on the product and say view. you can write the sql query as well but i'm lazy. so i'll say view data all rows and you got your table here. you can say we got id, name, description, price and quantity. and look at the types as well. additionally, this is a primary key. perfect. so table got created but table is empty. so this is a query is executing but the table is empty.",373,1282,1309,Lu8lXXlstvM
71,so this is a query is executing but the table is empty. you know what we have to do now? we have to basically save all this default data in the database so that we can fire the query. so we don't have the data in the database. so it's time to do that. now what i want to do here is whatever data you have here i want to put that in the database. the way we can do that is and i want to do that only once. okay? so every time i open this application it should check if the table is empty populate with dummy data. if you have any data just don't do it. now to achieve that i'm going to create a function which will do that and i will say this is initate db initate db that makes sense and in this let's do those steps now. so the first thing we need here is we need the connection object. so basically we need to get the session which we have done before. in fact we have written that here but we are not using it. so in fact for some time now i will just comment this part and we'll use it later. so once we got the db object let's try to add the product.,291,1309,1324,Lu8lXXlstvM
72,"so once we got the db object let's try to add the product. so for product in products yeah let's try to add that in the db so we say db do add okay and then in this i have to add the object of the product okay so i can simply say product here but there's one little problem okay so see see what we are trying to do is get the product from the list and add it in the database as simple as that right so this is the code for this but the problem is the db is working with the database model class of product because this is connected to the sql alchemy. but in the code, the product which we are using is not sql alchemy product. it is actually the model which we have created for with the help of pythonic here. and this will not work. let me try it out. so let me say init db and let's see if this works. uh okay. so there is a problem here. so you can see it says models.product product is not mapped because this thing is not mapped but this is mapped. so that means you can't pass the object of a product. you have to pass the object of database dot product object. this is what you have to pass. that means we have to find a way to basically convert this product into the product of the mapping which is database models. and how do we do it? it's very simple actually. you take this product which is from the pyic and convert that into the database model.",361,1324,1340,Lu8lXXlstvM
73,"you take this product which is from the pyic and convert that into the database model. okay, so this thing has this feature where it will accept the key value pair from you and then it will create the object for it. so if you can pass that key value pair from the product, your job is done. now in order to do that what you can do is from the product there's a function called model dump. now what this will give you this will give you a dictionary. so if you click here uh does it make yeah so you can see it gives you a dictionary. okay. now once you get this dictionary from the object so product is the object from that you are getting the dictionary of course even dictionary will have a key value pair but here for this product we don't need dictionary we need actual key value pair so how do you convert your dictionary to key value pair is through unpacking and in python we know if you want to do unpacking you use double star so when we say double star that's unpacking model dump will give you the dictionary from the object so let me reiterate this will give you dictionary and this will give you key value pair. and once we do that, there is no issue. perfect. and now the application ran. if i check the tables, if i execute this query, we're still not working. you know why it's not working? because we are adding but we are not committing.",343,1340,1353,Lu8lXXlstvM
74,"because we are adding but we are not committing. because in one of the configuration, we are mentioning auto commit is false and that's why it is not running. so what i will do is i will do the commit and i will do the commit after the for loop. once everything is done i will say db dot commit and our job is done. let's see run and voila. okay so it's that simple. okay so you don't have to write sql queries you just say add. it's just that the only point where we struggle is when you wanted to add the product and you have to make sure that you're adding a product of database models not of parentic model and uh to you have to basically convert the parentic object using model dump which will give you a dictionary and then you have to unpack it using star which it will get and this will just get those values and create the object for you. so a lot of things sql alchemy is doing for you. it's just that it's happening behind the scene and you just have to follow the protocols. okay. so this looks good but now we got an error because again i have saved it. now when you save it two times what happens is it will reload the server and every time you reload it will basically execute this the queries and then you can't have duplicate values right so id is a primary key. so you can't have duplicate values there and that's why you got error here. so it's trying to insert but we have a problem.",361,1353,1367,Lu8lXXlstvM
75,so it's trying to insert but we have a problem. so i don't want to call this every time uh we load this. it should do its job only when the table is empty. so we can basically check before you do that. so let's say i can use a count variable and the count variable is zero by default. and here i can check if the count is zero. in that case you do this steps otherwise don't do it. first i have to fire the query to database and then it should get returned the count. to do that i will just say here db dot query because you have to pass a query now. so when you want to fetch you use query if you want to add you can directly use add method and then here you have to specify for which particular table you are passing a query. in fact you have to specify the model itself and you you can have multiple models in your project right so we'll say database models dot product and then you will say i want the count. so this will fire a query which is select count star from the table which will give you the count and now this should not be an issue. so if i save this code let's see if the server is running so there's no problem. okay and anyway in the normal project we not we don't even do this right because data will be already there or maybe you want to add the data manually.,343,1367,1380,Lu8lXXlstvM
76,okay and anyway in the normal project we not we don't even do this right because data will be already there or maybe you want to add the data manually. i just wanted to show you if you want to have initial data you can do this but with this we have one little problem. the problem is with every function like get all methods or get product by id whenever you want to use database connection you have to basically do this line uh db equal to session and then every time you use this you are creating a session then you're not closing it uh that's not a good idea so what we should be doing is we should be doing that one place where when you need it it will use it and then the connection will be closed and also your fast api provides a feature of dependency injection. so we can use that as well. so now let's see how do you create this connection once or how do you write this code once and then you can use it in all of the methods and basically we can do the injection there dependency injection. okay but uh let's let's see this. so let's create a function and i will say get db as a function name and the session object which you are creating will do that here once of the method accessing this and when they want the database connection they should be able to get it and then they should close the connection.,339,1380,1386,Lu8lXXlstvM
77,"so let's create a function and i will say get db as a function name and the session object which you are creating will do that here once of the method accessing this and when they want the database connection they should be able to get it and then they should close the connection. so of course we have to also close the connection right and the closing of the connection should be done after they have used it. so in that case we have to use something called eield db and then after this you will close the connection. so you will say db do.t close. so basically this is where we are creating it. this is where we are waiting for the others to use it and then we'll close the connection. but it might happen something may go wrong with this. and even if something goes wrong or if it is not going wrong, i want to make sure that we are closing the connection. so if you want to achieve that, we'll have to say try and we'll put the close in the finally block so that it doesn't matter if something goes wrong here or not, it will close the connection just to make sure because resources are very important. so what we are trying to do here is we'll every time someone wants to use the database they will inject this object or this method there. now first of all what is dependency injection is so let's say if there's a method here which is uh get all products. now get all products needs the object of db.",356,1386,1397,Lu8lXXlstvM
78,"now get all products needs the object of db. so basically they want the session object. so instead of creating the object here what we can do is we can tell hey i want this dependency because i can't work without this db. so i want you to give me this dependency. so the fast api will inject this object. okay, that is called dependency injection. but then this get all products needs to ask for it. it will not give you by default. you have to ask for it. now how do you ask for it? so it's simple. you say i want db of type session and you have to make sure that this is not session which we have created here. and this is something we have to import from sql alchemy. so i will say import sql alchemy and okay so from import you need to get session so we want db of type session so you have to say this depends because this is where you are specifying that it depends on something and it depends upon get db. so basically it will inject it here and since we are using dependency injection we have to also import that in the fast api. so i'll say depends here or maybe after fast api anyway it works. so let's go come back and we got it. so we got db session equal to it depends upon this particular method which is get db. so this is how basically you inject dependencies. now once you have db you you don't have to create this you can directly use it. so this line we can avoid now i can directly use it.",369,1397,1417,Lu8lXXlstvM
79,"so this line we can avoid now i can directly use it. so how exactly we are going to use it. now i will create products or maybe i could say products from db. so we db products equal to and we'll use a db connection dot and we have to pass a query which we have done before but this time instead of getting the count i want to get uh all the details but of which type so i will say database models dot product dotall this will give me all the products and this is what we need to return now so db products simple right uh now once we have done this it's time to test it on the server. so go back to the browser and refresh this once. and now i'll say try it out. i want to fetch all the products. and this should give me all the products. but then i know you're not sure is it coming from database or is it just coming from uh the thing here. what i'm going to do is i'm going to make the change in the database. let's see if that works. so to change it, i will say update uh table name is product set. i want to set one thing. let's say i want to change this table to uh laptop table. so set name is equal to laptop table where id is equal to what is id? six. hope this will work. yeah, it it worked. updated. and now if i say select start from product execute and you can see this updated, right? table data. uh table. okay, we what i have updated?",367,1417,1439,Lu8lXXlstvM
80,"okay, we what i have updated? what's wrong with me? uh but okay, i should have said laptop table or something. i said table table. i need to watch the video again. it doesn't matter right something has changed. now let's verify that in the servers. if i execute this once more execute and you can see it stays table. so yeah we are right. so this data is coming from database. so yeah so we have done it. so data is coming from database. things are working out. i'm happy. i hope you're happy about this. and similarly we can do things for get product by id as well. so instead of getting it from the list we have to get it from database. so what i'm going to do is this thing here this will change because now we are not using a for loop because for loop will iterate from the elements. i'll be saying product equal to and this product will have in fact you know everywhere i should mention db product so that we'll know db product equal to db do.query. we have to fire the query and we'll say which for which one. so models dot product now you know dot now things will change here because here we are saying all here we have to say one we have we have to get only one not all but which one to specify that we'll use something called a filter. so in the select query as well we if you want to filter the elements you basically use a wear clause. this is how you do the wear clause.",357,1439,1461,Lu8lXXlstvM
81,"this is how you do the wear clause. you say filter and in this filter you specify for which particular table. so you do that with the help of product. you don't have to mention the table name. you use the model itself. and from the model you will compare the id. so the id should match with the id. so the id from the table should match with the id which you are getting here. and if you find one, get the first one. so of course this will generate only one. but in case if you find multiple things, then you will pick up the first one. okay. uh we have to make one more change because you can see db is giving error here because we have not added dependency. so i can just copy this line because this is what we are going to use everywhere. now we just have to add the dependency. now once you got the product now we don't have to compare that here. we we have done that in the filter. the only thing i want to check is if this product exist okay if this product exist return the product and this should not be outside right this should be in line. if this product exists, return the product. otherwise, you can return product not found. okay. so, even for one, this is working now. uh how do i verify? go back to the swagger and uh since we have made changes, let's refresh this and let's try to get one and we'll try to get this sixth one itself. id6 execute it should return table table. yeah, we got table table. things are working out.",371,1461,1487,Lu8lXXlstvM
82,things are working out. cool. okay. or maybe if you try to fetch let's say nine which is not which does not exist it will return product not found. so finally we are doing operations with database. and now it's time to change the methods like put and delete. in fact we also left with uh post. so we'll start with post first and then we'll move towards other methods. now you help me what to do here. so first of all we need to get this db session. so i can just copy it from here. i mean when i say help uh just think about it and i will i will know your thinking and i will get the answers from you. so here i will just paste it so that i will get the db. now once i get the db here i can use it to add the product. so i will not be using append i will be using add. but the question arise how will i add it? now if you remember when we worked with in db this is where we were adding data in the database right? so you can just copy this and paste it here and let's see do we have to make any changes not here not here.,284,1487,1504,Lu8lXXlstvM
83,so you can just copy this and paste it here and let's see do we have to make any changes not here not here. so basically we got the product here and that's the product i wanted to add but here it's basically pythonic model i want to convert that into a database model and that's the thing we are doing here using model dump and uh unpacking and it will add cool so simple right uh we can verify this and to verify this i will go back to the browser refresh this and first of all let me just try to get all the products to see how many products we have here so execute we got four products 1 2 5 and six let's add a new product and i will be using one of it to make the changes or to create a new product and for creating i have to go here and click on try it out replace with this or maybe i will just add it here so we got five and six let me add seven here we can also make this autogenerated so instead of typing the id seven maybe you can just pick up the last id and use it i mean plus one and and use right. uh name i will say book that's what i wanted to enter here and i will say my daily notes and the price is let's say we got 10 and quantity is let's say we got 100 of quantity of this and click on execute. this should add the product and there's no problem.,353,1504,1507,Lu8lXXlstvM
84,"this should add the product and there's no problem. response body is this uh status code is 200. so that's important. 200 means everything is okay. let me verify this by going to database. this i mean you can verify from the products as well but let me go to database and run and okay oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh something is not working you know what is why things went wrong uh it's because we are actually adding it it's just that we forgot one step commit okay so whenever you make changes you have to say commit otherwise it will not commit so it's a db dot commit and now let's reload code. go back there and execute the same thing again 200 and we can verify from database. run. yep, we got it. so, we got a new entry which is book my data nodes and uh price and quantity. we can also verify this from the get all products. i can just execute this part and this should print my daily notes as well. perfect. right. okay. okay, cool. uh, so we have done with the get, we have done with the post. let me just go with the put. now we can make it quick. now we know a lot of things. so when you want to update a product because update is taking two things the id and the product which is new information and you want to update the details. now see creating it is very easy. you can simply use add because it will create a new record.",367,1507,1529,Lu8lXXlstvM
85,you can simply use add because it will create a new record. updating means you have to fetch the existing record. first of all you have to check do you have existing record of the same id. if you have then you have to pick up the values from this product and add it there multiple steps. so what i can do here is uh first of all i don't need all these things. in fact let me remove all these things and let's make it from scratch. so the first thing i want to do is i want to check if the product exist and you can use this thing to check if the product product exist. so i can just copy this and paste it here because this will give you the product but also i need to uh use this thing. sometime it's important to re uh reuse the code. so once you got the db object here i can use it here. no problem. and once you got this you can check if db product if this exist i want to do certain things. if not i can write this in the else part here. else return no product. okay. but in the if block i want to write certain things. what are the things i want to write? uh first of all we know if it exe it it is coming to if block that means the product exist in the database and if it exists we want to change it.,332,1529,1546,Lu8lXXlstvM
86,"uh first of all we know if it exe it it is coming to if block that means the product exist in the database and if it exists we want to change it. so if this product exist and if you want to change certain things about it what you can do is you can say db product dot and you can change the name of it. let's say i want to change the name to uh a monitor. okay. so this is how you can do it. now you will say just by updating the value will it work? yes, you just have to do one thing which is db.comit. and when you do this whatever changes you have made to this this object it will be committed in the database. okay. but i don't want to set monitor. i want to set the values coming from the product. so name you can do something like this. and we don't even know which values have been changed. so we have to set all the values not just one. uh maybe you want to change the id as well but when you change the id do you want to keep this option of saving changing the id no i don't want to change the id i will just start with the name because changing id might create issues if you have a bigger application and they depend upon your ids so let's not change it so i'll say product dot description then db product dot price and this should be from product dotp price db product dot quantity and this will from product dot quantity okay.",358,1546,1560,Lu8lXXlstvM
87,uh maybe you want to change the id as well but when you change the id do you want to keep this option of saving changing the id no i don't want to change the id i will just start with the name because changing id might create issues if you have a bigger application and they depend upon your ids so let's not change it so i'll say product dot description then db product dot price and this should be from product dotp price db product dot quantity and this will from product dot quantity okay. now once you make all these changes just say commit. this will be saved in the database. if if is not executing you can simply print uh the else block which is return no object found. in fact here you can also return after committing product updated. so now let's see if this things works out. so i'll go back here and first of all let me just fetch uh let me just change uh seven here and i want to change this description here. so i want to change the price as well. so what i can do is i can go to i want to i want to go to put and here i want to change this. so i want to make the changes to seven id uh id number let's say try id number eight and when you try to execute id 8 it just it will say no product found but now i will say seven and i want to change id anyway doesn't matter whatever you mention will not make any impact. i want to change the name.,365,1560,1570,Lu8lXXlstvM
88,"i want to change the name. i will say notebook description i want to set. okay, there's a problem with the camel kissing. okay, so i want to set the description as my daily notes for work and let's say the price is now 9.99 and the quantity is let's say we got 80. so we are making the changes and let's say execute will this work? uh it says product updated but i want to verify that from the database. execute this and yes things have been changed here. okay, cool. uh we can even verify that from the all uh products. so you can see before running the query these are the old details and now if i execute and those are your new details cool even put is working. now let's try with the next one which is delete. so go back here delete the product what we are going to do. so of course i don't want to use the for loop here. for deleting we can directly delete the product but then we need to first fetch the product from the database. so the same thing we'll be using this query because whenever you have an id in the parameter you will be using this uh again we have to get the connection. so i will just use this copy and paste here. okay. now once you got the object first you will check if the db product exist. if this exist uh then we'll do certain changes. else will return product not found. so i will say else and block.",345,1570,1590,Lu8lXXlstvM
89,"so i will say else and block. now in this what we are going to do is we will simply say db dot delete and you can pass the db product and that's it. this will delete it. the only thing you have to do is you have to also say commit at the end otherwise it it will not work. committing is very important in life also when you are learning fast api completed. uh so let's go back now to the page and let's try to delete uh let's say i want to delete seven itself or maybe let's delete something in between. let's say i want to delete this six number. so id6 i will go down down down here. so come back here click on try it out. and now if you say seven click on execute. okay, i wanted to delete six one. i don't know why i deleted seven. uh, it says null because we are not returning anything but the status code is 200. that's great. so, if you go to the pg admin run seven gone. in fact, i would also delete two because we wanted to delete something in between. so, delete and again return null. but we got status code 200. let's verify from database. looks cool. we can even verify that from the all products. now that's a mutual right test it with all products and to gone. okay. so this is how basically you update delete stuff. now i know initially we started with the ui beautiful ui and now we are working with swagger.",347,1590,1614,Lu8lXXlstvM
90,"now i know initially we started with the ui beautiful ui and now we are working with swagger. don't worry once i make this thing working i will try to connect the react with this because there are certain changes which you have to make apart from db database connections. so we have talked about database and things are looking good. so now it's time to go for the next step. it's time to connect our front end and back end. so back end is running and we are testing that using swagger and things are working fine. but now it's time to connect this front end. now the front end is running here. so if i even if i refresh uh it has some issues but what i will do is because we have we have we ran the front end way back in the first video uh of second video. what i will do is i will just rerun the front end to show you the steps again. let's do step by step. right, that will that should make much more sense. okay, the first thing you have to do is open a new terminal here. you can just click here and open the new terminal. uh maybe i will do that again. so i will just click here. it is opening a new terminal and the only thing it is doing it is going into environment. i will i don't want to get into the environment. i will say deactivate.",328,1614,1632,Lu8lXXlstvM
91,i will say deactivate. coming out of the environment and then you have to move to the folder called front end and make sure you have front end here because we have done that in the first video or second video. okay. now once you have a front end you need to install the dependencies. so just to give you a glimpse of this front- end folder uh this is a react application in which uh basically you have this package.json json react is a library that's why we are adding that in a dependency and then if you want to run the scripts to run this server you have to say npm start so those are the things we have to remember and we have also done the proxy for localhost col 8000 because when you send the request here behind the scene this will work with this particular url so we have those things there next you have to make sure that whatever dependencies you have mentioned here are the part of node modules if you are doing this for the first time you will not be having node modules. so i will just say npm install. it will download the dependencies and keep it in the node modules. now once you have done that you can simply say npm start. this will start your front end and as you can see it started the front end. this is your front end. now there are certain issues. the back end is still running. there's nothing wrong with the back end but the front end is not working.,349,1632,1644,Lu8lXXlstvM
92,"there's nothing wrong with the back end but the front end is not working. if you can see it says failed to fetch the products and you don't have any products. let's see what is going wrong there. so i will right click here and click on inspect. let's go to console. and if i zoom the console a bit, it says there is a problem with uh the cause. okay, that's one problem. there are more problems which we are going to talk about now. and to understand those problems, let's go back to the project. now in the react project, if you go to src, now this app.js, this is where you are doing all the routings. okay, request sending response i mean accepting the response. so it is sending a request to this particular base url. okay, and that's what we have for the back end. so if you look at the back end, that's your url. apart from it, the each request url changes, right? so when you send a request for fetching all the products, let's say fetch products, you got this url called products. and if you try to match that with the main, let's try to match it now one by one. so get all products the same url. so no problem. now when you try to go for the next one which is this creating or updating a project in this case you are sending a request for the products. okay. so are we using products for update? no we are using products. so make sure everywhere you say products just for the simplicity i i went for product before but let's make it products everywhere.",369,1644,1667,Lu8lXXlstvM
93,so make sure everywhere you say products just for the simplicity i i went for product before but let's make it products everywhere. even in put we have products just to show you if you in fact yeah put will be using the same thing. but what about delete? so if you scroll down for delete down yeah even delete is using products. so i will say products. so everywhere you have to make sure that you have products. okay that's one of the problem. the second problem we have is which we saw in the console is this cause error. now cause stands for cross origin resource sharing. so if i zoom it a bit. now by default when you have two servers example if you are deploying uh two applications on the same server with the same port or maybe of the same server they are okay to work with okay because every time you request for something they know it's coming from the same server it is safe but what if the request is coming from some other server now in that case there might be hacking issues now how do you solve it and that's where you have to give the permission so by default course will always be blocked and you have to give the permission. so how will you give the permission? so you have to go to the main file again and this is where for the fast api this app you have to give the permission to accept from particular client.,336,1667,1679,Lu8lXXlstvM
94,so you have to go to the main file again and this is where for the fast api this app you have to give the permission to accept from particular client. so by default it will accept from the same file or same port number because if you remember this swag was running on the same port number. so there was no issue but your front end is running on port number 3000. that's where the issue is. so how will you give the permission? so you have to say app dot add middleware in which you have to pass in parameters and multiple parameters. so i will just say enter. so first thing you have to mention which middleware you're working with. so i'm working with course middleware and i think we have to import that. so course middle where will be coming from fast so from fast api dot middleware dot cause from this we have to import post middleware. now once you got course middleware what are the extra parameter you have to add the next thing you have to add is the allow origin so from where you want to allow it and you can pass the list of values not just one and i just want to specify one value at this point so it'sp col localhost col 3000 this is this is from this is where you have to allow it do we have to mention more properties let's try let's try with this and let's see if something goes wrong uh the server got reloaded no problem with that front end we don't have to change anything. let's try with this.,361,1679,1690,Lu8lXXlstvM
95,"let's try with this. and if this works, then we don't have to worry. refresh. yeah, it worked. so, we don't have to worry about the other parameters. and voila, you can see we got products here. okay. so, this is working. let me just try to add a new product and let's see if that works. uh let's say not minus one, two. name i will add laptop description best gaming laptop. price is 1599 and quantities. let's say we got only five quantity. okay, let's see if this works. add operation failed. let's see what happened. it says bad request for the products. so, we have changed it to products. let's see. let's go back to the inspect console. okay. so, for the homepage, there's no issue. i think it's issue with the methods. so, let's add a few more parameters. i'm just guessing that's the issue. so we'll say allow methods because we are changing something. so i will just add all the methods here. so star means all the all the methods. and now let's try click on add. okay. so basically what we have done is when you say allow origin the homepage was working because you are reading things. when you say submit you are changing something to the server and by default even submit does not work. so you have to say allow that as well. so when i say when you say start it will accept all the methods. so that works. let's refresh. no problem there. and if you go back here we got this laptop. let me try to edit uh let's say pen. so we got details of pen here. you can't change the id.",371,1690,1730,Lu8lXXlstvM
96,"you can't change the id. that's a good thing. and let me change the price to 149. and let me change the description to blue and black pen. you know sometime you have this combination of pens where you have single pen where you can change the buttons. uh, i used to love those pens in my childhood. so, submit and it says method not allowed. okay, something went wrong there. let me check the console. okay, so 40 methods not allowed. which method is executing? let me check for update. we do have okay, no here it is products. let me check here. what is the issue? put that's correct. we got products here. got products edit id. is this the variable which is creating the issue? uh edit id from where it is getting the edit id. so when you send this request, this is working. uh blue pen. okay. i think we have made one mistake here. when i was changing products, i have also removed this part. my bad. remember the dynamic data because the id is getting from here, right? i don't know how i messed that part. let's go back here. click on submit again and it worked. so, it says product updated successfully. but, uh, how do i verify? so, yeah. so, it's blue and black pen. perfect. so, edit done. and let me try edit one more time. let's edit this smartphone. a slim smartphone. price, let's say just 699 and let's say 51. update product updated successfully. okay, it works. it works. perfect. now we can also try deleting. so we can delete this part the pen and pen gone. okay. and all the searching part was from the ui side. cool.",378,1730,1778,Lu8lXXlstvM
0,"have you ever wondered when you place an order on amazon.com what happens behind the scene? what you see on the website, amazon website is basically called the ui code when you click on place order button it sends an http request to a back-end and in the back end there is a back-end server running, a web server running which serves those requests. now that web server can be returned in a javascript framework called node.js or if you're using python you can use a framework called flask. nowadays a new web from framework has come up which is called fast api and this video is a beginner video. for fast api in this video we will install fast api and we'll play with it a little bit to see what benefit it has to offer over flask. let's install fast api on your computer. first i'm assuming you have python installed which comes with pip so you will run pip install fast api and it is as simple as that. you also need to install something called uvicon so after fast api is installed you will run pip install uvicon. this is the server that you'll be using to run your fast api server so pip install first api and pip install uvicon. these are the two commands that you run to complete your installation. now i will write my first fla first api server. i have opened pyjam you can use any editor sublime notepad plus plus vs code, any editor that you're comfortable with. and here i'm going to go into zen mode. i will be meditating, all right?",354,0,13,Wr1JjhTt1Xg
1,"i will be meditating, all right? i will install the fast api first by doing this and then i will create a fast api instance so you're just creating an instance of this class you can call it app or anything that you feel comfortable with then you will write your first endpoint. so i will explain what that endpoint means. let's say i'm writing a simple function called hello and that hello endpoint will return me some fixed string. so i'm writing a python function it's an async function okay and i will say return for example welcome so my code is done so you see like only five lines of code and i have written my first endpoint. now to run the server you will use uv con command you will say uecon and then the name of the file the name of this file, by the way, let me show you is main dot pi. okay. so that's the file so i will use ubicon main. that's a file name colon app so you are not using main.pi you're just saying main and then colon app whatever is a variable name here and you will do hyphen hi for reload. i will explain what that reload option is but when you run it it says application startup complete and your server is available at this port. this is the host and port, okay? so we'll go to now browser and try to run this link. i'm using chrome here and when i run this link i will say hello see i get this thing back if so hello is essentially my entry point, okay?",360,13,25,Wr1JjhTt1Xg
2,"i'm using chrome here and when i run this link i will say hello see i get this thing back if so hello is essentially my entry point, okay? when you are working on less amazon placing an order you know you might be having urls like post order so post order is a typical name of the end point which will post an order you know or place an order so these are all called endpoint it could be post order it could be get, order it can be anything it's up to you it's not like fixed list of strings you can define it to be anything so here i'm using hello and hello is returning me welcome i can change my string and say welcome to fast api tutorial and i have to just save. i don't need to stop my server and rerun it again, see? i did not stop and rerun it and it automatically reloaded my new code and that that was the purpose of this option so now when i go back to my browser and refresh it you see you can see welcome to fast api tutorial. so my core changes i'm doing are dynamically getting reflected into my execution now let's make it more interesting. let's have it such that it takes a parameter where i can say okay the world and it will say welcome to first api tutorial how do you do that well here you can supply that parameter so here i'm saying name okay and i'm using this bracket this is more like a python format string kind of syntax where you are supplying name.",361,25,30,Wr1JjhTt1Xg
3,"let's have it such that it takes a parameter where i can say okay the world and it will say welcome to first api tutorial how do you do that well here you can supply that parameter so here i'm saying name okay and i'm using this bracket this is more like a python format string kind of syntax where you are supplying name. here and you can use the same word as a variable name here and now that name that someone is supplying you in a url is available here in this variable and you can just say this i will use python's format string and again, you don't need to reload the server it's automatically loading those changes on its own. now when i refresh it it's all right so let me refresh it so it says see the world you can give any name like say tom and that this string will be passed into name variable and it will come back in the message. here if we are using get method in http rest protocol there are a couple of other methods as well. so let's go over them one by one get is usually used to create data for example you are looking for iphone case on amazon website when you make that query the website is making a get request to get you all the iphone covers there could be another request called post which is used to create data.",321,30,34,Wr1JjhTt1Xg
4,so let's go over them one by one get is usually used to create data for example you are looking for iphone case on amazon website when you make that query the website is making a get request to get you all the iphone covers there could be another request called post which is used to create data. so when you place an order on amazon website let's say you are issuing a post query so this get post etc these are the rest api protocol endpoints the third one is put which will be used to update data so an existing order you want to update it and the fourth one delete obviously if you want to delete any data like deleting an order you will use this endpoint. now there are other endpoints as well but these four are the popular ones. now while working on this tutorial let's say you got hungry and you went to grubhub to order some food and let's say you're looking for indian food and this is showing you all the indian restaurant and it might show you all the indian kind of recipes. so let's say you are a backend engineer working in grubhub and you want to write an endpoint where you can say something like get item so let me just show you here you can maybe say let's say get items and indian and it will return you the indian food items which are available. okay so i'm going to just change this entry point and i will say get items and this is the name of my cuisine and that comes here as a variable.,361,34,39,Wr1JjhTt1Xg
5,"okay so i'm going to just change this entry point and i will say get items and this is the name of my cuisine and that comes here as a variable. this function name can be anything you it doesn't have to be get items. and let's say you know i'm retrieving my item records from a database but i don't have a database here so i will just use a simple dictionary where i'm saying okay indian cuisine means these many recipes okay i love samosa by the way american means these these recipes and so on and here you can just use food items dot get cuisine. so if someone is applying indian cuisine it will return this one so let's let's try this out really quickly. we are very hungry! okay this is hello hello doesn't work i will say get items indian get items indian is not working actually it was reloading i did not wait enough. so when i refresh this i'm getting see indian item samosa and dosa. all right. let's try some italian food ah ravioli pizza. okay how about mexican well. we don't have mexican so then this get will return null but this is not ideal. actually you want to give user a message that in my website the only supported cuisines are indian, american, and italian. now let's talk about the benefit of fast api over flask.",308,39,51,Wr1JjhTt1Xg
6,"now let's talk about the benefit of fast api over flask. so the first benefit is that in flask if you have to do this kind of data validation then you will do something like if cuisine in rf cuisine not in let's say indian so you will have to write a code like this where i have just said food items dot keys which will be indian, american and italian and you will say if cuisine not in this then return this message that only valid supported cuisines are indian you know american and italian now see this is a very simple function you might have a big function with a lot of validation and when you are using framework like flask you have to do all this validation yourself which is not ideal. what if the framework itself gives you some validation. so that's what fast fast api can do it for you it can give this validation for free and i will explain how. so i'm just going to remove this code here and i have imported nm in python and nm is used if you have fixed category of things and i'm going to create a class which will have all three cuisines are specified as inum. so see these are my available cuisines and now here you can just use python's type hint and say available cuisines so you're saying that cuisine has to be from these are these available cuisine now this is a type in python is not a statically typed language.",336,51,56,Wr1JjhTt1Xg
7,so see these are my available cuisines and now here you can just use python's type hint and say available cuisines so you're saying that cuisine has to be from these are these available cuisine now this is a type in python is not a statically typed language. so if you run the code just like that it's not going to do anything but fast api understands now that the user or the backend engineer is expecting cuisine to be one of these three and if you don't supply the cuisine which is you know part of this list let's see what happens. so here once again if i supply indian things are going to just work as fine but when i say mexican it will give me an error that it is not valid. the valid values are these three so see you don't have to write that validation code here and this is benefit number one that fast api offers inbuilt data validation it is a huge benefit it can make your code really compact because you don't have to do all this validation yourself. it can also reduce bugs so you see you see a very nice error here. i quickly wrote a second function. let's say on your grubhub website you have some coupon codes you know and one two three are coupon code ids and these are the relevant amount in percentages of for those coupon codes and you can have another endpoint called get coupon and code and now code you expect it to be only integer so using type in again you are saying it has to be integer.,358,56,62,Wr1JjhTt1Xg
8,"let's say on your grubhub website you have some coupon codes you know and one two three are coupon code ids and these are the relevant amount in percentages of for those coupon codes and you can have another endpoint called get coupon and code and now code you expect it to be only integer so using type in again you are saying it has to be integer. okay and let's see if it is not integer so i go back here. i will say get coupon and when i say 1 you know i get 10 percent when i say 2 i get 20 percent but if i do abc it will again tell me that value is not valid integer value is not a valid integer. it has to be integer so again the data validation in fast api is super awesome. the second big benefit of fast api is the enable documentation so you can do slash docs and it will generate the documentation for you. see i didn't have to write anything this could be very useful for front-end engineer who is using your back-end. bcause that way they will know what you are expecting in your api. for example, in get cuisine you are expecting only indian, american, chinese.",278,62,69,Wr1JjhTt1Xg
9,"for example, in get cuisine you are expecting only indian, american, chinese. so if you click on this try it out button see it is showing you that these are the three varied cuisines and you can also send a test request so if i say indian and if i execute you know see i'm getting very delicious samosa and dosa bag which i can eat and increase my programming productivity you know american execute hot dog, apple pie. so this is giving you a nice test pad to test out your apis you can also use a different kind of documentation so fast api. again, if you do slash docs you get this documentation if you do re doc. you know redoc that's another way of generating documentation. so see here it's some generates like response and response samples as well. so you can just explore it. it's pretty useful this is the official website of fast api and if you click on tutorials they have amazing quality tutorials. i have not seen tutorials of such a great quality. very simple easy to understand on any other technology frameworks. okay so i highly recommend you go through all these tutorials because i just covered the basics of fast api. i compared it with flask but there are so many things you know so many options available. for example- form data. form data is a usual thing when you're doing ui coding in your javascript and you can directly import that form class here in the past api back-end and you can do various things with it.",347,69,82,Wr1JjhTt1Xg
10,"form data is a usual thing when you're doing ui coding in your javascript and you can directly import that form class here in the past api back-end and you can do various things with it. you know you get some ready-made functionality so explore these options and you're going to absolutely love the documentation of this portal. while coding we covered the two big benefits of fast api which was inbuilt data validation and the second was inbuilt documentation support. there are few other benefits as well for example the benefit number three is that the fast api as the name suggests is actually very fast the way it is return it gives you the best performance so your server will run almost at the speed of you know node.js server. so the performance runtime performance is beautiful so that's benefit number three. benefit number four is the code that you're writing is very compact and as a developer, it will take you very less time to write fast api code so it's a compact code. the code development is very fast and there are very few bugs so just to quickly summarize four benefits- inbuilt data validation and build documentation support. the runtime performance is pretty good. it is very fast and the development time is also very less and there are less bugs. i hope you like this video. if you did, please give it a thumbs up and if you have any question post in the comment box below.",328,82,92,Wr1JjhTt1Xg
0,"fast api makes it quicker and easier to develop api's with python. tommy will show you how it works in this course, his first api crash course. first api is a modern, fast and iperformance web framework for building api's with python. in this video, i will show you how you need to get started working with fast api. first api also has a very good and easy to follow documentation, which i can also recommend. and it is also a great framework for building web applications with python. during this tutorial, there are some tips on keynotes that you might want to take down. but i already did this for you. so i made a fast api cheat sheet in which you can download for free using the link in the description below. and if you'd like more tutorials, ideas, don't forget to check out my channel as cody told me, where i teach more on python, and web development in general. the only requirement for this tutorial is that you have the basic knowledge of python, and you have python installed on your computer with a minimum version of 3.6 or higher. have you noticed said let's dive straight into this video. so the first thing we're going to be talking about is how we're going to install fast api. now to install fast api on your computer, your laptop is very easy. because first api is a python framework, we're gonna use the python package manager, which is pip.",327,0,14,tLKKmouUams
1,"because first api is a python framework, we're gonna use the python package manager, which is pip. so you can just open your command prompt if you're on a windows, whether you're on a mac, open your terminal, and we can just type python, i think m pip install bust api. now it's very easy to just do command line you need to install fast api. you can also do dinamo, we like peeping so fast cpi. or if you're on a mac, you can do pip three install for cpi, just the normal installation command lines. so i just like using python, by free, just personally, but you can install it anyways. so i that is running. and as you can see it says requirement already satisfied. now if you walk with python, you know that this means that i already have this particular library installed on my computer. so because i have it installed, it's not going to go ahead and install that again. let me just type cls to clear that up. so what that did was it install fast api. so if you do a fast api, you stored it, you go ahead and download some files or not just only fast api, some other stuff that you're going to use later, like typing or some other libraries, it's going to install that alone. and there's one more thing we need to install to let's say, pip, install you you'll recall. now the reason why we say p p. so you recall this recall is what we're going to use to run our web server.",349,14,29,tLKKmouUams
2,"so you recall this recall is what we're going to use to run our web server. now, if you work with other libraries, like for example, django, you know that django, you don't need to install any external team to run it or your web server. once you just type a command line, if you have django installed is going to run into a web server. but fast api is totally different. because working with fast api is a very light library. so you need like an external library just to run that file on the web server. so this uv con is going to allow us to run our first api project or file on the web server. once i press enter, now, i should also be expecting a requirement already satisfied, because i have this particular library installed. as you can see, it tells me requirement already satisfied, the ledgers closed our proposal. the next thing i want to do now, the first thing before i started this, i already gave it to my folder by came to this directory, and then i created a python file name my api.py. and in my visual studio code, i already open up the file. yeah, my api.py. so this is the file we're going to create in our api's are running everything we're gonna do. what i just want to do is to copy this particular directory for me to my command prompt and cd into that. if i press dir, right now, you can see that i have the file, just to make sure that we're in that particular directory.",352,29,43,tLKKmouUams
3,"if i press dir, right now, you can see that i have the file, just to make sure that we're in that particular directory. and now that i know i'm in that directory, let's just go ahead and create our first api. now the reason i just opt into this directory is just simply because when i want to run these on our locals on our web server later, i need to make sure i am in that particular directory. so i can run the file in that directory. so oh, this is going to make sense later. but let's just go into visual studio. now, first of all, let's just import past api. so we can just do import past api. and if we save this and come into our command line and run that python, my api spy, so if that runs without any error, or anything like that, so as you can see it doesn't give us any early response does just because there is that's the teller, the first step has been installed successfully. so let's just close that. now let's just go ahead and create our first api. now, well, we need to do to create our first api, we'll say from first api, import past api. exactly. so this is just import first api, like an object to work when you use this object, or an instance of this object to create our api's later. so now that we have this is imported, we're gonna set up is equals to fast api.",335,43,57,tLKKmouUams
4,"so now that we have this is imported, we're gonna set up is equals to fast api. we're taking it from this first api, which we imported to our creating like an instance of the first api object, so we can assess this later. now this object, it has plenty attributes like the get, let me do let me go into that now by just some other things that we're going to use to create our api later. so now that we have that created, what we want to do is to create an endpoint. now let me explain what an endpoint is, to an endpoint is one end of a communication channel. so that's why so complicated. but what that just means for an api, an endpoint in a url, so let's say we have a url like on our local hosts, slash, delete user. list if a user so for this particular url, the endpoint is this delete user. so this is like the path in a normal url, we call it the path. let's say we have something like amazon.com. and then we have something that yeah, just delete user or something like create user. so this is we can say this is our endpoint in a normal url. this is the path, we're dealing with api. this is like our endpoint to just have that at the back of your mind on each endpoint, where you're specifying your urls do my like, perform different operations. so my just get a particular information, a particular data store might be too late, for example, this great user, the endpoint is self explanatory is creating a new user.",362,57,71,tLKKmouUams
5,"so my just get a particular information, a particular data store might be too late, for example, this great user, the endpoint is self explanatory is creating a new user. so that means he will have to post some data to that endpoint, so we can create a new user. so again, you're gonna understand this when we get straight into it. and there are different type of endpoint methods, body cones, there, there are lots there are not too much, but they are a lot, but the core ones get not this get, we have posts, we have routes. and then we have delete, let me just explain this quickly. so the get is used to just get an information or return on information just to get or return a data or something like that. and then of course, this is just to create something new, and put it like that with something new, or create like a new object in the database. and therefore put this port is used to update a data or to update something in a particular object. so all these again, you are going to understand when we are putting all these in all this practically, for updating on delete is self explanatory, is just used to delete something. so as i said, these gets used just to get or return data or information post is to create something new or just to add something new input is used to update something that already exists in that particular database, and delete it just to delete like a data from a database or something like that.",352,71,80,tLKKmouUams
6,"so as i said, these gets used just to get or return data or information post is to create something new or just to add something new input is used to update something that already exists in that particular database, and delete it just to delete like a data from a database or something like that. so now that we have all these known, i already understand all these, let's just go straight into creating the route hiv, the real api. now for me to create a new api, i'm going to do at the add sign up dot get. and as you can see, for this api, i'm using git, and according to the explanation i did like a few minutes ago, it shows that i want to return on information or we just want to show an information. so if i want to, like do something else, like posts, put delete, we're also gonna do all that later in this video. but let's start with get laughter you say app dot get a note that this app is getting from the first api instance, which is what we imported, say app dot get. and then we're opening a bracket to the current output slash. so this is our endpoint, our api to the home or right slash, it means our own and our own can be like, what what we show when we just come to our own page, like google.com does the homepage. so whatever our domain is, does just our own page is something like slash, bessie like delete. so that is another like url, it can be google.com slash delete.",357,80,89,tLKKmouUams
7,"so that is another like url, it can be google.com slash delete. but now this is just the homepage just to make sure you get that correctly. but now that we have that we're gonna have a new function, let's name this index, our own or anything wants to name it. and the no ledger's return particular data, series name, ciphers data. so what this is doing now, as we say we are taking from the app, which is also taken from the past api object hours in a get method. and the endpoint you specified is the home page or the home url. and then under that, we're just going to have a new function, we can name this anything we want, what we just do is to return this particular data. now, this is just a dummy data that we just input. but later on, i'm going to show you how you're going to use it as a python dictionary. and yeah, normally when we're returning data to be written in python, your response. this api first api uses json, but later on, so i'm going to show you how to use a python dictionary and automatically converted to json data. but for now, this is just a normal json data is being returned. let's save this particular file. and first run this and really see what we are doing on our web server. that's when uv call comes in. but let's go back to our command prompt. first, to be able to run these, let's just make sure that we're in the directory of this particular file.",351,89,105,tLKKmouUams
8,"first, to be able to run these, let's just make sure that we're in the directory of this particular file. and to make sure we just press dir, and then we see our file does good. now for us to run these we need to press after it is the first one we need to add is the name of the file, which is my api.py. the second one is the name of the variable you use, most of the time, it's always up. and then there's basically two things but ledger says we know that ledger's roll this on the web server, what i'm going to do now i'm going to say yuvika gonna leave a space. and i'm gonna say my api, which is the name of the file, but now we don't write about api.py, we just write only the name of the file, not with the file extension. and then we'll put a colon, i'll say up. and this is coming from right here, which is the variable we give to the object instance of fast api. and then we just put through iphones, save reload. this is the basic command line just to run our fast api project for our server. once we eat, enter, that you can see it says you've gone running a pc application startup complete. and it gives us this particular url. that is the url that says where we should go to to see our project. i'll just come into my browser here, paste our url, hit enter.",335,105,118,tLKKmouUams
9,"i'll just come into my browser here, paste our url, hit enter. and boom, you can see that i have named first data, which is exactly what we returned right here as a response new first data. so this is how you can basically just have a very simple api. now, pass api as is very cool thing that i like most of everybody that works with fastpay. like, we just got the documentation to automatically generate these good looking ui documentation for your api. if we go to slash doc's. and we hit enter, ccs fast api swagger ui. this is just like a basic documentation for all the api's you have on these your like website, or like your api application, whatever you want to call it. but as you can see, it says default, we have from the endpoint of ui, just slash. and the function is index, which is right here, function, give it to the index. and as you can see, using a get method, that's what it shows you it is it gets metal, they just basically generate these, like api for, like this documentation for our api. and we can test our api directly from this place. but we don't need to use postman or any external service to test api, you can just come here, click on try it out. i will say it execute. and i come down here, he just give me the response, which is named first data of whatever is being done here. now, we're also gonna go complex, more in the video and show you how to like manipulate all this data very well.",360,118,133,tLKKmouUams
10,"now, we're also gonna go complex, more in the video and show you how to like manipulate all this data very well. but for now, this is just the response that is being given back to this page. and of course, if we go to the oh, students same response, so to this point we've talked about, first of all, we created our first api, we made sure we installed all our past api, you recall on everything we needed. i explained, endpoint, what endpoints are and get post, put, delete, implemented. and yet again, we created our first api, and also showed you how to use documentation of fast api. so that's basically all about reaching our run our first api. now, let's talk about our endpoint parameter. name parameter is basically used to return a data relating to an input in the part or in the endpoint, basically. so we can do that using either a part or a query. so we have two input parameters, which are path parameter and a query parameter. i'm going to show you i illustrate these right now. but first of all, let's have low year as a new dictionary, our name is students. it just filled out correctly, the day dense. and there's a key with an id of one, unless you have that as another dictionary, lesson name. give you like john. these are the age this c 17. there's a class yet. so this is just a basic, make that small caps. this is just a basic python dictionary, we just have one field india, this is a key, and this is the value.",357,133,151,tLKKmouUams
11,"this is just a basic python dictionary, we just have one field india, this is a key, and this is the value. so what we want to do now is that we want to return the data of this particular student using the student's id. so in the field, by key, let's say slash, we can have a parameter like slash, it's then slash one. so while you have one it means is you return the data with the student the as the ideal for. so before i get into these, let me explain what i mean by a pass parameter. more. so let me just come down here. let's say i have a website like google.com, slash, and they have get student then this is just the basic url to get a student. and it can just basically show me all the data we have in this dictionary. but let's see how to get only a specific study like this to date, which has the id of one, and i can add another endpoint. but now this endpoint will be dynamic is going to be like a parameter that the user inputs, that will be something like slash, then can be one, then when we go to this url is the user that has the idea of wanda is going to be returned to his to his user as the idea of two. so let's cancel this and just go straight into it. so we have this object right here.",326,151,163,tLKKmouUams
12,"so we have this object right here. and then what want to do right here in app dot get place first of all, have a new endpoint, app dot get also why isn't using i get, and this time around, let's say get student. now we have this endpoint name gets to them. and we'll put slash again, i now want to take a parameter dummies want to take an id of what a user should input. the first to do this, we'll use the curly braces, and they will say student underscore id. this is just telling us the previous id huge basically be collected. and now they jeff, this creates in the normal function of cassie gay student. now, remember right here, it was just blank in euros not in india, but now we're collecting students id underscore id. and this should be equivalent to whatever you put right in here. that does what he just being collected. and we have to specify the data type of this, which is our integer so the id should be our integer now, or we can just do is to return students. we're returning students, which is from year is in the student id. so let me explain this. one more time, what this is doing now is that we have these adult guests slash guest today, slash why now why isn't it curly braces, that's fine like a variable mpg like variable a dynamic variable. so whatever these are input here is what is going to be used to get the student.",340,163,177,tLKKmouUams
13,"so whatever these are input here is what is going to be used to get the student. so d f, get student there now, initializing the student id thing we're giving is like a data type of int, that means it must not be a string or a boolean value, it has to be an integer. and then we just say return students who are turning the student with the students id. but what i mean by this, you know, in a normal python dictionary, if we just want to return these students, because it has a key of one because the students want. and this is basically just going to return the value of this one. but since one is to be dynamic, i want you to do what the user inputs, we change this one to the students id damage whatever the user inputs, the key of that is what should be, and not, let's say user input like five, and we know of that here, just gonna give you like the channel found or something like that. so let's save this. first of all, let's just go test it out. if we come back here, and let's refresh this page. now you can see that we have a large guest student. and instead he knows the names are roscoe id that we call me utilizes students id is required. that means we can just test this out, i say execute the we try to try this and execute t, it says that we need to input something here. now we put one, i say execute. we see now that gives us a response of named john 17.",364,177,190,tLKKmouUams
14,"we see now that gives us a response of named john 17. on year two, that is basically this one right here. so if we come here now, and then we try to put something that is not the like three, i'll say execute. you see the isis internal survey rodas because there is no detail like the, of course, let's just go to the url direct directory directly, we say get student finish student then slash one. you see now that we have john age, we just have basically dat data proceed to internal server error. let's go back to the docs, the does what's the path parameter is basically all about what we can make it more we can add more details to it. or we can say let's say we can make it compulsory. so let me come back here. let's say we want to add more details to this student id like, you know, when we are serving this api to a user, the user manual really good like you really name this today's idea, we just say student ism, i know understand what this is like, do we need to pass the student name, or the student's age or the class or the id. so if we don't have a surface financial name, we can use like, something we need to import. and then that is going to allow us to have a description of it that the user can know what to input. but first of all, let's come up here and input path.",338,190,201,tLKKmouUams
15,"but first of all, let's come up here and input path. to say that we don't need to have an dialogue, we can just press comma inputs path, because it's from past api inputs in it. so now that we have parts imported, we can just say student id, below int equals two, then we can say path. first of all, we'll say none. now this non is so there, if the student, if the person doesn't input anything is just going to leave it blank or is not going to once iran is just gonna like bring out an empty data. so just to like catch the error. remember, when we did it back year, when we tried to get without inputting something, he just told us that we need to input something. that's just what this is doing. and now we can add a description. now in this description, we can do something like we want the id of the student. you want to view like that. so now this is just the description we are giving it. and once we have this, if we save it on then recall to our code right here. and let's just try to refresh the page to refresh dogs. and then we say get student. no, because he writes in distribution. he says the idea of this today you want to say just more, giving us more description or more details of what we really want to collect.",321,201,217,tLKKmouUams
16,"he says the idea of this today you want to say just more, giving us more description or more details of what we really want to collect. what you also more attributes is not just only does it come back to our code, we have more attributes in this path, we have attributes like less than two, let me just quickly come down here again and experience it. but we have let greater than gt gt, this just means greater than, is self explanatory, we have less than, we have d, which is greater than or equals to also have ld, which is less than or equal to. so this is just telling us since we are collecting an integer, we can specify whether the integer want to collect must not be greater than 50, or must not be less than one or zero or something like that. let's say we don't want our integer to be greater than zero. but we want it to be greater than zero, so it must be greater than zero. let's save now, we come back here, it refresh. now, let's try it out right here. let's say we try to use zero and say execute the gives us this error say, ensure the value is greater than zero. but that's because we specified in yellow, we don't want it to be less than zero, it must be greater than zero. and we can also add to, let's say we want you to be greater than zero, but less than, less than, like three. that means the number should be from one to two.",351,217,228,tLKKmouUams
17,"that means the number should be from one to two. so if we go against any of these rules, for me again, in the refresh button, and right here, we tried out the let's say we do three presses acute trauma give us an error, we say ensure the value is less than three, when i we do two is gonna give us internal server error does because that data is not found there is no data with the key of two with do one, you know, we have that data right here. right here, we just go here, the key of one to come back here. now you see that gives us a data that does our we can just add more values. or we can yeah, we can add more values and details to this particular api. now let's talk about query parameters. the query is used to pass a value into a url that is quite similar to the path parameter. let me quickly explain that again. now in a url, like i say, google.com, slash, and we have results. now when we have these question mark, and then we have variable name equals to something like python. now in this url, the query parameter is this search equals to python. so this is just basically giving us an like a key and a value. so this is the name of the variable as we pass undefined is very, very, very similar to the path parameter, right here. and the path parameter, you know, we have a name of student id, or we have a value of whatever the user inputs, that's just our which is zero.",364,228,241,tLKKmouUams
18,"and the path parameter, you know, we have a name of student id, or we have a value of whatever the user inputs, that's just our which is zero. so we can have like a url, and then instead of having slash or something to, you know, be on that same endpoint, for right now, is gonna just have a query attached to that end point. to having this said, let's just go straight into using these practically, what i want to do now is i want to create a new person a new endpoint, i'll say app dot get also. and then let's say slash get, i think by the want to get the student data by the name, this right here, we're getting by the students id for the difference between the query parameter and the path parameter is that in the patch parameter, we need to add whatever the parameter wants to collect in the url in the endpoint right here. when it query parameter, we don't need to do that. or we just need to do so coming to our function. this fire function, i say get student. and then we'll just say, name is a string. so right here is very similar. you see it in the path with the student id is equals to an integer. but we already defined our students id in the url. but here we didn't define any team but we are now coming into the function and defining it. so once you see something like is it ready, takes it as a query parameter. and then once we have that i'm just going to these our particular function.",362,241,254,tLKKmouUams
19,"and then once we have that i'm just going to these our particular function. and now, once a user comes to this url and do something like this, i'll say name equals to like john, we want to get the data there as the name of john, to this as duties now, and i use a for loop and say for a student, id, student. if students were brackets, student id, name. posts are the name which was passed. and what we just want to do is return to dance. that particular students id, none of this is just return. data. thought. what this is doing is, first of all, is looping through the values we have in this student wise in the key of students id. in this student, we say if the students have this student id, the name of it is because with name which has been passed, let me explain his will to we are looping through this student's id using while looping through this students dictionary using the students id. and this jeanette id is this. so we can name this anything right here, we can just say id anything we don't want to name. but if i say id pattern automatically see that as a keyword. so i'm just going to put a student's id so it's original. so now we are looping through it using these, i will say if the student with this student id, the name of it is equals to the name which is being passed, they want to return that particular student. if not, you just say data not found.",352,254,269,tLKKmouUams
20,"if not, you just say data not found. now to make a query parameter, let's just go and test it out. let's come back here. now issue. good. so now we have guests by name, or we see yet we are name required. we can just you know, we say john, once we execute that should work. so as you can see now is named john, because it gets that is really a value by data with the new module where we try something like let's say, a team. we execute, gonna say data loss, because we don't have any data rights in year with the name of team. so that's basically how to just create a very simple query parameter and fast api. now to make these our query parameter option now, we can do the so is very simple. let's say we just want to make it optional. like we don't want to require we come here, you see that it says it is required right here. let's make this option now. we can just say name, right here in this string kathy equals to not equals to no is no more required, we can go year. so dogs always say get by name, katsina, it doesn't say da required again. that means we can try that's gonna give us an error or something. they just says data not found. but now is no more require. now does one way to do it. but the best practice is to use the default option now method that past api is for us. for us to use these, what we just need to do.",356,269,290,tLKKmouUams
21,"for us to use these, what we just need to do. first of all, we'll come right here is very easy, we just say optional. and then equals known. so this is just going to make it optional for us. but before we can use this, we need to make sure we put that option now, they from typing import option. that is just to make it the best practice. because these are facts epi recommends that if we need documentation, also do it the normal way or just the other way, not the normal way. this is the normal way. so we can also do it that way to keep things simple. we just want to you have a clean code and a more readable code as the main points to make it more readable. we can also use this so right no, this is gonna be like, not required is going to be optional. so now, let's say we want to add another query parameter into eloquent to have to query parameters. how do we do this? i'm going to show you something. yeah, let's just have a comma, i will say test it. so we just have a query parameter of json is an integer. so if i try to run this, we should get an error. and i'm going to show you why we have that error right? now. let me call me not here, a browser. so okay, you can see that it's not even going if i come here. now you see dices syntax error, non default argument, follows default argument.",347,290,311,tLKKmouUams
22,"now you see dices syntax error, non default argument, follows default argument. so what this is just saying is, we have, as you can see, right here, we have named optional. and then we have test, which is optional. so python doesn't allow us to have like, an optional argument before, a required argument. so fast the back covers is about python doesn't just allow us because python is a language writing. so we also have to stick with python. if we come down year, one thing we can do is to just switch to places like changes, or bring it here or something, we can also do that. but the best way to do this, so we can have a limited query parameters and have them anywhere we want. it's very easy, what we can use to say asterik sa, coma. so once we have these, we can now write it if we have a default parameter or optional, or whatever required is going to work since we have this now. gonna be able to take care of anything that comes after it. never call me on it, refresh. it that should work. well. let's see, let's have that a reloading? yeah, there we come, you know, she's ready reloading. and that should work. now. let's give it one more trial. okay, that's taking a while to reload we shouldn't. but that's just basically how to solve that particular error, or when you're having an optional parameter before a required query parameter. so i hope you understand that concept. now let's talk about combining path and query parameters together.",351,311,333,tLKKmouUams
23,"now let's talk about combining path and query parameters together. now, sometimes you might want to do this, well, most of the time, you might not. let's say you want to get two different values from a user, like let's say his id on his name, or the class or whatever, you might want to combine a query parameter alongside with a path parameter. first to do these, you know, right here, we already have weary parameter of name and test, the length of the path parameter, let's also call it the student id, the slash k, students underscore id. and then since we are defining initializers, to net id in the path, and we also come here, and then say, dent underscore id, and then we give it int, j comma. so since we have eta on here, that is a path parameter, which only in the function that is a query parameter. so that's just the basic difference you just need to take note of. so we are between these two places does that. so now we can just easily test this out? actually, we even when we collect the students, id one organizer for anything? yeah, because everything was doing is basically getting it by the name. but that's just how you can combine a query parameter with us come back here. now. we hit enter. so if we say get, no, this is not what we need. we want to get by name. we say get no, you see that we are name test. we should also have students id to strike out as the key. if we come back here. did i okay, we didn't save our file.",367,333,352,tLKKmouUams
24,"did i okay, we didn't save our file. let's make sure we save our file back ducks it and now we have student id which is also required. that just the basic concept of like combining parameter on a query parameter together. this part we're going to talk about request body and the post method. so the request what it is, is like the information or data pass when creating a new object using the post method. now to do this, first of all, let's import something right here for the base. so say from by downtick import base model, just to be more elaborate. the request body is like the data pass when you want to create a new object or a new data of t. so, i'm going to explain this later, like, you're gonna see what we're talking about, you don't need to like know the definition or whatever that is, just to know how to use it in a practical case. so now that we have this base, moodle importer, we're going to create a new class. and this class, we just want it to be similar to the student dictionary, and let's name the class student going to take from the base moodle let's have a name should be a string is of an age and use that as an integer. and then they have a class. now i would have loved to have a class boy, cassie is seen it as a key word. so let's change out to like year, the year of the student and that should be string also, they just call me mr. of class here.",358,352,366,tLKKmouUams
25,"of class here. so we can use it, you know that we have these, what we can just do want to create a new path now. but this time around, we're gonna use a post method. first to do this just similar to say arts app dot post proposed. and now we will say create students now wants to create a new object of students are going to take a path parameter, or students id, i know what we do, when we take a path parameter, we have to define that in the function. so create, underscore, the desk. router, define our student underscore id as an integer. and what the next you want to specify is student. the students know what it is, is basically the details that the that the user submits. so in order to create a new students object, for using this particular class, to know the details, let's say the details, the name that the user saw me the age and the year is what is going to be saved in this student. so want it to be because to this student moodle, you can see that these with a small letter and capital letter. so that's what's really different. so this like key value, but now that we have that just go straight to say, if the students id that was passed. in students, what we just want to do was say, return like an error, say student exists.",318,366,379,tLKKmouUams
26,"in students, what we just want to do was say, return like an error, say student exists. so this means that if the student id in is already in students, and the student id right here, then what we just do is to say return an error, does history already exist, damage the concrete, the same student with a student with the same id twice. what he does not the case, justice students. within the student id equals to student i tell you that the student is whatever the user broad or submitted using this post metal, but now that we have these, we've created a new object of a student. and then we can just basically return students of that student id. this is just the code we need for this. so before we test this code, or let me explain what we did in this code, again, what we're doing in the score above register, we are taking a path parameter of an id. so this is per parameter alongside with all the details of the student, and they were using that to create a new student object. so please just go ahead and test this code to save the permian. ducks. now you can see that we have posts not get any more great students, our students id is required. and they can see it says request body also required. that's what i talked about. talked about the request body is basically the details as the name phd year it needs orders, let's try it out. one of our students have to, though, we'll say, just say, tim, give tim age of 12. see nine.",361,379,394,tLKKmouUams
27,"see nine. now, if i execute this, see now that it just gave me the response of what i just created. but now we have a student id of two, if i call to get by student's id, so let's just get the students using the id, we should love that now safe. so let's say to say execute. now we can see that we have this team, with dummies, it has been saved right here. but one thing about this is that it's saving it in like a memory. so if we refresh this page, that is gone. but you know, we are working with real world applications, you want to save like your database or something like that. so you can see that we created a new object of that now. and then we just use the get to get that url, if we want to read what already exists like true, and want to create it again, let's execute, we say says error students already exist, we can create with the same id twice, does what's good about that? let's go back to our code. so this is basically how we can just use the post method and the request body to create a new object in your new object in our data in our database, basically. let's talk about the ports method. as i explained earlier, in this tutorial, the put method is used to update something that already exists. so to do these, first of all, let's have a new endpoint policy up those goods. this time around, we'll say patch update. affairs today student and they want to collect the id of the student.",364,394,410,tLKKmouUams
28,"affairs today student and they want to collect the id of the student. so we know the student in which we are updating. now let's have a function saying date. student we also have to defend us today and id as an integer. now students just similar to what we did here, we know our collecting. vantage goes to this student. well, now, i want to explain something. so right here you can see that it says to that is because of studio. but when we are posting these posting a particular data to this url, i will say this student is equals to student, everything has to be essentially all the field in the composer. so you've become here now. and we give a name field and we don't give an age field not going to work if we are using the students module for the update. so we need to create a new model, but this time around to make all of these optional. so the reason for these is so that because when a user want to update a value, you only want to update the name and not age only wants to update the year and not the name of a new our body we are using this student class to for the update method. also. the scenario as the user as to input all the whole data now means he has to change audio data every time he wants to update something which is not a good practice. let's just quiet and create. plus our name is update. student and does also take from the base model. same thing name. now we'll say optional.",362,410,429,tLKKmouUams
29,"now we'll say optional. trained person just to make it optional. to do the same thing for the age. optional, trained integer person and then exactly simply for the class right here we are using year optional and this is a string equals no now we can use this of this students model as a reference here. so, these notes tonight again these objects today now with these each of it is optional see we use i gives only one value the other values can still remain the same does very good. and now, what we just want to do to say if student id not in student will say return an error of like these particular student does not exist or something the world this is okay let me quickly write that though that under students does not exist, what this is doing is first of all it checks if the student id is not a student the damage if the the id which was being passed to what we're doing is if we want to update this today we're going to pass the id of the student then whatever we want to update. so, it will first check if the id is present in students will not present it to you the student does not exist today. you also update does not exist at least you have to have a student before we can update that data. we have to check that first.",318,429,437,tLKKmouUams
30,"we have to check that first. but if it exists, we can just do something like students didn't id equals to student and then right now we can just return student this is exactly the same thing we did right here dance id exactly what i'm going to show you where the arrow is in this particular is not an arrow but where the touch is if we come near now on test days see the refresh button now you can see is the put we have the put method and no let's first of all let's create a new student so straight out this are creating with to let's say team 12 nine now we execute this we have a new objects or new data or the counterpart want to update this student. so let's try it out. and what's up jd students with the id of two witches team when i want to change the name to tom not team we don't need the age or the dummies this option also we don't want that he doesn't know what to do. if we execute this now you will see what will happen. so now it says named tom age no yeah no because we didn't specify these he says no. they were all the way to get your student by id and i just want to get this student into an execute what is going to happen now you can see he doesn't even show us the age again. he doesn't show that he's 12 years old. the is in year nine.",343,437,445,tLKKmouUams
31,"the is in year nine. so it has been overwritten so it because we didn't provide this age on year when we are updating it also via ride rides as no damage has no value. and this arrow is coming because we just did students with us today id is equals to student needs to know is whatever the user passes. so since we didn't pass anything, it just updated our student that already has a value of this it has nothing which is but the first take care of these we are going to have to do it in like a manual way to come up here to say if a student let's name is not posting non numbers if there is something in what the user past then doesn't want to update on say to dance. dance id does the name will be updated to the student. this one does name is exactly the kind of code wants to use i'm also going to do the same thing for the age and the class or the year, the eve student that age is not equals to none. see, the dance with a student id, the particular student does age equals to the dent. but ah, now, last time, we're gonna have to do it for the year, that's here, now equals to none, i mean, you're giving us an empty value, then that's the only time when we now want to update sort of data particular value, but a particular key, so you don't give us something, we are not going to update it.",345,445,452,tLKKmouUams
32,"but ah, now, last time, we're gonna have to do it for the year, that's here, now equals to none, i mean, you're giving us an empty value, then that's the only time when we now want to update sort of data particular value, but a particular key, so you don't give us something, we are not going to update it. when we give us something, we're going to update it, then to a year. this is just the code to take care of that error. so now we save this. now, this is very good, because we've taken care of that particular error. and now let's just go ahead and come back into your and then we eat fresh, this. great, but what wants to do first of all, let's create a new dent to him. both yes or no. let's execute. and now we have that, let's come to our outputs and update that team. for now we want him to be 15 years old to let's leave this name as blank. well, now the age wanted to be 15. so you can see that now we didn't pass the name or the year. but if it executes, only so let's see where the error is coming from this expecting proper name equals interval codes. okay, so it wants this to be in double cordial because it's json data. let's do this or execute. okay, especially improperly named interval codes. okay, this gives us an error enclosed in double quotes. let's see.",330,452,470,tLKKmouUams
33,"let's see. now let me tell you where the arrow is from if you don't have anything after it, like you do have another value right here something, just remove this comma, we don't need this actually, we should not need that because an integer to make sure you don't have a comma at the end if there's not enough to read. so now let's hit execute. and if we come here now you see there are only 15 was updated and alighted at 15. but only 15 was updated. now we have team a nice to intact. so this is what we want, we can also do it again. and now we can just update our name. and we wanted our name. now be tom. the execute procedure, we now have tom age of 15. year over year nine, this is exactly what we want. if we come back, you know and try to get to didn't try it out. let's try to do it, you can see that we have the named song working perfectly the age on the year. so this is exactly what we want to get. this is our main go. i'll also come down here to get by name. let's test that out. so let's try it out. we have to we have the name of tom. and as i said this can be anything because we're not using it. if we execute this now it gives us an internal server error does because of course that doesn't. it doesn't. now there are so let's see slash to the name of song on test 32.",352,470,493,tLKKmouUams
34,"now there are so let's see slash to the name of song on test 32. so the reason why is because we definitely changed some values right here are some code right here in where we're start to get to take care of the liter. but now that's basically how we can just update some data in our in our database if we are using a real data. it's not like a memory or like i will say a cookie because the cookies in the browser or is it like a memory once we refresh or like a session once we refresh that is gone. but now, this is how we can just simply do this on i hope you understood why we went about doing these. so if you read this code will you see our saying if this, whatever the value is is not known, that means it is not empty, then that means there's something there that doesn't want to update it. so i hope you understood what exactly what we did in this bar. and this next part, we are going to look into the delete method. the delete method is self explanatory, just delete a data object from our database. what we can just do know is to add a new endpoint, we can say add up dot delete, and then slash delete shouldn't blush let's take the idea of the student want to delete as a path parameter, and then define our function. elite student new student this guy id wants you to be an integer as well we're collecting and now we'll check if the student id is not in student.",361,493,503,tLKKmouUams
35,"elite student new student this guy id wants you to be an integer as well we're collecting and now we'll check if the student id is not in student. so if he's not a student, that means it doesn't exist and we just return an error that what you're trying to delete is not even in our database, it needs to be in the database for you to be able to delete it to arrow judgment does not exist then you have to create us to know first before you can try to delete but it does not decay, go ahead with our code and delete that that's two different questionnaires id from the students and they just return back a message thing that didn't deleted successfully now this is a basic way to just be able to delete data or value from a database first come here and test this out. now if we play around with the pose first of all we have to create a new object before we can do that. so let's see do this a for jerry 20 years let's see year 13 now let's execute this case study exit so as you can see i press that twice so that's why i say students exists but i come here to my get without really created it when i press it the first time when i mistakenly press it again so let's say to execute we can see we have jerry 20 years return. if i come down now to delete you can see we have this delete function.",344,503,507,tLKKmouUams
36,if i come down now to delete you can see we have this delete function. now that we have this delete function is is right out now by just typing to an eating execute j student deleted successfully. now we might not know if they are related it let's just go ahead and press check to see if we execute now. he says internal server error does because the student doesn't exist again. so you also come in now and try to execute again. he says student does no it is i'm a student i'm trying to delete does not exist. so this is how we can use the delete method. the fast api geysers gonna be all for this video. thank you so much for watching to this point. in this video we talked about fast api's and all the basics and fundamentals you need to start working with fast api. thank you once again for watching this video and i'll see you in the next one.,218,507,517,tLKKmouUams
0,"hey everyone, welcome back to ml works. in this video, we'll learn how to scale machine learning model with fast api, docker, and kubernetes. in the previous video, we built or trained a machine learning model and served it using fast api endpoint. in this video, we'll take that forward and containerized that fast api application and deployed in kubernetes cluster. to complete this task, you have to uh install docker in your machine and you also have to install mini cube. so i'll be sharing the video for mini cube. i'll be putting that in the description and you can check that out. and to complete this task, first you have to create a docker file and then you have to build a docker image from the docker file and save that to the remote registry. and once that is done, you can spin up your mini cube cluster and then you can deploy your deployment service yamel and run your pods and services in kubernetes. so uh we'll come back to this diagram when we are talking about kubernetes cluster. first what we'll do is we'll just overview the code that we have written for model and fast a application. so for model uh first we trained a model with the help of logistic regression and we trained it on iris data set and we created a model like this iris_model.picle file. once that is done then we move on to main. py where we have our fast api application.",327,0,13,6WMXI0izClk
1,"py where we have our fast api application. so first we import all the necessary packages and then we create a class for uh validating the input request that is coming in with the help of pyantic and then we do uh create a lifespan method which will act as a startup and shutdown for the fast api. and when you start up uh the fast api app you want to make sure you load the model. loading uh logic is written here. and once we have done with the lifespan function, we go ahead and create the app where we map our lifespan function. and once you create this instance, this will be executed. this function will be executed and your model will be loaded. so once that is done, then we create our api endpoint predictor. so what will happen is once you send a request it will contain four features and that will be validated with high-risk features class and if it is correct then we'll go ahead and predict it. so for prediction also we need to format it into array or list of list. so we do that here and then we go ahead and predict and get the response. so if you want to uh deep dive into these two files, you can go and watch the previous video and we'll now go ahead and deploy the uh application in docker and then move on to the cluster. now let's talk about docker file which is present here. to create this docker file, first we load the base image where we have python 3.11 installed.",352,13,26,6WMXI0izClk
2,"to create this docker file, first we load the base image where we have python 3.11 installed. so from this image we'll be setting the working directory inside that image which is working directory app. inside this directory we'll have our application code. so if you see here once we set the working directory inside the docker image we copy the contents of our application. for instance we pass the requirements.txt txt and to the current directory position which is ash app and then we install the packages inside the docker image and then what we do is we move our model file and then we move our fast api application to the current directory which is the app. so we expose 8,000 port inside the docker image and then we run the uvcon server which is the final command when you're trying to run the docker image. this will be the final command that will be executed. so here you will have uicon server and this is the file name and this is the app instance of the fast api and this is the host information and this is the port number on which the app is running. so this is with respect to the docker file. now what we'll do is we'll create our docker image. so i'll go to the terminal. i'll clear this up. now if you see here this is the folder where my docker file is present. so i'll go and build docker build tag command. this is fast api fast api ml. and then what we do is we load it from the current directory the docker file. so this will build my docker image.",366,26,42,6WMXI0izClk
3,"so this will build my docker image. and once this is built we'll push this into remote repository. so here this is my docker file not docker file docker image. now what i'll do is i'll go here in the next terminal i'll do docker login. so here it will since this is already stored the authentication information is already stored. it directly succeeds in logging in. if you are doing it for the first time you have to do docker login. give the username for your dockerhub and the password for your docker hub and then it will uh login into your dockerhub uh instance in the back end. so now it's already logged in. so what i'll do is first i'll take this image which is fast api ml. i'll tag that into my remote repository. so, docker tag fast api - ml which is my latest docker image and the source or the destination is this one may 28 this is the username if you see here this is the username this is where i want to push my docker image so fast api - ml colon 0.1 version so this is done now i have to just push this image. so this will push my docker image which is fast api ml 0.1v into my docker hub repository. you can go ahead and check it here. i'll go into new window docker hub. i'll just log in here. so it is mounting the image. you will see it here. uh let me refresh this one. so digest. yep. so what was the name? fast api ml. let me refresh again.",356,42,65,6WMXI0izClk
4,"let me refresh again. so fast api ml is present which was less than 1 minute ago was pushed. and this is the tag that i used. so my image is present inside the docker hub repository. now i can use that docker image inside my kubernetes deployment yl. so let's move towards deployment yl now. so but before we do that let me start the mini cube server. so clear this mini cube uh start. so this is this will spin up my kubernetes cluster in the back end. so while this is happening let's go to deployment yl. so this is our deployment yl. so there are only two things to remember here. so one is deployment, the other is service. so let's understand with the help of this diagram which is present here. so the way uh it communicates. so finally the user will be outside. okay. he won't have access to the pod or the service that is running inside the cluster. so what we have to do is we have to provide access. so to do that what we have to do is first we'll create deployment yl which will contain all our uh fast api application. so currently we have one container which is one docker image which is one pod. now if you want to handle multiple requests simultaneously or if you have high traffic then you have to run multiple instance of the application. so what we do is we create replicas. so we create three replicas and then that is deployed as a deployment inside kubernetes.",344,65,88,6WMXI0izClk
5,"so we create three replicas and then that is deployed as a deployment inside kubernetes. now that will be running on port number 8,000 which is the port which was suggested or used by fast api for running the application. now that will be exposed as part of your container and then what we do is we connect that deployment to the service and that service will be connected to the user or will be accessible by the user. so the user will hit service. service will be providing access to the parts. so when you have multiple requests coming in. so service has the ability to send the traffic or load balance the traffic between the parts such that one of the pod is not overloaded with the high number of request. so that's the flow that is happening here. so we'll see it here in the deployment yl. in deployment you will see we have first our this is the deployment name. this is the deployment name and this is the app that app name that we are giving which is past api ml api predictor. now this label this is coming under label and label will be used to map your uh service to the deployment. so if you see here uh when you are going to the service you will see there is a selector. so you will map this fast api ml api predictor which is mapped to this one this label.",321,88,101,6WMXI0izClk
6,"so you will map this fast api ml api predictor which is mapped to this one this label. so these two are connected with the help of this label and under specs the key point is we have our replicas which is three and so there will be three instance of your container running and we specify the container information here and then we have our resource details here. so what is the image that we created? let me go back here. so this is fast api - ml. so i'll go here. fast api. so let me remove this fast api - ml. and then this is 0.1 v i guess. yes. so this is where we mentioned all the details related to the container and what is the resource that is required for running that specific image and how we are going to expose that container which is 8,000. so if you go back here right the deployment it will be exposed on 8,000. so each of the parts are exposed to 8,000 port and then that 8,000 is connected to the service. so if you go back to service now this is one of the resources in kubernetes and this is used for networking. the key purpose is to ensure there is uh persistent ip address because what happens is sometimes due to some issues okay due to some issues random issues like resource issues okay there's a chance that one of the pod might fail and the ip address and it restarts and it creates a new pod. so when a new part is created a new ip address is assigned.",357,101,116,6WMXI0izClk
7,"so when a new part is created a new ip address is assigned. so there is uh too much of dynamic changes in terms of ip address. so we create a service which will have a persistent ip address. with the help of that you'll be communicating towards the to the ports. now coming back. so here we connect to the deployment and then we expose our service with the help of port 80 and the protocol we are using is tcp. and here the type is node port. so since i'm running my local machine, i'm sending the node port. i'm using node port because i want to expose the app that is running inside the cluster to the node that is my local machine. but if you are uh if you don't want to expose your uh service outside, you can put cluster ip. so that will be only accessible inside the uh cluster. then if you are using something called as like cloud service provider like gcp or aws then you can use load balancer. now coming back to uh this replica we have set it as three which is primarily for handling multiple requests simultaneously. but that is in production setting normally what happens is we don't set it as some static value like three. what we do is we create something called as horizontal pod autoscaler where we keep minmax replicas. so when the traffic is low, we just want one replica to run. as the traffic increases and the cpu utilization increases beyond like 50 , you can start increasing the replica to five. so this is how you will slowly scale and scale down your application.",367,116,133,6WMXI0izClk
8,so this is how you will slowly scale and scale down your application. so this is with respect to scaling and high availability for your application. now what we'll do is since we have mapped our docker image. now what we'll do is we'll run this uh yl file. so i'll go here to the terminal and if i just do cubectl. so so if you see right cube uh mini cube start is done. it is up and running now. now we can do get parts to see what are the parts that are running. so nothing is running already for with respect to the fast api. so what i'll do is cubectl apply -f i'll give the deployment yl. so now it has created the deployment and the service. now you can go ahead and check get pods. you will see three containers getting created 7 seconds ago. now let's go and see get uh deployment. so you will you will see fast apml ai api deployment. now we can go ahead and see the service get service. so you will see here fast api ml api service. so everything is up and running. now what we want to do is we want to expose our application that is running inside the cluster to the outside world. so what we can do is we can get the ip address. so for that what we have to do is mini cube service and can give uh the service name. i think this should work and get the url. so let us see if it brings up the url. so yeah service not available. no pod running for okay. so let us see get service.,370,133,158,6WMXI0izClk
9,so let us see get service. so we have it running uh get parts. so yeah there is some issue with the uh image. so let us check what is the issue. cubectdls. so let me copy this. i think there is some issue with respect to uh the image. so let me go here. this is pass cp ml 0.1 v. so this is correct. so i think maybe this is where things are going wrong. so what i'll do here right i'll just modify this one predictor to there is other way to what debug this one. so let me resolve this issue and come back to understand the issue more. so what we do is we do cubectl describe pod and pass the pod name and we see that it is taking an older image which is not present. so what we have to do is we can modify the deployment yl. so i what i'll do here is i'll just go here and modify this one ml and then this i'll remove it here and let us see this should update everything in the back end. so this is done. now what we'll do is save this and trigger this again. not this one. we'll apply the file again. now let us see uh cubectl get ports. so uh now this is done. this is the previous run. now it is creating container is getting created in the back end. so let's wait for this. so it should not throw an error. so pubectl get service. it should create that service which is this one. so let's wait for the part to up and run. okay.,364,158,187,6WMXI0izClk
10,"okay. so here if you see it is up and running. so it has managed to download that specific image and we are able to what we'll be able to run now. so before we do that let me see right if i can go and get the yaml for this one. so i'll take this uh pod num name and get pod - o yl. so here you will see yeah this is the updated image. okay so it is not referencing to the older image. now we have our updated image. now what we have to do is we have to get the access to that service because if you go back here we want access to the service such that we can able to access it from external uh browser. so what i'll do here is first i'll do clear this one mini cube service and then what was the service name? let me go here get the service name. if i go here uh that will be ml api service the name of the service and then uh i'll get the - url. so this will give me the ip address and the port on which that fast api application is running. so i'll copy this now and i'll go to the browser hit this one with docs and it will show me that iris classification here. i can go and test it out. so just do some random testing and execute it. see the response. yes, it's working fine. now one thing to understand is the reason why we have ubectl get pods.",348,187,205,6WMXI0izClk
11,"now one thing to understand is the reason why we have ubectl get pods. so the reason why we have three deployments running is because if you have higher traffic coming in then you can pass your traffic towards any of these pods such that you can handle the request. if you have only one pod running then your request will be ceued and there is a very tendency there will be an expiry time after which the request will be dropped. so better to have multiple replicas keep it up and running and such that you can handle higher higher number of request in parallel. so uh what we have seen is uh we have created an ml uh model then we used fast api to expose that as an fast api application and then we use the docker file to create a docker image. then we push that to remote repository. then we updated the yaml file here which contains both deployment and service. deployment is where you will pass your container details and the resources required and then we have a service that will communicate between user and the deployment. so with that i'll be concluding this video. i hope you understood how to scale a machine learning model with fast api, docker and kubernetes. thank you for watching.",288,205,215,6WMXI0izClk
0,"hey friend and welcome back to the channel. today we're going to discuss an end-to-end machine learning project, a classification problem. this is a churn project where we try to predict if the customer is going to leave the company or unsubscribe uh based on the features that we have. so the difference between this project and many of the projects that you'll see on the internet is that usually on the internet you work in notebooks and you just do a few nice models. you focus on accuracy or one metric, but you never deploy really in the real world and no one can really use it apart from you in your own computer. but today we're going to go through the whole process from row data into deployment where you can share with anyone this project and they can also go and input their numbers and get the predictions based on the model that you fine-tune that you worked on. so just to give you a very high level overview, the things that we're going to cover, we're going to go from row data. i'm going to show you from where we're going to we're going to download it. we're going to do data validation with a quality check tool. we're going to do pre-processing of the data or cleaning. we're going to do some feature engineering, model training. we're going to train a few different models. and then we're going to move into model packaging, which we're going to use docker and ci cd pipelines with github actions.",339,0,12,luJ64trcCwc
1,"and then we're going to move into model packaging, which we're going to use docker and ci cd pipelines with github actions. later on in the ci cd automation, we're going to deploy to aws and we're going to do some basic monitoring uh within aws to see how our model performs and if everything is doing well. so as you can see this is quite comprehensive and it's h quite different from what you see usually on the internet very straightforward notebooks. this will be way more comprehensive and therefore i won't be walking you through each and every part of the course writing it from scratch. i'm going to show you what i've built module by module. you can have access to everything in github which i will show you in in a second. you can clone the same repository and just do the same steps as me and you can be and you should be able to reproduce all this uh by yourself. but obviously if you're more interested in doing a deep dive and step by step, you can start the whole project from scratch and get inspiration from everything that i'm going to share with you. so first of all, you could go to uh this link in here and i will link it in the description below. this is where the repository for the this project lives and you're going to see that we have everything within it.",317,12,21,luJ64trcCwc
2,"this is where the repository for the this project lives and you're going to see that we have everything within it. uh you're going to find a readme file and that one will give you more of an idea of what's the purpose the problems we solved what we built and some even some roadblocks that i faced while building this because as you can imagine it's not as straightforward as just watching a small tutorial and being able to reproduce everything in 1 hour. it took days of work, but i will try to package everything for you so you get more of a highlevel idea of how data scientists and machine learning engineers work and not just uh a very high level tutorial, you know. so, you're going to find all in here. you just go uh in your vs code. i'm going to show you and you literally just take on the link. and if we go to um to vs code, i'm going to just do a new terminal here. at the bottom and literally just do get clone and you put in the link and that's how you can clone all the projects in your new uh vs code page or if you use cursor or any other ide it should be roughly the same. so that's the first thing. uh so let me walk you through uh the data set and this project is from telco and telco is a telecom company uh basically and we want to analyze if uh their customers are going to churn basically are they going to leave the company for whatever reason.",356,21,30,luJ64trcCwc
3,"uh so let me walk you through uh the data set and this project is from telco and telco is a telecom company uh basically and we want to analyze if uh their customers are going to churn basically are they going to leave the company for whatever reason. so this data set has uh 7,000 customers and it has roughly 20 features which means columns and what we want to predict is if they're going to churn or not if they're going to leave or not based on the features. we try to build a predictive model here. it's a classification model, a yes or no based on the features. in a project like this, uh it's very important to uh identify what is the metric, the kpi that matters the most for the business. so when we think of classification, we have a few key metrics that we look into. there is the accuracy, there is the precision, there is the recall, there is the f1 score. uh so you need to understand which ones matter the most for your business. and in our case here, the thing that will harm the business the most is people leaving the business, the company, they unsubscribe or they just cancel the subscription. and that's the best metric for it is the recall. so that's something very important to understand. so first of all, we can go into kaggle in here. i will of course share the link in the description below. so you can just go and click it. and you go at the bottom here. and you have this button here to download.",356,30,45,luJ64trcCwc
4,"and you have this button here to download. you just go click on it and it will download uh the csv file for you and you just go and put it inside of your project uh so that you're able to work with it. so if you go to uh my github, you see that the data is not in here. this is just for me to not put too many things and make super heavy github. i just keep it locally. but you can do it. the the data itself is not that big. you just download it and go and put it inside the folder. so if we go and look into briefly our code, let me just close here everything. just want to show you briefly the structure that was used in our project here. so you see that if you do get clone, you're going to be able to bring all these things in here and the data will live in a folder like this data. so um you have the row data which is the one that will be downloaded from the kaggle that i showed you. and then when we have a few other steps like we're going to process this and everything, we're going to just include it in here. so this is where you're going to put your data set inside of the folder data. and inside of it, inside of row, that's what we're going to put it. and the rest obviously of the code, we're going to walk through it. but this is one of the structures that i use.",351,45,61,luJ64trcCwc
5,"but this is one of the structures that i use. and this one is just to um have a nice structure of what's going to uh happen step by step. so i know where i have my notebooks. i know when i have where i have all my python scripts. i know what i have for example just some read me to go and read and add some details about what's going on so on and so forth. so for now we're going to take things step by step. if i go back in here i can briefly walk you through the tools that we're going to use so that you have more of an idea uh what you're going to learn today and what you're going to play with today. so uh first we're using python to do this project. in my case, i use vs code is the ide that i use. you can use cursor or you can use pycharm. it depends on your preference. it shouldn't matter. um, we're going to use great expectations. and this is just a python library that is going to help us uh basically just do a quality check of our data. so, when we receive a new stream of data, it needs to fit some business requirements. and that's something that a lot of people completely neglect in their tutorials. we're going to do it. i'm going to show you how we implement it. uh remember this will be a high level video in terms of coding. i won't be coding live. i'm going to be explaining to you what's in there because it's a heavy project overall.",361,61,81,luJ64trcCwc
6,"i'm going to be explaining to you what's in there because it's a heavy project overall. uh we're going to do some modeling and the uh machine learning model we chose is the xg boost classifier. i've tried a few. i'll show you briefly, but that's the one that gave us the best uh performance overall. we're going to use docker uh for uh building a container for our project. so what happens in most tutorials is uh you get people that do a project in their own machines and everything works. they have the libraries, everything installed, the python version, and then when they try and share with other people, it completely breaks. and that's because there's a mismatch in the dependencies, libraries, and the setup. and docker removes all this headache. when you take your setup of the project and everything within it and put it in a into a container then anyone uh from anywhere like yourself you can literally take this and reproduce it without having any issues. um so we're going to walk through that as well. we're going to do uh and use mlflow and this is for uh tracking the um experiments that we do in machine learning. so all the uh metrics, artifacts, everything will be stored for us. so for example, let's say you work within a team and you try 10 different machine learning models or you tried to fine-tune a few things and you want to roll back to the one that worked the best 3 months ago. this is how you do it. you will find all the historical of it.",354,81,96,luJ64trcCwc
7,you will find all the historical of it. we're going to use fast api uh to have an https endpoint uh which means that our model will be exposed to the internet so that it's usable from the internet. and that's a very important step what we're going to deploy because when you go to the cloud you need to a way to be able to communicate uh with your model and if it's not exposed to the internet you can't work with it. so that's a very critical one. we're going to use fast api which is the one used in production um in most companies and we're going to use aws. it's one of the most famous clouds. we're going to use aws ecs which is the elastic container service if i remember. uh and this one we're going to use a serverless uh solution which is called fargate. and lastly we're going to create a ui a web ui with radio in python just so that any user is able to plug in some numbers and u get the machine learning model to work for them. so for me to give you a demonstration of what it will look in the end here i create this created this um ui with gradio and this is exactly all the features that we have and when it's now it's deployed if i just go and play around with all the values that we have in here and then i just plug in and click on submit we get the prediction not likely to churn or likely to churn.,349,96,105,luJ64trcCwc
8,"so for me to give you a demonstration of what it will look in the end here i create this created this um ui with gradio and this is exactly all the features that we have and when it's now it's deployed if i just go and play around with all the values that we have in here and then i just plug in and click on submit we get the prediction not likely to churn or likely to churn. so this is like in real time we're using the machine learning that we built model and we're getting the output required. obviously in reality you'll have the stream of data that it comes every day and this output will be generated automatically. but since we don't have that stream of data we're working with the static data that we have but we're going to do exactly the same steps as you would as you would do in production. so let's take things a step at a time. and just so i remember when you go here and you want to clone the repository, you can take the link in here. you can give this repository a star as well to support me and to support this kind of content because i will be doing more of it and it doesn't cost you anything. and then when you go on on your vs code, you just do uh as we did earlier, you just do get clone and you put in the url and that's all. you should be able to clone it.",341,105,113,luJ64trcCwc
9,"you should be able to clone it. but for those that want to start a project from scratch, i'll briefly show you the way i do it in a quick way. i just save some commands that help me start a project from from scratch and create all my here the folder, the project structure, the environment, the requirements, and the libraries within just a few commands that i will show you right now. but this is not necessary if you clone the repository. it is not necessary. this is only necessary for your next project when you started from scratch. you can use these commands for yourself. so what you would do, you'd create a an empty folder, for example, your desktop. you're going to put it into vs code, like literally drag and drop to start a new session. then you're just going to go to terminal. you're going to type this. it will create the whole structure for you. and then uh let me do a demonstration for you so you get an idea. so let's say i i go and i create a folder test. so i'll come to vs code. i'll start a new project. let's say i do here. i'll just drag and drop a new empty folder. so to create a new session for me, i just called it here test. let me zoom in here. uh i'll close this. i'll just go to the terminal at the bottom. and i will literally just copy paste uh the command that i showed you on my notion. so it's this one. and i'll just press enter.",353,113,137,luJ64trcCwc
10,"and i'll just press enter. and you see all the project structure was created for for me within seconds. no need to create manually. then we can go back here and look into what's next. so after the project structure, i'll just include a quick readme. you can also do it programmatically from the command line as well. or you can just go and inside your project just create readme. md and here you type anything about the project. this is just showing you if you're doing it from scratch. i won't spend too much time in here. then we're going to go and add requirements.txt which is the one that will have all the libraries that you want to install. you can add them just by using this command that i did in here. let me just run it here for you at the bottom. so we just did it here. and you will create as you can see here our requirements.txt for us. you can add any libraries that you want. and in here we're going to create our python environment. and this is very important. we have here we specify which python version we want with the name of the environment and we're going to activate it and install all the the dependencies. so if we do that you can do it one at a time but if you do that you're going to end up creating a new environment and you're going to activate the environment and you're going to be creating and installing all um all your libraries in there. as you can see we've installed all of them and now we are inside the virtual environment.",366,137,157,luJ64trcCwc
11,"as you can see we've installed all of them and now we are inside the virtual environment. so this is just tips that i'm sharing with you here uh in case you want to be reproducing yourself and starting new projects. that's how i start them. but now we're going to jump into the project itself. so now that i showed you how to set up the environment and do everything, especially if you're doing the project from scratch, we're going to jump into the exploratory data analysis. and this is where you do most of the things that you would like to then modularize and deploy uh in the cloud and and everything. so this is the phase where a lot of data scientists that are good at modeling or data analysts that are good at visualizing, exploring, cleaning, they shine in and myself as well. um so what i do usually is that i would uh do the whole project or most of it anyway i will do it inside a notebook and the reason why is because i can explore easier. i can open the data in here. i can clean it. so i can get a better idea of what's going on and then afterwards i will think okay how can i make this production ready and i will try and split each and every task done you know loading the data uh cleaning the data uh building the features and all that kind of stuff. so here i included a basic about the data.",336,157,168,luJ64trcCwc
12,"so here i included a basic about the data. so we just talk about uh what we do with this data what what is the content and things like this which is not we already spoke about it. so what's interesting is to look into the data. so we here imported the libraries that needed pandas to work with tables sns and mplot li um here seabboard and mplot lib to visualize basically. so we just bring the data we just bring it into a data frame and then we read it just one row and here the goal was to just look at what u let me run it all here the goal was just to basically look at what the data looks like and what are the features that we have. so here we can see the data the features that it has. we can see all the columns and here in the end we can see the column churn. uh so initially before the processing the column churn uh will have either a yes or no. and after doing the processing, we're going to turn things and encode things into numerical values because in machine learning, you will need to have uh your um features which are the columns that we have numerical so that they can be read and processed by the machine learning model because it only understands numerical values and not categorical values that has uh letters in them and characters basically. so yeah, we're looking at this data set and then we're going to just basically try and explore it.",347,168,177,luJ64trcCwc
13,"so yeah, we're looking at this data set and then we're going to just basically try and explore it. try to see if there are no missing rows for example, if there are any outliers, weird averages. that's like an exploration phase i will call it. but basically here uh it's clear for for us that uh the target is churn and the um the other features are the ones that we're going to use to try and predict the target. if it's yes or no churn, we're going to use all the others features that we have there. and we can already see the payment method, how many they charge per month, how much they already spent and all these kind of stuff, the contracts, what do they have, online security, um the tenure, how how long, etc. uh so we're going to look deeper into this. i'm going to skip this part. and what we're going to notice is when you look at the data is that we have uh some of the columns or the features that have two different values. for example, gender here we have male or female. uh dependence we have yes or no. uh multiple nines we have three options and some most of them have three options and the churn one has the payment method has four and churn has only two. so in a classification model where we have a few categories, there are different ways of approaching things. if it's a binary classification, it's a yes or no. it's usually very straightforward. we can turn them into zero and one. nothing uh difficult about it.",357,177,193,luJ64trcCwc
14,"nothing uh difficult about it. but if it's more than two, there is the possibility of choosing another uh type of um encoding that we're going to use. this is a multiclass encoding. and in this one using a one hot encoding is the best option because we have more than two features but we don't have too many either. so if we had more than 10 or something like this we will need to look into something completely different again because one hot encoding will end up creating more columns. it's like um binary. think of it as binary. if you have three uh categories it will give them a numerical value of 0 0 1 or 0 1 0. so it will like create for each and every category a separate column. so you can imagine if you had 100 and you have 100 times that you need to do those binary new columns, you'll have just um too many columns to deal with which will make it very complex for the computation. so that's something to keep in mind. so when you do the encoding, if it's a binary encoding, you just do 0 and one like we did here. yes or no will be 01. male female will be 0 and one. and you'll just literally just turn those uh into zeros and ones. and that will help us for our machine learning model to to just have the numerical values instead of textual and characters. and for those that have above two, which means they have three, four, five categories, we used one hot encoding with this um function here.",357,193,209,luJ64trcCwc
15,"and for those that have above two, which means they have three, four, five categories, we used one hot encoding with this um function here. um and what it will create, as you can see, will creates more and more columns for us that have boolean values false, true, false, true. and that's why i'm telling you if you had more than 10 categories for example, it will become so big and so difficult to to deal with. then obviously we'll need a cleaning phase. and this one is for anything that was unusual that needs to be cleaned. and in our case, we're going to drop the customer id column because it's kind of irrelevant to predict the churn. it could we could have removed others, but that one seemed to me from the side that is completely irrelevant. so just remove it. we're going to turn all the columns that were boolean uh because all these that had true false are boolean. uh so we're going to turn them into zero and one. so false zero and um true one. as simple as that. so that's all our columns that we're going to use are numerical either integers or floats. that's very important. and then we're going to move into um looking at a the correlation. we're going to create a heat map to see the correlation to the churn. and let me zoom in a little bit more. and we're going to see here we have the positive positive correlation up until the negative correlation. so we can see here the positive correlation. we can see that there are two that kind of stood out.",361,209,228,luJ64trcCwc
16,"we can see that there are two that kind of stood out. the fiber optic option for the internet. they correlate a lot with churn. for some reason, uh people in this data set anyway, people that have fiber optic are more likely to churn. and those that have a payment method of electronic check also are 30 likely to churn. and on the other hand, those that have uh contracts of 2 years, which is like long contract or they stayed long with the with the company are less likely to turn. and i added a few notes in here just saying that negative uh correlation or positive correlation both are quite important because sometimes a high negative correlation says that when this is high we are less likely to churn. so that's also something to to optimize for. um it's very important to know because if we had a set of a thousand categories or 100, we'd want to only keep the ones that are relevant and not compute for 100 which are unnecessary. uh we look into this multicolinearity in the classification problems when we um suspect that some of the categories or the features that we have might be um multiolinear which means that both of them basically give us the same information which makes the model very redundant and when we do this vif uh we look at this and if the number is above 6 7 8 10 maybe there is a multiolinearity and we can see all these features here have a very high multiolinearity. so there are a few options here to consider.",352,228,238,luJ64trcCwc
17,"so there are a few options here to consider. we either drop some features or we can do some regularization techniques like there's l1 and l2 ridge and lasso to to use. we can see them in here. added notes or use a treebased model something like the decision tree or the xg boost or the light gbm uh because usually they handle this scenarios uh really well and in my case that's what i did i i jumped into a tree based model so that's uh we can completely not focus too much on this and start modeling already so here in modeling we can see first that i'm looking at the column of churn and what i easily notice here is that there is a class imbalance. uh we have 5,000 not churning and only 1,800 churning. so there is an imbalance and in your data set you have to do everything possible uh to get a a data as balanced as possible because uh if you don't balance it well then what's going to happen is that um your model will start predicting more of the value that appears the most which is fair enough. and in our case, we'll just need to make sure that um we we make it more fair. and there is a way of doing it. we can just adjust a threshold. so when we uh do the model when we adjust the threshold, we can adjust the how how much one um category is compared to the to the other. so that's one of the ways that you can do it.",351,238,248,luJ64trcCwc
18,"so that's one of the ways that you can do it. obviously there are other ways of you can resample and and things like this or maybe duplicate uh some of the uh churned ones to get a more balanced data set but that takes time and there is a lot of also uh validation that needs to be done there and it needs to make sense otherwise you'll just end up messing with your data. so we're going to use this threshold. so one thing that we uh need to to focus on and talk about is what is the metric that we're going to work with and the metric that we want to focus on as a business. so if i was in a telco company and i need to build this machine learning model, what is my main thing? what is the thing that i'm focused on is not losing clients. which means that we don't want uh the false negatives. and the false negatives the one that focus on focuses on it the most and try to optimize is the recall. so here if we look business context prediction is asymmetric in cost. so false negatives. you predict a customer will not turn, but they actually leave. you miss the chance to intervene and keep them. and this is the worst thing that we want to be able to avoid. we want to spot all the people that were going to turn and maybe give them an offer, give them a discount, make them stay. and this is uh what we're going to focus on in in our project is optimizing for recall. and obviously there are trade-offs.",366,248,263,luJ64trcCwc
19,"and obviously there are trade-offs. when you focus on recall, uh you end up losing precision. so that's something that needs to be discussed with stakeholders and it's very important to keep that in mind because you're not building just for the notebook. you're building to benefit the business and you don't want to harm the business with your model. so as i mentioned earlier, i've tested the random forest classifier. i've also tested the light gbm classifier and lastly i tested the xg boost. i don't want to walk through each one of them. that's not the purpose of the video. i ended up choosing the xg boost. it was uh slightly better performance than light gbm in terms of recall and it was faster. so faster is something to consider when you're in production. uh you you want to have things to work fast and with less compute. uh xg boost doesn't mean less compute. it just means in this case it was faster and that's why i went for it. so we can see here uh it run the training time in 1 second and the prediction time was like in no time literally. uh here we can see precision, recall, f1 score and support. so it's very important to know that accuracy is not the only metric in classification and accuracy it's in many cases is just when the data is super balanced and when we don't really care about uh the precision or the recall that much in terms of none of them really affects too much our client base or our result.",348,263,279,luJ64trcCwc
20,"so it's very important to know that accuracy is not the only metric in classification and accuracy it's in many cases is just when the data is super balanced and when we don't really care about uh the precision or the recall that much in terms of none of them really affects too much our client base or our result. in our case, we want to optimize when recall equals to one, which means that we want to spot all the people that we're going to leave the company that we're going to churn. and this is the metric that we want to spot. so, we have here 81.8 that were able to spot with our model. so, uh here what we did next is that we tried to look because we use the um we use the threshold 0.3 and this one is to try and unbalance the data. and here what i did is i just tested a few different thresholds just to see what is the recall because that's what matters to me the most uh based on what's the threshold. so here with different thresholds 025 03 we get different recalls and also we can see precisions and the f1 score which is kind of a mix of of both of them which is a very important one when you want more of a balanced response. uh so here i can see that uh if i go for 025 i get a better recall but the precision drops a lot. so i might just leave it like this. but this is obviously a business decision to discuss with the stakeholders.",356,279,288,luJ64trcCwc
21,but this is obviously a business decision to discuss with the stakeholders. uh but in our case i am the person making the choices. so let's say i'll keep it 0.3. so i chose the xubus. it gives us a decent recall and it's faster to train. uh so that's that's great. one step that a lot of people kind of neglect as well is uh fine-tuning our hyperparameters. so uh xg boost has all these parameters in it and by choosing and finding the right parameters we can very much enhance our model and what we do here is we use a uh library called optuna. it's a great one and we can just give it like a range of things to experiment with. so for example the estimator from 300 to 800 the learning rate from 001 to 02 and so on for all these it will try a variation of things and then it will give us the one that gave the best results. so if if we keep going in here we can see we're going to optimize and we're going to do 30 trials. so it's basically going to try 30 times and it gives us the best one of them.,267,288,299,luJ64trcCwc
22,"so it's basically going to try 30 times and it gives us the best one of them. and you can see here if we take just the trial zero we can see it gives us here the recall it gives us what are the hyperparameters that was used for that exactly and in the end if we go down it will give us here best parameter let me zoom it a little bit more best parameter and it gives us it was this one 300 estimators this learning rate this max depth this all all of them basically i don't want to walk you through all of them and that one gives us the best result. and then what we're going to do is that we're going to just bring those best parameters instead of writing them ourselves uh manually. we're going to just bring this one best parameter and we're going to just do a random state of 42 so that when we rerun we get the same thing and then we're just going to time it to see how long it took and everything. so basically the same as we did before and here we see that we improved to 92.8 8 looking at the recall equals to one meaning that uh the the possibility of the client leaving the company which is very important. yes, we get a lower precision but in our case we said we're going to optimize for recall and we see that we optimized uh by from 81.8 to 92.8 . so it's 11.1 of people that were going to turn that we kind of saved with this model.",358,299,305,luJ64trcCwc
23,"so it's 11.1 of people that were going to turn that we kind of saved with this model. yes, we're gonna target more people that weren't going to turn with with our new campaigns, but you'd rather save those that were really going than uh you know just sending a few more emails to people that were going to stay. so this tuned model here gave us 92.8 of recall. so it's the best. uh the precision is very low. so many loyal customers are being flagged, which means that we're going to end up contacting them thinking that they're going to leave, but they won't. and this is the um in the classification metrics tradeoff where you have an increase in recall but a drop in precision. and you always need to find that balance where it's acceptable. and in our case we aimed for a high recall and we got a high recall for now. so now if we just wrap this notebook and you can see what all we did we explored the data we loaded the data we explored it. we did binary encoding for the categories that had two categories in them. and we did one hot encoding for those that had three or four uh categories in them like values. and then what we did is we cleaned the data. we turned the boolean columns into numerical columns. we dropped some columns.",309,305,319,luJ64trcCwc
24,"we dropped some columns. um then we looked at the multiolinearity but we found out a few features what were high highly multiolinearated and we decided to use a decision tree based or a tree based model like xg boost we tried a few machine learning models like um random forest like gbm xg boost and we decided to go for xg boost and then we fine-tuned the hyperparameter to to boost uh our results. uh the experimentation here with mlflow, i left it for the next phase and i'm going to show you how we're gonna do it. so, usually if you did everything in here, this looks a lot like the tutorials that you usually follow and then you stop and you're like, i've built a great model. i've got a great recall. i'm great. but no one can use this model. and therefore that's why the importance of going through the the pipeline of going next to modularize everything we've done into python scripts and then adding mlflow for tracking machine learning models. then we're going to use fast api to serve that model into the web. and then we're going to put that all into a container so that it's reproducible from anywhere. then we're going to use github actions so that we're able to test, run, and push that container into the cloud and in the end use aws so that we we're able to um deploy and also visualize with our grad um ui and share it with anyone from the internet. so we're going to keep going step by step. the next phases will be less code oriented, less technical.",360,319,331,luJ64trcCwc
25,"the next phases will be less code oriented, less technical. i will still go through them but not in a very detailed way as we did now with the notebook because with the notebook you got the idea of all the steps. now i'm going to show you how we split them and focus on the important modules as well. so let's do a quick recap of what we did initially. we just uh went through the environment setup, the library installation, the github repository um and um i showed you how to do it from scratch. you also need to go to github and create your own repository and i also showed you how to just clone it and bring the same project as i have in your environment. then we walked through the exploration and here we did we went through the problem the data we analyzed it we built the model we did the hyperparameter tuning and our conclusion is the model that we chose with those hyperparameters and uh here uh what we did we focus on the recall which is the business metric that we focused on. so i left all this information here. i didn't have to read too much through it because we built and that's the best way to do things. the next phase will be uh to turn the notebook everything we did into python scripts. and why do we do that? this is very important. we do that because first it's nicely split. so we can modularize each part of the notebook in a different python script.",347,331,344,luJ64trcCwc
26,"so we can modularize each part of the notebook in a different python script. and that is because we can easily troubleshoot or test each and every part of it. we can for example load the data and we can test that. we can process the data and we can test that. we can build the features and test that without having to have everything in one and we don't know what broke and we don't know how to change things. and it makes it easier for our pipeline to to be nicely organized and call each and every one of the scripts when needed. so this is a very important step and a lot of people when they go from notebooks into python scripts they get lost because you don't have the same interactivity of the notebooks when you write python scripts. anyway, so this phase is very important and we're going to walk through it very slowly. so if you just think of what we did in the notebook, it already gives us a hint on the uh step by step that we need to take. so for example, we initially loaded the row data. let me see if we can zoom in here a little bit. we initially loaded the row data basically the csv that we loaded it. we're going to have a script for that. we're going to run data quality checks which i'm going to show you now. there is a library called uh great expectations. then we pre-processed the data. we cleaned and we did some transformation.",343,344,360,luJ64trcCwc
27,"we cleaned and we did some transformation. we did some feature engineering and basically this is the encoding that we did for the binary and the multiclassification and multiclass basically encoding with one hot encoding and we handled categorical numerical transformation. so basically that's what we did in here. then we train the model. we're going to train machine learning models. we're going to do hyperparameter tuning and then we're going to do some evaluation. basically evaluation that will be with testing and we're going to also use uh ml flow so that we keep tracks of the uh machine learning experimentations that we did we uh create and i will show you around pipeline.py pi and that one will basically have all these modules. we're going to be able to call them in one so that we're able to run the scripts and we're going to have the experiment tracking with mlflow which will log some of the important metrics that we want from the model like the accuracy, the recall. um you got to save the hyperparameters and everything. and here i didn't include the recall but it's it's the metric that we focused on here basically. so let me here show you just a simple flow. if we just look in here, we can see uh we have loading the data, we have quality check, pre-processing, we have the encoding that we did. so it's the features that we've built. the model we're going to train, tune, and evaluate. and here we're going to do some testing. it's basically just unit test just to make sure that everything runs well.",354,360,375,luJ64trcCwc
28,"it's basically just unit test just to make sure that everything runs well. we have a run pipeline which basically will orchestrate all the training. will do everything that we've done in the previous scripts and will run them for us and the results of those uh machine learning models the recall the accuracy everything that we get is going to be saved in ml flow and then we can go and track and use whatever machine learning model that we want to go to the next phase which will be to more containerize and productize and deploy everything that we've built. but this phase is very necessary and it's one that a lot of people skip. so in here in our environment, if i zoom in a bit more, we can see that we have this. let me close this. save. we have this src here. and this src is where i'm going to put all my python scripts that i need. we have the data which will have our initial phases of loading and pre-processing.",230,375,384,luJ64trcCwc
29,"we have the data which will have our initial phases of loading and pre-processing. we have the feature where we have the building the features part which will cover more into all the uh encoding the models everything that is touching the model which is training tuning and evaluating we have the serving part which we're going to have the inference.py pi which i'm going to show you things step by step and here important one is validating the data with great expectations the library that i told you where we can check the data and this one is will be a step just after loading the data we want to check the quality of it before going to the next step. so we're going to walk through each and every one of the python files uh in a very brief way uh but i'm going to still explain things. so we're going to go first to the load data. and this one is very simple and straightforward. we're basically loading the data. we're given the file p the name of the file or the path that of that file and then we're just going to return pd read csv the file path. this is what we do basically even in a notebook. we just do import pandas as pd and then we just read with pd do read csv. then we're going to pre-process the data. and this one is just the cleaning step that we did initially. so let me do this. if we look at this, i've added some comments so it makes it easy. we just trimmed some column names. we dropped obvious columns.",361,384,398,luJ64trcCwc
30,we dropped obvious columns. so we removed the the customer id column. uh we fixed the total charges to numerical and we just did some mapping in here and we handled simple um na values which means that they didn't have any value in them. so you're going to see each and every one of these ones is the what was mentioned at the top. so for example here total charges often has blanks in this data set. we just want to turn that into float so that we don't run into a lot of errors when doing the machine learning model. here citizens should be zero or one. we just made sure that if it was missing we just put zero by default because that one that was the one that appeared the most. obviously here can be optimized as well but we don't want to spend too much time in each and every one of the things. there's always something to optimize and here is just the simple uh na strategy to fill in with zero things that are missing basically in our case in here. so this pre-processing call it also cleaning is a very important step and it leaves inside of src and inside of the folder data. the next one will be the features and in the features we're going to build the features. we're going to have two functions.,305,398,410,luJ64trcCwc
31,we're going to have two functions. one is for the binary um category like churn for example yes and no male and female and this one is when the length of the categories how many we have basically is equal to two and any um feature that has only two categories is going to have this zero or one mapped into it. so if we know the name of the features then we just specifically mention which ones they are and if we don't know maybe a few other features will appear over time then we just do in general generic binary mapping in here. and in the second one this build features this one will focus more on if the features have more than two categories. we can see here explicitly if they have more than two categories then we're going to do a one hot encoding. basically we're going to use the um df get dummies or pd.get dummies in here so that we turn multi- uh category columns into all those multicolumns that are boolean for each and every one of those. so this basically has two uh two main functions. they might seem a little bit complex because we try to handle a lot of things in here and i don't want to dive too much into the code and how to write these functions. you can go play around with them. uh you can use chat gpt or something on the side to explain to you if you want to deeply understand how each and everything is is is done.,344,410,419,luJ64trcCwc
32,"uh you can use chat gpt or something on the side to explain to you if you want to deeply understand how each and everything is is is done. and i've added some descriptions here for you that might help you understand things a little bit better. but before going to modeling, i forgot i needed to mention the validation of the data. so this goes after the step of loading the data. we're going to validate it. and this is important because we want to make sure that the data that will come in afterwards is um qualitative. for example, if we have male and female and then we end up having something that comes and it's not within these two, this is an error. we don't want to bring into our data because it will break everything afterwards. so from the start, we're going to go through each and every one of the features. we're going to check that that column exists in the first place. if it doesn't okay throw an error so that we know that there is something new coming and then for each and every one of these columns we want to set what we expect to receive. for example, for gender want to we expecting male or female partner yes or no. and we're going to do that for each and every one of the columns. and for those that have a numerical value for example a tenure uh we want to look that the minimum value is zero. we don't want to have one that has minus5.",344,419,433,luJ64trcCwc
33,"we don't want to have one that has minus5. we need it needs to throw an error because otherwise it will end up affecting all of our data and the maximum value for example is 120 people that were there for 10 years because you know maybe in your data there's no one that has been there for 10 years or the company is maybe just 10 years old. so it's impossible to have someone that is over 120. so this is just like a set of boundaries that you set for the new stream of data when it comes uh so that it doesn't break everything that you've done. so that's very important. and you're going to see it's done for each and every one of the columns. that's why the the code seems to be big, but it's just for each and every feature that we have. and we have over 20 features. so, so that's why it might seem a bit long, but it's just a quality check. and all of this is done in a function called validate telco data. we're basically just validating the data. now, we jump back into modeling and we're going to look into training the data. and in training the data here we can see that we have mlflow and we have our model xg boost and we have scikitlearn so that we can train test split check for the recall the accuracy in our case we're looking for the recall primarily and here we create a function train model. so basically this is what we did in our notebook. it's just that we're now turning them into functions that are reusable.",365,433,447,luJ64trcCwc
34,"it's just that we're now turning them into functions that are reusable. and in our case here we just x is for the target churn y is the other columns uh and just we do 20 of tests we put in here uh the hyperparameters and then here what's interesting here is mlflow. so mlflow what it does it's basically tracks the machine learning experiments that you're doing. so every time you run your model, you will have this folder here, ml runs, that will keep a record of that model that was run. let's say this one was the one that was run the latest. we see that we have artifacts. it saves the model into this model.pkl. it saves the metadata. it saves the metrics. so we can see the recall. look at the recall here. 082. and it saves the precision. it saves the parameters. everything. and this is fantastic when you want to go back and look into a specific model that uh you want to maybe revert back to because it performed well or anything like that. so let's look into it. we have this mlflow. we start run mlflow. that's how you start it basically. and here everything that we want to log or keep, we're just going to use this log parameter log metric.",287,447,467,luJ64trcCwc
35,"and here everything that we want to log or keep, we're just going to use this log parameter log metric. and here log metric recall accuracy and estimators and we want to log the model itself and this is very important because this is exactly what's going to show in the ml runs in here and that's exactly what you're going to find the artifacts the metrics and everything this is a very important step and then in the end we just want to print the model trained the accuracy the recall whatever is the uh metric that you focused on so when we train we want to afterwards tune and we saw that we used optuna. optuna is the library that we use. and basically instead of me again going through it because we did it in the eda. that's why it was important to go through each and everything step by step in the eda because here we're literally just taking the logic of those things and just splitting them into scripts so that it's modularized. it's called modularize in python scripts. so we go optuna and we're going to basically just tune the model and we get here say that we're going to use optuna to fine-tune xg boost. we're going to again set this um like boundaries that we're going to try in and then we're just going to expect optuna to give us the the best um the best parameters based on a number of 20 trials. we can do 50 but this will be more computation. you can do 100 that's completely up to you. and then we can just evaluate the model.",362,467,477,luJ64trcCwc
36,"and then we can just evaluate the model. basically just getting the classification reports and everything as we did earlier to see the tradeoff of recall precision and all that kind of stuff. we've done it here as well. here if you look on the left we have the src the one that had all our python scripts but we also have this script spot. and here this one is very interesting because we have a few python scripts that help us test the pipeline. for example, when i wanted just to test the phase one where we did uh build the features or the phase two where we did the modeling, it's just a quick way to test our python scripts or here for example if you want to run the pipeline. and this one here is interesting because this is the one that will englobe everything that we've done in those python scripts. you can see here load validate pre-process and feature engineering and the modeling and the hyperparameter tuning will be done in here. so the way you do it basically we call all the libraries that we need and then we're going to just go and call each and every script by doing this. so if we go back we can see for example here if we want to load the data we're going to need to go to src dot data dot load data. that's what we did here exactly. and that's how we import load data uh load data.py. and then we do the same for pre-process. if you want to get the pre-process, we do it from the same.",358,477,490,luJ64trcCwc
37,"if you want to get the pre-process, we do it from the same. and if you want to get the features, you basically just go and track where it came from the path and you just call it. and this is very important. otherwise, it won't work. it will break and you will have to figure that out. something that used to to happen to me all the time and still does. and um you just get used to figuring these things out. and basically what we're going to do afterwards is that we're going to create the the machine learning model in here. we're going to do the data quality validation. we're going to do the data prep-processing. we're going to be calling our functions that are already inside the python scripts. and basically it's just like bringing all those scripts in one and running them. so that uh we do the loading, we do the pre-processing uh we do um the quality check before the pre-processing then the pre-processing that we handle the class imbalance that we're going to run our xgb uh xgb classifier. we're going to bring in a few hyperparameters and everything all done in one script. this is a very heavy one, but think of it as just regrouping everything that we've done in the python script so that we're able to run the pipeline from here directly.",304,490,504,luJ64trcCwc
38,"this is a very heavy one, but think of it as just regrouping everything that we've done in the python script so that we're able to run the pipeline from here directly. you just run your pipeline and then you're going to have a new entry in your ml runs based on whatever you've put in here and you can change it as needed and you're gonna get again another new uh ml model that is saved with mlflow and that's the beauty of mlflow is that everything is tracked and as you can see i've already done a few runs and each one of them was saved. so then i just need to go back and look for which one had the highest recall and the one that i want to work with and just save it and then i can use it in my models and i can just here have the ones that i wanted to use and just focus on them. great. well done. now we we've done so much already. so the whole logic is made. we've done it in a notebook. we then split it in python script. so if we go back and we just look at each and every step, we modularize everything. so the step three and four that's what we did now. we modularized all the steps into python scripts. we created some tests files as well and then we trade the model and did the hyperparameter tuning and called all those scripts in our run_pipeline.py and that one runs our whole project.",341,504,516,luJ64trcCwc
39,"we created some tests files as well and then we trade the model and did the hyperparameter tuning and called all those scripts in our run_pipeline.py and that one runs our whole project. now it's more of a taking it from here on my computer into making it work from anywhere in the world and even yourselves. so for that we're going to go through a fast api layer which basically will serve our model into https which is serving it to the web being accessible from anywhere. then we're going to put the project into a container with docker. then we're going to run test and push that docker container from uh github into um docker hub or ecr. in our case, we're going to push it to docker hub. dockerhub basically is where you store your docker containers in the cloud. there is a service for that. it's free. and then we're going to deploy in aws. here the aws deployment is kind of optional uh because it will cost a little bit of money. it cost me 10 to run this project for a few days. i will show you later the cost and a few things to be aware of. so if you don't want to go through that step, no need to. but i will still show you how to set up things in there because the goal here is to really bridge the gap from just a tutorial mindset into like a building real stuff mindset. and i wanted to use real tools that cost money sometimes. uh the rest it was all free.",351,516,532,luJ64trcCwc
40,"uh the rest it was all free. uh but aws deployment depends on how much you run it, how you optimize it. we're going to keep things very small. if you have the free tier, that's even great. i'm going to show you if you've never used it, you can have the free tier and most of these things will be for free anyway. so let's focus on this layer. uh the number number five is the fast api layer. so if we go here, so fast api in our project, fast api acts as the serving layer. it turns our machine learning model into an api that can receive data and return predictions. it's necessary because without it, the model will just sit in python files and no external app script or user or user could interact with it. so it's very important. so there is this also this phase of uh building the ui. but in my case, i've built the ui after i deployed into aws so that i have a nice access to the project as i showed you initially where we can just input the numbers and we can get the responses because it's better than calling an api and having the responses in the terminal. just wanted to show you things that are nice. so if we look at fast api, if you go onto their website, you can see a lot about it on their website. but fast api is a framework high performance, easy to learn, fast to code, ready for production.",338,532,547,luJ64trcCwc
41,"but fast api is a framework high performance, easy to learn, fast to code, ready for production. and basically what it does is just a framework for building apis with python basically and we want to expose our models into the internet and therefore we'll need to use fast api and believe me that a lot of companies use it. so it's a great skill to have and the thing is to start it's very easy if you just look on their on their website. it's literally a matter of just bring in fast api import it and call app equal fast api and then just by writing this code you'll already have an an endpoint in they give you which u ip address you just open and you will see hello world already in there. so it's as easy and that as that to get going obviously there's more complexity uh the more complex the project is but to get started it's very straightforward. you can find some tutorials on it. let me know in the comments if there's any of these topics that you want me to dive into by itself. uh maybe how to build a fast api um endpoint in a simple manner like a very beginner tutorial or how to turn notebooks into python scripts or how to build docker for for data projects and all these things. anyway, we'll go to our project and here inside of src we have the app and inside the app we have the main.py. pi and main.py is where i have my fast api and gradio uh ui in one place.",354,547,556,luJ64trcCwc
42,"pi and main.py is where i have my fast api and gradio uh ui in one place. now we're not going to talk about gradio that's later. so i have like two functions uh or two sections, sorry. the first one will be for the fast api. we're going to just call the app fast api as we saw on the web of fast api and we just give it a title, a description. want to do things nice. we're just gonna call it get so that we can get the root of it. this is just to get a health check endpoint for monitoring and load balancing which we will be using a load balancer in aws. this is just to check that the status is okay. basically that we are able to call that uh we're using here uh pinantic model. this is just to do an automatic uh automatic validation and this is for the documentation as well. so, it's basically going to just go through each and every one of the features that we have and just making sure that they're all okay. basically, that's all we're doing. and this part is the most important is we're going to predict. and what does it mean? we're going to use our prediction, the machine learning model that we did to make predictions also in uh with our fast api. so, that's the goal is to be able to do it from our machine uh and be able to do it from anywhere via this endpoint. uh the second part is for the grad gradio web web interface or the ui. that's something we're going to look into later.",365,556,574,luJ64trcCwc
43,that's something we're going to look into later. so basically as you see we have three main parts or four main parts. we're going to just call our app and give it some details. we're going to do a health check in here by doing this slash. it's exactly how you do it. and we're going to use here pyantic model for automatic validation. that's an extra step but it's a very interesting one. and then we're going to make predictions. so this is all i mean i added a lot of comments so that it's clearer for those that are just exploring but overall it's not that many lines of code to to be able to to to use fast api. 20 30 40 lines of code that's not that much for a project like this. now that we focus on the fast api endpoint we're going to talk about containerization. basically using docker to save all of what we did into a docker image that can be shared and reproduced from anywhere. so if we go into docker just to get an idea and you can go onto their website and read their documentation and what is docker is a faster way to build share and run applications.,273,574,586,luJ64trcCwc
44,"so if we go into docker just to get an idea and you can go onto their website and read their documentation and what is docker is a faster way to build share and run applications. so a lot of people when they start building anything whether they're are software engineers, data scientists, machine learning engineers, they always get into the point where uh things work for them but they don't work for other people and they say but it worked on my machine and this kind of talk is because they don't know how to use docker and when you learn how to use docker you stop talking like that because you put everything in a docker image and you can share it with anyone and it will work from anywhere. so the step here is the docker image bundles the application code the mlflow model artifacts and all the required dependencies. and what happens is that in production why your model doesn't work on another's person person's machine is because they don't have the same python environment. they don't have all the required dependencies and things keep breaking. but if you package that all into one container and give them that and they will run the app from within it, then they have the exact same environment that you had on your computer, which means that 99 of the time, if not 100 , you should work for them because you're giving them the same conditions to make their model thrive and work.",331,586,591,luJ64trcCwc
45,"but if you package that all into one container and give them that and they will run the app from within it, then they have the exact same environment that you had on your computer, which means that 99 of the time, if not 100 , you should work for them because you're giving them the same conditions to make their model thrive and work. so if we go into our project and now we're going to go into this docker file and you can see here it doesn't have any extension because that's the way you write it. you literally just create a file call it docker file and it has this nice logo here of a whale in here. and the docker file is an interesting one and quite a simple one to work with. and you're going to see it's split into a few key sections in here. so here initially we're just going to use one of the python uh base images that we have from docker. we can go onto their website. there are plenty. i'm using this one 3.11 uh or hyphen slim. and this one if you click it says that it has high vulnerabilities. and these ones are not relevant. i checked them. not relevant for my project. so it's always worth having a look and reading the documentation. but in my case i'm using this one. then we're just setting the working directory. it's basically just giving a root directory for this docker. then we're going to copy everything everything here. the dot here refers to everything that is inside requirements.txt.",351,591,609,luJ64trcCwc
46,"the dot here refers to everything that is inside requirements.txt. and requirements.txt is where we had all of the libraries that we used in the project or that we don't we didn't necessarily all use, but everything that we have in this file. um, where is it? yes. and then we're going to just install uh pip install upgrade. we want to have the latest pip and we're just going to install all the requirements and just clean basically the path that we have. so here we just install everything and add the curl if we use mlflow local tracking uri and then we're going to just copy the entire project into the image. so we first gave it a python environment. we installed all the libraries that we had as well. and then we're going to copy all the rest of the project. so this is the flow of taking uh everything that we've made locally. that's exactly what we did. we uh put and created the uh python environment. we put all the libraries and installed them and then we started building. that's exactly what we're doing here. the next phase here is to copy the models, the machine learning models. and here, why i did that is because i put explicitly some of the models in here because those are the ones that i wanted to use, one of them anyway. and i did that because usually you don't push these ml runs into uh github. so you usually put them in a file like git ignore so that you don't send everything there. so um that that's one of the things that you do by default.",364,609,628,luJ64trcCwc
47,"so um that that's one of the things that you do by default. you know, you don't you don't send your api keys. you don't send some of the important information. you can also avoid sending data as well as i did in here because sometimes the data is a few gigabytes and you don't want to send a few gigabytes to github. it will bug and you don't want that. so basically this is just to give it the the path to get uh to those models. and here it's even further is just for us to give it a path to get the artifacts uh to get to the features and the pre-processing because uh without the pre-processing the features will still be categorical instead of numerical which will end up breaking the pipeline. and this is just me ensuring that things will work well. and this is why a lot of tutorials will not go over this because it will break many times and you'll have to keep figuring things out step by step. and that's why people don't want to do things live. uh me included because some of these things took me some time to figure out and to debug and doing it live the video instead of being an hour and a half uh will be forever. so yeah. um then here we just uh do this the serving and app importable about the cr src. so basically here we just tell it that we get the uh the models from this. this is where we get all the python files and all the scripts. and this ensures that logs shown in real time.",364,628,643,luJ64trcCwc
48,"and this ensures that logs shown in real time. so this is the buffering. so the logs are shown in real time. and this one this path lets the import module using from app instead of from src.app. so basically instead of using here from app directly, it will use src.app. this is just just a way for us to make sure that all the relevant data is found in the in the right folder, subfolder, and directory. then we're going to expose the fast api port to 8,000. that's one that you can tweak. in our case, it's 8,000. and this one is we're just going to run the fast api uh using uorn, which is this. we just run it. we use python-m and we use uicorn to run the fast api. we give it where it lives. so if we look we have srcapp dot main and that's do here this double dot here app is the root directory of our docker and here we're just going to give it access from anywhere and the port is 8000. that's what you do. and when you do this you've already created your docker your docker file. it might seem a little bit complex because we've added additional steps like this. but in general, you could think of docker as having these main components and you see the python environment. you can see the setting the directory for docker in this app setting all the copying all the requirements. .txt and this is a dot that means we bring everything from it. then we just install it. we copy the entire uh project, the rest of it.",361,643,664,luJ64trcCwc
49,"we copy the entire uh project, the rest of it. we expose the fast api to this port and then we just run it with uvon. this is for me just additional to be able to run specific models because we're not running for example the just the latest model because we don't necessarily want to run every latest model because there is no guarantee that it's the best model. here in my case, i've chose something specific. i don't want it to be used when deploying it to to the internet. so this is how we do file. next step will be to use github actions workflows. so why do we use that? so there's a concept of cicd. it's continuous integration, continuous deployment. so now that we have this uh container that we've created in docker that has all of our project, we need to do something with it. and in order to do that, we want to use github actions. and that github actions will help us test, run and push that uh container into docker hub. and here just to have a quick look in docker hub, if you go and sign in, this is our project in here. and you can see it ran 2 hours ago last time pulled less than a day ago, etc. so this is where all your containers will live. and our goal with github action is to push to here. so how we do it is we're going to create in this github uh folder. we're going to have another subfolder inside of it called workflows. this is like a typical way of doing it. and then we have this file called ci.yml.",369,664,684,luJ64trcCwc
50,"and then we have this file called ci.yml. it's yaml language basically. and we're going to go in here and we're going to look into it. and here you see we are going to build and push to dockerhub. that's our main goal. so first when do we push? we push every time we push a change into our main branch on github. so every time for example i've made changes i can come in our terminal i can just do g status. i see that there is nothing to change. um g does not exist in this case. g status. yeah status. and then i'm going to do g add everything and get commit. g push. and when i'm going to push, i'm going to push to my g main branch. here we have the main branch in here. and when we push to the main branch, whatever we push, it's going to trigger our ci pipeline. it's going to trigger it. and when it triggers it, what's going to happen? so i can show you more in how it does. it's going to come into this actions in our repository. and you're going to see it here running something like this. and you're going to see it running. and in real time, you're going to see what's happening. what's the setup that is happening? and you're going to see everything one by one and if anything breaks it will give you what's the message error and you need to go back and try and fix it. so let's keep going with the code here.",345,684,710,luJ64trcCwc
51,"so let's keep going with the code here. here we just define when it's going to work and start is when we push anything to github on our main branch. if we have many branches, we don't want to keep triggering this for other branches. we just want to trigger it for the main branch. here we just set uh which os we're using, the operating system. here's the checkout code. what matters here to us is this. this is very important is having those dockerhub username and the password basically. and where do you get it from? you get them from docker hub. when you go to docker hub, you can go to um to your account if i remember. let's see. let's see all together. yeah, we go here to your account. you will go here to personal access tokens. and here you're going to generate a new token. so that's how you do it. and when you generate them, you're going to go into github. you're going to go to settings. you're going to go down to um secrets and variables actions. and in here, you're going to create um you're going to create new repository secrets. one you'll call it dockerhub token. and you're going to put the token that you got from docker hub. and one you're going to call it dockerhub username. and you're gonna give it your username in docker hub, which in my case annas riad 8. that's all.",321,710,735,luJ64trcCwc
52,"that's all. you're gonna put them in there, save them, and then when this um when this script runs, you will be able to from github uh secrets to grab exactly what's in there and give you access to push into docker hub. otherwise, it will not work. and then after we just see here build and push to docker hub. it's basically just gonna uh use the content of what we have in docker file which is what we saw in here. and the tag here is what you saw in docker hub. if we show you just briefly you can see here that this is the tag and you saw here latest and that's exactly we're just pointing into the right to the right direction. so this is how you build the continuous integration pipeline with github actions. so everything will be run inside of github actions in here and you will see all your runs that you did before. so this is basically every push that i did that triggered this workflow, the github actions workflow. so now we have the last phase which is the aws uh deployment. so before jumping into it, uh what i want to do is briefly just show you how you can go and sign up yourself. if you just put aws and you do sign up and you can sign up to the console, it takes you here and you can just put in your email. if it's the first time you did, it will give you a free tier, which is great. you're going to use a lot of things for free.",354,735,749,luJ64trcCwc
53,"you're going to use a lot of things for free. uh if not and you feel a little bit lost, just go in here and the resources to create the account and it will show you um how to do things and what you need to add and choose a plan and you you will need to put in your your debit card or credit card. um, but they don't charge you anything. i think they charge you the first some 1 just so that they have your card, but afterwards it's u pay as you go basically whatever you choose. and just to show you things in a proper way, let me sign in first and i'll show you the cost and everything. so here i am in my root account and here if i go on the top so the root account is the one that you use to create uh this environment and if i go to billing and cost management um i can see that i've spent 10.82 uh and it was just on this project. so if i see in here we can see that the um amazon ecs so the elastic container is the one that used the most. than the elastic uh load balancing. those are the ones i use the most. so these to be careful about, you know, if you don't want to spend any money, obviously you can skip this step. just watch me doing it or explain it to you on a high level.",331,749,759,luJ64trcCwc
54,"just watch me doing it or explain it to you on a high level. uh but if you're willing to get your hands dirty and really learn because it's valuable to know this stuff, then you can you can follow what i'm going to show you. so you have a root account. so this is the one that you create initially and then you create users and those users you give them permission to do certain things. so it's advised to not use your root to develop anything. you just create the root to create users and those users you give them levels of access needed to perform certain task. so for example you're a startup you will have five employees. uh each employee does something different. so based on what they need you'll give them permission to do those things. you don't want to give a permission to anyone to do everything and end up, you know, just skyrocketing your costs and end up paying thousands of dollars in bills. when i do my tutorials, i try to keep the cost as low as possible, but i worked on this for the past 2 3 weeks. so, 10 is not much. obviously, i didn't spend 3 weeks on the deployment, but still, there are things that i learned myself to not keep running overnight either. so, uh we go to the user that i created.",306,759,772,luJ64trcCwc
55,"so, uh we go to the user that i created. so when you're in aws uh in your root account it's best to go to am which is the identity and access management and in there you're going to have your users uh you can put those users into groups for example data people marketing people sales people in this case i have just myself and in in here uh give the level of permissions needed you know like for example it's just me i'm giving administrator access and a few access to a few services that i want i use uh chach is great at helping you figure out what kind of permissions you need and how to work in the console. just send screenshots and it kind of guides you. it did help me a lot. so this is the first and necessary step. uh the second step will be to go to vpc and it's the virtual private cloud which is necessary to run the ecs the elastic container service that we're going to use later. and here we can see the vpcs your vpcs. you will have one by default.",252,772,779,luJ64trcCwc
56,"you will have one by default. i kept it but you can clearly go and create a vpc and you can just just see what what's needed in here and uh you will find good tutorials on this topic but i kept the the default one it's the one that i used myself and in the vpc what's good to know is that the ecs will run inside the vpc we will need at least two subnets subnet is like two uh distinct ip addresses to reach and we will need security groups to allow inbound and outbound traffic. one thing also that i need to to mention here, obviously aws, it's such a massive topic that i can't cover everything, but i'll give you as many nuggets as i can. uh you can see at the top here, i'm in europe, london. um, and this one is because i'm using the servers that are closer to me. so, if you're in the us, you're going to use something closer to you. if you're in asia, you're going to use something closer to you. africa, and so on. um, i don't even know if there's an african one. um, no, i don't see it here. anyway, so you use the one that is closest to you. so that usually have a better, faster performance and all that kind of stuff. so now that we spoke about the uh vpc, we're going to go to the ecs, which is the elastic container service. and this one is very important here. uh, first uh what we're going to do is that we're going to create a cluster.",357,779,793,luJ64trcCwc
57,"uh, first uh what we're going to do is that we're going to create a cluster. and you come in here, you create your cluster by just doing create a cluster. and you will give it a name. uh you will choose the infrastructure. this is very important because we in this project worked with fargate which is serverless which means that we only pay for what we use and every time we make calls it says here pay as you go. uh it says if you have a tiny batch or a big batch and this one is just to avoid all the overhead of managing your servers and ec2 instances is basically uh using servers that aws provides and you pay obviously depending on how big those servers are. so i'm not going to create because i already have one here and in this one if we see uh we can see that it's active. we can see that it has a service. a service is something that we need to create inside of it. we can see the tasks performed and all these things in here. so if you look at the services here, it's very important to have a service. so the service what it does is that it runs tasks and those tasks need to be created beforehand. so think of this ecs as three steps. you have the cluster, the service and the task. and the task you go in here on the left and you define a task. you come in here and you just do a task definition.",345,793,808,luJ64trcCwc
58,"you come in here and you just do a task definition. you can either do it via json or you can do it uh via um just doing it in the console like this or also you can use the cli command to write things programmatically. so basically so i don't go too much into details. i'll show you the one that is already created here and i've created a few revisions but the latest one what it has is that it's using fargate. it's also uh if we look at the json here it's very important. it knows that we're gonna bring the image of the docker from dockerhub in here because it has the path. it also knows that we allowing the port 8000 tcp protocol. and here we're just allowing all the logs, aws logs, everything will be logged for us. and basically just knows that this task execution role basically just allowing some um some some stuff to run on aws on our behalf. we're also here declaring the memory, the cpu, and the operating system that we're going to use. so this you don't have to write it yourself. if when you create you put every detail in in the right order and it creates the json for you. but afterwards, if you feel like you want to uh make some adjustments, you just copy this uh you can go and you create a new revision with json and you then go and maybe upgrade the cpu or downgrade, etc. if we go back to the uh ecs service here, something that is interesting is to look at the update service.",358,808,821,luJ64trcCwc
59,"if we go back to the uh ecs service here, something that is interesting is to look at the update service. and here we have the desired tasks. and if you set it to one, it will be one desired task that is uh available and running. if you do zero, then ecs won't be charging you for nearly anything. uh so that's a mistake that i did overnight. i just kept it for a few days. so make sure here that if you're not using it, you set it to zero or you completely delete it and recreate a new one. but that's an important one. so now since i have it as one, we have a service that is running. and we can see that we have an active service. and if we look at this, we can even see that we had one task. and that's why we're able to access our ui and be able to do predictions is because everything is up and running. so if we go back now in this there's another very essential step in here is to have a load balancer. so if we go to ec2 and ec2 is where we manage servers even though we're not man managing any servers in here we will look at three different things. the first one we're going to look at security groups and here we have created this security group in here that i'm going to click on. and this one is very important because it gives us some inbound rules and and outbound rules as well. so inbound rules we're giving access to these po port ranges.",358,821,837,luJ64trcCwc
60,"so inbound rules we're giving access to these po port ranges. so 80 and 8,000 we're giving them full access for example for this one. and this one has access access just to the vpc and the outbound rule we're giving it access to everything. so it's open to the whole world. and this is very important because when you're going to here go and build the load balancer uh you will need to have those security groups in place. so when you look in here, you're going to see that we're internet facing. we have the vpc. we have two subnets as i mentioned earlier and uh and we're able to run in http80 in here. so this is very important. and lastly, the target group. in the target group, we're going to see if what we've run is healthy or unhealthy. and this is great for debugging. and you can do all the monitoring and health checks and everything in each and every step of it so that you can see if things are working properly or you have to debug something. so when this is all in place, you usually just um you run and you push your model and you go to ecs back again. we can go to ecs and if you have made any changes and you want to push them again, you just go to the cluster, you go to the service and in here you just force a new deployment and it will force the deployment. and then what will happen is that i'll show you briefly the grad ui. if we go back to src main.py, pi.",358,837,853,luJ64trcCwc
61,"if we go back to src main.py, pi. so as i told you we we had it split into two. the fast api and the second phase was uh gradio here. so gradu is a is a python library to create interfaces. and here you see all the interface every basically area that we had and the data that goes into it and uh and basically just the options that you have just drop downs drop downs drop downs. i don't want to bore you too much with this. uh you can look into the code and play around with it. um and uh and then what happens is when you deploy all this and you get the um the url from the alb. so i'll show you when you are in ec2. this is the url that you get. basically you go to load balancer, you click in here and you get this uh this one this url here and that's the one exactly that we're using here. so that one is running. if i remove the ui because you have to include it, it just gives you a status. okay. but in our code, we specify that it has to be slash ui and now it's taking you here. and when you input the numbers and the values, then you're going to get the prediction based on what you did. so now i'm honestly just put in random numbers. so i don't know what's going to be predicted there.",326,853,870,luJ64trcCwc
62,"so i don't know what's going to be predicted there. but uh this is how in reality when new streams of data is common we're going to know that um whether it's the person is likely to churn or the person is likely to stay basically. so we're going to submit and we're going to see that this person is likely to churn based on the information that we've provided in there. so here i didn't look too much into showing you exactly the architecture of aws, but basically we use an application load balancer to handle incoming requests. that's what i showed you how to do. we used ecs fargate as a serverless container running the docker image from docker hub. basically by creating task definition, a cluster and a service and cloud watch. we can use it for monitoring. i didn't show you this, but uh you can use that to keep track of what's going on and monitor the health, the logs and the performance metrics. so everything that we saw in here is how to go from a basic just notebook style let's optimize the accuracy style projects into a real world production ready going from a toz with all the steps that we spoke about from setting the environment from doing the eda from modularizing all into python scripts to using fast api to um to get the uh the model to to be used in the web via https endpoint.",319,870,879,luJ64trcCwc
63,"so everything that we saw in here is how to go from a basic just notebook style let's optimize the accuracy style projects into a real world production ready going from a toz with all the steps that we spoke about from setting the environment from doing the eda from modularizing all into python scripts to using fast api to um to get the uh the model to to be used in the web via https endpoint. points to build a docker container so that we can package the whole projects to be reusable from anywhere to use github action to be able to test run and deploy the whole projects in the cloud and in the end also adding a ui so that it's nice and pleasant to play around with our model. so you've literally learned how to do an end toend machine learning project. this is a churn uh classification problem, but you've learned so many things that are reproducible and reusable in so many different projects. and i hope that you found this interesting. please leave in the comment section if there are anything that you want me to expand on in other videos. maybe just specific topics you want me to talk about docker or github actions and how you can deal with them. or maybe a specific video on aws uh which will kind of force me to also try and explain it to you and learn more about each and everything. i will be doing a series of projects like this where i go end to end in projects that can grow in complexity.",352,879,887,luJ64trcCwc
64,"i will be doing a series of projects like this where i go end to end in projects that can grow in complexity. i will try and keep it as real as possible so that you can learn this and use it in your next career project or in your freelancing or in your job. and i hope that you really benefited from this. if you did, please remember to like this video, share it with your friends, comment if you want me to do anything similar or there is something that wasn't that good or something that you want me to dive more into. i tried to keep it as compact as i could. 1 hour and 40 minutes, that's quite a lot. or 1 hours and 20, 30 minutes is quite a lot, but i tried my best to give you something that is quite complex, but a big overview of it. and you have access to the code to play around and build these things yourself. so, if you find any problems, just leave them in the comments and i'll try and help you uh on my end. i hope you enjoyed it and i'll see you on the next",263,887,896,luJ64trcCwc
0,"so, if you've been wondering how you can take your local ai demo and turn it into an actual proper ai application with a proper backend, then this video is for you. i'm going to walk you through my fast api tutorial where i will show you how we implement these ai backends for all of the client projects that we work on at data luminina. in this tutorial, i'm going to specifically focus on the very basics that you need to understand in order to build ai backends. so there is really a lot that you can do with fast api. if you if you for example want to build out an entire web application, there's a lot more that you need to know, but actually when you're building ai backends, it's actually quite simple. and fast api is currently really becoming the go-to library to build api endpoints with for ai applications. so it's very worthwhile learning about this library. so i am going to explain essentially how we can do the following. so by the end of this video, you'll understand how you essentially can connect from any type of application or or web hook or whatever to an api endpoint layer that we are going to create where we can then validate that incoming data and then for example process it using ai logic or send it to an llm. so that's also the flow of operations when you are working with an api and when you're working with the back end like this.",335,0,9,-IaCV5-mlSk
1,"so that's also the flow of operations when you are working with an api and when you're working with the back end like this. so really the goal of setting up this api endpoint is to serve as an integration layer so that you can turn your local demo where you run locally on your laptop you run some examples you can now actually expose that logic and make it available in an application so that other users can actually start to use it. that's the goal of integrating an api. all right so that with that out of the way what i want to do right now is i want to first go through this quick start with you and you can follow along if you want. that's what i recommend if you really want to learn this. and then we'll cover the four files that are uh that are in here. and they're very simple. this is a basic setup. this is really all you need to know. and now in this video, i am assuming you are already familiar with the basic api concept. so i won't go into all of the specific concepts about what is an endpoint, get post request. so we can keep this tutorial lean and effective. so if you're entirely new and have never worked with an api, it might make sense to do a little bit of digging, for example, with jbt to ask, okay, what is an api? how does it work? and where does it fit into an entire architecture of an application? so let's do that. we have the repository. then we did already did the uc uv sync.",365,9,26,-IaCV5-mlSk
2,"then we did already did the uc uv sync. and now i can come in here and i'm going to copy paste this command over here. i'll explain what it does in a bit, but for now i'm just going to cd into the app. let's see. we're in here. all right. so we can come in here and run uvorn main app reload. so this is now going to spin up our server and this should be running on localhost port 8000. right now we can now access our api by for example going to localhost port 8000 and then going to the docs. and this is one super nice feature of fast api because by just defining your api endpoint you get documentation like this. and now you might be wondering okay what's going on? this is cool. it seems to be working. we have some documentation, but that's the quick start and i want to start there first so that we can now start to understand all of the different building blocks that we're working with right now and how all of this works. so let's come back to the tutorial over here. so first of all, let's understand what we're doing with this command over here and then look into the files. so whenever we spin up our uh fast api application, we can use uicorn which is an asgi server that actually runs the application. so fast api defines really your api structure. that's what we're going to do inside the python files. but uicorn is a server that handles the http connections and really serves your application.",354,26,45,-IaCV5-mlSk
3,"but uicorn is a server that handles the http connections and really serves your application. so it's really facilitates the communication between all of the different components here. and what we're doing over here, so we're using that uvicorn command and then we say main and main refers to the main file that we have in here. and the app refers to the variable app that we're creating over here. so we're uh importing the fast api library and uh we're initiating that using the app variable. so that's what we're seeing over here. and the d-reload simply means that as we go about changing and adding our files over here that this automatically reloads. so if you're following along, congratulations. you have now set up your first fast api endpoint. so now let's dig a little bit deeper. so really with the idea of starting with the end in mind, showing you end to end of what we're going to do. now let's drill down and see how all of these components work together. and i have split the application up into uh three parts. so we have main, router, and endpoint. and i did that on purpose to emulate essentially how we are using this in our production applications. so already out of the box if you follow this tutorial you'll get a boiler plate that you can reuse because in a lot of tutorials you just find they have one big file and they add everything there just for the sake of simplicity. but i'm going to show you how we can make this more modular.",349,45,61,-IaCV5-mlSk
4,"but i'm going to show you how we can make this more modular. so out of the box this already is something that you can actually put into a project. so let's start with the main.py. so that's really the entry point over here. and as you can see, there's not much going on over here. that's also really what i love about fast api. it's just super simple. it's super lightweight. and there's really not that much that you need to actually understand about the library if you're building lightweight uh ai endpoints like this. so like i've mentioned, we import the fast api library. and all we're doing right now is we're including a router. so you can see from router we import the process router and that is simply the next file that we're going to go to. so what this essentially allows us to do so we keep our main we keep our main.py very lean and minimal. so there's not much going on over here and essentially from here we route we essentially create routers so that we can move the data to the right uh api endpoints. so now let's get into that file. so that's the router. so route incoming requests to the appropriate endpoint handlers. so let's have a look at what that looks like. so come in here you can see that now from fast api we import an api router and we we initiate that router over here and then we essentially include that router and you see one more import over here that we're importing from the endpoint.",349,61,79,-IaCV5-mlSk
5,so come in here you can see that now from fast api we import an api router and we we initiate that router over here and then we essentially include that router and you see one more import over here that we're importing from the endpoint. so what this essentially does is now we can say we want to create an endpoint forward events so that we have our application running in this case on localhost host 8000 but when you put this into production this is going to be a url and then port 8000 and we're going to mention forward events. so that's going to be the url that then our application needs to send information to. i'll show that in a bit on how that actually on what that actually looks like and how that can come together. now just know that we're creating a router as a way to say okay everything that's in this endpoint we use the prefix forward slashevents and we can also give it some text but uh the prefix is really the most important one. so then we get to the endpoint.py so this defines the data model and the endpoint logic for the processing event. this is really the only part right now where we implement some custom logic. this right now is all pretty much boilerplate. and here we can play a little bit with let's see let me close this down a little bit. here we can actually start to implement our logic.,331,79,88,-IaCV5-mlSk
6,"here we can actually start to implement our logic. so now if we go over here and we have a look at the files or the code that is uh that lives within this file we can see it's also very lean and we start by initiating the router. again, this is all just to follow the the same syntax that we can use to essentially say, okay, we want to have this event endpoint that we're creating over here and we want to tie it to the router. now over here you can see that we have a pyic model. so here is that pyantic integration is coming in. so out of the box if you install fast api, pyantic will be installed as well. and here i have defined an event schema. so let's say you're building an ai application and the incoming data looks something like this where you have an event id, you have an event type and you have some data in here which could be a dictionary. so let's say you're integrating with an email surface or with your gmail and you set up a web hook. so every time you get an email that is now sent to or that is the information that we want to send to this api endpoint. so then the first thing that you want to do is figure out what that schema looks like and implement it into a pyantic base model. now again for the sake of simplicity we're just using this schema over here but i'm just explaining how this would actually what this would actually look like in the real world.",360,88,99,-IaCV5-mlSk
7,"now again for the sake of simplicity we're just using this schema over here but i'm just explaining how this would actually what this would actually look like in the real world. so you would first figure out your data source implement the pantic base model and then go from there. and now if we then go to the actual endpoint. so now we get to the actual python logic, python function where we can decide with our application about how we want to process this data. so we have a simple function called handle event and we plug some data in there and we make sure that we set that uh we set the data type to the event schema that we have just specified. so in a bit we'll see how that works. but because of this we can now validate the incoming data. whereas if the incoming data does not match this schema, our endpoint will give us an error. for the sake of demonstration, i'm just going to print this data. but this logic that you apply over here, this should be the starting point of your ai logic. so if you're for example already working on an ai demo, you might have a simple processing pipeline or workflow where you make multiple llm calls, you implement some guardrails, you do rack, whatever, it doesn't really matter. all of your logic that you already implemented, this is the entry point for you to then kick that off.",326,99,110,-IaCV5-mlSk
8,"all of your logic that you already implemented, this is the entry point for you to then kick that off. so you most likely want to wrap that into some kind of workflow or class or function where you have a single entry point where data is coming in and you can send it to your application and now the whole thing starts to work and then simply what we do is we return a response. so we import that response from from starlet and that is another um library that's working in the back end when using fast api. and now we essentially give an http status code of accepted. so if if all goes well, this will return a 200 or 202 message status. and then it will essentially ask the content. it will say we have the data received. so that's pretty much all you need to know right now about how you set up endpoints. you initiate the app, you create a routing point so that now we have forward events and then we have the endpoint in here. and now everything that's in here essentially all the uh events or sorry all the endpoints they they will live with the prefix forward events. so that if now we use this forward slash here this can be accessed by simply using the forward slashevents. and so now we tag the endpoint with decorator over here and this is just following the fast api syntax. so this now becomes an endpoint. so i have another file in here which is the request.py.",344,110,123,-IaCV5-mlSk
9,"so i have another file in here which is the request.py. so what this is doing is this is using the uh request library to send an an event to send some data to our endpoint. so for this to work you need to make sure that your application is running. so in the terminal below here this should be running. and now what we can do i'm going to run through this in the interactive window. so we can really do this step by step. first of all, the url is we're running this on localhost port 8000 and we discovered that our endpoint is available at forward slashevents. so we have that as our url. now we are going to create an event data and this should match the pyantic model that we have specified. i will show in a bit what happens if we don't do that. if we right now come in here and we have our event data so we can have a look at that. this is all nice and neat in the correct format and we add some headers to this. what we can now do is we can now use the request library to do a post request. we have the url which we have over here and then we have our json data which we dump. so this is a python dictionary and we dump that to json and we include the headers and we can now send the response.",323,123,137,-IaCV5-mlSk
10,"so this is a python dictionary and we dump that to json and we include the headers and we can now send the response. so we can also see right now below here in the terminal that we print our data that is coming in and that's because in the endpoint we simply do a print print statement. now if we then come back to our response over here we can see okay we get a 202 status code the data is received and we get some additional metadata to see what's going on behind the scenes. so we have now created a simple setup where let me scroll up. we can now if this is available, if this endpoint is available, we can now send data to this fast api endpoint. it will validate the data and we can then send it over to our a ai or llm service to process that. so it's really about creating a communication layer that we need. and now of course the api layer is just one component of your entire ai infrastructure and system that you need in order to really build production ready ai applications. and if you want to learn how we do that at data luminina for all of the client projects that we work on, you might want to check out our genai launchpad, the first link in the description, you can get access to our entire production framework that's available in a private github repository.",323,137,145,-IaCV5-mlSk
11,"and if you want to learn how we do that at data luminina for all of the client projects that we work on, you might want to check out our genai launchpad, the first link in the description, you can get access to our entire production framework that's available in a private github repository. and also, you'll get access to all of the training tutorials that we have for this to really show you how you can set it up from a to c and start shipping real production ready ai applications. let me show you one more time what happens if, for example, this is not a dictionary, but this is a string. so now we do it like this. now we get a status code 422 because this is an unprocessable entity because if we look in the endpoint data is coming in we set the data we match it against the event schema but the event schema is not matching right now because this is uh a string instead of a dictionary. so now we can also start to catch that error and maybe also to the application or to to the user to instruct what they need to adjust. so for example here you can also see the input should be a valid dictionary.",283,145,151,-IaCV5-mlSk
12,"so for example here you can also see the input should be a valid dictionary. so that's out of the box such a nice feature of fast api that it comes integrated with paidic and as you know if you've been building with uh language models you know one of the core methods to make llm applications more reliable is to work with structured output and most of the structured output approaches that you'll find within the python ecosystem are all based around pyentic and that's why we really love working with fast api because it just follows that same methodology and if you go back to the documentation, you can also find how you can create uh synchronous versus asynchronous endpoints in fast api. and it is super simple. so in the current example that we have over here, this is a synchronous endpoint. but we can also make that async simply by turning the endpoint into an asynchronous functions and then essentially waiting awaiting the processing logic. so this is super helpful uh or super simple as well with fast api and this will already make your application more scalable. all right. and then what if you want to protect your api endpoint which you should do if you're going to put this into production. here is an exercise for you to go through how you on how you can set up security using a bearer token. so i have an example over here how you can set it up and how you can also work with fast api and with the imports in here.",347,151,160,-IaCV5-mlSk
13,"so i have an example over here how you can set it up and how you can also work with fast api and with the imports in here. create a security token or an api token and then how you can uh send an authenticated request. so that's going to be an exercise for you to implement on your own to not only better understand the code that we have over here but also to understand at a deeper level like okay how do all of these components work together you can test some things change some things and then also implement the security components so that now you fully understand how to use fast api to start really serve as the starting point for your ai backends. all right and then that's it for this video. so, if you found it helpful, please leave a like and also consider subscribing. and then, since you're into building ai systems because you watch this video, you might want to check out this video next where i cover my entire strategy to build effective ai agents in pure python without relying on any other external frameworks. so go check that out right",260,160,166,-IaCV5-mlSk
0,music hello everybody and welcome to another youtube video so in today's video i'm gonna be showing you the python web framework known as fast api now as the name suggests what this web framework lets you do is create apis in python the reason it's named fast api is because you can make apis very very quickly in fact we can write our first api in about four or five lines of code and then these apis are very quick themselves they have very high performance with that said this video is not designed for absolute beginners you should have some knowledge with python however i will explain to you what an api is i'll talk about the common api kind of method so get post uh put patch update all of those kind of ones and i will discuss json and just kind of how an api works if you're unfamiliar with apis don't worry but you should have some knowledge of python to be able to follow along so that said let's dive in after a quick word from our sponsor before we get started i need to thank intel for sponsoring this video and having me talk to you about their openvino toolkit and the intel dev cloud for the edge the openvino toolkit is free software that helps developers and data scientists speed up computer vision and ai workloads streamline deep learning inference and deployments and enable heterogeneous execution across intel's platform from edge to cloud on the openvino website you can find free resources like a tutorial on how to perform a style transfer using a deep learning model and,358,0,0,-ykeT6kk4bk
1,a cheat sheet containing everything you need to know to get your ai application up and running as fast as possible if you want to get started with the openvino toolkit you can use intel dev cloud for the edge this is a cloud-based development sandbox that gives developers access to the latest intel edge hardware and software to build test and prototype their ai applications with these technologies and resources you can build optimize and deploy your applications with ease start building your ai and machine learning applications today by clicking the link in the description and taking advantage of the openvino toolkit thanks again to intel for sponsoring this video alright so as we enter the video here the first thing i'll mention is that there is timestamps linked below if you want to skip to a certain part in the video or you want to kind of skip through the more beginner explanations maybe if you're more advanced regardless i'm going to start here by discussing kind of the core advantages of fast api and why you may actually want to use it so fast api is very fast to make an api with right the name is very fitting and one of the reasons for that is that when you create an api in fast api you're actually going to be defining the types of all of the data that your api is expecting so traditionally when you write an api or some web framework or something or web app whatever in python you don't actually explicitly set what type all of the information that your endpoints so kind of a url,358,0,0,-ykeT6kk4bk
2,on the server are going to be accepting now what that means is you have to do a ton of data validation you have to check to ensure that you know you actually got an integer you got a stringer you got some json object or something like that and while this is kind of a lot of grunt work and you're just writing a ton of stuff to essentially check that the information that was sent to your api is correct now in fast api all of this is actually automatically done for you so if someone sends the wrong piece or wrong type of information to your api endpoint it will automatically return to them kind of an error message saying hey you know this was supposed to be an integer and i got a string whatever but that's kind of the first main advantage of fast api is it does all of this data validation for you because when you create endpoints with fast api you're going to define explicitly what type all of the information that's going to be past that endpoint will be so whether that's a path parameter a query parameter or actually the kind of uh request body you're going to define exactly what that should look like so fast api can automatically handle the data validation next thing that's great about fast api is that it auto documents your entire api so since you are actually giving all of the types for what's expected for the api fast api can automatically generate documentation that also works as kind of like a test script so i'll show you this,358,0,0,-ykeT6kk4bk
3,in a second but it actually generates a web page that you or maybe your front end engineer if someone else is working on this could go to to see all the api endpoints and exactly what they expect and any description or information that you would have provided for them and then last thing since you're defining the types of all of these kind of all this information related to the endpoints you're gonna get really good auto completion so if you're working in an actual ide like vs code or pycharm i'm not doing that right here i'm in sublime text then you get really good auto completion and just better completions that you would typically have because of the fact you're defining these types so anyways enough talking let's get into it the first thing we need to do is install fast api so if you're on windows open up command prompt if you are on mac or linux open up terminal and you're going to type the following command pip install fast api to install the fast api module if for some reason this doesn't work for you try the following command you can see i already have this uh requirement satisfied try pip 3 install fast api if that doesn't work for you try python hyphen m pip install fast api if that doesn't work for you try python 3 hyphen m pip install if none of those work for you then go to the links in the description i'll have a video for mac and a video for windows showing you how to fix this command now once we've installed,358,0,0,-ykeT6kk4bk
4,fast api i'm going to assume you've done that at this point we're going to install something called uv corn now this is what we're actually going to use to run our api kind of as like a web server which you'll see in a minute but regardless you need uv core as well again if this command doesn't work try those sequences pip 3 python m so on and so forth all right so we can leave the command prompt open we'll be using that later then what we need to do is open up a python file in some editor for me i'm working in some directory this on my desktop it's called fast api you can kind of see it at the top left hand corner of my screen for the path for this file i just have a python file called working.pi obviously place yours wherever you want name it whatever you want but make sure you know where it is because you are going to have to kind of access that all right so now what we're going to do is make sure our fast api installation is working so we're just going to import fast api save our script run it i'm running mine with ctrl b in sublime text and you can see that we got no errors and so we're all good fast api is installed so now we're going to write kind of our first api i'll go through this fairly quickly just showing you how to set this up and then i'll kind of walk through each line and discuss what i did so i'm going,358,0,0,-ykeT6kk4bk
5,to say from fast api import and then make sure you look at the capitalization here fast api this is the first thing we need to do we're going to import this fast api class module whatever and then we're going to say app is equal to and then fast api with an open parenthesis and a closing parenthesis what this will do is creates kind of i guess like let's say an api object something that's going to initialize our api and say okay this app variable right here this is telling us we just created our first fast api regardless is what you always need to do when you are starting uh working with fast api okay so now that we have this we can create an end point and at this point in time i'm going to stop for a second quickly discuss what an end point is so if you hear me say endpoint or root or route or whatever what that really means is something like slash hello slash you know get hyphen item so when when you're talking about an endpoint you have a kind of base server url in our case our base url is going to be localhost because we're not distributing this app we're kind of i guess deploying this app we're just hosting it on our local machine so the base url would be localhost and then the endpoint would be slash hello and so you have all of these different endpoints and when you go to these different endpoints well different stuff happens right different information will be sent back to you maybe you're sending,358,0,0,-ykeT6kk4bk
6,information to an endpoint but regardless that's what i mean by endpoint slash something essentially right it's kind of like the ending path after the main domain so if you were looking at you know like facebook.com an endpoint would be like slash home right that would be one of the endpoints for facebook.com regardless that's an endpoint so to create an endpoint what you do is you say app dot and then the method that this endpoint is going to kind of accept or that is going to be so when we talk about http hyper text transfer protocol i believe that's what that stands for at least we have some main different methods so whenever you set up an endpoint you can set it up to kind of be a different method if that makes sense i'm having trouble coming up with the exact word for it but the idea is we have these kind of core http methods and they all mean something different now one of the main ones is get we have post we have put and then we have delete and we have a few other ones as well but these are kind of the four core ones now when you set up a get request or you have an endpoint that has a get method what this means is that this endpoint is going to be returning information that's kind of all it's doing is you're asking this endpoint to get something for you and return it to you that's kind of the get http method now when we talk about post this actually means that you're going to,358,0,0,-ykeT6kk4bk
7,be sending information to the post endpoint or this end point here that is method post will be creating something new whenever you're creating data kind of adding something to the database you're doing this with a post request so you would be maybe posting a new user login or something right or a new user sign up right you would post that to an endpoint because you're going to create a new user in the database because they just registered or signed up or whatever and then put this is to actually update something that's already existing in the database just you know kind of modify information essentially and then delete well that's straightforward you're deleting something getting rid of information now of course there's a few more but these are the core ones the ones that i'll show you so depending on the method you want your endpoint to be you're going to say app dot and then the method so in this case i'm going to go with get and then you're going to put the end point right here so i'm going to say slash and make sure sorry before the app you put this at symbol right here i'll discuss why we need that in a second so for now i'm going to set up what i'm going to call my home endpoint and what this is going to do is just return some data that says test so what i've just done is i've said at app.get i've defined my kind of root or endpoint as slash i've then created a function i've called this home make sure that your,358,0,0,-ykeT6kk4bk
8,kind of root or endpoint is right above the function that will be triggered when you go to this root and then what you're going to do is return some python dictionary this will be kind of the data or the information that's going to be returned when you go to this endpoint so hopefully that kind of makes sense but if we go to slash now once we kind of run this web server what will happen is the information data with the key test will be returned so this is how you kind of set up a root you say at app dot whatever the method is again the root you define a function right underneath it you can name this function whatever you want and then you return some information okay there we go so let me just show you how we can run this then i'll talk about what's known as json and kind of what an api is and actually how that works so what i'm going to do here is go to my command prompt and i need to change directories into the folder or into the directory where my python script is so in this case i'm on windows i'm starting in my home folder so i'm going to go to cd desktop and then i'm going to cd into fast api then what i'm going to say is uv cord this is the thing we just installed and i'm going to say the name of my file which is working notice i'm not adding the dot pi you don't want the dot pi so make sure you don't,358,0,0,-ykeT6kk4bk
9,have that and then oops what did i just do here okay i've messed up my cursor somehow anyways uv corn working colon you're gonna then say app app being the name of the variable that you have fast api stored in and then space hyphen hyphen reload now what this hyphen hyphen reload will do is tell uvcorn to constantly reload the web server every single time you make a change to the python file that's kind of storing the api so uvcorn the name of your python file mine is working notice i don't have the dot pi extension then the name of the variable storing fast api colon colon or sorry hyphen hyphen reload and then press enter and you should see that it says application startup complete and it shows you the url right here that you need to go to to access this website or access this api sorry so in this case it's http colon 127.0.0.1 colon 8000 pretty much the same thing as localhost so what i'm going to do now is go to my browser and say http slash 127.0.0 colon8000 and i'm not going to go to the docs page quite yet and here you can see that we get this data saying test because we went to the slash endpoint so when you just put slash that means if you go to any endpoint essentially on the server like if you go to just the default one so you don't have anything after then it's going to return or uh treat it as if you're at this endpoint so i think i think you guys get,358,0,0,-ykeT6kk4bk
10,what i'm saying i'm kind of mumbling around here but the point is that like this just that is equivalent to the endpoint slash regardless you can see we're getting data test so now let's do a quick change this i'm going to say testing i'm going to save that now i'm going to go here and refresh and notice it says testing that means our reloading is kind of working great now what you guys saw i accidentally loaded this docs page so we might as well just go to it right now if you go to your uh server url which is just 127.001 colony thousand then slash docs it will have automatically generated documentation for your api so at this point we only have one endpoint it is slash the function name is home right so that's why it's saying home and then if you press on this it says okay it doesn't take any parameters and what is it if we try it out so we can try it out by pressing that button and press execute we can kind of see the sample response body we can see the request that we sent to get this response this is a way that you can actually test your api from this docs page right here alright so i'm going to quickly discuss what an api is so an api stands for application programming interface and really what an api is is some web service that provides an interface to applications to manipulate and retrieve information so if we're talking about something like amazon amazon almost definitely i don't know the internal makeup,358,0,0,-ykeT6kk4bk
11,of amazon but almost definitely has an api now they probably have multiple apis but one of their apis may be responsible for handling their kind of inventory system right figuring out what items are in stock what items are not in stock how many items are in stock how much they cost whatever we can think of a common api that amazon may have as an inventory api now this api here is separate from the different front ends that display this information so if you're talking about amazon they have a web app right they have a mobile application they have something that works on like alexa or google home or whatever they have all of these different services that all rely on the same underlying information they need to know inventory they need to know what's in stock and so rather than writing you know five inventory management systems they would write one in the form of an api application programming interface and now any of their applications that are used by users can access this same information by sending requests to the api so when you go to the you know website and you look up i don't know let's say graphics card right none of those are in stock right now what's going to happen is there will be a request that is sent to the amazon api that says hey i'm looking for graphics cards right and then it will return all of this information to the front end and display all of that for you and then when you click on a specific graphics card it will then say,358,0,0,-ykeT6kk4bk
12,oh i need all of the information related to this graphics card again a request will be sent to the api the api will then send all of that information to the front and so hopefully that gives you an idea of kind of how the communication works but the same thing that happens on the mobile app right it sends a request to the api and this way you don't need like five different backends to handle every single one of your applications and all of their information you have one api responsible for kind of distributing and giving you all of the information that you need and this is just good practice whenever you're kind of writing code it's a very good idea to separate your front end and your back end so if at any point in time you want to create a different kind of representation of your data on the front end you can just use the same api and the only thing you need to change is kind of the way that you're displaying so hopefully that gives you an idea of what an api is and kind of how that works now i don't know the internal makeup of amazon's web services and all of that kind of stuff so that could be completely wrong what i described but that's like an example of where an api might make sense to use with that said what i need to quickly talk about is the data that apis kind of exchange or that i guess anything over http really exchanges so right here we returned a python dictionary now this,358,0,0,-ykeT6kk4bk
13,python dictionary is actually automatically converted to something known as a json javascript object notation as soon as it is returned from this function so fast api actually handles jsonifying all of our information so we can work with strictly python types in our actual api now this might seem a little confusing but whenever you are returning information from an endpoint it's kind of standard that that information is in the format of json now you don't really have to know what json is or what it looks like it's just a little bit of a different syntax than kind of a standard python dictionary and well you can just understand here that a fast api any data that you return from your endpoints is automatically converted to json and so whoever receives it on the other end may have data that looks a little bit different than what you returned just because it's all going to be converted from kind of vanilla python types to this json format now same thing when these endpoints are receiving information whenever they get like maybe a query parameter they get some request body that will come in as json and be converted into vanilla python types so we don't need to worry about kind of dj sonifying or jsonifying all of our data just make sure you understand that all of the data exchange kind of between apis is in the format of json we're just lucky here that fast api can convert all of this to kind of vanilla python types for us anyways that was a lot of talking i hope i didn't bore you,358,0,0,-ykeT6kk4bk
14,guys too much with that but if you guys are beginners hopefully that helped you out let me know in the comments now let's continue working here and let's create a few more endpoints so i'm going to make a new endpoint here i'm going to say app.get and let's make this slash about now what this is going to do i'm going to say define about and oops if i can type about properly and here i'm going to just return some data and i will return is data and about just so that we have another endpoint so we can see kind of how this works when we look at the auto documentation so now my web server should have refreshed if i go to docs now notice i have another endpoint called about right and if i press on this i can press try it out i can press execute and when i execute that notice that we get our data we get about now same thing here if i go to slash about we can see that we have data about awesome so that is kind of two examples of using an endpoint that has the get method now we also can use an endpoint that has the post method the patch method and the delete method but before we do that i want to show you something known as path parameters and query parameters and to do that we're going to kind of get into our first example here so i want to treat this api that we're creating as kind of an inventory management system like the example i gave,358,0,0,-ykeT6kk4bk
15,you from amazon so i'm going to say inventory i think i spoke that correctly is equal to and we're going to make this a dictionary now inside of here we're going to store a bunch of different like stock right all of the different items that we may have in stock and all of these items will have a unique id so the key in this dictionary will be their id so i'm going to say you know item with id1 the name of this item is a let's just say milk maybe we'll treat this like a grocery store or something the price of this item is i don't know how much is milk let's say 3.99 and we will say maybe the expiry uh this could be actually i don't want to do like a date time thing right now let's do something else milk maybe we'll just say brand and actually i don't even know like a milk brand we'll just say uh regular now of course there could be a lot more information we're just kind of doing this simply for now because we don't need to go into anything like crazy advanced okay regardless we have name we have price we have brand and then we have an id for this item so what i'm going to do is set up an endpoint that can retrieve for us item information based on its id so what i'm going to do is say at app.get this is going to be a get method for this endpoint i'm going to say get hyphen item and then what i want is the user,358,0,0,-ykeT6kk4bk
16,to actually pass me some id for this item and so what i'm going to do is put inside of curly braces item underscore id now what this means when i do this is that whatever is here could be anything right this item id could literally be anything and based on what this item id is i'm going to return something different from this endpoint so now i'm going to say define get underscore item and now what i need to do is define a variable to represent this item id so i'm going to say item underscore id needs to match the name here colon int now what this is is a type hint in python whenever you do a colon beside a parameter and then you define the type it's known as a type hint and the reason we do this in fast api is to tell fast api that this item id is supposed to be an integer so if you try to pass something that is not an integer to this endpoint for the item id it will automatically return to you we don't have to do this fast api will automatically return an error message saying hey this wasn't an integer it needs to be an integer and i'll show you how that works but this is our first example of what's known as a path parameter so now what i'm going to do is just return and i'm going to return my inventory at the item underscore id so let's just go ahead and have a look at this and see how this works i'm going to go to,358,0,0,-ykeT6kk4bk
17,now the end point slash and this was get hyphen item and then slash one now when i do that notice it gives me all of the information relating to that item so i have the name which is milk price 399 brand regular awesome but now if i try to go to something that is 2 we get an internal server error the reason we get that is there is no item with id2 and so this line right here cause an error i'll show you how we fixed that after but just want to show you that and then finally if i try to do something like go to milk notice that we get this detailed error message saying hey this value is not a valid integer you need to give us a valid integer and even tells us the item id is the thing that is incorrect and so that is how you set up a kind of path parameter now you can also set up multiple path parameters maybe we have items that have the same id or maybe we just want some more information from this user this example is going to make too much sense but i'll just show you how this works we can take an item id and we can take something like i don't know maybe a name right now that we have this name here we would need to go inside of our uh what do you call it uh parameters here and define name colon string saying that okay this name right here this is expected to be a string and then what we could,358,0,0,-ykeT6kk4bk
18,simply do is we could return inventory item id and maybe we could add something to this i'm trying to think how it would do this i can say dot update and actually no this isn't going to quite work uh there's not really a way for me to return the name string that's going to make sense so anyways i just want to show you we could take multiple path parameters so now if i go here and i refresh this so let's go get item 1 slash test notice that this still works this is totally fine we took another path parameter in and well all is good so hopefully that kind of makes sense but this is how you take multiple path parameters inside of your kind of endpoint all right so sorry for the abrupt cut i had some issue with my editing software anyways what i'm going to show you now is how we can add some more detail to our path parameters so i'm going to remove name because we don't really want this one anymore so i'll get rid of this from the actual parameters and now what i'm going to show you is something known as path so we're going to import this function called path and what we can do is set our item id colon int equal to and then this path kind of function right here now what this is going to do is allow us to add some more detail or kind of some more enforcement or constraints on our actual path parameter so for example if i wanted to add a description to,358,0,0,-ykeT6kk4bk
19,this path parameter to tell the user what this actually is like the information they should pass for item id i could say something like description is equal to and then i can write a description and for my description i'll just say the id of the item you'd like to view okay and then one thing here before i do description i always need to give a default value for this so i'm going to say none and then description the id of the item you'd like to view now i understand this might look a little bit weird but whenever you use these type of functions inside of the parameters for an endpoint you always have to start with the default value for this parameter so if item id wasn't passed what should this default to in this case it's going to default to none so if you don't pass an item id by default it will be none however you'll see since we're talking about path parameters here it is actually required that you pass an item id so we will never end up using the default value i know this might be a little bit confusing this will make more sense once we move on to the next thing which is the query parameters which can be optional regardless i'm going to put none here i'm going to say description equals the id of the item you'd like to view now if i go back here and let's just refresh this so get item one you can see all is good and if now i go to my docs you can see,358,0,0,-ykeT6kk4bk
20,that says get item item id and notice that now there's actually a description it says the id of the item you'd like to view because we added that description inside of this kind of path thing right here now what we can also do is have some constraints on item id to make sure it's say greater than one or greater than some value so what i'm going to do is say gt this stands for greater than and then i'm going to say this is equal to and make it equal to zero so now what this is saying is okay this item id must be greater than zero now another few ones you can use here is lt that stands for less than that means this must be less than zero you can do l e that's less than or equal to and then you can do ge that's greater than or equal to and you can do kind of any combination of them so i could do maybe uh let's say g t zero and lt equals two so now the only valid thing you can pass is an id of one if you pass anything else you're going to get an error so let's test this out let's first just refresh this here and i don't think it actually tells me the greater than like enforcement here but if i go to slash get hyphen item slash two notice we get a problem it says message and sure this value is less than two right because uh this did not meet the less than constraint now if i do one all,358,0,0,-ykeT6kk4bk
21,is good and if i try to do zero we get an error here saying no this is not valid make sure this number is greater than zero hopefully that makes sense but that is kind of the basics of using this path thing right here okay so now we've talked about path parameters the next thing to talk about is query parameters so a query parameter is something that comes after the question mark in a url so sometimes you'll see something like i don't know maybe let's go facebook.com and then some endpoint slash home and then there's something like question mark and it says like you know key equals whatever it says like redirect equals and then maybe some page and the thing is this is what's known as a query parameter so whenever you have a question mark and then you have some variable name equals and then some value this is a query parameter and you can have multiple of them let's just make the redirect like slash tim and then we would do an ampersand and then we would say you know msg equals fit and now we have two query parameters redirect which is equal to sim and msg which is equal to fail so how do we accept query parameters for our endpoint so i'm going to make a new endpoint here and show you how we do this i'm going to say app.get and i will make this get by name get by colon name and what this is going to accept is one query parameter and this query parameter is going to be the name of,358,0,0,-ykeT6kk4bk
22,the item that we want to kind of retrieve and so what i'm going to do here is say name colon str now what this is saying is okay we are going to accept one query parameter named name so by default if it does not see this variable that you've defined as a parameter in the path to the endpoint it will by default be a query parameter so this means we are looking to accept a parameter called name which is equal to a string so i'll show you how this works but that's kind of what this means and now what we would need to do inside of here is we need to look through our inventory and find something that has the name of whatever the name is that it was passed so what i'm going to do is make a for loop here i'm going to say 4 item in inventory and actually this should be 4 id in inventory actually i can't call it id i'll call it item underscore id i'll say for item id in inventory if inventory at item underscore id at name is equal to the name then what we will do is return inventory at item underscore id otherwise so if we get to the end of this and that doesn't work we'll return something that says data not found okay so now we've created an endpoint it's taking one query parameter named name so let me show you how we would actually call this endpoint so if i go here now and i change this to be get what do we call this get,358,0,0,-ykeT6kk4bk
23,by name so get by hyphen name and then i do a question mark and i say name is equal to and then milk notice that this works and we get the item that has the name milk now if i make this equal to something else i say name equals tim we get data not found because well there is no inventory item that has the name tim now if we don't pass any query parameter and we just do this it's telling us hey this field is required we need a query parameter you can't call this endpoint unless you have this query parameter name and we can add multiple query parameters as well we could add like price we could add a brand whatever if we wanted to do that and so anyways now let me show you how we can add more detail to our query parameters and how we can make them potentially optional so right now we put name in here it's equal to a string and this by default is a required query parameter but maybe we don't want this to be required maybe we want it so that you can call this endpoint without this query parameter in that case we would simply set this equal to none and now what this means is since this has a default value this query parameter is no longer required it automatically becomes optional so if i save this now and i go back and i just run this notice that this works we don't get an error anymore because now this query parameter name is strictly optional we can pass,358,0,0,-ykeT6kk4bk
24,it but we don't have to and by the way it's recommended from the fast api docs that you do the following when you have an optional parameter you say from and typing import and then you're going to import optional and then you make this type here optional string now you don't have to do this you saw that this worked when we didn't do this but this is just going to give better auto completion for your editor when you decide to do this so optional is strictly for like yourself like the developer it just makes it easier to get better autocomplete when you're writing code here you do optional string to note that yes this parameter here name is indeed optional so if i run this you'll see when i save this this still works this is totally fine okay so hopefully that's clear that's how you do a query parameter just to show you quickly we could do another query parameter i could say let's just make this one test maybe this would be an int and maybe we want this one to be mandatory you do have to pass test so now that i've done this this means that if i refresh this now let me go here and just just hit enter oh what is this name string non-default argument falls ah okay it's actually a good error to run into let me discuss this so we just got an error here saying that a non-default argument came after a default argument so for fast api it does not matter the order in which you kind of write out,358,0,0,-ykeT6kk4bk
25,these different parameters so it doesn't matter if you have like you know item id first or if you have name first if you have test first fast api will be able to figure it out doesn't matter what order you put it in however for python it actually gets kind of mad when you put something that's not or put something that is mandatory after something that's not mandatory and so the way to fix this is to well you could reorder this put test first when i say that i mean we could do test like that or what we can do is simply add an asterisk like this and then do a comma now if we do this everything will work essentially this is uh it's kind of hard to explain exactly what this does but this says okay let this function accept unlimited keyword arguments or unlimited sorry positional arguments and then the rest of them should be treated as keyword arguments i'm not really going to explain why that works but if you get some error saying hey you know your parameters are all ordered wrong just put an asterisk first and then this will fix it for you so if i go here and i refresh this now you can see it says field required we're looking for oops get by name message field required type query test okay sorry i'm just trying to decipher that so now it's all working but if i now say test is equal to 2 you can see we're all good because we passed the mandatory keyword argument and i can pass another keyword,358,0,0,-ykeT6kk4bk
26,argument say name is equal to milk if i do this correctly press enter and uh what is it message value is not oh sorry this needs to be an ampersand not a question mark so let me try to fix this ampersand okay and now we're good now we get the item milk okay hopefully that's clear that kind of covers the uh what do you call it keyword arguments or sorry not keyword arguments query yes query parameters that's what it's called now let me just show you how we can combine query parameters and path parameters together so let's say get item by name we wanted to accept a path parameter as well as a few query parameters maybe we wanted the item id and we wanted the name i don't know why you'd want that but maybe you do so in this case we can say item underscore id and now what we need to do is make sure we have a variable called item id that is inside of the get name parameter it doesn't matter where we put it i'm just going to put it first i'm going to say item id colon int and now this will work we can now accept our item id as a path parameter our name as an optional keyword sorry optional query parameter and then test as a mandatory query param so now if i do this and i go here and i refresh we get not found but if i go slash one slash and then question mark notice this works right we're all good and yeah that's that's fine so hopefully,358,0,0,-ykeT6kk4bk
27,that kind of illustrated to you how that works but that is how you can combine path arguments and query arguments okay so now that we've looked at query and path parameters we're going to move on and talk about the request body so oftentimes especially when you're trying to kind of add information to a database you're not going to be sending all of this information in query parameters or path parameters you're going to be sending like a bunch of information as what's known as the request body so i'm going to set up an endpoint here i'm going to say at app.post so a new method this time i'm going to say slash create item now what we're going to do is kind of change this a little bit so that we are now going to have an endpoint that allows us to create a new item in the database so i'm going to say define create underscore item and what i want to accept here is a request body i want some information relating to the item so i want the name of the item the price of the item and potentially like the brand of the item as well and so what i'm going to do here is say item and this is going to be equal to a type that we haven't defined yet which is called item so whenever i'm looking for a request body so i want something that is not a query parameter and i want something that's not a path parameter i need to set it equal to a class that inherits from something known as,358,0,0,-ykeT6kk4bk
28,base model now i'll kind of discuss this in more depth in a second but we're going to go up to the top of our program and say from pi dantic import base model now what i can do is create a class i'm going to say class item this inherits from base model and i can now define in this class the kind of structure of the data that i am looking to accept as this item parameter right here for create item so i'm going to say well i want a name this is of type string i also want a price this is of type float and then what else do i want well i want a brand and this is going to be optional because i don't know if i i'm going to take a brand every time string equal to none so just like it worked for the query parameters and the path parameters if you want to make it optional you can add this kind of optional thing that's not required but it's good practice and then make it equal to none or make it equal to some default value and that now makes brand optional awesome so now that we have that i'm going to save this and what this can do uh let me actually just go here and let me just return kind of an empty dictionary for now just so we can see how this works but since this is equal to this class right this is now telling fast api that okay this is for the request body this isn't a query parameter so it's,358,0,0,-ykeT6kk4bk
29,not expecting me to do something like question mark item equals and then type it all out it knows that i'm going to be sending this item information in the request body okay so i'm just going to go here and refresh this and let's go and see if if something went wrong okay looks like everything is good so now if we're looking at our documentation we see we have this new endpoint called create item so look it says that the request body is required we look for a name a price and a brand uh and if we wanted to try this out we could although right now it's not really going to work it's not going to do anything although if i press try it out and i i just i can just send this and i press execute notice that we're just getting kind of an empty response because we haven't typed anything out so what i want to do now though is i want to take this item and i want to actually insert it into the inventory and what i can do is rather than like trying to create a structure that looks like this i can just insert this actual item itself which you'll see in one second so what i'm going to do here is now change create item so it also accepts an id so i want an item underscore id the reason i want an item id is because if i'm going to insert this item well i need an id associated with it so now i also need an item id here so i'm,358,0,0,-ykeT6kk4bk
30,going to say item id colon int now this is going to be assumed to be a path parameter because it's in the path right so we have our item for the request body and we have our item id that is a path parameter awesome so now the first thing i'm going to do is check to see if this item id already exists i'm going to say if item id is in and then this would be inventory let's go here then what i want to do is just return and we'll say error item already exists or item id already exists great now if that's not the case we can just add this item into uh the inventory so what i could do is say something like inventory at and then item underscore id is equal to and i could say name is equal to item.name i could say brand is equal to oops to item dot brand and then price is equal to item.price now we'll leave it like this for a second but i'll show you kind of a better way to do this but i just want to show you this how you access all of the fields from this item right you just use dot name dot brand and dot price like that and now we would have added this item to the inventory and now what i'm going to do is return a response and what i'm actually going to return is just inventory at item id just to indicate that hey this was all good now your item is in the inventory and we just return the,358,0,0,-ykeT6kk4bk
31,same item back to them so now i think that's all working okay good so let's refresh this uh just sometimes it glitches out and you have to like press enter in your terminal to like refresh the web web app web service whatever it is anyways we're going to have create item now so if i go to create item what i can do is say try it out notice that now we have a required item id since this is required what i'm going to do is pass an id of 2 i'm going to say name and we'll make this name equal to eggs this price will be like 4.99 and then the brand well we actually don't need the brand so let me just remove that and now let's press execute so now when we do this notice that we get our response body saying name eggs brand null price 499.,201,0,0,-ykeT6kk4bk
32,that's now added into the database so now if i go to get item and i try to get item of id two and i press execute you're gonna see oh that we get this issue and sure the value is less than two so obviously we need to fix that we need to make it so we're no longer checking if the uh what is it if we're less than two because we don't really want that constraint okay so if i save this now and then i go back you'll actually see that if i try to run this we're not going to get anything the reason we don't get anything is because this these items are being stored in our memory so as soon as this server refreshes any items that we added are automatically going to disappear hopefully that kind of makes sense but since it's just a python dictionary it's not persistent it resets every time the server restarts and so that item we just added is now gone but i promise you it would work now we would be able to get that item but as i was saying this probably isn't the best way to insert items because well i'm just kind of like copying stuff that we already have what i should do instead is just insert the item object into the dictionary and then if i actually go ahead and return this this will still work the exact same way because fast api is smart enough to take this object and convert it into json since it inherits from base model so we don't need to do,358,1,1,-ykeT6kk4bk
33,anything fancy to convert this to a python dictionary we can just return the item itself and this will still work but what that means is that now if we're going to be inserting kind of item objects into our dictionary we need to change the way that we're looking up items so rather than saying if item or if inventory item id at name we're going to say if inventory item id dot name equals equals name then return inventory at item id and now that should be good and all should be working but we have one problem here we need to change uh this inventory object right here to be an instance of item or we can just remove it and have an empty inventory to start which is actually what i'm going to do hopefully that kind of makes sense i know i'm going like kind of fast through this but there you go that's how you create a new item and that's how you take in a request body so let's just test this out let's go here and just refresh the docs okay now let's go to create item let's make an item so let's try it out let's say item id 1.,271,1,1,-ykeT6kk4bk
34,let's just make this eggs 2.99 again we won't have a brand here execute looks like everything is good we inserted this item so now let's do another item let's do id2 let's make this milk okay let's make our price 4.99 and we can add a brand for this one let's say brand is equal to large and let's go execute and there you go we got that item so now if we go here to get item by name let's look for an item named milk notice that we do get that item let's look for an item named eggs notice that we get that item and now let's go look by id and if we go for id 1 we can see we get eggs if we go for id 2 execute we see we get milk all right so that is awesome that is all working next thing i'll show you is how we do a uh how we actually update an item so if i say app dot and then i'm gonna use put put is to update right i'm gonna say slash we'll call this update item and then what we'll take here just to kind of mix it up is actually a query parameter so the query or sorry not a query parameter we'll take a path parameter so i'm going to say item underscore id and i guess we're not really mixing it up because we just did it here but regardless i'll call this update underscore item and what we'll take here is item underscore id that's an int and then we want a request body as,358,2,2,-ykeT6kk4bk
35,well and so i will say item and i'll make this equal to item so now all i'm going to do here is the exact same thing i did previously i'm going to say if item id and inventory literally have the exact same so if item id in inventory oh sorry not in inventory then i'll say error item id does not exist because notice we're updating an item here not creating a new one so item id does not exist and then if the item did exist what we'll do is just override that item with the new what do you call it the new item that was just passed to us and in fact i'll show you kind of a fancier way to do this i'm going to say inventory item id dot update with item so what this dot update will do is take in the kind of dictionary or json that is this item and it will use it to update the item that we have now what that will mean is that if we don't pass name price brand so on and so forth it will not change the name and the price and all of those other things so if we just passed name then it would only update the name of this item if we just pass price it would only update the price if we passed any combination of these would update any combination of them the only problem however is though since we have this equal to item that means that it's requiring us to take name and take price in so instead what i'm going,358,2,2,-ykeT6kk4bk
36,to do is make a new class and call this update item and all i'm going to do is change these all to be optional so i'm going to say equals none equals none and then we'll make this optional like that so let's go optional like that okay so now this should be good we're going to change this type now to be update item like that okay so hopefully you guys are clear but this should just update the item for us so now we're going to have to go and add a few sample items so let's refresh this here notice that now we have this update item which is a put request so let's just go to our post and let's create an item item id 1 we can just make this milk again price of 299 and then we won't include a brand okay so now if i execute this i think we're all good nice so let's now update this item so to update this item what i'm going to do is try this out item id1 and now i will only include a brand so if i only include a brand now and i make this equal to large and i press execute internal server error what is the error here item object has no attribute update oh okay sorry guys i had a little bit of mistake here i kind of was thinking that this inventory item id is equal to uh a python dictionary which we could use the update method on it's not equal to a python dictionary so instead we're going to have to update,358,2,2,-ykeT6kk4bk
37,doing something a little bit differently what i'm going to do is say inventory at item id dot name is equal to and then this is going to be item.name but we'll say if item.name does not equal none else we won't do anything so actually i think we'll have to change this a bit okay i'm just going to say if item dot name does not equal none then inventory at item id dot name is equal to item dot name and then we'll do the same thing for brand and the same thing for price kind of an annoying way to do the update here but just what we have to do so i'm going to say item.price item dot price item dot price and then same thing here if item dot brand and then brand and then brand okay so again the reason i have to do this is because i thought it was a python dictionary thought i could use that update update command but since it's not a python dictionary it's an instance of this object we have to kind of manually update it and so what i'm going to do now is oops i guess running that's not really going to do anything let's rerun our server here let's go and let's refresh we're going to have to create an object so let's just make one fast one we'll say this is of type milk say price 299 and then we won't include a brand okay let's go execute uh all good what does it say executing property name enclosed in double quotes um okay so the reason that was,358,2,2,-ykeT6kk4bk
38,uh glitching out was i had a comma here and i can't have a comma at the end if i don't have another value after so anyways we just created the item you can see we have name price and brand is equal to no okay so now that we have that uh what i'm going to do is try to update this item still not quite sure if this is going to work but let's go to item id uh what was the id we inserted i think it was just one okay one and now let's just include a brand so we're going to say brand and we'll just make this equal to large and let's execute and now notice that we updated the brand here and we made it large so now if we were to change this and we remove brand and we make this name and we change the name which is supposed to be lowercase to be eggs because right now i think it's milk yeah it is milk and we execute this notice that now we've updated the name of this to be eggs and so that is how you can update an item and now if we go and we look for an item so get item by name we can look for the name eggs and notice that we are getting eggs now of course we could do this the manual way get underscore item or get what is it hyphen item actually what did i call this i want to make sure i don't mess this up get by name okay get by name and then,358,2,2,-ykeT6kk4bk
39,this is question mark and the name that we pass is name equal to eggs it works the manual way as well awesome okay so that is kind of it for the put and for the post now let's do one for delete okay so to do the delete method is pretty straightforward we're going to say at app dot delete we're going to say slash delete hyphen item and then we'll just take an item id actually let's take the item id as a query parameter just to do that we'll say define delete underscore item we're going to say item underscore id this will be an int and we'll make this equal to a query and just add some kind of info here i'll say dot dot and to make sure that this is going to be required not optional and then i will say description is equal to the id of the item to delete and then i will say greater than equals zero so that it must be greater than zero okay now what i can do is check if this item exists so i'll say if item id is in the inventory then we can delete it so we could do this other way actually i'll say if item id not in inventory then what we want to do is return error id does not exist okay otherwise we will simply say dell inventory at and then item underscore id awesome so now this endpoint should be working so now let's go to our docs so let's go slash docs now notice we have this endpoint for delete it's automatically documenting,358,2,2,-ykeT6kk4bk
40,all of it right another great thing about fast api if we go to create item we can make an item so item id one let's just leave it default we'll just we'll just execute one with name string and now let's delete this item so first thing let's just make sure it exists so item id try it out one we get this item okay now if we go to delete and all we do is try it out we pass item id 1 and execute this response body no oh because we didn't return anything from here that's fine but it should have deleted the item because now if i go and look for this item so i go here to get item by id and press execute we get an internal server error because well we don't have that item now let me just return something here let me just return you know success item deleted or something okay item deleted exclamation point great okay so that's good we now have the update create delete and then get by name and get by id all right so now we're nearing the end of this tutorial and i'm going to show you how you can actually return different status codes from these uh these endpoints because you can see here that for example i'm returning like some data that says error id does not exist but the status code when i return this is still going to be whatever the default status code of delete is and that's no good now if you're unfamiliar with status codes every single time you call an http,358,2,2,-ykeT6kk4bk
41,endpoint it will return to you some status code that indicates kind of what happened the default is 200 that stands for ok 201 is created 404 as you've probably seen before is not found there's a bunch of status codes you don't have to memorize them obviously but i'll show you how you can return like an error status code rather than just returning some data that has the same status code as whatever the default you know return status code of delete is and the way you do that is you go up to fast api you import http in all capitals exception like that and then you import something called status all right so import both of those now i'm going to go to a place where we have some error message so here it's saying data not found and rather than returning data not found when you want to actually give some type of error message with a different status code you just raise a python exception so if you raise exception and this exception should be http exception what you can do is indicate the status code so you say status code like that make it equal to whatever code you want in this case i can say something like 404 or i can use the status dot then http underscore 400 or underscore 404 underscore not underscore found i think that's what it's called so this kind of status module here has like the actual names of all these status codes in them if you'd prefer to type it out this way when you do status dot http underscore 404,358,2,2,-ykeT6kk4bk
42,not found this whole thing is just equal to 404 it just makes it easier to kind of read it out anyways i just want to show you that i'm not going to use status but you guys can use status if you want what's known as like the enum for each value and then you can say let me check here in my cheat sheet i believe it's detail and you can give the detail of why this this error was returned so i can say detail and say item id not found or something like that or item name not found okay so let's just copy this and let's paste this in all the other places we have an error so rather than raising this error we'll say item id already and then exists and i don't know what status code we should use for this one i'm just going to use 400 i think 400 stands for bad request i'm not too fussed about doing this all properly i just i'm trying to show you how we can raise these errors and then let's raise another one here so rather than item does not exist we'll just do 404 we'll say item id does not exist and then do we have any more errors this one as well so we can just copy this exact same thing so raise http error let's copy it here and there we go we are now good and just to give you an idea of what's going on when we do this in the back end so in fast api it's kind of waiting for one of,358,2,2,-ykeT6kk4bk
43,these exceptions to be raised when it's raised it will automatically return the equivalent kind of http response so that we don't have to do it manually instead we can just raise an exception so let's now go here and let's refresh our docs i'm going to have to press enter in here okay reloading that's all good now what i'm going to do is go to get item let's try to get an item id1 when we do this oh i forgot we haven't fixed get item by id okay let's let's not do that one let's get item by name let's look for an item let's look for eggs when we do this notice we get detail item not found and our response code was 404 as opposed to 200 which we would have gotten previously all right so hopefully that kind of makes sense let's go and try to create an item so let's go try item one let's just execute that okay let's try to do it again when i try to do it again you're gonna see here that we get detail item id already exists and we get status code 400 telling us hey this was a bad request you can't do that right now if i go to update item and i try to update item id 2 let's go like that again same thing item id does not exist we're getting the correct response all right so i think with that that's actually going to wrap up this video hopefully this gave you and i'm just going to zoom out so you guys can look at all this,358,2,2,-ykeT6kk4bk
44,code here on kind of one screen hopefully this gave you a decent idea of how to get started with fast api i know i went into like a lot of probably unnecessary detail for a lot of this stuff we just want to make sure you guys understand the difference between the path query and kind of request body arguments or parameters that's very very important and once you understand that like you saw here we created pretty well fully functioning api in not very many lines of code now obviously you would want to change this inventory to be an actual database but if you're just working on something simple and want to get it up and running fast fast api is a great choice and yeah i mean i hope this helps you understand the framework if it did make sure to leave a like subscribe to the channel i will see you in another youtube video,208,2,2,-ykeT6kk4bk
0,"knowing how to architect your application is a big deal when it comes to building a fast api product we don't just want to build a backend application we want to build something that is clean and easy to scale so in this video we'll be creating a fast api application that is following all of the best practices which includes the domain layer like where the enti development lives the application layer where all the business logic of the application lives the infrastructure layer which is where the database and rate limiting lives and then we're also going to include all of the unit testing and end to end testing this video will cover everything you need to get started with a clean architecture for your next product if you're new to the channel i'm eric roby a software engineer with over a decade of experience and i've helped over 100,000 developers learn and grow within their craft all right so with that let's dive into some code all right so i already have a environment created so i have a directory that's already created called clean architecture which is hosting our application inside our directory i just have a source directory with a empty main.py file and then i have a requirements. text file which has all of our dependencies that we need to run our application in a production environment so not necessarily what we need to run on our local machine but what we need for a production environment now in my requirements dd.",335,0,1,H9Blu0kWdZE
1,text file which has all of our dependencies that we need to run our application in a production environment so not necessarily what we need to run on our local machine but what we need for a production environment now in my requirements dd. text this is what we need for for our development environment which is like you right now on your personal machine if you want to code so what we can do here is pip install - r requirements ddev dotex this will install all the dependencies that we need for our entire project so now what we can do is let's go ahead and jump into our main.py file the first thing that we want to do is just add all of the imports that we're going to be creating in the future and that we need for this main.py file which is just our database our entities our api and our fast api and then what we want to add here is simply our app which is equal to fast api and then we're going to have this base. metadata.,240,1,3,H9Blu0kWdZE
2,create all bind equals engine this we only want to uncomment when we're creating new tables when we run our application for the first time this will break our tests later on so i leave it uncommented but it is a good thing to have just in case you want to rapidly be able to create new entities perfect now we're going to get a bunch of errors and that's okay cuz we're going to be building everything step by step now the next thing i'm going to just implement just so we have all of our configurations down pth is something called logging so i'm going to create logging piy and inside logging it's going to be a way for us to be able to document kind of all of our endpoints so if something happens in production or happens on our environment where we don't have to necessarily jump in and start debugging everything with code we can go ahead and just kind of look at the logs that our application is creating and and just be able to identify hey that's where the error is just by looking at the terminal so in here i'm going to add this logging which we're going to import logging we're going to import an en from our string and then we're going to just have this log format debug which is showing us how our log is going to format um debugs then we're going to have our log levels which is going to be info worn air and debug and what this does is when we add these into our code it'll tell us like,358,4,4,H9Blu0kWdZE
3,hey this is just information like maybe this is just something that you might want to know we might be like hey this is a warning in our code if this happens this is specifically going to cause um downstream effects then there's like air like our application's broken and then we can have debug errors for like in the future so if you have like an object that's getting weird we can just throw in a debug so then when we're running our application it'll be like hey this is um the object debugged now what our configure logging does is it takes in one of these log levels and then we kind of just do different things so it takes it it capitalized which it already is capitalized we find what level it is in our log levels and then we're just kind of setting up different permissions based on the log level and then we just enable our basic configuration to be logging and if you've never logged before we're going to be implementing this in our application so you'll be able to see exactly what's happening but overall this is what it's going to look like when you want to set it up from scratch the next thing we're going to be setting up is our rate limiting so inside our source directory let's go ahead and create a new file called rate limiting and inside here we want to implement our slow api which was what we installed now this will allow us to be able to do rate limiting on an endpoint so what that means is we are saying,358,4,4,H9Blu0kWdZE
4,hey maybe our register we only allow a certain user to be able to hit our register three times before maybe we're saying you need to take a break and this allows us to protect from like dds attacks or some other kind of attacks where people are trying to spam or use bots to call our endpoints over and over and over again just to kind of collect data we're putting rate limiting on in this git remote address means it's going to collect the ip address and we're going to say hey based on your ip address we are not going to allow you to call certain endpoints you know x number of times because it could be security maybe the the endpoints cost money it's something just to be like whoa slow down you're calling way too many in points way too um too fast so that's what we're implementing right here and then we're going to be adding it in our endpoints here in a little bit and now what we want to do is make sure we make this a python package by saying underscore uncore and knit underscore uncore .p that means we can now call these very easily within our project and now let's go ahead and create our first package now our first package is going to be called entities now let's go ahead and just say entities just like we did before inside entities we need to say new file uncore and nitpy that makes entities a python package and now in this project we're going to have two entities one for our to-dos and one for,358,4,4,H9Blu0kWdZE
5,our users so we can go ahead and create um two more files in here to-do and users now first let's go into our to.py and let's add some code right here so what we're saying is we are simply going to be adding our sql alchemy we're going to be using uu id for our um primary key and then we're going to be using date time and some other things in here so here we're just saying what the priority of this to-do is which is going to be 0 to 4 we're going to create this now to-do class and we're going to name this table todos plural we have an id of column uid as uu id is true it's going to be our primary key and the default is to is to use the uu id for we're going to have a user id which is going to be a foreign key back to the users which we haven't created yet and that's also going to be a uu id then we're going to have description due date is completed created at and completed at and then um our priority which is going to be part of our numerations up here and then simply we're just going to have this string which returns all the data inside we now want to do something very similar for our users where we're going to have a new table of user which is going to be called and this table is going to be called users our id is also going to be a uu id and we're going to have an email first,358,4,4,H9Blu0kWdZE
6,name last name and password now the only difference is we can see that it's the to-dos have a foreign key back to our users but our users don't have a foreign key to our to-dos and that's because it's a one to manyi relationship there's only one user who can have multiple to-dos but these todos can't have multiple users right so we're going to have one user that can hold multiple todos now the next thing we're going to add is this database and i kind of want to put this in its own folder which is going to be called database now inside our database we're going to create a new file of uncore uncore and nitor dopy and inside here we're going to create a new python class called core dopy now this is going to have quite a bit of information inside so we're going to have everything we need we're going to load from ourv if you want to stick this url in a env file or we could use sqlite or we could just hardcode postgress right here we're going to have our create engine session local base and our git db which creates a database session and then we're going to have this db session which equals our dependency injection db session that we can pass into functions so just for um for practice let's go ahead and create a new file called env now env files are often a way for you not to have to pass in your private secret information into a source code repository so what we could do is instead of passing this,358,4,4,H9Blu0kWdZE
7,directly into code we can create a new thing called database url which is going to be equal to something and we can just grab this information paste it right here and now i'm going to comment out this as well and i'm going to un comment here so what's happening right now is we are grabbing the database url from a env file and we're making sure that's set as our database url so when we create our engine now let's go ahead and create a new package right here called off and inside our off we're going to create a new file of uncore anitore dopy and now is when we're really going to start seeing this clean architecture design where we're going to have a controller which consumes an endpoint a service which does the um business logic and then the models which is the pantic data validation schemas so inside here let's go ahead and create a new file of controller.,214,4,4,H9Blu0kWdZE
8,piy and a new file of model.py we're going to go backwards from the model to the service to the controller just so we can see how it's created so what are the models that we're going to need well we're going to need a uuid and pantic as imports but we're going to really just have a couple different ones for the authentication which is to to register your user and to be able to sign in a user so to register you a new user we need a a email a first name a last name and a password and then we're going to do whatever we need to do with that to save it to a database we need a class of token which just returns the token back to the user for um authentication which returns an access token and a token type which is going to be like jwt bear token and then we want a class token data which is what are we going to give back to the user once we validate a token and this standpoint we're going to get their uu id so then we can fetch whatever information we need for that user based on their primary key let's then jump into our service doy which this is going to start having quite a bit of data here where we are going to have a secret key and a secret algorithm again we can move that into a env file just like we did with our database core and i kind of showed you how to do that so feel free to go ahead and do,358,6,6,H9Blu0kWdZE
9,that this is definitely something that you're going to want to keep secret and probably not push around in you know different um repositories we're going to say the algorithm is hs256 and then the access token expires is 30 minutes so our uh token is going to expire in 30 minutes oaf 2 bearer token is going to be inside our off.,81,6,6,H9Blu0kWdZE
10,token path so this is our um which we haven't created yet in our controller but it's going to be slof sl token and then our bcrypt contacts we're going to be using bcrypt as our hashing um contact so we're going to be saving all of our database at database passwords as a bcrypt hash um we have a couple functions here like verify a password where we're checking the plain password a user passes in with the hashed password to make sure that they're validated we're going to be able to fetch hash passwords and we need to be able to authenticate users so if a user you know passes in their email and password we want to be able to query by that user fetch them verify that user if it fails well then we're going to log a warning for anybody to see in our actual like logging of our application which says failed authentication attempt for the specific user that tried to log in if not we're just going to return a user and you'll see that we have this authenticate user getting called below we have our create access token which is our jwt code which takes in a sub email id and expiration date and then we encode that jwt with our secret key and algorithm so to make it like valid we can then verify a token which returns a models of token data so if we look up here we can see that we're importing our models and then we have our payload our user id we return the token data if everything works out or we,358,7,7,H9Blu0kWdZE
11,throw a warning and we throw an authentication error of token verification failed same thing for our register user we're passing in a new database session we request a users's register request we validate it and do everything that we need to do for that and if not we throw a registered user and we log it we can get our user this is what we're going to be using for um our dependency injection for validating current users when we're trying to deal with to-dos we have this as an annotation now to fetch the current user which is going to get our current current user and then we have our login for access token which just uh takes in a username and password and we just hey does it check out we call different functions or we throw an error and then what we want to do here is add in our controller our router for our api routing which we're saying you can only call register user five times an hour so we're like hey you just can't sit here and just keep creating new accounts over and over again we're going to you know slow it down now this request looks like it's not being used but it's required for our rate limiter to fetch the ip address and then we also have a login function which returns the token now this is from rate limiting limiter we need to make sure this is from rate limiting and perfect that's how we're going to implement our off we're then going to create a new folder called users inside users we want the,358,7,7,H9Blu0kWdZE
12,same exact stuff so we're going to saycore uncore init.py we also want to just say um controller.,23,7,7,H9Blu0kWdZE
13,and model.py let's start with our model.py where we're going to have a user response which is going to have an id email first name and last name and a password change of our current password new password and new password confirmation so a user once they log in so once we hit the off endpoint we log in a user we're going to return a jwbt token and then when a user tries to log in we'll grab the user token we so when a user logs in they are able to fetch information abouts well they can't fetch all the information we save in the database like their password cuz why would we just give that out to people but what we will give them is their id their email their first name and last name and then we're also going to allow users to be able to change their password so you can pass in your current password what you want your new password to be and then kind of the confirmation for the new password to make sure the passwords match and these are going to be the base models for the pantic data validation that we're going to be using here inside our service um we're going to say get user by id where we pass in our database session and then here we can just kind of fetch our user and we're going to say get user by id we're going to fetch that user and then if everything works out we're going to return our user as a user response so a user can say hey i want to,358,9,9,H9Blu0kWdZE
14,fetch my information by my id after i validate that i'm a the actual user and then we'll have change password which takes in the database session the user id and our password change mod model that we just created in our model where we can fetch the user by their id we can verify um their passwords or the current password so the password that they typed in is that really their password do the new passwords match so the new password and the new password confirm and then we can finally update the password at the end and then lastly in our controllers we're going to have it prefixed as users where we're going to have our current user as our current user and we're going to get our user by the id in the the same thing right here perfect and now let's go ahead and check out our to-dos so we can create a new folder called to-dos where again we're going to do the exact same thing of uncore uncore in nit uncore uncore dopy then we want to create a new file called controller.,247,9,9,H9Blu0kWdZE
15,py a new file called service dopy and a new file called model.py now inside here we can just paste some information we can say we want our to-do base which is going to be like if you want to create a new to-do description due date priority create is going to use exact same thing and then our to-do response is just going to have our id is complete completed at and then we're going to say hey we want this model configuration to be um config dictionary so we can consume and use the data a little bit easier let's then jump into our service where we can say hey we want this to um be create todo with our current user token data d session to-do model.,169,10,10,H9Blu0kWdZE
16,create where we've done this a ton of different times where we're just passing in data and we're creating a new to-do log it if necessary same thing for our to-do get to-dos so based on an authenticated user fetch all the to-dos based on the uu id get to-do by id so we can just pass in the uu id after we verify the user fetch the specific to-do by its id return it back to the user update a new to-do which allows us to be able to just up update to-dos and then we have complete to-do which makes the is completed value the opposite so true or false and then we save it to our database and then we have our delete to-do which allows us to delete a to-do and then lastly we have our controller which allows us to be able to call each of those endpoints so like create a new to-do get to-dos get to-do update to-do complete to-do and delete to-do all based on validating a specific user now one thing that we are and this is all model let me just go through this real quick and now one more thing that we want to implement is exception handling so we can see like in our service we're throwing some exception handlers and let's just go into here create a new file called exceptions.,304,11,11,H9Blu0kWdZE
17,piy and here we just want to add some exceptions so our to-do error um to-do not found to-do creation error userbase error and we're just doing a super call back up to the initialization of the error that we're calling beforehand and this just allow us to be able to see a little things a little bit cleaner be able to throw different status codes and be able to kind of just be able to scale our application a little easier now the last thing i'm going to add here inside our source i'm going to say new file i'm going to name this api. py where we're going to add now all of our routers right here and then in our main.py we already have register app register routes we can go over here and just say hey we want to register our routes of our app and then we also want to make sure that we add our configuration for us to be able to throw some logs and then add them right here so with everything that we just added if we jump into our terminal and we say uicorn source.,253,12,13,H9Blu0kWdZE
18,main colon app-- reload we're getting an error inside our source do off oops cannot import models oops where am i saying models that should be model that is my bad and we also doing it in here yeah that should be model see even when you think you have everything planned coding can throw some errors at you oh goodness i did it in all of them all right let's go back into users uh should be model model model model all right now everything should be good all right so now if we open up our application we go to our port there it is here's our whole application everything looks great and if you try and get something it's going to throw a not authenticated because you have to register an account and then grab the token stick the token up here to be able to to handle all the endpoints but now let's go over how we can test this application so there's two ways we can really test it we can test it with unit testing and we can test it with in to end testing and how we can do that is let's jump into our source let's go ahead and say new folder oops let's jump outside of our force um our source and let's create a new folder called tests and now inside our tests we want to create a newcore uncore and nitor do pi file and the first thing we're going to create in here is something called a com test file so com test.py this is more of like all the stuff to get our,358,14,14,H9Blu0kWdZE
19,application ready to be tested so here we know this is actually that and this is limiting but what we're doing here is we're just setting up a fake sql light test or sql light test database for our application we're um doing everything we need so we're setting up all of our entities for our test database and then we're just setting up our high test fixtures which are more or less ways to set up data for our tests to be called and we're doing that all over in here first thing we're going to do is our off test service so we can come in here and we can say test off service.,150,14,14,H9Blu0kWdZE
20,we just add in all of this information where we're going to be able to test against our verify password authenticate user test login for access token be able to create a user or testing a against everything inside our service we can come over here and create another file called testore users service. py where we're just adding everything inside of here where we're now going to be testing our um g user information and then be able to change a password and we're testing really every way someone could try and change their password and making sure that all of our configurations are correct and then the last thing is our test to so our testore todos um service.,157,15,16,H9Blu0kWdZE
21,py we want to make sure all these are good so here there's a whole bunch of different tests about creating a to-do getting to-dos get todos by id completing to-dos deleting to-dos everything we need there and really what we need to do here is we can just say pie test if you run pie test we can see our um tests start running and everything looks good we have all of our things passed we can ignore this right now it's saying something is going to be potentially removed in the future but it doesn't matter because a lot of times these deprecation warnings last way past the version date or sometimes they don't even ever get fixed so that's okay we can see that all of our all of our tests pass and we have our test db but now we want to do in to end testing so what that means is we are starting at the end point and we're testing everything down and everything back from the endpoint so inside our test let's create a new file called e2e which stands for end to end and i'm going to say new file of testore off uncore npoints dop this is now going to test all of our endpoints literally calling the endpoints we can say we're calling the off sl token we're calling the off sl token again we are doing our endpoint testing all the way down to our service and back instead of just our services we're going to do the same thing here for testore users uncore endpoints dopy and this is going to test everything about,358,17,17,H9Blu0kWdZE
22,a specific user based on the endpoint all the way down we're testing hey are we getting the the right status code are we getting the right responses we're just making sure that everything looks really good and now lastly we want to say testore todos uncore um endpoints we're again we are just testing everything from a to-do perspective based on the endpoint so awesome stuff let's go ahead and just do pie test again and now we should be testing all of our endpoints as well and this percent over here is just like it's incrementing up to 100 where 100 is done that's not like code coverage so now we have all of our tests passing cool so now for remember we added our postgress right here and we want to make sure that that is like the new information so i'm just going to say uh source.,196,17,17,H9Blu0kWdZE
23,env from my standpoint that sets all the environmental variables for um my my local environment but what we want to do now is let's deploy all of this on docker so if you don't already have it um go ahead and download docker from your application and here you can see that i don't have very much installed on my docker at the moment and what we're going to do here is make sure that we can run our application from docker and then also postgress will be created in docker so we're going to deploy our application on docker and we're going to deploy our postgress on docker so the very first thing we need to do is is create a new file inside our main thing called a docker file and inside here we're saying use python 3.11 from our work directory of app we're going to install our requirements.,198,18,18,H9Blu0kWdZE
24,text we want to copy the project files of our source source and then expose on port 8000 using our command that we use to run our application and now we need a docker a docker compose yaml file which is going to do everything else for us so right here we can see that it's going to spin up a postgress environment which is going to be called clean fast api on our port and we already set this information up in our docker in our um database core file and then we're going to deploy this postgress environment so we're going to deploy our postgress environment and we're going to run our fast api application and we can do that by saying docker compose up-- build when you do this it's going to pull the image from the latest postgress if you don't already have it it's going to spin up that postgress environment and then it's going to deploy our application our application is going to be communicating with that postgress environment so it says application startup was complete if we come back over here and we refresh this application is literally running from a postgress database now in our docker environment if we open up docker we can see right here we have our postgress running and our uh product running if we like but it's all in the same container so if we just wanted to stop it we can just stop that container and our application crashes just like expected because we turned it off in docker so awesome awesome stuff guys this is i know this was a lot,358,19,19,H9Blu0kWdZE
25,um the the code is you can download the code directly into descriptions i just kind of wanted to walk through what exactly is happening in case you wanted to you know dive more into it so you can scale and build from here this is already a very professional application which is using docker tests in to end tests unit tests structured in a way to really scale our application so use this how you like i hope you enjoyed it and i will see you in the next video,118,19,19,H9Blu0kWdZE
0,"hey, what's going on everybody? it's caleb. welcome to your fast api tutorial. we're going to build out an api from scratch using the fast api framework in python. so, this is the perfect starting point if you're relatively new to building out apis. we're going to lay the foundation, get that hands-on experience, and we'll have additional material in future lessons. so, i want to call out the playlist link. you can start from the beginning of the playlist to get all of the api concepts, or you can start with this video if you want to jump in on hands-on material. additionally, i will have extensive notes for anything we talk about in this lesson, as well as copyable code samples and references for any other topics you might want to do additional research on. so, i'll have a link to get the notes in the description as well. so, let's get started with first talking about what fast api is. well, it's one of a couple popular frameworks for web development in python. two other popular ones are django and flask. i've also done videos like this covering flask and django, but there are some very unique benefits to fast api. so this is the fast api website. a couple of quick things to call out. well, first it's fast, but a big thing is that it's fast to code. we're going to build out a pretty intense api relatively quickly. not only building the api, but also having very clear documentation for that api being generated for us. and this is huge. it's going to make consuming the api so much easier.",358,0,20,k5abZLzsQc0
1,"it's going to make consuming the api so much easier. by the end of this, you will have an api that aderes to the open api spec, which will give you a very nice interface. and as mentioned make it much easier to consume the api and it'll say the types of data and the structure that the client the consumer of the api should use to interact with the api and this structure will be enforced on the backend fast api is built with pyantic and this is the most widely used data validation library for python so not only will we tell the user of the api what structure we're expecting we will enforce that with some data validation and this is pretty much out of the box ready to go which is something that for other libraries and frameworks could take a lot of additional work. so before we jump into the code, let's take a look at what we'll build in this lesson. and this is just the starting point. as i mentioned, we're going to continue building upon this in the upcoming videos. so there's a lot of things we won't get to cover in this video, but i'm going to cover the most thorough foundation so that the future lessons will be able to continue right where we left off. so here's a look at what the final products of this video is going to be. we will have a series of crud capability api endpoints. we'll be able to read our list of campaigns and we'll be able to try it out in the browser.",353,20,29,k5abZLzsQc0
2,"we'll be able to read our list of campaigns and we'll be able to try it out in the browser. so you can see all the different campaigns that exist. we'll be able to get a campaign by id. so you can pass in id one for example. this will give you a specific campaign. we'll be able to update a campaign. give some new data and this will update it in the database and give back that new change. we'll be able to delete data. so we can go in here, delete id1, get a 204 response saying it was deleted and we will be able to create new campaigns as well. this one is new and this will give you a 2011 and give you back the data from the database. additionally, these will give you the potential error codes or response codes you could get. so you can get validation errors for the data as well as successful responses and if there's any other special response possibilities, they'll show up there. and then lastly, we'll have the schema. so these are the different types of data you might expect to work with. so we have a campaign. this can be expanded and you can see that structure. we'll have a slightly smaller campaign create with a name and a due date. we'll also have these other things such as a response with a list of campaigns or a response with a single campaign. so, this pretty much gives the full interface for working with the api. so, the goal of this video is to build this out. now, i'm going to be running this off from mac.",364,29,49,k5abZLzsQc0
3,"now, i'm going to be running this off from mac. if you're on windows, you can follow along as well, but the commands might be slightly different in certain scenarios, or you can run wsl if you would like. but the very first thing we're going to do is we're going to create a directory for our project. now you can decide what you want to call this directory. i just came up with a project name for what i'm trying to build. so the idea that i have is an app that will help with copy writing content for marketing campaigns. so if you want to write some copy and you want to distribute that across social media, newsletter, website, everything basically have it in all locations. that's the idea of this application and i decided to call it omnicopy. so you can follow along with this example or do your own. but we're going to say mkdir omni copy. we will open that directory. and now we will create a virtual environment. so we'll use the venv command and we'll call the virtual environment.ve venv. then we can activate it. so dot space.veenv bin activate. i think if you're on windows this is where it will differ. it'll be like slashcripts slashactivate or something like that. but yeah, you can look up the exact commands if you're on windows. now, we're also going to set this up with git right from the beginning. so, we're going to create a ignore and we're going to put in that the.vev directory. so, we'll put this in.",345,49,69,k5abZLzsQc0
4,"so, we'll put this in. so now we should have if you do an ls- a dot get ignore and av and the git ignore just hasve venv in it so far. cool. so far so good. now what we're going to do is we're going to upgrade pip the package installer for python and then we will use it to install fast api. so, pip install--upgrade successfully installed. and then we'll say pip install fast api. and you have the option to specify what features or portions that you want to install. to do this, you wrap it in quotes. and then we'll just say standard. now, when you finish installing stuff, you will want to add this to a requirements.txt file. so, we'll say pip freeze. redirect that into requirements.txt. txt. so now we have this file and you can see what's inside of that. it's just all of the packages that we've installed. so that way we can use this list to install these later if we download this project on a new computer or need to recreate our virtual environment. now we will say get init to initialize an empty repository. get add dot get commit initial commit. now we'll create a file main.p py and that's where we're going to write all of our python code. now at this point i'm going to switch over to visual studio code. so i'm going to open this directory in code. so this is what our file structure is going to look like and then we can open a new terminal. now we can run everything from here. now my visual studio code is automatically activating that virtual environment.",365,69,93,k5abZLzsQc0
5,"now my visual studio code is automatically activating that virtual environment. if yours does not then you will need to do that. so again, that's just going to be dot.ve bin activate and then you'll see.venv over here on the left, which will tell you that the virtual environment is active. now getting started is quite easy. we will open up main.py. we'll say from fast api import fast api and then we'll say app is an instance of fast api. now basically we create a function that we associate to a path. so we'll say app.get get and then in quotes the path to hit this function and we'll say defaf and these are going to be async functions and then we'll just return a value. we will return a dictionary and we can say something like message is hello world. so this is the most basic fast api app we could build. and then to run you'll say fast api dev and then the name of the file main.py. py and then this will show up some stuff here as well as give you the urls to access the api. so we will open this and this is our site. additionally, you will also have a documentation site. so you can go to for doccks and this is our swagger interface and you can see the end points here. so you can expand them with this dropown and see the potential responses you could get and what the value might be. so, as we go, i'll talk about how to customize all of this. but the reason this works is because the fast api framework generates for you.",364,93,110,k5abZLzsQc0
6,"but the reason this works is because the fast api framework generates for you. you go to slash open api.json this json file which is used to generate that swagger interface. so there's swagger and then there's open api very closely related. just think of open api the actual spec the structure here for your api and then you can think of swagger as this interface. so we have the open api specification and then swagger is a suite of open source tools that implement and work with the open api specification. so yeah, you might hear some overlap between these two. closely associated but slightly different. but yeah, this is really awesome because this makes documenting and sharing your api with others so much easier. so this is awesome. i built a rust api and i had to build all this out manually. it was such a pain. i mean, it wasn't that difficult. you can just build these api json files manually even. but even the library i was working with was supposed to make it easier and it was still much more difficult than doing it with fast api. so this is probably my favorite feature. so let's just review each thing here real quick. so first we import, no big deal. then we create an instance and we assign it to app. so then whenever we want to refer to our fast api instance, we use app. we use a decorator here and this describes how we associate a web page visit to a certain function. you might hear path operation function.",345,110,130,k5abZLzsQc0
7,"you might hear path operation function. so in open api and within fast api an operation would be one of the http methods such as get. so for every method and every path we're going to have a function that handles that request. so we have the basic structure here. but one other thing i wanted to mention is from the extensions you will probably want to get pylance. so make sure you have the python extension and then the pyance extension. and then from within your user settings, i added this python.is type checking mode strict. having this extension, it will give us the most likely chance that we'll have issues pop up here in the problems. and you might need to reset visual studio code if you just installed it. so now if i go in here and i put something bogus, it'll give me some warnings saying that these don't exist. and you can see it comes from pilance. so now let's take a look at building an api. i gave you a brief overview of what app i wanted to build. doesn't really matter at this point because we're not going to get into too many domain specific things, but i'm going to paste a basic data structure you might expect to work with for this app. so, it'll look something like this. and i'm just going to comment this out. so, i'll just make a giant multi-line comment. we won't keep this the whole time. i just want to show you guys without having to look at a bunch of errors. so the idea is you run some marketing campaign.",357,130,149,k5abZLzsQc0
8,"so the idea is you run some marketing campaign. the campaign has a date where the event launches or ends. basically some date to tie everything to. and then within that campaign you have a bunch of pieces. so marketing pieces, content pieces. you can think of a social media post, a blog post, a newsletter. any of these things would be considered a piece. just getting started. i only want to worry about one of these. so, we'll worry about the one you have to create first, which would be the campaign. since a piece relies on a campaign being built, we're going to look at campaigns. then, when we get to relationships, we'll be able to make an association between these two types so that we can look at all of the pieces for a certain campaign, for example. so, that's on my to-do, but right now, i just want to focus on the campaigns. so, this will have an id, a name, some due date, and i guess i'll just be consistent here and use underscores, and then a created at. now, the created app, that's something that's typically just going to be generated either by the backend or the database. whereas the due date, that's going to be set by the user. so, to distinguish between those, i usually will end the timestamp automatic stuff with at. so, you know, this was created at this point in time. whereas due date sounds a little bit more like a manual value that you would be expected to put into the database. hopefully that makes sense. but as we get into building out data structure, this is what we're working towards.",367,149,169,k5abZLzsQc0
9,"but as we get into building out data structure, this is what we're working towards. now, as for this campaign id, that could be either campaign id or just id. either one of those is fine. i've kind of grown to like having the table name prefixing the id. i think it makes joins easier, but i think just having id is also fine. so, let's see what it would look like to create a new endpoint to retrieve the campaigns that we're working with. we can say app.get. and i will have these problems running, but i'm not going to keep that open because i find it very difficult to look at those problems constantly updating or i find it quite distracting. so, we'll make a new get. and then what do we put for the path here? well, the path i want to have is basically slash api slashv1 for version one and then slash campaigns. however, if we have a bunch of these endpoints and we're constantly repeating this section, that can be a bit annoying. so, what you can do is you can take this and you can remove it. i'll cut it. and when we instantiate fast api, we can provide into this a named parameter of root path and paste that value and you'll want that to be quoted. so now what it's going to do is it's going to take this root path append the path for the endpoint to give you the full path. now it doesn't worry about local host or any of that stuff. this is just a path after that point. and then we'll create a function down here, a sync defaf.",367,169,187,k5abZLzsQc0
10,"and then we'll create a function down here, a sync defaf. and then the naming for this is typically going to be the action. so create, read, update, or delete followed by the resource. so for example, we'll have read campaigns. and to get started, just so i can see this endpoint in the browser, i'm just going to say return campaigns and then a colon. and then i'll just say example. so let's check if we can first hit this endpoint and then we'll worry about filling in the data. so we'll go over to the browser. we'll go to slash api slashv1 slash campaigns and make sure your server is running. i'll probably forget to do that about 300 times. so just bear with me. fast api dev main. py. all right, we got that running. we can go do a refresh. okay, so we know we can hit that endpoint. now we can worry about getting data to show up here. now, what i'm going to do is very quickly mock the data with a dictionary. so that way we're not working with a database. and i know this can be a little bit annoying because we have to build up all these steps. but i think jumping directly to working with the database is quite the jump. we are going to do that in this lesson, but there's a little bit of a learning curve. so what i want to do is i want to first build out all of the endpoints we're going to work with and have a very basic structure. so that way we have the interface to working with our api pretty much done.",366,187,210,k5abZLzsQc0
11,"so that way we have the interface to working with our api pretty much done. so we'll be able to create that mock data very quickly and then it'll make the next step a lot easier where we just have to swap it out for the database. so what's that going to look like practically? we're going to create a variable here data and this is going to be a dictionary. i lied guys. i'm i'm terrible. it's going to be an array of dictionaries and that's because we're going to be getting multiple campaigns. so that's going to be an array full of dictionaries, but ultimately we're going to put that inside of a dictionary. i talked a bit about this in the api concepts video earlier in this playlist. so we'll say campaigns and then the value for this will be data. so now let's go and fill out what data looks like. it's going to match this structure here. so we'll say campaign id one name summer launch. next we will have the due date and for this we can say datetime do now to just generate a datetime for us. and this will need imported. so you can say from datetime import datetime or just type that out manually. and then for the timestamp of when it's created, it'll be the same thing. so created at datetime.now. that'll just make it easier for us. and then we'll do the same thing down here for the next one. we'll just change some of the values. so we'll make this two. and then for this we could say black friday. now taking a look at these problems, it's going to complain about the types.",371,210,233,k5abZLzsQc0
12,"now taking a look at these problems, it's going to complain about the types. now, we can get into the details of how typing works and all that right now, but it doesn't really make sense until we get to actually working with the database. so, i'm going to show you now how you can basically ignore this as this is just kind of a fill in so we can get the api interface built. so, to do that, we'll say from typing import any and then you can put a colon any. so again, we'll circle back to types here soon, but for now, we should be good. and our endpoint will return that array inside of campaigns. so let's go refresh. and you can see our new json structure here. now, if you want, you can go into inspect, and you can take a look at the network tab, do a refresh. you'll see that request here, and you can get more information about it. so this will format the response data. you can also click this button here, which will format it, but you can also take a look at the headers. so we have a response code of 200 and the request method was get and then the content type for the response application json. next up, let's talk about how to get a single campaign. so just the details of one of these campaigns, not the full list. so this will teach us how to get an id through the url. so we'll say app.get and the path will be pretty much the same with an additional slash id.",356,233,249,k5abZLzsQc0
13,"so we'll say app.get and the path will be pretty much the same with an additional slash id. so campaigns slash same structure as we talked about in earlier lessons with the curly braces and this is going to be given as a parameter to the function. so our function will be read campaign singular and then we can say id with a type int of int and then we will return just to test this one out we're going to say campaign and let's just return the uh id that we actually passed to it. so we can go to the page, go to campaigns slash one and it'll just echo that number back to us. so we can see that the number travels through the web page and comes back. so now what we can do is we can use that id to grab the appropriate campaign. so how would we do that? well, basically keep this real simple for campaign in data. if campaign.get get and then here you will put the key of the dictionary you're looking for the campaign id compare that to the id that was passed in so when we access this it'll give us a number we check if it's the same number we're looking for if so then we return that campaign otherwise if we get through the loop we can raise an http exception and this is imported from fast api. and then you'll pass a status code here. so status code is 404. so let's check this out. we'll go to campaigns and we'll try a five. it'll give us not found. and then we'll try a one.",362,249,263,k5abZLzsQc0
14,"and then we'll try a one. and it gives us that id. but i don't actually want to return the id. i want to return the campaign. all right. so now we do a refresh and we get that data. looks good. and this is what the structure looks like when we pretty print. okay, cool. so, we've shown one way of working with this api. works pretty well. we've also talked a little bit about using the docs page, but we haven't used it a lot. so, maybe let's take a look at that real quick. so, we'll just go back to docs, do a refresh, and you'll see we have a couple additional endpoints here. and this will give you some extra details. we haven't talked about putting the correct type structure here. we'll get to that once we start working with the database. but what you can do is you can try it now. and you can put in the value here. execute. and that's going to give you the data as well. so either way you want to work with it is fine. it'll be handy once we get to other methods such as post. you can just do that right from within the docs page. now let's go in and create app.post. this is going to be at campaigns with no id. and then the function is going to be called create campaign. you can name it whatever you want. by the way, i'm just following the crud acronym create, read, update, delete. now we have the ability to read data from the request body.",351,263,292,k5abZLzsQc0
15,"now we have the ability to read data from the request body. so for this we'll say request is request with a capital r and this will be imported from fast api and to get the body we'll say body is request.json and this being an async function we have to await this call. now we can craft a new campaign which will be a dictionary following this similar structure. so i will copy this and we'll paste that here. and instead of a two here, we will actually just use a random id for this. so we can say random or rand int. and we'll just get a value between 100 and a th00and. this will need imported from random. so from random import randt and then for the name, we can get that from the body. so you can say body.get name. and then for the due date and created date again we can just ignore those and generate values here. well actually with creating a new one we might want to specify the due date. so let's practice that as well. so we'll say body.get due date. so we'll take a look at how to pass a date time from the request body. now once we have this new dictionary we can just append it to our data list. so we'll say data.append append passing in new and then we will return this newly created element. so we'll say campaign is new and we will type this to any. so now how do we actually make a post request? well, we're not going to be able to do it from the url up here. so that's where the docs come in handy.",370,292,312,k5abZLzsQc0
16,"so that's where the docs come in handy. so we'll see the post here. try it out now. and we have the ability now to provide a body. i think we were pretty close. i think we just need to change one thing to get the request body to show up in the docs. and that is instead of working with this request and then getting the body by saying request.json, we can just work with the body directly. and when we do that, fast api knows that we're expecting some content in the body for the request. so, it's going to add that option to submit the body in the docs. so, you'll see what i mean in a sec. if we just replace this with body and then we type that to a dictionary, which is a generic, and a dictionary is going to be key value pairs string to pretty much anything. so we'll say stir any and then we can remove this line here. so now fast api looks at this and says oh we're expecting a body in this request. so we do a refresh and now we see this request body. so when we try it out we can fill in the values. so name example campaign and then execute. scroll down you'll see the response. we get a 200. it gives us a campaign id of 999 which was generated for us and name example campaign and then the due date and all. so if we wanted to pass in a due date we would just put in a format of this nature.",351,312,331,k5abZLzsQc0
17,"so if we wanted to pass in a due date we would just put in a format of this nature. so example campaign two and then we can say due date and then as a string the year month day and then the time hour minute second and then microsconds execute. and there we go. so now when we go in and grab all of our campaigns, try it out and execute, we should get those new ones in that list as well. so those will persist as long as the application is running. once i reset it, it'll go back to just the two. so it's all in memory. i want to talk now about updating data, which works pretty similar to creating new data. the only difference is we use an id of something that already exists and we're going to use a put request. so we'll basically replace that entire campaign. so anything we want to stay the same, we'll just leave it as is, but we'll keep that in our request. so we'll say app.put path will be campaigns slash id and then this will be update. so update campaign same idea we'll have a body which will be a dictionary containing a string and then anything for the value and now let's find that element in the dictionary and update it. so we will use an enumerate loop. so for that we'll say for index campaign in enumerate passing in the data list from up here. then we'll check if check the id campaign.get campaign id is equal to oh we also need to get this id as a parameter.",357,331,346,k5abZLzsQc0
18,"then we'll check if check the id campaign.get campaign id is equal to oh we also need to get this id as a parameter. so that'll be id and that will be of type int. so then we can use that here. we can now craft an updated campaign. so it's basically going to look like this new one. so i'm going to copy and paste that here. then we'll just adjust the indentation and we will call this updated. the campaign id will come from the existing id. so that'll just be the same as the id for the campaign we're looking to replace that we pass in as an argument. and then for the created at, we're not going to actually replace that with a new time. we'll keep the same time. so this should act like a patch where we don't have to provide that as the client. it's just handled by the back end. and not providing it is not going to cause that attribute to be removed from the campaign. hopefully i'm making some sense, but probably not. basically, we just need to have the backend handle the created at. and for an update, we don't want to give a new created at date. it's an update, not a new element. so, we'll just grab the original created at. so, campaign.get created at. also, i just realized i had a typo there. so, at this point, we have an updated campaign. i'm just going to fix some of this indentation so it's clearer. so what we need to do now is go into the original campaign list and replace the one with this id with our new updated data.",369,346,369,k5abZLzsQc0
19,"so what we need to do now is go into the original campaign list and replace the one with this id with our new updated data. so to do that we'll say data and then the index will be from this index thanks to the enumerate and then we'll set this to updated. and if we get through all of the loop and still didn't find it, we can raise an http exception with a status code of 404. so far so good. i think that'll do it. the only other thing is what do we want to return with an update? typically, that's going to be the updated data. so, i'll put that inside of a dictionary and we'll just say campaign and set that equal to updated. so, let's give this a run and see how it goes. we'll do a refresh on the docs and let's just get the latest data so we can see what we got. so, the server restarted. we got one and two. let's take two. and then for our put, we will try it out. we'll replace this document entirely. let's change it to cyber monday. we don't need to provide the created at. and we can change his due date to whatever we want. let's just move it a couple of days forward. execute. oh, and then the id, we don't want to pass in the body. we want to pass it in the url path. so, we'll remove that there. scroll down. execute. let's take a look. all right. so, we're getting a 422, which basically means something's wrong with our syntax. and that is this trailing comma. so, execute 200 response body is the new updated data.",374,369,398,k5abZLzsQc0
20,"so, execute 200 response body is the new updated data. you can see has the same id, should have the same created at, but it has our updated name and due date. then when we go and retrieve all of the elements, that change should be visible there as well. last one i want to worry about is deleting. just so we don't have to start from zero, i'm just going to copy and paste the update, but we're going to change it from put to delete. we will still need the id of which one to delete, but we're not going to provide a body, so we can remove that. and then we'll iterate through if the campaign id matches. it's very simple. we don't need to do all this. we can just say data.pop and pass in the index. and then we won't return the campaign. we can just return a response and pass in a status code. and i don't think we're getting that suggestion because this needs imported. so from fast api import response. and then status code is 204. no content. an additional place where you can specify the status code to be returned such as in the case of creating we can add an additional argument to the decorator here and say status code is 2011 and that will give us back the data but it'll come with a 2011 status code. let's check our campaigns. right. okay, so we have one and two. let's delete two. so we'll go to delete. all we have to do is provide an id. execute. and we get a 204.",354,398,422,k5abZLzsQc0
21,"and we get a 204. if we run it again, we get a 404, which is expected because that document doesn't exist any longer. and if we go up and retrieve all of them, it's not in that list. all right. so at this point, we have a pretty decent structure. most of the end points are what they're going to look like with our final product. we're just going to add in the database now. so, we're actually storing that on disk instead of just in memory in a dictionary. with this, we're also going to introduce custom types. so, the interface for the api will change slightly. it'll pretty much be the same way to interact with it, but the documentation will have specific types. so it's actually just going to get clearer, not actually change the way you interact with the api. this is why i like to mock data for simple examples or when i am learning a new framework because basically i can think of the end user what they're going to interact with separating that from complexities that may make it more difficult to progress. so now that we have that interface, we can just substitute out the data layer for something better and continually improve on our api. plus, there's a little bit of a learning curve here. some of the database setup is a little complicated, but it's not too bad. once you have it set up, you can pretty much ignore it. but for that initial structure, you will need to do a couple of things because we'll need to define our models and so forth. so i'm going to walk you through that.",367,422,440,k5abZLzsQc0
22,"so i'm going to walk you through that. now, before we get started though, i'm going to add everything to the repo. and i haven't pushed this anywhere, but if you wanted to, you could get push origin main or whatever. so, we have that created. i didn't add the pi cache to the get ignore, which probably would have been smart, but it's not the end of the world. so, what do we use to connect to a database? there are various options. i'm going to go with an om, an object relational mapper. this will basically take database records and convert them to objects in our code. so it's very easy to work with the database just through those objects. it allows us to kind of skip over sql because it does a lot of that for us. oms can have limitations and challenges, but i think it's a great place to start. now when it comes to what om to use, there are a few different ones. big one here is sql alchemy and sql model. now sql model is often recommended with fast api. it's by the same creators. and you can think of it as basically a combination of sql alchemy and pideantic for typing. so that's something i want anyways. i'm going to go this route, but you could of course use sql alchemy or any other om you're interested in. so we will say pip installs sql model and then pip freeze to our requirements file to update that. now much of the database connection information can be found in the fast api docs.",352,440,460,k5abZLzsQc0
23,"now much of the database connection information can be found in the fast api docs. so tutorial user guide and then there's a section for relational databases and they have the connection code here. so this is where i'm getting most of the stuff and if you want to know all the juicy details i have additional information in the notes for this lesson and you can go to the docs as well. but basically we start by creating an engine variable and this is our connection to sql light. this being an om you can easily swap this out to another database. though even if you're going to be using postgress or some other database, this is going to be pretty much the same. the only major thing that will change is the connection url. getting started with sqlite is probably recommended though for learning sake. and then we use check same threads to false allowing fast api to use the same sqlite database in different threads. we then create the initial tables. we will create a session and this will use the engine to communicate with the database. the session keeps track of any changes needed in the data and is the thing that actually interacts with the database. we will create a fast api dependency with yield that will provide a new session for each request. this is what ensures that we use a single session per request and then basically we can pass that session around very easily. so we don't have to do a bunch of work. so we'll create this session dependency and then pass that to anything that needs it.",360,460,475,k5abZLzsQc0
24,so we'll create this session dependency and then pass that to anything that needs it. so an example of that would be this here. we just provide that session to our handler and then we can use it in our code. and then the last thing really is this on startup. we will create the database in tables. i changed it a bit from what's here as i was getting deprecation warnings. so i found a more up-to-date way of doing it. and then here's an example query. we use session.exec. this will allow us to do arbitrary queries or if you're just reading one thing then you can use session.get. so we'll see that as well. so in our code we will first create the sqlite url. sqlite file name is database db and this will just be created in your current working directory. so from where you run the file in this case running it from the same directory it'll just put it right here. from this we will create the connection string. this will be an f string. sqlite colon and three slashes sqlite file name. making sure to spell all that correctly. and then any additional arguments. the big one here is being check same thread. setting that to false. and then we create the engine variable that we'll use for the rest of the interaction with the database. create engine passing in the sql url and then the connect args is a named parameter. so connect args is connect args create engine. this will need to be imported. if you're given the option go for the sql model version.,356,475,500,k5abZLzsQc0
25,"if you're given the option go for the sql model version. a lot of these things will exist in sql alchemy as sql model is built on top of sql alchemy. so if you end up switching to sql alchemy at some point you're already going to have a lot of this practiced. next we will create our function to create the tables. create db and tables sql model dot meta data create all passing in the engine. this will be imported from sql model. next we will have get session. we'll use a context manager here with session engine as session yield session and then we'll import session from sql model and here we will create the session dependency that we will pass around. we will use annotated passing in session and depends on get session and this should not be within get session this should be unindented and then depends is imported from fast api. now for creating the database on startup, this is what is recommended in the docs. however, then i found these lifespan events and this will be something that runs at startup or shutdown logic using the lifespan parameter of the fast api app and a context manager. you can see an example. it'll have a yield in it and everything before the yield runs at startup and everything after the yield runs after. so this will be used for releasing resources and then you'll pass this lifespan to fast api and it'll invoke this when the app runs. so this is pretty much the structure we're going to follow. so we'll come down here async context manager.",356,500,515,k5abZLzsQc0
26,"so we'll come down here async context manager. honestly don't even know what some of this stuff does. definitely need to sharpen up on my skills, but that's okay. so we'll have a yield here, which will basically pause execution for the next run of this. anything before will be done at the beginning. so create db and tables. and then here where we are creating our app instance we will say lifespan is lifespan. so when we run this you should see in your files a database db. so it should create the database for you. if it doesn't, then you might not be connecting this lifespan properly to execute this function or you forgot to put this function here or this create db and tables function is not doing anything. so check all of those pieces. but yeah, that's a cool thing. basically, anytime you mess up or you just want to restart, you can just delete that. no problem. just recreate it. and because of that, i like to usually seed the database with some data. so basically start off with two rows sort of how we did in this example but instead of just this dictionary we will insert some data into the database table. so i'll show you how to do that as well. so right after the database and tables are created we can say with session engine. so this is a context manager that will automatically free any resources. so we have a connection with the database. if not session.exec. and this is where we can invoke a query and see if we get a result back. if so, that means we've already done this.",366,515,538,k5abZLzsQc0
27,"if so, that means we've already done this. so, it's kind of like create table if not exists, but instead it's with an insert. so, we're going to check to see if the table has any data. if it does, we're going to do nothing. if it doesn't, we're going to create two rows. so, we'll say select, and this will be imported from sql model. and i'm actually hitting roadblock here because we need to use our custom type which we haven't actually created yet. so let's just pause on this because this is a great idea. so we have some data to work with right from the get- go, but we'll pause until we get our type built. so let's take a look at building our type. we'll scroll up to the top. and here we can craft our data type. so class campaign and we'll pretty much match this structure here but we have to do it with the special syntax to create a model for sql model. so to do that we use inheritance. so sql model this is how sql model is going to know that this class is a type for the database. additionally we'll explicitly say that this is associated with a table. so table is true. and this is important because we'll create some other types for our api based on this campaign, but it's not directly connected with a database table, but this one will be. so we'll set that to true. and the syntax here is you will use class level variables for all of your fields. so the first one we're going to have is going to be campaign id.",365,538,558,k5abZLzsQc0
28,"so the first one we're going to have is going to be campaign id. and this is what the syntax will look like. we'll say the type that it can be either int or none. and then we will use equals field passing in any customization such as a default or if it's the primary key. so campaign id will be int or none. assign this field. this will need imported. default will be none. this will be handled at the database level. so it should auto increment. but we will want to make sure we set primary key to true. next up we will have the name. this will be a string and we'll set this field. and you can also set indexes in here. so we'll say index is true so we can easily work with the name attribute. next up we will have a due date. this will be a date time or none. and we will make this a field. we will have a default of none. and we will index this. so index true. so this due date i intend for the user to be able to set. so you can basically say, hey, this is when the campaign finishes. so that way everything can be described relative to that date. like, hey, this post needs to go out a day before the due date. then we will also have a created at and this i'm thinking more of a timestamp at the database level. so this will be a datetime. this will be field and we're going to do something a little bit different here for the default.",356,558,585,k5abZLzsQc0
29,"this will be field and we're going to do something a little bit different here for the default. now you could generate this at the application level or at the database level. i'm just going to generate a datetime value here. so i'll come back to this in just a second and we'll say nullable is true and index is true. so for the default you can put a function name. however, you don't actually invoke the function here. so you don't use parenthesis. now what i want to do in this specific scenario is i want to put a function but also give that function some arguments. and to do that without using the parenthesis, you can use a lambda function. basically, an anonymous function will define here. so, it's going to look a little strange if you haven't used lambda before. but basically, it's going to look like lambda colon date time now. and now you can use the parenthesis here because the actual datetime.now is not going to be invoked immediately. and then here you can say timezone. uutc. and i think i have a couple issues here. so we need to fix the spelling. and then this is going to need imported time zone add from datetime. so let me just run through this one more time. basically i want to invoke the datetime.now function. so you could just use that name directly without the parenthesis. but because i want to provide a value for this, i basically have to wrap it in another function that will be invoked later on.",344,585,606,k5abZLzsQc0
30,"but because i want to provide a value for this, i basically have to wrap it in another function that will be invoked later on. you can think of this as a function that all it does is invoke another function passing an argument to it. okay, cool. so this is the model that we're going to do everything through. and what we can do now is we can go create two of these right after we create the database in tables. so i'm going to uncomment these lines and first we will check to see if any data is there. so we'll say campaign select campaign and then we can use dot first. so if this doesn't return any data then what we can do is we can insert two rows session.add all this will take a list and in here we'll just define these as instances of the campaign type. so name summer launch due date can say datetime now and we'll do something very similar for the next one. and we'll just change this to something else such as black friday. and then separate these by commas. after the session add, we'll say session. and let's see, we got an error. let's check what's going on. i think it's just not seeing those columns. so it may be the case that the table's not being created or there's some other disconnect. so let me just look around here. so we have the create db and tables. that all looks good so far. we have our model here.",338,606,625,k5abZLzsQc0
31,"we have our model here. it could be the case that maybe once we created the database initially, we didn't have these there and we didn't have our model complete entirely as i know the server was refreshing as we were changing. so just to be on the safe side, i'm just going to delete the database, make sure it creates the right tables based off of this model, and then inserts that data immediately after. so everything's lined up. so i'll just close out of the server and run it again. that wasn't quite it. i have one other thing i'm going to check. and yeah, it's this default here. there's one little change we need to make here. because we're actually using a callable, we're not using a value. this is going to be default factory. so save that. and it looks like the application was able to start up. so that's great. and what we'll be able to do now is we'll be able to retrieve data from the database. and we should already have these two campaigns preloaded in that database. so, i came down here to our end points, and i'm going to comment these out, all of them, because basically, we're going to recreate these. and it's probably honestly easier to just use them for reference as needed, and just focus on creating new ones that use the database. so, let's first recreate this one here, slash campaigns. so, we'll say app.get c get slash campaigns async defaf read campaigns. now since we'll be working with the database, what you'll be doing is you'll be passing around this session object we created somewhere this here.",365,625,645,k5abZLzsQc0
32,"now since we'll be working with the database, what you'll be doing is you'll be passing around this session object we created somewhere this here. so we'll copy that and inside of our parameter list we'll say session is session depth. now to retrieve the data we'll say data is session.ex exec select campaign and we will want to invoke all. so that's what you'll use when you want to retrieve multiple values and then we'll return data but we want to wrap that in a dictionary as we've done. so similar idea to what we did here. so we'll say campaigns is data. so taking a look at our endpoints now we have the list of campaigns. we will try it out now. execute. and you can see we get two campaigns back. one being the summer launch and one being the black friday sale. now the due date is pretty much going to be the same because i just generated it as datetime.now. but you could definitely set a custom value in the database seating up here. so far so good. but i mean it works. so now we just have to continue on by replacing the rest of these functions. however, before we do that, i want to take a quick break to talk about the types. so if you look back over at the documentation, we can see the response structure here. but if you look at the expected structure, it just says string. basically, it doesn't know what the expected structure is. so it doesn't give us much information. so we need to change what that structure is.",354,645,666,k5abZLzsQc0
33,"so we need to change what that structure is. so that way people who are using this docs page know what to expect and that's really important. so when we look at an overview of our api will be able to see what types are used and how to interact with the api properly. additionally, you might be able to generate client code based on this documentation. so the easiest way to do this is to create a class campaigns response. this is going to be our custom response type. we can structure it however we want. now for basic validation you can inherit from base model. for this we will import it from paidantic and then inside of here we just structure what we're expecting which is the campaigns and this is going to be a list of type campaign. you can then specify the response model. response model is campaigns response. save. it looks to be good. what we can do now is go over to the docs, do a refresh, and you'll notice that we now have the proper structure expected as a response. now, in the notes, i have a bunch of variations you might see for the typing, but ultimately i decided on this approach for for various reasons that make this better, so you can read additional information there. this allows me to get the exact structure that i'm expecting, and i think it's the cleanest of all the different options. let's now work on replacing the next function here which is to get by id. so we'll copy a lot of this here. app.get slash campaigns slash id.",357,666,684,k5abZLzsQc0
34,"app.get slash campaigns slash id. we're going to ignore the response model for a moment. let's just get this function structured. we'll say async defaf read campaign id is int. and now we can say data is session.get get. so if you're going to be retrieving multiple values or anything else, you'll use session.exec. but if we're retrieving a single value, you can do session.get. you'll say from what table and then the id you're going to use. and then the session, we need to pass that in as well. so that's going to be session is capital session depth. and then it's just a simple check. if not data then we can 404 raise http exception status code 404 otherwise we can return data similar idea we'll wrap this in a dictionary so it'll be campaign is data so i think that's good let's check it and then we'll worry about the type in a moment so we'll come back over here do a refresh we should see campaign id notice notice there's no good typing down here, but ignore that for a moment. try it out. we'll try id2. let's check what the actual ids are here. so you can see them. campaign id 1 and campaign id two. so let's do two. we run that and we get just that campaign back. okay. so for the response type, basically you can do the same exact thing, but instead of it being a list, it would just be a single value. so you would have to create a new custom type. and then you're going to be creating a bunch of custom types for all the different endpoints.",364,684,706,k5abZLzsQc0
35,"and then you're going to be creating a bunch of custom types for all the different endpoints. so because of this, i want to introduce the idea of a generic type. basically, what i want to do is i want to create a type where we don't specifically say what the type here is. we keep it general that we can reuse for all of our responses. so we'll just change this to be a little bit more general. this will just be a response we can use for all of our endpoints. and instead of the type being a list of campaign, it will be a list of t. and to specify that this is generic, you'll use another parameter here. generic square brackets t. and then the last thing, instead of campaigns, we'll use a general word for the dictionary key such as data. all right, let's see what else needs imported or anything like that. add from typing. import generic. now, we haven't really talked a ton about types. we've been using them. we've been using the type hints from python up in the models like we did here. but basically throughout this, if you've seen square brackets after a type, it's a generic type. so i think in the previous example we just had, we had, let me back up a second. we had here list and then campaign in square brackets. this was basically saying, hey, we're creating a list and that list is going to be of type campaign. so everything in that list would be a consistent type.",344,706,726,k5abZLzsQc0
36,"so everything in that list would be a consistent type. that same idea will be done now, but we're creating our own type that we can provide a type to. so if you have never done generic programming, it's basically a layer of abstraction, one level higher thinking and that can be pretty challenging if you're new. but uh basically what we're going to do now is we're going to provide what type we want to use when we use this response type. it's basically like a placeholder that we can provide a value here for, but instead of that value being a value like a specific campaign, it's a type. so the thing that we're substituting in is a type, not some value. so let's see what that would look like. instead of using campaigns response, it will now use the response type, the generic one we created. and then because this is a generic, we have to provide the type that we want to substitute in right here inside of square brackets. so here we can just put what we used to have there which was a list of type campaign. so this functionally works the same way. we're just creating our own generic type now. now last thing to get this t to work we'll just say t is type var and then the string t. and then for type var this will be imported from typing. all right. and it looks like everything is good to go. all right. so now we have this custom type. a little bit of extra work but we can use this type for both of these.",358,726,744,k5abZLzsQc0
37,"a little bit of extra work but we can use this type for both of these. so now down here for getting by id we can do a similar thing where we say response model and the type will be response and the type we're going to provide to substitute here for t instead of this being a list of campaign it will just be a single campaign. so campaign. so now when we go over to our docs page, it's going to say the response has a data attribute. and that's one of the trade-offs here. basically, it's always going to be data, but then the user knows it's consistently going to be the same attribute. so what that means is for these returns, we'll just change this to data. so now they're consistent, but the structure for data here is a little bit different. one is a list of campaign and one is a campaign. all right, let's do a refresh now. check it out. so, we have data. it's an array. and then we have by id. it's data where it's an object. so far so good. i also have noticed here a import of name response, which is the same name that we're using for this response here. i don't think we're using response for anything. so, i'm just going to remove that. i think we're using that down here for the delete, but we're not going to need that anymore. but if you were going to need that, then you might want to just alias this or um change the name of your custom type. all right, cool. so, we replaced this get by id. so, let's remove that one.",370,744,767,k5abZLzsQc0
38,"so, let's remove that one. now, let's talk about creating data. so, we'll say app.post and we will have this the same path campaigns status code 201 response model. the create typically returns that new campaign. so it's going to be the same type as getting a campaign by id. and then we'll create the function create campaign. and what we can do here is we can specify a custom type that we expect from the body. so this will be pretty much exactly what we did here. however, a couple of small differences. instead of, you know, manually saying, hey, it's a dictionary with string keys and then any value, we can use our now custom type that we created up here, the campaign type. so, it's a more direct type where it knows what the different attributes are going to be instead of just saying, hey, they're string keys. additionally, we can rename the variable. instead of just body, we could say that it's called campaign. so it'll look something like this. campaign is of type campaign. additionally, we will have session as we've done for the other functions as well. now to add data, we'll say session.add pass in the campaign that was passed in the body session. and then what you can do is you can do a session.refresh. and this will make sure that the campaign that you're working with has the most up-to-date data from the database. and then we can return data is campaign. so basically when you create the data in the database, the database might do additional stuff. for example, it gives it an id.",356,767,788,k5abZLzsQc0
39,"for example, it gives it an id. it gives it a created at timestamp or it might do any transformations. and we need a colon right there. so what this refresh does is just make sure that we get all of that information in the campaign variable. so when we return it back to the user, it has all that new data. at this point, we're not going to need rand int anymore. so we'll just remove that al together. and let's run this. give it a shot. so do a refresh here. you can now see we have the post. it gives us an example value, which is pretty handy. so we'll try it out. execute. and we get an internal server error. so let's take a look at this error. we'll go to the terminal and it complains about the date time and date objects as input. this has something to do with the timestamp values and the created at. so if you look at what we're providing, we have the due date and the created at being given here as well as the campaign id. so basically what's happening is the expected structure is a little bit different than what we actually want. we don't want the user to provide the campaign id or the created at and that's causing issues at some point with this error we're getting. so what we actually want to do is we want to create a type that's just a little bit different where it doesn't expect the campaign id and it doesn't expect the created at since those are going to be done server side.",357,788,809,k5abZLzsQc0
40,"so what we actually want to do is we want to create a type that's just a little bit different where it doesn't expect the campaign id and it doesn't expect the created at since those are going to be done server side. so really this is what the structure we want to use should look like. so to do this we'll create another type here and this will be a campaign create. so for this i prefix with the larger type campaign and then follow it up with the operation. and i'd say that's a pretty good convention. so then we have if we have multiple variations of this they all start with campaign and it's very easy to find that type. and then this is going to inherit from sql model. but we're not going to set table to true because this is not going to directly associate to a table as the table has four attributes. this is only going to have two. so it's a little bit different than this campaign up here. so now for the attributes, all we need to do is say the type since we're not mapping directly to a table. we're just worried about the python types here. and then due date will be date time or none. and we will provide a default value of none. now we'll go back down to our create function. so right here and we're no longer expecting a campaign. we're expecting a campaign create. however, this is now going to lack some of the automatic processing. so what we need to do is we need to do one additional thing.",358,809,827,k5abZLzsQc0
41,"so what we need to do is we need to do one additional thing. we basically need to validate it against the database structure. so db campaign then we'll use the type that is associated with the table which in our case is campaign and there should be a model validate and this will take a pantic model meaning just like a a structure for validation that's not associated with the database. it'll take an instance and validate it against the database structure. so we can pass in our campaign object here. then we will use this new variable db campaign for the rest of this code here. so let's test this out now. you can see the expected type is now proper. try it out. execute. we're still getting an internal server error. so let's check it out. so it says unhashable type list. and that is a mistake on my part because this should be a string key. so not the data variable which is a list from somewhere else in our code. so let's try now. we'll do a refresh. try it out. execute. scroll down. and here is our response body. you can see it created the data. it gave it an id. it gave it a created at. and it kept our due date and name. so now we could go in here. we could create any with custom values. so maybe it's a christmas sale. execute. and there you go. wow, that was quite a bit of work, but we got it working, so that's good to know. all right, cool. so, we've completed creating a campaign. we can now worry about updating a campaign. app.put slash campaigns id.",370,827,861,k5abZLzsQc0
42,"app.put slash campaigns id. response model will be our custom response type. and then this will return a single campaign. and then we'll say async defaf. i think earlier i got these uh function names backwards. i don't know how i managed to do that. but we had update campaign and then we had delete campaign. so update campaign for this new one we'll take the campaign id which will be an int and then we will parse the body call it campaign and this will be of type campaign and actually we can just use the campaign create type here for the update as we only need the name and the due date. those are the only two things we would change. so you could call it something else or we could create a custom campaign update but i think this is fine. and then lastly, we will need the session so we can work with the database. now, for this, we're going to first retrieve data from the database. so, this will be similar to what we did up here. so, i'm going to copy that and paste it here. and we'll make sure this is all lined up properly. so, we first get a campaign by id. if we don't find it, we will raise a 404. but before we just return data, we'll need to update data.name. name. set that to the campaign that was passed in the bodies name. same thing for data dot due date is going to be campaign. then we do a session add passing in this data and a session. and then a session.refresh to get the latest changes from the database.",365,861,883,k5abZLzsQc0
43,"and then a session.refresh to get the latest changes from the database. so we're making sure we're actually giving back the correct data. we'll pass in data here as well. and then if all goes well, we can return data. so let's try this out. we will go over to the web page, do a refresh, grab the put, and we will do campaign id 4. or you can just grab your list of campaigns to see what ones are available. we will change the name to something new. change this due date to 2026. execute. internal server error. let's check that out. i think this is just a naming issue as i'm using campaign id for the parameter, but then i use id for it down here. so id is a built-in thing with python. so throughout this i i've been overwriting that, but if it's not something you're directly using, it's not going to be the end of the world. it's probably healthier to use uh the campaign id prefix so you're not overriding that. but i guess for consistency sake, since we've been using id throughout this whole thing, i'm just going to change this to id. let's try this out again. i did a refresh just to make sure. hit execute. and you can see it updated. awesome. so, we have update campaign done. now, let's talk about deleting a campaign. so, app delete campaigns id. we'll do a status code equals 204 async defaf delete campaign id is of type int and then we'll have the session we will retrieve the data and then we'll just say session.delete passing in data and then session commit and that's pretty much it.",370,883,908,k5abZLzsQc0
44,"we'll do a status code equals 204 async defaf delete campaign id is of type int and then we'll have the session we will retrieve the data and then we'll just say session.delete passing in data and then session commit and that's pretty much it. let's try this one out. we'll go over to the web page do a refresh open up delete. we will delete four. execute server response is a 204. great for i was like, oh, validation error. but no, these are just the possible responses. cool. so, we've pretty much finished up the basic crud. overall, not too bad. i mean, it was pretty brutal getting all of these crud capabilities with the model set up. at this point, i think everything's pretty good. we don't need this mock data anymore. and at this point, everything's still in one single file. so obviously, as you grow, you're going to want to break things out into separate files, make things more organized by the type of data you're retrieving. so you might have some dedicated files for campaigns. there's different architecture approaches. so for example, you could put all the campaign stuff together, or you could do more like an mvc structure where you put all the models together and the views and the controllers. for now, we've covered so much in this one video. i think this is a great stopping point, but i do want to continue this project. so, please, if you're enjoying this material, give my channel a subscribe and check out the playlist link for you to go through all of the development content.",353,908,928,k5abZLzsQc0
45,"so, please, if you're enjoying this material, give my channel a subscribe and check out the playlist link for you to go through all of the development content. reminder, i do have notes with all of the material we talked about and additional resources and links. and additionally, i will commit this to the repo. so get add get commit basic db crud get push origin main. oh yeah i didn't make a repo so awkward. so we'll create a repo fast api example. create repo and then i will push this up to github. awesome. so now you should be able to find this at this url as well. couple other minor changes. i'm going to remove this nullable true. i don't think we need that. if the database is always creating the timestamp, then we don't want to be able to set it to null. for the due date seated data, i'm going to make sure i do timezone.utc just for consistency with how we're creating timestamps, but it's not really a huge deal. thank you so much for watching. hopefully this is helpful. there's still a lot to learn. some ideas i have are talking about relationships, which is a huge one for databases, pageionation and url parameters, organizing our code and different architectures, middleware, that's a huge one, and user authentication, and so much more. so, please stay tuned for the upcoming lessons. i'm really excited for this content. i look forward to seeing you there. goodbye.",326,928,949,k5abZLzsQc0
0,so apparently data is the new oil and if that's the case let's learn how we can control that data and store it oursel and then refine it ourself now what i really mean here is we're going to be building out an analytics api service so that we can ingest a lot of data we can store a lot of data into our database that changes over time or time series data the way we're going to be doing this is with fast api as our micros service this is going to be our api inpoint that has well mostly just one data model that we're going to be ingesting from there we're going to be using of course python to make that happen that's what fast api is built in but we're going to be storing this data into a postgres database that's optimized for time series called time scale i did partner with them on this course so thanks for that time scale but the idea here is we want to be able to keep track of time series data time scale is optimized postgres for time series data it's really great i think you're going to like it now we're also going to be using docker in here to make sure that our application is containerized so we can deploy it anywhere we want and we can use the open-source version of time scale to really hone in exactly what it is that we're trying to build out before we go into production which absolutely we will be now the idea here is once we have it all containerized we'll then go,358,0,0,tiBeLLv5GJo
1,ahead and deploy it onto a containerized cloud called railway which will allow us to have a jupyter notebook server that can connect to our private api inpoint our private analytics server altogether now all of this code is open source and i did say it's done in fast api which really means that we're going to be writing some fairly straightforward python functions that will handle all of the api inp points and all that the thing that will start to get a little bit more advanced is when we do the data modeling and specifically when we do the querying on the data modeling now these things i think are really really fascinating but i ease you into them so i go step by step to make sure that all of this is working now i do want to show you a demo as to what we end up building at the end of the day we got a quick glance at it right here but i want to go into it a little bit more in depth now i do hope that you jump around especially if you know this stuff already and if you come from the d jango world the models that we're going to be building i think are a little bit easier to work with than jango models there's less to remember because they're based in pantic which is what sql models is based in as well which allows you to write just really simple models it's really cool it's really nice to do so those of you who are from d jango this part will be very straightforward,358,0,0,tiBeLLv5GJo
2,fast api itself is also very straightforward and a really really popular tool to build all of this out now the nice thing about all of this is all the code is open source so you can always just grab it and run with it and deploy your own analytics api whenever you want to that's kind of the point so if you have any questions let me know my name is justin mitchell i'm going to be taking you through this one step by step and i really encourage you to bounce around if you already know some of these things and if you don't take your time re-watch sections if you have to some of us have to and that's totally okay i know i had to repeat these sections many time to get it right so hopefully it's a good one for you and thanks for watching look forward to seeing you in the course the point of this course is to create an analytics api microservice we're going to be using fast api so that we can take in a bunch of web traffic data on our other services and then we'll be able to analyze them as we see fit and we'll be able to do this with a lot of modern technology postgres and also specifically time scale so we can bucket data together and do aggregations together let's take a look at what that means now first and foremost at the very end we are going to deploy this into production and then we will have a jupyter notebook server that will actually access our private analytics api service,358,0,0,tiBeLLv5GJo
3,this is completely private as in nobody can access it from the outside world it's just going to be accessible from the resources we deem should be accessible to it which is what you're seeing right here once we actually have it deployed internally and private we can send a bunch of fake data which is what's happening this is pretending to be real web events that will then be sent back to our api as you see here so this is the raw data so from this raw data we will be able to aggregate this data and put it into a bulk form that we can then analyze now of course this is just about doing the api part we're not going to actually visualize any of this just yet but we will see that if i do a call like duration 2 hours i can see all of the aggregated data for those two hours now this becomes a lot more obvious if we were to do something like a entire month and we can actually see month over month data that's changing but in our case we don't have that much data it's not really about how much data we have it's much more about how to actually get to the point where we can aggregate the data based off of time based off of time series that is exactly what time scale does really well and enhances post cres in that way so if we actually take a look at the code the code itself of course is open source feel free to use it right now you can actually come,358,0,0,tiBeLLv5GJo
4,into the src here into the api into events into models you can see the data model that we're using if you've used sql model before this will actually give you a sense as to what's going on it's just slightly different because it's optimized for time series and time scale which is what time scale model is doing now if you've used d jango before this is a lot like a d jango model it's just slightly different by using sql model and something called pantic don't don't worry i go through all of that and you can skip around if you already know these things but the point here is that is the model we are going to be building to ingest a lot of data after we have that model we're going to be able to aggregate data based off of all of that which is what's happening here now this is definitely a little bit more advanced of an aggregation than you might do out of the gates but once you actually learn how to do it or once you have the data to do it you will be able to have much more complex aggregations that will allow you to do really complex you know averaging or counting or grouping of your data all of it is going to be done in time series which is i think pretty cool now when we actually get this going you will have a fully production ready api endpoint that you can then start ingesting data from your web services i think it's pretty exciting let's go ahead and take a look at all,358,0,0,tiBeLLv5GJo
5,of the different tools we will use to get to this point let's talk about some of the tools we're going to use to build out our analytics api first and foremost we're going to be using the fast api web framework which is written in python now if you've never seen fast api before it's a really great framework to incrementally add things that you need when you need them so it's really minimal is the point so there's not a whole lot of batteries included which gives us all kinds of flexibility and of course makes it fast so if you take a look at the example here you can see how quick we can spin up our own api that's this right here this right here is not a very powerful api yet but this is a fully functioning one and of course if you know python you can look at these things and say hey those are just a few functions and of course i could do a lot more with that that's kind of the key here that we're going to be building on top of now if you have built out websites before or you've worked with api services before you can see how easy it is to also do the url routing of course we're going to go into all of this stuff in the very close future now of course we're going to be using python and specifically python 3 the latest version is 3.13 you can use that 3.13 3.14 you could probably even use 3.10 maybe even 3.8 i'm going to most likely be using 3.12,358,0,0,tiBeLLv5GJo
6,or 13 but the idea here is we want to use python because that's a dependency of fast api no surprises there probably now we're also going to be using docker now i use docker on almost all of my projects for a couple of reasons one docker locally emulates a deployed production environment it does it really really well and you can actually see this really soon but also we want to build our applications to be production ready as soon as we possibly can docker really helps make that happen now another reason to use docker is for database services so if you just want a database to run on a per project basis docker makes this process really easy and so of course we want to use a production ready database and we also want to use one that works with fast api and then is geared towards analytics lo and behold we're going to be using time scale time scale is a postgres database so it's still postres sql but it's optimized for time series now yeah i partnered with them on this series but the point here is we're going to be building out really rich robust time series data and fullon analytics right that's the point here we want to actually be able to have our own analytics and control everything so initially we're going to be using docker or the dockerized open- source version of time scale so that we can get this going then as we start to go into production we'll probably use more robust service is from time scale directly now we're also going to be,358,0,0,tiBeLLv5GJo
7,using the cursor code editor this of course is a lot like vss code i am not going to be using the ai stuff in this one but i use cursor all of the time now it is my daily driver for all of my code so i am going to use that one in this as well now i also created a package for this series the time scale db python package so that we can actually really easily use fast api and something called sql model inside of our fast api application with the time series optimized time scale db that's kind of the point so that's another thing that i just created for this series as well as anything in the future that of course is on my private github as far as the coding for entrepreneurs github all of the code for this series everything you're going to be learning from is going to be on my github as well which you can see both of those things linked in the description so that's some of the fundamentals of the tooling that we're going to be using let's go ahead and actually start the process of setting up our environment in this section we're going to set up our local development environment and we wanted to match a production or a deployed environment as closely as possible so to do this we're going to download install python 3 we're going to create a virtual environment for python projects so that they can be isolated from one another in other words python versions don't conflict with each other on a per project basis,358,0,0,tiBeLLv5GJo
8,then we're going to go ahead and do our fast api hello world by installing python packages and then of course we're going to implement docker desktop and docker compose which will allow for us to spin up our own database that postgres database that we talked about previously and of course it's going to be the time scale version the open source version of that once we have our database ready then we'll go ahead and take a look at a docker file for just our fast api web application so that that can also be added into docker compose and again being really ready to go into production then we'll take a look at the docker based fast api hello world now all of this you could skip absolutely but setting up an environment that you can then repeat on other systems whether it's for development or for production i think is a critical step to make sure that you can actually build something real that you deploy for real and get a lot of value out of it so this section i think is optional for those of you who have gone through all of this before but if you haven't i'm going to take you through each step and really give you some of the fundamentals of how all of this works just to make sure that we're all on the same page before we move to a little bit more advanced stuff and advanced features so let's go ahead and jump in to downloading installing python 3 right now we're now going to download install python and specifically python 3.13 or,358,0,0,tiBeLLv5GJo
9,3.13 now the way we're going to do this is from python.org you're going to go under downloads and you're going to select the download that shows up for your platform now there are also the platforms shown on the side here so you can always select one of those platforms and look for the exact version that we're using which in my case it's python 3.13 and you can see there's other versions of that already available that you can download with now this is true on windows as well but the idea is you want to grab the universal installer for whatever platform you're using that's probably going to be the best one for you this isn't always true for older versions of python but for newer ones this is probably great so we're going to go ahead and download that one right there and if course if you are on windows you go into windows and you can see there's a bunch of different options here so pick the one that's best for your system now if you want a lot more details for those of you who are windows users consider checking out my course on crossplatform python setup because i go into a lot more detail there now the process though of installing python is is really straightforward you download it like we just did this is true for windows or mac and then you open up the installer that you end up downloading and then you just go through the install process now one of the things that is important that it does say a number of times is use,358,0,0,tiBeLLv5GJo
10,the install certificates we'll do that as well just to make sure that all of the security stuff is in there in as well so i'm going to go ahead and agree to this installation i'm going to go ahead and run it i'm going to put my password in i'm going to do all of those things as you will with installing any sort of software from the internet now i will say there is one other aspect of this that i will do sort of again in the sense that i will install python again using docker and specifically in docker hub so yeah there's a lot of different ways on how you can install python and use it but both of these ways are fairly straightforward okay so it actually finished installing as we see here and it also opened up the finder window for me for that version if you're on windows it may open this folder it might not i haven't done it in a little while but the idea is you want to make sure that you do run this installation command here which if you look at it is really just running this pip install command we'll see pip installing in just a moment but that's actually pretty cool so we've got pip install uh you know the certificate you know command basically to make sure all that security is in there now once you install it we just want to verify that it's installed by opening up another terminal window here and run something like python 3 - capital v now if you see a different version of,358,0,0,tiBeLLv5GJo
11,python here there's a good chance that you have another version already installed so for example i have python 3.12 installed as well you can use many different versions of python itself all of these different versions are exactly why we use something called a virtual environment to isolate python packages again if you want a lot more detail on this one go through that course it goes into a lot more detail on all different kinds of platforms so consider that if you like otherwise let's go ahead and start the process of creating a virtual environment on our local machine with what we've got right here part of the reason i showed you the different versions of python was really to highlight the fact that versions matter and they might make a big issue for you if you don't isolate them correctly so in the case of version 3.12 versus 3.13 of python there's not going to be that much changes in terms of your code but what might have a major change is the thirdparty packages that go in there like what if fast api decides to drop support for python 3.12 and then you can't use that anymore that's kind of the idea here and so we need to create a virtual environment to make that happen which is exactly what we talked about before so back into cursor we're going to go ahead and open up a new window inside of this window we're going to go ahead and open up a new project so i want to store my projects in here so i open up a new project i,358,0,0,tiBeLLv5GJo
12,find a place on my local machine as to where i'm going to store it and we're going to call this the analytics api just like that and i'll go ahead and open up this folder okay so normally with cursor it's going to open you up to the agent i'm not going to use the agent right now i'm just going to be using just standard code and we're going to go ahead and first off save the workspace as the analytics api we'll save it just like that then i'm going to go ahead and open up the terminal window which you can do by going to the drop down here or if you learn the shortcut which i recommend you do it can toggle it just like i'm doing okay so the idea here is we want to just of course verify that we have got python 3 in there you can probably even see where that version of python 3 is stored this actually shouldn't be that different than what you may have saw when we installed called the certificates here but the idea of course is we're going to use this to activate our virtual environment so i'm going to go ahead and do python 3.12 or rather python 3.3 13 and then do m venv v env so this is the mac command for it this also might work on linux if you're on windows it's going to be slightly different which is going to be basically the absolute path to where your python executable is so it's going to be something like that then- mv andv v andv okay,358,0,0,tiBeLLv5GJo
13,so this is another place where if you don't know this one super well then definitely check out the course that i have on it uh which was this one right here so just go ahead and do that okay so the idea is now that we've got this virtual environment all we need to do is activate it now the nice thing about modern text editors or modern code editors is usually when you open them up they might actually activate the virtual environment for you in my case it's not activated so the way you do it is just by doing source vmv and then b activate this is going to be true on linux as well windows is going to be slightly different unless you're using wsl or you're using powershell uh those things might have a little different but more than likely windows is going to be something like this where it's uh vmv scripts activate like that where you put a period at the beginning that will help activate that virtual environment and so now what we do is we can actually do python dv notice the three is not on there and here it is and of course if i do which python it will now show me where it's located which of course is my virtual environment and so this is the time where we can do something like python dm pip or just simply pip both of those are the python package installer and we can do pip install pip d- upgrade this is going to happen just on my local virtual environment it does not affect pip,358,0,0,tiBeLLv5GJo
14,on my local machine which we can check by doing pip again if i do that notice that it's not working right so it works in here where i do pip but it does not work in my terminal window nonactivated terminal window if i do python 3-m pip um then i'll get it or rather just pip that will give me that actual thing and it's showing me where it's being used just like that same thing if i came back in to my virtual environment scrolled up a little bit it will show me something very similar here's that usage if i do the python version it will show me the same sort of thing that we just saw but it's going to be based off of the virtual environment just like that so that's the absolute path to it of course it's going to be a little bit different on your machine unless you have the same username as i do and you stored it in the same location but overall we are now in a place to use this virtual environment so let's see how we can install some packages and kind of the best approach to do that now let's install some pyth packages it's really simple it's a matter of pip install and then the package name that is how the python package index works if you're familiar with something like mpm these are very similar tools but the idea here is if you go to p.org you can search all of the different p published python packages and pip install can install them directly from there so if we,358,0,0,tiBeLLv5GJo
15,did a quick search for fast api for example we can see here is the current version of fast api that's available and this is how we can install it now the key thing about these installations is just like many things there's many different ways on how you can do this there are other tools out there like poetry is another tool that can do something like poetry ad i believe that's the command for it but the idea here is there's a lot of different ways on how you might use these different package names i stick with the built-in modules cuz they are the most reliable for the vast majority of us now once you get a little bit more advanced you might change how you do virtual environments and you also might change how you install python packages but the actual python you know like official repository for all of these different packages is piie and they still say pip install so it's still very much uh a big part of what we do okay so the idea here now is we need to install some packages so how we're going to do this is we're going to go ahead and once again i'm going to open up my project and in my case i actually closed it out mostly so i can show you the correct way to install things here's my recent project here with that workspace all i have is a virtual environment and that code workspace in there now if i were to toggle open the terminal it may activate the virtual environment it may not so the,358,0,0,tiBeLLv5GJo
16,wrong way to do this is to just start trying to do pip install fast api in this case it says commands not found the reason this is the wrong way is cuz i don't have the virtual environment activated so i have to activate that virtual environment just like that if i need to manually do it if for some reason the terminal is not automatically doing it then we can do the installation so we can go ahead and do pip install fast api and hit enter and just go off of that and that's actually well and good except for the fact that i don't have any reference inside of my project to the fast api package itself so i have no way to like if i were to accidentally delete the virtual environment i have no way to like recoup what i did so what we need to do then is we create something called requirements.txt once again this is another file that could be done in different ways and there are other official ways to do it as well but one of the things that's nice about this is inside of this file we can write something like fast api and then i can do pip install d r requirements.txt which will then take the reference from this file assuming that everything's installed and saved and so once i save it i can see that that's the case it's now doing it this comes in with the versions then so what we do then is we can actually grab the version and say it's equal to that version right there in,358,0,0,tiBeLLv5GJo
17,which case i can run the installations now this is actually really nice because it kind of locks that version in place now if you go into the release history you could probably go back in time and grab a different version like 011 uh 3.0 if we do that so 0113 and then 0.0 i save that now i run that installation it's going to take that old one and it's going to install the things that are relative to that uh inside of my entire environment now in my case i actually want to use this version right here and a lot of times i would actually do another tool to make sure that this version is correct that other tool is called pip tools which i'm not going to cover right now but the idea here is we want to keep track of our requirements and fast api of course is one of them now within fast api we have something else called uvicorn that we will want to use as well this is often hand inand with using fast api so once again we see pip install uvicorn and you can just do something like that now there is something else with uvicorn that we might want to use and that's called gunicorn so g unicorn and we do a quick search for that this is for when we want to go into production g unicorn and uvicorn can work together so once again i'll go ahead and bring that in uvicorn and gunicorn they are um great tools for running the application in production but we usually don't have to lock,358,0,0,tiBeLLv5GJo
18,down their version i think once you go into production you need regular things then yeah you'll probably want to lock down the version the more important one is probably fast api but even that we might not need to lock down our version at this stage like if you're getting something from what i'm telling you right now then you probably don't need to lock down the version yet if you already know oh i need to lock down the version then you'll probably just do it anyway that's kind of the point that's why i'm telling you that um but the idea here is of course we've got all these different packages and then of course my package that is definitely a lot newer you do a quick search for it and it's time scale db it is not a very like there's not that many versions of this package so this is another one that i'm going to go ahead and not lock down because i definitely don't want that so within that package there is something called sql model which is a dependency of the other package so it's this one right here this one also doesn't have that many versions itself uh but it's pretty stable as it is and this one works well with fast api and you'll notice that it's powered by pantic and sql alchemy which means that we probably want to have those in there as well just so we have some reference to it so i'm going to go ahead and bring that in here with pantic and sql alchemy there we go so now that,358,0,0,tiBeLLv5GJo
19,we've got all of the different versions here i can once again do pip install r requirements.txt and then i'll be able to install all of the packages as need be now here's the kicker this is why i'm showing you is because what you want to think of your virtual environment as just for the local environment you are not going to be using it in the production environment you'll be doing a completely different one what's interesting is my terminal now opens up that virtual environment and attempts to activate it which is pretty funny but i don't actually have one yet so i'm going to go ahead and re bring it back now i've got that virtual environment back of course if you're on windows you might have to use something different here but i'm going to go ahead and reactivate it with bin activate and then we'll go ahead and do piv install r requirements.,206,0,0,tiBeLLv5GJo
20,txt hit enter and now i'm getting all of those requirements back for this entire project this is going to come up again we will see it for sure because it's really important okay so now we've got our requirements we've got our virtual environment it's time to actually do our fast api hello world before we go much further i will say if there are file changes within the code it will be on the official github repo for this entire series as to what we're doing going forward so in the in other words in branches of start this is the branch we're going to finish off with of course this one doesn't have everything in it like we just did but it will and so that's the start branch that's where we're leaving off but as you see there's license and read me those things i'm not going to show you how to do or the get ignore file uh they're just going to show up in just a moment but now that we've got this let's go ahead and actually create our first fast api application it is very straightforward we now want to do our fast api hello world so the way we're going to do this is by creating a python module with some fast api code and just run it the key part of this is just to really verify that our package runs that it can actually go so to do this we're going to go ahead and go back into our project here and of course i want to open up my terminal window if the virtual,358,1,1,tiBeLLv5GJo
21,environment activates that's great if it does not activate like that then we want to activate it manually every once in a while when you create a virtual environment you might see this where it automatically activates we hope that it will start to automatically activate i'll mention as to when it might happen but the idea here is we want to activate that virtual environment so source vmv bin activate or of course whatever you have on your machine then i'm going to go ahead and do pip install r requirements.txt it's not going to hurt to run that over and over and over again right it never will because if it's already installed it's not going to do anything cool other than tell you that it's already installed so the idea here is we want to make sure that it's installed so we can actually run it the next question of course how do we run it well if we go into the fast api documentation and we go to the example we see here some example python code so it says create a file main.py with all this stuff so the question then is where do we actually put this and that's going to be as simple as you could do it right here in main.py you can copy this code paste in here save it with command s which is exactly what i do a lot i won't actually show you that i'm saving it i'll just save it i just saved it like 10 times just there so now that we've got that we actually want to run this code right,358,1,1,tiBeLLv5GJo
22,so how do we actually run it well going back into the docs again you scroll down a little bit you see that it says run it right here and so i'm going to go ahead and attempt to run it with fast api dev main.py hit enter i get an error so there's a couple things that i want to fix before this goes any further but the idea here is this may might make you think think that oh we didn't install fast api correctly and in a way we didn't we didn't install it to use this particular command line tool it is a pce ma command line tool it's not necessarily going to be there by default in this case it's simply not there now in my case you could i could consider using this as my development server for fast api what i actually want to use is uvicorn because it's closer to what i would use in production and it's actually what fast api is using anyway as you look in the documentation you can scroll down a little bit it says we'll watch for changes uvicorn running and all that so it's basically uvicorn anyway so that's what we want to do is we want to have uvicorn run our our actual main app so you use uvicorn then this is the command if you hit enter you should be able to see this now if you're on windows you might actually have to use waitress at this point waitress is just like uvicorn but that might be the one that you end up using we'll talk about solving,358,1,1,tiBeLLv5GJo
23,that problem in just a little bit when we go into dock but for now um i'm going to be using uvicorn and of course if you are on windows go ahead and try that fast api standard that will probably work for you as well okay so the idea here is now we want to actually run this application with uvicorn and then the way we do that is we grab main.py here and then we use colon looking back into main.py we look for the app declaration for fast api which is just simply app and then we can do something like d- reload hdden enter oh that trailing slash should not be there so i'll try that again we hit enter and now once again it says we'll watch for these changes and it's running on this particular port just like what we see right in here great it just doesn't show us this serving at and docs at and stuff like that as well is there's not a production version of this just yet that we'll use we will see it though okay great so now i can open this up with command click or just copy this url you can always just control click and then open it up like that but the idea is hello world it's running congratulations maybe this is the first time you've ever run a fast api application and it is up and running and it's working on your local machine now don't get too excited because you can't exactly send this to anybody right so if i were to close out this server which we,358,1,1,tiBeLLv5GJo
24,have a few options to do so one is just doing contrl c which will do that it just literally shuts it down another option is just to you know kill the terminal and then you can open up the terminal again in which case ours right now we need to then reactivate it and then we can run that command again by just pressing up a few times we'll be able to find it and there it is okay great so now we have it running what can we do we well we need to actually put it in a location that makes more sense than where it is right now main.py is in the root folder here this is basically something that no one ever does when you start building you know professionally you often put it inside of src main.,185,1,1,tiBeLLv5GJo
25,p like that notice that i put that that slash in there that's important because as soon as i hit enter it actually creates a folder for me with that new module which is where i'm going to go ahead and actually paste all of that code again and then i'll just go ahead and delete this one with command backspace that allows me to delete it of course there's other ways to delete it but there we go okay so now we can verify that fast api is running it's working it's ready to go so if you're on windows and you have docker installed or if you just have docker installed the next part is going to allow for us to build on top of this or the next half of this section is going to help us build on top of this so that we can actually use this code a little bit more reliably than we currently do let's say you want to share this application with one of your friends and you want to help them set it up to run it what you'd probably tell them is hey download python from python.org make sure that you create a virtual environment install these requirements and then use uvicorn to run main.py and then you remember oh wait you also have to activate the virtual environment then install the requirements then run with u viacor then you might remember oh make sure you download python 3.13 because that's the version we're going to be using so there is some nuance in just the setup process to make sure that this is working,358,2,2,tiBeLLv5GJo
26,correctly that of course you and your friend could figure out by talking but it would be better if you could just give them something and it just worked docker containers are that something what we end up doing is we package up this application into something called a container image and we do it in a way very similar to what we've been doing which is download install the runtime that you want to use like python 3 create a virtual environment activate that virtual environment install the requirements and then run your p your you know your application your fast api application so we will get to the point where we actually build out these things very soon but right now i want to just show you how you can run them how you can just use them by downloading docker desktop so if you go to dock.,195,2,2,tiBeLLv5GJo
27,and hit download docker desktop for your platform you'll be able to get the docker desktop user interface just like this but you will also more importantly get docker engine to be actually able to run docker containers to be able to create them and to be able to share them the key parts of using docker now docker itself can be really complex so i want to keep things as straightforward as possible so it's very similar to what we did with python when we downloaded and installed it but we're going to do that the docker way right now so at the point that you download it you're going to go ahead and open it up on your machine and it's going to look something like this ui right here now if for some reason this isn't popping up and it's just in your menu bar you might actually see that as soon as you exit out it might be something like this in which case you'll just go to the dashboard and that will actually open it back up that's just off of the recording screen for me right now which is why you're seeing it this way but the idea here is we now have the docker desktop on our local machine now i also want to verify that this is working correctly because if i don't verify it then it's going to be hard to run in general so opening up the terminal i should be able to do docker ps docker ps really just shows me all of the things that are running on my machine we'll come back to,358,3,3,tiBeLLv5GJo
28,this in just a moment but the idea is if this runs an error you don't have it done correctly the error will look something like that so just type in gibberish that's the error right command not found in this case so i want to make sure that that's there and i also want to make sure that docker compose is there we'll come back to docker compose really soon but for now we're just going to have those two commands now i'm going to be giving you a very minimal version of using docker and learning how to use docker over the next few videos or next few parts of this section now the reason for that has to do with the fact that we need it for a very stable local development environment but also so we can put push this into production so let's actually take a look at how we can actually use a docker container image right now so on docker desktop there's this docker hub right here there's also docker hub if you go to hub.,237,3,3,tiBeLLv5GJo
29,deer.com both of these things are just simply docker hub and you can search for other container images it's not surprising that if you look at the docker hub on docker desktop versus the website they look basically the same that's of course makes a whole lot of sense now in our case we're going to go ahead and do a quick search for python we want to do the python runtime so when we went to python.org we went to downloads and you could have gone to one of your platforms here and you could actually grab a specific version of python so this is true on dockerhub as well but instead of saying download it's just a tag so you just go into tags here and you can find different versions of python itself you can get the latest version which often is a good idea but just like what we talked about with the actual versions of you know packages or python software that we're using a lot of times you want to use a very specific version to make sure that all of these things work as well so going back into uh you know the dockerhub we'll do a quick search for python 3.13 and so what we see in here is python 3.13 the one we've been using here's 3.1 3.2 and of course if we go back into you know actual python releases we can see there's 3.12 right there and and it's the same one exact same one is through docker one is directly on your local machine so the one through docker is going to work on,358,4,4,tiBeLLv5GJo
30,linux windows mac it's going to work crossplatform it's really great that it does that but i actually don't want to use the version that i already have on my machine i want to use an a really old version so python 3.16 and we've got relevant and it has vulnerabilities there's a lot of security concerns with using this old one for our production systems but for this example there's no con concerns at all we can delete it it's really simple to do so the way we actually get this thing working is by copying one of these pole commands so we can scroll on down i'm going to go ahead and just grab the one that is 3.6.1 i'm going to go ahead and copy this command here go into my terminal and i'll go ahead and paste that in with that pole command so this is going to go ahead and download python 3.1 or 3.6.1 5 directly from dockerhub it downloads it in the image or the container form so we can now run this with docker run python colin 3.,241,4,4,tiBeLLv5GJo
31,6.15 and hit enter and it immediately stops so the reason it immediately stops is because docker itself has a lot of features it can do a lot of things one of those things is we can do an interactive terminal for this which is- it with that same docker container image and we can hit enter there now it actually opens up python for me it's quite literally in the python shell much like if we were to open up a new terminal window here and type out python 3 in there that is also a python shell right the difference here is if we actually clear this out i can just run it at any time and i can change the version there's also another difference that says linux up here here and darwin down here that's because on my local machine i'm on on a mac but in the docker container i'm on a linux so docker containers are basically linux with a bunch of things already set up now the other cool thing about this is i can exit out of here of course and then i can run python 3.7 and now it says it can't find it so it's going to go ahead and download it for me all the while on my local machine if i try to do python 3.7 it's not available python 3.6 not available very similar to using a virtual environment but it's another layer of isolation and it allows us to have these packaged run times that we can use at any time which is fantastic so the other part about this of course,358,5,5,tiBeLLv5GJo
32,is that inside of docker desktop we can delete these old ones that we don't want to use and you're going to want to get in this habit sometimes to make sure that you don't have these old versions of python on your machine because they're rather large that's why there's other versions or other tags of it right so if you go back into docker hub you'll see that there's this slim buster look how much smaller of a an actual python project it is it's way smaller so you can just grab that version and then you can come in here and do the same thing docker run-it that version right there this is going to download it it's going to be much smaller which means that it probably doesn't have nearly as many features out of the gates it's still on linux but we can do you know all sorts of python things in here as you would and then you can then see inside of docker desktop all of the things that are downloaded in here and then the slim buster is quite a bit smaller than these other ones and so another thing about docker desktop that's really nice is you can come in here you can search for python and you could do a quick search which gives us all of these ones that are using the python image all of these are running versions of the application which you can stop and delete which you would want to do and then you go into your images you can also delete these as well just like that which gives you,358,5,5,tiBeLLv5GJo
33,back 2 gigs of space which you're definitely going to want to get in the habit of doing in which case then when you want to run it again in the future it will just go ahead and redownload that image right so it's really meant to be ephemeral like this you're meant to think of docker as something that is temporary so you can add it remove it you know run it when you need to and then delete it when you don't right and so every once in a while you'll see hey you can't delete it because it's running so then that means you just go into the containers here these are ones that are running you just go ahead and delete that you go back into the images and then you can delete it worst case scenario you just literally shut down docker desktop open it back up and you can see what's running with docker containers in here or you can use docker ps to see if anything's running right now i don't have anything running which is very clear in the desktop as well as my terminal okay so we'll continue to use this i don't expect you to know all of docker at this point you shouldn't know all of docker at this point instead you should just be able to benefit from using docker which will allow for us to do do all sorts of rapid iteration and isolate our projects from each other on a whole another level well beyond what virtual environments can do and it will allow us to have as many database instances as,358,5,5,tiBeLLv5GJo
34,we might want which is what we actually want to do very soon by using time scale itself so that is the docker desktop and some things about docker compos now i actually want to get fast api ready we want to actually build our own container for fast api or at least the makings of it then we'll go ahead and take a look at how we can develop with docker itself we are now going to bundle our fast api application as a container the way we do this is by writing some instructions in a specific syntax so that docker can build the container from those instructions and our code so this is called a docker file so we've got a production docker file for fast api the reason we're doing it now is because the sooner we can have our project optimized for production the sooner we can actually go into production and share this with the world or anyone who has a docker runtime those things are the key part of this so the actual docker file we're going to be using is on my blog so it's here this is a production version that you can just go ahead grab and run i'm going to make a modification to it for this project but that's the general idea here here so i'm going to open up my project now i'm actually going to copy these steps you don't have to copy them but i just want to lay them out so i know where i want to go with this instruction this docker file so if i go into docker,358,5,5,tiBeLLv5GJo
35,file like this no extension i'm just going to go ahead and paste this out and these are the instructions i wanted to do first off is download python 3 create a virtual environment install my packages and then run the fast api application those four steps are really what i want to have happen now the way this works is very similar to like when we ran our python application itself right when we ran it just with docker itself we can see that it is in linux and here is our docka application so it really starts at this tag here the container and its tag so the way we find this of course is by going into docker hub looking for a container image like we did looking for the version that we want to use and then also the tag that we want to use which of course is going to be 3 uh 13.,206,5,5,tiBeLLv5GJo
36,2 which we had locally that's the one we downloaded in the first place we're going to use that same one for our docker container so the big difference here though is we don't want to use the big one because it's massive it's massive all across the board we want to use a small one which is called slim bullseye so that's the one we actually want to use and so the idea here is very similar to what we have with the slim buster from the original python download in the docker container we're going to do something very similar to this the way it works is we say from this is the docker um you know syntax for it the actual image that we want to use which we could use latest this might be 3.13 this might be 3.14 this might be 3.15 we want to be very specific about the version we're going to use which again 3.13 and then i think it was2 is the baseline one if you do slim bullseye slim blls eye like that it will be a much smaller image as soon as you do a much smaller image we lose some of the default things that might come within linux itself when we lose that that means we need to also install our linux environment so the next step might be you know set up linux os packages right so if you were going to deploy this directly on a linux virtual machine you would need to do that same idea that same concept here now i could go through all all of these steps,358,6,6,tiBeLLv5GJo
37,with you or we can just jump into the blog post and copy it because this is not a course about docker so i'm going to go ahead and copy what's in this blog post and we'll go ahead and bring it right underneath those comments and i'm going to go ahead and modify this a little bit to make it work for our project the first thing is notice that i've got this argument in here we actually don't need that argument we're just going to go and stick with that single one right there and then i'll just go line by line and kind of explain what's going on first off we create that virtual environment no big deal this time it's going to be in a specific location this is like as if we were setting up a remote server we would want our virtual environment in one location because that one server is probably going to have only one virtual environment for this particular application which is what we're doing with our container this one container is going to only have one python project but we still want to use the virtual environments to isolate any python that might be on the operating system itself so now that we've got this virtual environment here this little command right here makes it easy so that we don't have to activate it each time it's just activated and so we can run the pip install command and upgrade pip we do python related stuff here's the os dependencies for our mini virtual machine or our mini server here we can then install things like,358,6,6,tiBeLLv5GJo
38,for postres or if you're using something like numpy you would have other installations in here as well or if you wanted to have git in here you could install that as well so a lot of different things that you can do on the os level and that's it right there so that's one of the cool things about using docker containers themselves is you also control the operating system not just the code so what we see now is we are making a directory in this operating system this mini one called code we have a working directory in here also called code in other words we are just going to be working inside of there then we copy our requirements file hey what do you know requirements.txt into an absolute location this doesn't matter we could actually not have it in an absolute location but it is nice that it is in one because later when we need to install install it that's what we do and so what we see here is it copies the code into the container's working directory in other words it's copying main.py into a folder called code no big deal now we've got a few other things that we probably don't need in here um and then the final one is actually running a runtime script a bash script to actually run this application and then removing some of the old files to reduce image size and then finally running the actual script itself so what i want to do before i say this is done is i actually want to create something called boot slocker run.,358,6,6,tiBeLLv5GJo
39,sh now the reason i'm doing this is because all of us are going to need to know what it is that's going on with our application at any given time and this is what it's going to be so we first off make this a sh file or bin bash file so that the linux virtual machine will be able to run it the linux container will be able to run it then we want to cd into the code folder we also want to activate the virtual envir m then we have runtime and variables to actually run our application which is going to be an src main app and that is a little bit different than what we've got here so i'm going to go ahead and just keep it in as main app just for now we might change this in the future but this is going to be our script to actually run our application and so to use that script what we do then is we are going to go ahead and copy theboot slocker run.,236,7,7,tiBeLLv5GJo
40,sh and we're going to go ahead and copy that that into opt runsh so instead of this script here we've got a new one and that's going to be the name of it then we want to do the cho mod to make it executable then that's what we're going to end up running is that script right there and make sure it has a.sh and that's it so of course i still need to test and make sure that this is working it's not necessarily working already so i will have to make some changes in here just to make sure that that's the case that's not something we're going to worry about yet the main thing here is that we have a docker file and that we're going to be able to build it really really soon this docker file most likely won't change that much if anything the blog post will give the updates to the change or the actual code itself will have this docker file in here that's the key of docker files they don't need to change that much the only thing that will change most likely would be the python version that you end up using over time and then the code that's going in but everything else related to this is probably going to remain pretty static that's also why in the blog post the way we run the actual application itself is written right in line it's actually a script that's created right in line but having it external makes it go a little bit faster than what we have right here now i realize some,358,8,8,tiBeLLv5GJo
41,of you aren't fully ready to learn the inss and outs of docker but we want to be as close to production as possible which is why that blog post is exists and it's why you can also just copy this stuff personally i think looking at this it's hopefully very clear as to the steps that need to happen to recreate our environment many many many times over so that it's a lot easier to share it whether it's with somebody else who has a docker runtime or with a production system over the next few parts we're going to be implementing the docker compose based fast api hello world but before we get there we need to still see some things about docker just so you have some foundation as to what's happening in docker compose so jumping back into our project here we've got this docker file these are the instructions to set up the tiny little virtual machine or the tiny little server to run our application right it bundles everything up on our code so we need to actually be able to build out our application so there's really two commands for this i'm going to put them in our read me here so we'll go ahead and do docker in here and we'll go ahead and do the two commands and it's docker build and then it's going to be docker run okay so build is what it sounds like it's going to build our bundled container image and the way we do it is we tag it something hey these tags what does that remind you of hopefully it,358,8,8,tiBeLLv5GJo
42,reminds you of a few things a tag like this and in our terminal a tag like this right so we need to tag it in the same way it's going to be tagged on our account if we were pushing it into dockerhub which we're not but we still need to tag it so either way we're going to build it and tag it then we want to say hey where are we building this file well we're going to build it in the local folder which is just period there now we also want to specify the docker file we're going to use which is how we do it like that now if you don't specify the docker file it's just going to go based off of that as in no other docker file the reason being is you can have multiple docker files like docker file.,193,8,8,tiBeLLv5GJo
43,web we aren't doing that we're just using the one single docker file but it's important to know about if you were going that direction on how you go about building it once you build it then you run it hey what do you know build it then run now we've already seen that command a little bit as well too which was docker run and then that python command like this now the thing about this run command is there are a bunch of arguments that can go in here there can be the- it argument like we saw with python we ar going to spend any time with the arguments in here in fact this is all we want to see for our arguments at this point because we actually want to use docker compose because it will do all of this for us as we'll come to see so the idea here is we want to build out the container let's go ahead and do that in the root of our project right next to docker the docker file itself i'm going to go ahead and run this command and it's going to go ahead and build this out for me now in my case it actually went really fast cuz there's this cache in here i actually did test this and built it already so if i go into docker desktop i can see the image that was built in here which will be my image my actual docker analytics app image in here so i have got a few of them in here and the reason that i have a few,358,10,10,tiBeLLv5GJo
44,is because i was testing this out but of course you can delete these just like we've seen before in this case we've got one that i can't actually delete yet so go back into my containers in here and let's go ahead and get rid of the search bar and i'm going to go ahead and stop all of my containers and delete all of them and then i'll go back into my image here and i'm going to delete this one as well just so i can see it being built out i also wanted to show you that's what you do if you want to get rid of it whether it's your app or someone else's and so once again it still has cash in here so it's going really really fast which is super nice but sometimes it will take a little bit longer than that now there's this other legacy warning that we've got in here cu one of the docker files needs to be changed a little bit to having equals instead of that space bar there so i'm going to go ahead and do that and i'm going to try and build it again cool so it builds really fast great so now i've got a container image that i can run it's not one that's public it's only on my local machine it goes public when i push it into dockerhub which like i said we're not going to do at this point so to run it i just go ahead and do docker run and then whatever that tag is in our case that tag is anal,358,10,10,tiBeLLv5GJo
45,analytics api which we can also verify inside of our images in here there's the the tag itself well actually it's the name of the container with latest in here so it actually def defaults to latest that's the tag that's how you actually can do it it's going to default to that if you don't specify so if you were to specify one it would just be like that again we'll do some of this stuff with docker compose in just a little bit so now all i really want to do is verify that i can run this application by doing docker run and there it is it's now running the application itself if i try to open this application it's not going to work we will make it work in a little bit the reason it's not working is because of how docker works itself everything needs to be explicit to make it work so in order for it to run on a specific port we also have to let docker know about that the application itself doesn't have to let docker know about that we've got a lot of control over how all of that works so what i want to see now is how to do these two things inside of docker compost music off the video i went ahead and stopped that container and deleted the built image so that i could then run the command docker run analytics api which of course failed it's not locally and it's also not on docker hub so it just can't run it so that's a little bit of an issue that actually,358,10,10,tiBeLLv5GJo
46,is overcome by docker compose so if we go in here and do compose diamel we can actually start specifying the various services we might need so the very first key in this yaml is going to be services in inside of there are going to be all of the container images we might want to use like a database or our app in this case we'll go ahead and just work off of our app the nice thing about modern tooling is a lot of times you can just run individual services right inside of the ammo file it also might depend on an extension that i have installed but the point here is we want to make sure that we have these nested key value pairs here app is just what i'm calling it i could call it src i could call it web app i could call it a whole lot of things what we call it is going to make more sense or it's going to matter more later when we use something a little bit different than just app okay the idea here in then is we've got our app and now we want to specify the image name so what do we want to call it well we could call it the same image name as in analytics api and this time we can say something like v1 okay great next up what we can do is define the build parameters here and that is going to be our context which is going to be the local folder this should remind you of this dot right here so that's the,358,10,10,tiBeLLv5GJo
47,context that we're looking at and then the next part is specifying the docker file we're going to use relative to the composed file so we'll set docker file and it's going to be docker file just like that now if we had this as docker file.,61,10,10,tiBeLLv5GJo
48,web which you might do at some point then you would just change this to web as well let's actually keep it like that so it's a little bit more clear as to what's going on in here now if we do change it one note i will change is inside of my little command here i would want to change that one as well just in case i wanted to build it individually okay so the idea now is i've got my docker image and some build parameters now you can actually add additional build parameters in here this is all we're going to leave though is the context and the docker file in part because it actually matches what we did to build it in the first place but the other part is well we probably don't need a whole lot of context just to build it because that's what the docker file is for the docker file has a lot of those context things that you might need to build it now to run it it's a whole another story so to run it what we did was well actually if i just leave it like this and try to run it we'll see something interesting so i'm going to go ahead and clear this out and then do docker compose up hit enter what this will do is it starts to run it but it actually ends up building the application itself and then it goes to run it which it is now running and of course if i were to go to this actual place it will still not work right,358,11,11,tiBeLLv5GJo
49,okay so we'll get to that in a second but it built and ran it basically the same way but now we specify the image here and so the command i need to remember is just simply docker compose up and if i want to take it down i can open up another terminal window and do docker compose down that will stop that container application from running which i think is pretty nice okay and then we could also go into docker desktop we can take a look at the images in here and look for our analytics api and what do you know there's that v1 tag in there as well uh which makes things a little bit nicer and easier to see what's going on so the image was built with that tag is kind of the point okay great so now what we want to do though is we want to be able to actually access this inpoint this url here so the way it works with docker this is true whether it's docker compos or just straight docker itself as in this run command we actually need a specify ports so part of the reason that i actually had ports in the first place like inside of my docker run is so that i can specify a different port right so this port value this environment variable we want to play around with this in just a moment so the way we play around with this is going to happen uh next to ports so before i actually do the ports let's go ahead and change the environment variable so we're,358,11,11,tiBeLLv5GJo
50,"going to go ahead and come in here and not to use entry point but yeah the rather use environment and set key value pairs so the port value you want to use so let's go ahead and use port value 8,000 and2 i'm not going to do the ports just yet we'll just change the port and then i'll go ahead and do docker compose up notice that the environment variables have changed if i try to open it once again it still is not accessible okay so this is in part because what i changed was runtime arguments this little thing changed a pretty big change inside of the application because of this environment variable or more specifically this one right here so inside of our composed.",167,11,11,tiBeLLv5GJo
51,gaml we've got this port value in here now the reason i'm mentioning this is because at some point we will have a database url in here and we'll be able to pass in what that argument is another thing that we can do inside of using you know just docker compose is do an env file and you can do something more like env and which case you could just come in here and dov and this being port like 801 so right now i have conflicting environment variable values so let's actually see what that looks like i'm going to go ahead and call docker compose down and then we'll go ahead and bring it back up in just a second okay took a few seconds for that to finish but now if i do docker compose up it's still port 802 in other words these hardcoded environment varibles override the environment variable file so if i were to get rid of that port it would still be uh you know whatever the environment variable value is uh so for now i'll just leave them the same so there's not any confusion but the idea here is we have the ability to have environment variable files now we can add more of them by just using another line here you can do something like em.,295,12,12,tiBeLLv5GJo
52,sample or any other kind of environment variable files and of course it has to be in this format of key equals some value okay great so the final step here is really just to expose this port so i'm going to go ahead and bring this down and the way this works is we declare ports in here and our this is super cool so what it shows us is our host port and our container port so if i click on that we've got host port of 8080 goes to container port of 80 if you're not familiar with what the container port is that is going to be this number right so port 802 and then the host port is our system what port do we want to access port 802 on this so this sometimes doesn't work as intended so we're going to go ahead and try it out we've got docker composed up here's 802 and then we exposed port 8000 so inside of my local host here i'm going to try to do port 8080 and see if i can connect so depending on how your system ends up being designed this might work and it also might not work uh so what you want to typically do is default to your local host port to that port itself the reason for this has everything to do with how our port and host and all this sort of stuff is being mapped it sometimes works very seamlessly sometimes does not basically the general rule of thumb is if you have the ability to use the same port you should but,358,13,13,tiBeLLv5GJo
53,the reason i wanted to show you all these different ports here is because this is kind of confusing it doesn't really show you what's going on you just need to remember the first port is going to be our systems port the second port is going to be the port that the docker container app is running on great so now we can go ahead and bring that back down and then we'll go ahead and bring it up in just a second okay let's bring it back up and here we go so we got port 802 we open this up and now we've got the hello world in there so congratulations you have docker compose working now this stuff i realize might be a little complicated if you've never done this before but really these are just a bunch of arguments that we are telling docker to use for this particular container image that's being built locally you don't always build things locally as we saw before when we actually had a python project but when we do build things locally we have a whole another set of stuff that we can do so for example we can change the docker file command here the one that starts the application the runtime script we can change it to something different something like uvicorn main app and then the host port and reload the nice thing about this then is it's actually going to be built off of that command it's not going to be using the gunicorn script at all so if you made any mistakes with that you could just use this,358,13,13,tiBeLLv5GJo
54,command to run it and of course you would take it down and make it all work just like that um as it need be so the next thing about the docker compost stuff is you can do something like this where you can actually watch for changes on files and it will rebuild the entire container we'll see that in just a moment now if you wanted to rebuild the container or if you want to be able to do your own development you're going to do one more thing which is attaching a volume to this container so this volume here is going to go ahead and grab this folder of source and it's going to go ahead and put it where we want it to right now this says slash apppp which may or may not be the correct location we want to mount source so this means that we go back into the docker compost file and we scroll down and we see where we copy our src folder which actually goes into slash code so we just make a minor modification to this volume here and that means that we are going to go ahead and basically copy our code into the container constantly and then we'll be able to rebuild it so this might be confusing so let's go ahead and see what what i mean by all of this we do docker compose up d-at so what's happening here is we see that watch is enabled if i were to change something to requirements.txt let's go ahead and say bring in requests here like python requests and i just,358,13,13,tiBeLLv5GJo
55,go ahead and save it this rebuilds the container image it's going to happen right away now this might take a few moments because that's exactly what does happen when it comes to building out containers but there it is it rebuilt it and then it restarted the application altogether so the next thing is actually changing the code but before we do that let's take a look in here we see hello world at port 802 now inside of main.py if i change this to hello world earth or something like that we should be able to refresh in here and it automatically changes which is super nice so really the command we want to use from now on is d-at in here so that we can actually just change our code we now have a docker based development environment all you'll need to run now going forward is docker compose up and watch and then if you do need to get into a command line of this docker image you can do so with simply docker compose r and then the service name which is in the composed damel in our case it's simply app so if we come back into our read me here we can just go ahead and say app and then you can put in something like bin bash which will allow you to go directly into the command line shell now you could also see something like or uh you could do something just like that but instead of b bash you could just go ahead and say something like python right that should actually work as well which,358,13,13,tiBeLLv5GJo
56,will give you the virtual environment version of python so let's go ahead and try either one of these out so you can take a look at what we're doing here and then we'll be able to see that i have this runtime ability in here as well so every once in a while you'll see this removed orphin you might want to just add that in to a command as well uh whenever you run it but we're going to leave it like this here is that version if i import os and then i print out os.,128,13,13,tiBeLLv5GJo
57,pi which is quite literally this code right here and so we can actually see that as well by doing cat main.py this will show that code in there if i were to change main up high in here to something different to like hello world and then let's clear this out again and then run cat main.py it changes and shows that actual code right there as well so now we have a mostly complete development environment that we can use through docker um there are other things that we might want to expand on this but if you've ever thought about hey i want to use a more even more isolated environment inside of my project this would be the way to go but the main thing about this is mostly to be prepared for production which i think we are well prepared for production now in the next section what we want to do is actually start using another service in docker compose to actually start building out the data schema that we want to use the key takeaway from this section is that we now have a production ready development environment of course this is thanks to docker containers and this docker file right here now the composed file helps us with the development environment the docker file file will help us with the production environment but both of them together give us that production ready development environment now this is more general right it's not necessarily about our python application which of course means that we also need to make sure that our python application is ready for a local development,358,15,15,tiBeLLv5GJo
58,environment as well which is why we started it there some of you may or may not use the docker composed stuff during development and that's totally okay the key thing is that it's there it's ready and we can test it when we're ready in other words if you need to just run your uvicorn and activate your virtual environment and run that on your local machine like that that is totally okay and it's more than acceptable when it comes to the actual docker compos though we can also just have that running if we need to because the way the actual docker composed file is developed is it will react to changes that you make with your code or at least sync those changes and a lot of that has to do with all of the different commands we put in here so like we saw when we changed our requirements.txt the entire docker container was rebuilt and then started to run again thanks to docker compose watch so this really gives us this foundation that we can build off of now does this mean that we're not going to change anything related to docker or docker compose no as our application develops we might need to change how it actually runs whether that's in docker docker compos or whether that's locally just through python and virtual environments but the key thing here is this foundation can be used on other python applications if you want and with slight modification to the docker file you can also use it in node.js applications now node.js applications will also follow this same sort of pattern,358,15,15,tiBeLLv5GJo
59,that's the cool thing about the docker file itself is that it has well very stepbystep instructions that are going on here and of course you could use this to deploy things manually as well so if you don't want to use containers when you go into production this docker file will at least help you with that as well so we're really in a good place to start building on top of our application and really just flush out all of the features we want to use now we still will use docker compose for other things we still will use the docker file for other things but the point here is we have the foundation ready and that was the goal of this section let's take a look at how we can start building new features on our application to really make it the true analytics api in this section we're going to be building out api routing and data validation we are creating something called a rest api which has very static endp points that is predetermined url paths that you can use throughout your entire project and so fast api is really good at building these things which is the reason we're using it but the idea here of course is you're going to need to have something the code going and i'm going to be using my docker compose version of this with watch running so if you don't have docker compose up and going you can use just straight python of course what you'd want to do is clone the repo itself and if you are going to use straight,358,15,15,tiBeLLv5GJo
60,python you'll create a virtual environment in there and then you'll navigate into src in which case you will do the docker compose command which is this one right here and you would just maybe change the port or you could use the same port so in my case i have both of these things running and they are allowed to access so either one works the point here is being able to develop on top of what we've already established previously and so we can actually build out the api service now there's one other thing i'm going to be doing here is i'm going to be using something called jupiter notebooks and i'm going to go ahead and create a folder called nbs here and it's going to be simply hello-world doip python notebook or iynb and so in here i'm going to go ahead and create some code and print out just simply hello world and then i'm going to go ahead and run this with shift enter this will then prompt me to select my python environment which is going to be my local virtual environment right here this of course means that i'm not using docker for this the reason i don't want to use docker for this has everything to do with how we're going to test this stuff out and so i'm going to go ahead and install this this uh you know it's asking me to install the ipod python kernel in here and we want to do that obviously you can add this to requirements.txt but cursor vss code wind surfer all really good at actually just,358,15,15,tiBeLLv5GJo
61,running jupiter notebooks right in line which is why we're going to use it right here so this is pretty much it for the intro now using these jupyter notebooks we will then test out all of the api routes we put in and make sure that they're working as intended let's jump in the purpose of rest api are really api services is so that software can communicate with software you're probably already well aware of this but since we're having software automatically communicate with each other we want to make sure that we implement a health check almost first and foremost so that if the software has a problem reaching it it could just go to the health check to see if the api is down or not so that's where we're going to start and this is a really just a lwh hanging fruit way of seeing how we can build all this stuff out now inside of main pi i'm going to go ahead and copy my read root here and i'm going to paste it underneath everything and i'm going to just change the path to simply health with a z health check api endpoints are usually just like that it's not usually health but it has a z in there not really sure why but basically what we want to do here is rename the function to something different and we'll go ahead and say read api health something along those lines and we'll go ahead and say status being simply okay in other words we can tell the designers that are going to be using this api hey if,358,15,15,tiBeLLv5GJo
62,you need to just go ahead and run this health check this is also going to be really important for us when we go into production so now what we want to do is actually test out this health check so inside of my nbs here i'm going to keep this hello world going and i'm going to import a package called requests so python requests is a really nice way to do api calls this might be what you end up using in the future there's another one called htpx both of them are really good and have a very similar way of doing htp requests now what i see here is i actually don't have the module called requests or python requests itself so jupyter notebooks one of the other nice things about these sort of interactive environment is i can run pip install requests right here and what it's going to do is it's going to use the environment that you set up at the beginning of this one right it's going to use that virtual environment to install the packages you might need in which case you can just run something like that every once in a while you might need to restart the kernel which you just hit restart and there it goes once it restarts you have to run the various things and at this point python request is installed on my local virtual environment okay so i'll leave that out for now i don't actually need it any longer so feel free to comment these things out and realize i'm doing shift enter a lot to actually run each,358,15,15,tiBeLLv5GJo
63,cell itself okay so how do we actually call this health check itself well the way i think about it is my endpoint the endpoint i want to use which is going to be health z and then i'm going to go ahead and do like my api base url which i often do something like this base url and it's going to be htp col local host and then the port we're going to be using in my case i'm using port 802 now if you're not aware local host is also the same thing as doing on 7.,130,15,15,tiBeLLv5GJo
64,0.0.1 those two are interchangeable with in most cases okay so endpoint maybe actually is not what i want to do i want to say path here and i'll go ahead and say the endpoint is the combination of these two things which i'll just use some string substitution to make that happen just like that so now we've got our endpoint here and i can print that out and that's pretty straightforward now if i could do a command click or control click i can actually open up the web browser and see this right here now of course we want to build towards automation since we're using our our uh you know our api service we're going to go ahead and say the response equals to request.get this endpoint right here and then all i want to do is print out response.,187,16,16,tiBeLLv5GJo
65,okay okay so i'm going to go ahead and run this and what it does is it gives me a true value great if i change the path to something like abc on it and run those two cells again i get false fantastic okay so i'm going to keep it in as just simply health and this is what we're going to be doing we're going to be building on top of this with different paths and different data points and also different http methods now let's go ahead and create a module for our actual api events so inside of our src here i'm going to create a folder called simply api inside of that folder i'm going to go ahead and create another one called events and then inside of there we're going to go ahead and do routing.,184,17,17,tiBeLLv5GJo
66,so what i also want to do is make sure that each one of these things has an init file in here to turn events into its own python module so we can use dot notation appropriately as we'll see in just a moment the idea here is we want to have very specific routes for our events resource so if you think of main.py this is kind of generic routes what we really want to have is something like sl api events and then that being all of the events stuff that's going on in here so very similar to main.py but just slightly different in the way it's termed so let's see what that looks like so we're going to go ahead and do from fast api we're going to go ahead and import api router and then i'm going to go ahead and declare router equaling to the api router now this router right here here is basically the same thing as the app but it's not an app it's just for this one small portion of the larger app and so in here then i'm going to go ahead and do router and we'll go ahead and do.get the http method of get which we'll see in just a moment and i'll just put a slash here and then we're going to define and this is going to be something like you know get or let's say read events eventually in here that we will have some sort of more robust response and i want to return back just let's go ahead and return back in a dictionary of items and i'll,358,18,18,tiBeLLv5GJo
67,just go ahead and say one uh two and three now the data that's coming back we'll definitely look at and change over time but for now we'll just say read events like that so in order for this to work what we need to do is we need to bring it into main.py because right now the actual fast api application doesn't know about this routing module because we have nowhere it's not orted anywhere right this right here has all of the definition that we have in place at this point so the way it works then is we need to import it from api so we'll go ahead and do from api and then the fact that it is a python module because of this in it we can do do events like that and then i actually can also then go one more do notation and import routing and then we can import the router that comes in and then we can change it as something like as event router now the reason that i do as event router is so that i can just riable name this if you will um so that i can have a lot of different ones with the same sort of organization or the same sort of setup that we might have had before now that we've got that we can come in to our app itself and we can use do include router and we can just pass in that event router but as you recall the event router itself has a slash here but we really want this to be sl api sl events,358,18,18,tiBeLLv5GJo
68,like that so this is the prefix we want to use for this route now of course the way i can actually use this path is i could set it right here and that would generally be okay it would technically work but what i actually want to do is go back into main.py and set the prefix itself to that actual prefix without the trailing slash okay great so that is now an api endpoint route that we can use of course we still need to test these things out and we will but before i go much further i will say that this routing is something i actually want to change the way we change that is by jumping into init.py and in here i can do from routing import router and then i can just go ahead and exported by putting it into this all list and we just put in router just like that notice it then gets highlighted and now i can just come back into main and i can get rid of routing and just imported that way it's just a subtle way to do that it's a nice thing that those init methods are able to do now the init method in here i could probably also import that router in this init method as well i don't want to do that i just want to keep it like somewhat isolated from each other um or somewhat packaged in this way but now that we've got this router what we can do of course is jump into our api endpoint and just check it out by going api and,358,18,18,tiBeLLv5GJo
69,events and there we go items 1 2 3 since we have this api ino let's go ahead and create a notebook to verify it so i'm going to go ahead and do one- verify router or let's say api event route something like that ipython notebook will bring this in here now i'm going to go ahead and continue to number the notebooks themselves so you can always reference them later but the idea here is we want to do something very similar to this hello world where we're going to go ahead and import the requests here so let's go ahead and open up new code then we're going to go ahead and bring in all of these inpoint stuff in here so we can do some various tests itself now i want to run this with shift enter it's going to tell me what environment i want to select again we're going to use our local virtual environment and then our endpoint our actual path is going to be a little bit different than this and that of course is going to be api events and we can have a trailing slash in here and then i can do my response equals to requests.get and then that path itself now the.get method here is correlated directly to this git method right here we'll talk about that more in a little bit but for now i'm going to go ahead and run these two things and what we should get is the actual api endpoint but of course i did path not endpoint so that's a little slight little mistake so we need to,358,18,18,tiBeLLv5GJo
70,change it and then that gives us back a okay response which we can now print out with response.,24,18,18,tiBeLLv5GJo
71,okay and there we go we got a true value there so what we can do then is say something like if response. okay like that we can actually print out the data that's coming from that response by doing response. json like that and then we can print out that data okay so we hit enter and there we go we got items 1 2 3 now the reason we're doing response. json is because this is now a dictionary so if i actually do type of that data we should see that it is a dictionary itself right oh this is not dot but rather comma we can see that the class is a dictionary which in other words means that i should be able to do something like data.,171,19,22,tiBeLLv5GJo
72,git items and do the dictionary value stuff that you might want to do now of course if we change our api endpoint this might have some issues in here as in it items might be none so if we actually did change our api end point let's change it real quick to being you know uh results something like that slight change made it happen real fast i refresh in here this is still the same data because i didn't actually do the res the request again so i didn't actually hit the api endpoint again now when i do hit it it gives me something a little bit different in terms of the actual data that's coming back so this is one of the main reasons why it's really important to think through all of those api end points in the beginning you want to make sure that these stay basically the same going forward that's why we're going to be testing them out first to make sure we've got exactly what we want then we can always of course improve it later but something is simple as just returning back results here could have a drastic effect on somebody who might want to use your api itself even if that somebody is just you let's keep going we want to take a look at the impact of data types on our rest api now what we saw in the last part was when i changed the key value of this data from items to results the actual notebook itself had a different result altogether and so i actually copied that notebook and just,358,23,23,tiBeLLv5GJo
73,made it into one or two cells here what we run now is we can see that result still so what i want to do though is i want to take a look at how this impacts a single api result itself so in other words i'm going to go ahead and copy this and we're going to call this instead of read events we'll call this get event like a single event itself and then we're going to return back what that single entry might be so if you think of this in terms of a single row and this being a you know list or a bunch of items in a table more on that soon but the idea is we want to look at this single row here now the way this works is we put in something like event id so this is going to be a you know a variable that's going to be passed after that endpoint here that variable will then come into the function itself so whatever you name that you put it in here and of course if you declare it as a data type like int then it's going to look for a specific number we'll see that in a second and then of course we can return back the id being that event id okay so back into this new notebook here i'm going to go ahead and pass in let's say 12 in here for example and i'm going to go ahead and run this and what i should get back is exactly this data it's still a dictionary but it's now returning back,358,23,23,tiBeLLv5GJo
74,id of 12 if i change it to something like a and then try to run this i get nothing back in other words i can print out the like you know okay and then do response okay and i see that it's not okay and i can also open up the url itself and we see that it says un able to parse string as an integer because the input was a the url everything like that we're seeing that the input is a so there's a couple ways on how we can solve this number one we could change this back to being a actual number because it's an invalid request number two if we did want to support an a we would change this from a integer right here a very specific data type to string which is a little bit more generic in the sense that we can then you know now support that api inpoint for that string now in my case i want to keep it back as an integer because that's how we're going to approach this inpoint itself which it requires it to be an integer all across the board which means then our notebook will also require that as well but we're not quite done yet that's the input that's the data coming through what about the data going back like what if i change this to being abc id and run it that way what we end up seeing here is we will actually still see the data come back and in some cases oops let's make sure we save that and then run it again,358,23,23,tiBeLLv5GJo
75,and there it is now this is now the output data it is now incorrect so we need to change that to something different that something different is going to be called a schema which is basically just designing the way the data should flow in this case we're going to be using pantic so we'll go ahead and create schemas dopy and i'm going to go ahead and do from pantic we're going to import the base model and this base model is going to help us a lot so if we look back into our requirements.txt we did add pantic in here pantic itself most likely will be in with fast api as well so we're really just using the basics of pantic here and we're going to go ahead and say the event itself is going to take in that base model and this is where we're going to design how we want it to be returned in this case we're going to do an id which has an integer so that's the schema that is going to be used this is also a lot like data classes but it's going to end up turning into something along these lines where it's a dictionary that's coming back so that's kind of what's expected by this schema now the way this ends up working then is back into our api routes we can import that schema so from schemas import the event this is probably better than calling it event we would call it something more like event schema so let's call it that you could call it model but we're not going to,358,23,23,tiBeLLv5GJo
76,and you'll see why when we start using the database stuff database models is how i typically call models schemas like the design that might go into a database model or come from a database i'll call schemas okay nevertheless we now have this event schema in here and it's in here as well now what we can do is we can actually just return that data type so if the colon and then the int is how we declare the incoming data type this arrow and then the data type is how we declare the outgoing data type the response data type itself and so as soon as i do this what i should probably see inside of my fast api application or at least very soon i should see some error with that data type okay so let's go ahead and do a request here and i'm going to go ahead and run it in two places so i've got my notebook here and then my fast api application running below as soon as i run that i see that it's missing the response id field required the input again is abc id of 12 so the input's incorrect so all we need to do then is change our route itself and that is going to be of course id or basically to match that schema from before and now we can run everything again and this time it actually ends up working great so now we've actually made our system a little bit more robust it's a little bit more hardened because it's harder to make a mistake now or that mistake will,358,23,23,tiBeLLv5GJo
77,be a glaring problem here so if i put in id that is possible you make that mistake on accident but as soon as you go to try it out you'll get that same error like what is id don't know got to fix that and we'll go back okay great so then we bring it back just like that co cool so of course solving errors also comes back to using something like git so tracking the changes over time will become important if you were using something like this if you're building out a rest api of course you're going to want to use get i'm actually not covering that because it's outside the scope of this series but it is something important to note right now because git would help identify that problem because then you could just do something like get diff and of any sort of thing if it was already in your database so let's take a look at that right now or rather in your data itself so i'm going to go ahe and add it and we'll go ahead and do something along the lines of the actual name itself which i believe i called it this will be something like uh five and we'll do basic data types there we go and so now i'm going to go ahead and do that slight little change here save it and now if i do get status i can see that there was a change and now i can take a look at that difference and what i should be able to see uh oops not schemas but rather,358,23,23,tiBeLLv5GJo
78,the routing itself so get diff of that i now see exactly the problem that's coming through so this would be another way to catch it and hopefully catch it early on of course you can also use automated testing using something like pi test to actually catch that as well which would be even better to harden the system but the point here is we now have a way to validate a single piece of data how about a list of data let's take a look now that we can return reliable data for a single item let's go ahead and look at how we can do it with a bunch of items so there's a couple different ways and how we could think about doing this one of the ways would be to do something like this where we just bring back a list you can just use the list element just like that and then put brackets there and put in that event schema that is a way to return back a list but that actually changes the result schema from a dictionary to a list so you can't exactly do a dictionary like this that's not really how it works so what we would end up doing is we would actually bring in a whole new schema for the result results themselves so what i would do then in here is inside of schemas dopy i would do something like maybe the event list schema in which case this is now going to be results of the data type itself which will be a list of schema right and we could also,358,23,23,tiBeLLv5GJo
79,bring in the other class from typing so we can do from typing we can import the list class and we can use that instead of the list element so this is a better data type decoration itself instead of doing the python built-in list both work but this one is more verbose okay so now that we've got this we're going to go ahead and bring it back into our routing and we'll go ahead and use that list event schema here and we're going to go ahead and return back that a list event schema and just like that we don't need to change it at all because inside of this schema we've got this results here great so the actual result itself well let's actually take a look at that one as well so if we save this i'm going to go ahead and duplicate this basic one and now we'll do list data types or something like that we'll go ahead and come in and change this to simply events and we will update this and let's go ahead and run it now and we get false once again okay so if i open this up i get an internal server error and what's likely happening is the actual input is incorrect so each one of those items has invalid input data itself and that's because the actual data that's coming through in here is not an event schema so we would actually want it to be at least of that type of event schema in other words it needs to look closer to this for each instance in here the way,358,23,23,tiBeLLv5GJo
80,we do that then is actually turning these into dictionaries themselves so id equals to that id and then we would just repeat this process for each element which i'm just going to go ahead and do really quickly and of course i probably could have just copied pasted but there we go so now the results are dictionary values and we can see if that solves the problem for us it looks like the application rebooted and now we can go back in here and we can run it again now the results are coming back so this is actually super nice so that means that our routes themselves are now hardened to what we want for our results and of course if we wanted to add something to this list scheme i say something like count and we'll go ahead and do count being something like three three right then we would go back into our event list schema come in here and say count and this is going to be an integer h here uh we could also have it as an optional value but we'll just leave it in there just like that and then we'll come in and run this again and now we're getting false once again so let's see for instance uh we don't have the results coming back correctly so let's go back in the route we've got our count in here uh this actually should work just fine let's make sure that everything's all saved up and let's try that one once again with it all saved and there we go so now we've got the data,358,23,23,tiBeLLv5GJo
81,coming back here it is with that count in there so the point of this of course is to make sure that how we're designing our api when it comes to communicating the data is going to be very hardened it's not going to change a whole lot altogether so of course this is going to be even better when we start entering this data into the database uh and also ex and actually grabbing the data from the database but before we even start grabbing the data from the database we need to learn how to send data beyond something like the url route in other words we need to take a look at the postp put and patch methods to see how we can send data to our api let's take a look up until this point we've been using the http git method this becomes even more obvious when you look at something like requests.get and then inside of main.py we've got app.get and then we have a url path and then in routing.,230,23,23,tiBeLLv5GJo
82,we have router. git so the g method is very very common so we've got git like this we've got get down here for like some id those methods are what you'll use to grab data from the database or from your api itself and so our notebook is really just a small example of that this notebook is really meant to be like as if you were building out another app to work with the api we're also building at the same time but now what we need to do is we need to be able to send data back to any given uh you know server or to our api so what we want to do is we want to use the post method that's what we're going to now is we're going to use that post method in here and so the idea then is we've got our endpoint all of that stuff can be the same to change the response here all we need to do is change this to not get but rather post so now this is going to attempt to send data but in this case it's actually not sending any sort of data at all because we well we didn't define any data to send so if we actually look at the response itself so i'll go ahead and do response.,297,24,25,tiBeLLv5GJo
83,git so the g method is very very common so we've got git like this we've got get down here for like some id those methods are what you'll use to grab data from the database or from your api itself and so our notebook is really just a small example of that this notebook is really meant to be like as if you were building out another app to work with the api we're also building at the same time but now what we need to do is we need to be able to send data back to any given uh you know server or to our api so what we want to do is we want to use the post method that's what we're going to now is we're going to use that post method in here and so the idea then is we've got our endpoint all of that stuff can be the same to change the response here all we need to do is change this to not get but rather post so now this is going to attempt to send data but in this case it's actually not sending any sort of data at all because we well we didn't define any data to send so if we actually look at the response itself so i'll go ahead and do response.,293,25,25,tiBeLLv5GJo
84,text and we can see detail method is not allowed as in the http method is not allowed if we do get and we see that as well we can see that it actually unpacks two different things so we'll look at this as well in just a moment but the idea here is going back into the post method we get a detail not allowed that not allowed error is because of our routing here so we don't actually have a route to handle the htp method the only routes we have right now are to handle git methods at these particular endpoints if we want to handle a post method as well we can just duplicate our function here and then just change it from router. git to router.,170,26,27,tiBeLLv5GJo
85,poost which is now going to be this as an send data here type of thing and then this instead of read event it will be something like create event now the important part to note is the endpoint is the same as each other but the method is different so inside of fast api we create a different function to handle the different http method as well as the different http route sometimes what you'll see on ap apis is something more like this where it's create and it's quite literally a different route as well to handle the different method now that is not nearly as common in the case of creating data so we're going to go ahead and leave it in as this right here so we're going to get data this is going to be more of a list view this is going to be more like a send data here or a create view right and so let's go ahead and get data here type of thing right so the idea then is in this function when we go to create an event the response should be very similar to the detail view because when we're going to add data into the database what we could could return back is hey we added that data oh yeah and here is the data we added because if we modify things inside of this function which we might then we want to send back whatever that modification would end up being so in my case i'm going to return back the original event schema and we'll just go ahead and say we'll,358,28,28,tiBeLLv5GJo
86,just do an arbitrary id here because we don't actually have a database that will help us with that id just yet okay so we're almost there now that i've got the actual inpoint let's go ahead and try it again because now the method should be allowed so let's go ahead and run it again and now i get the method being allowed and then the response is just echoed back in here so the post method is basically the same as g method at this point it's just echoing back some data but that's not what we want to do we want to actually send data here we want to actually see some data so i'm going to go ahead and say data and i'm going to put this equal to an empty dictionary in here and i want to actually print out that emp empty dictionary or rather data dict equals to empty dictionary rather so i'm basically declaring the data type and then an empty dictionary in here so we want to see what that data is and so this is actually very similar to what we saw down here where we passed in an argument into the route a wild card if you will into the route it's very similar to that but slightly different okay so let's go ahead and take a look at that and we'll do this by sending that event again and if we look in here we see that there is no data coming through so there's that dictionary okay so if i actually want to send data though i need to change things a little,358,28,28,tiBeLLv5GJo
87,bit before i do though i'm going to go ahead and jump into the response and grab the headers that are coming in here and so what we see is a bunch of headers but the important part is the content type here so what's happening on the server side in our fast api application is is expecting a certain data type to be sent especially with sending out data in other words the data itself is going to be a json data type so i'm going to go ahead and say something like our page is equal to you know test plus or something like that so that's the data we're going to send now and we're going to go ahead and say data equals to data but there's a caveat here so if i actually run this now i will get an error right so it says the error there and of course i can say something in here something like else print out the response.,218,28,28,tiBeLLv5GJo
88,dumps and put in data there hit enter and now we see this string in here this is actually quite literally a string which you can check by just putting type around it and you can see that as a string it's no longer a dictionary like what we have up here so so in other words this is string data but it's in a very specific format that format is you guessed it json data and so we can send this data now to our back end so let's go ahead and try that so i'm going to go ahead and use instead of just pure data here i'm going to go ahead and grab this json dumps pass that in run it now this time i didn't get that error it actually did send that data end and we can actually take a look at that data with this right here so what ended up happening is this data was treated as json still even though i didn't tell it to treat it as json so one of the other things that you do every once in a while is you create headers and you declare what the data is that's coming through in this case it's application json this is not the only kind of data so for example if you were sending a file like an image file you would not use application json you'd use something different but the idea here is you can pass in headers in there as well and send that same data these headers are going to send to our back end our actual api will look,358,30,30,tiBeLLv5GJo
89,for those headers and be like okay cool i see you headers you say application json so i can treat this data as if it were json but in in the case of fast api it actually unpacks that json data for us and turns this into an actual an actual dictionary which we can verify by coming in here and saying type of what that data is and then we can come through in here send that and there we go we've got a dictionary in here so in other words what's happening here under the hood this is all happening for us but we still need to understand what's going on when it comes for apis communicating with each other it's usually through json data it's not always there's other data types that you can pass but it's usually through json so fast api was designed in a way that said hey when we skit post data we're just going to expect it to be json so then we can just infer that the data that's coming in is going to be a dictionary in other words i should be able to respond with this data assuming i did it correctly but of course the schema that i have set up won't allow for this data to respond as an echo so i'm just going to get rid of this schema for just a moment so we can see the echo come through now so with data test i'm going to go ahead and run this all again and now we should see that echo coming back as soon as i bring that schema,358,30,30,tiBeLLv5GJo
90,back it is going to run an error as we'll see in just a second so if we run that back everything's saved run it again what i get is now an error because the schema is invalid so we need one more step with our data and that is going to be an incoming schema in other words this can't be generic like this it needs to be something else so for now i'll leave it like that and we'll look at that incoming schema in just a moment now i will say one of the cool things about this as well is we can go even further i can copy this router here down here and we can grab something like put as in we are going to go ahead and update this data now i'm not actually going to cover patch but put is very similar to this where we have the event id coming through and then we also have the actual data that would be coming through from the back end this actually would be not called data but rather payload that's what we would end up calling it for the put events so we'll see this one as well but in this case we are just returning back this here and this would be something more like update event now in the case of an analytics tool i don't know why you would be updating the event to directly instead of just entering a new event but this is kind of what we need to understand is these different methods now there are other methods as well that i'm just,358,30,30,tiBeLLv5GJo
91,not probably going to implement for a bit but one of them being like delete where you would delete a specific event itself that would be another one that you might end up using once again we're not going to be implementing that just yet but now we want to actually validate the incoming data to our api we need to ensure the data that's being sent to our api is correct or at least in the correct format so very similar to when we send data back needs to be in the correct format in this case as the event schema we need to make sure that the data that's coming in is in the correct format not just in the correct data type so not just a dictionary for example so what we want to do here is create a new schema that will help us with this validation now we already saw some elements of this validation with the git lookup where we said event id is an integer we want to make sure that it's only a number of some kind you could obviously change it to being an str str in which case that actually sort of breaks this endpoint where it's now a wild card it's no longer an actual number so we want to keep that one as an integer the same sort of concept exists for our create event as well so what we want to do then is we want to jump into our schemas here and i'm going to go ahead and create a brand new schema this one is going to be called our event,358,30,30,tiBeLLv5GJo
92,create schema this one will no longer have an id in there but rather it's going to have a path in here which for now will just leave as a string value and so this schema itself is now going to be brought into the router in here so we bring it in just like this i'll go ahead and pass it in here and we'll do something along these lines right here there we go and so now this is going to be our schema so this is the data in here all i can do is bring it in as create event schema or the event create schema and then i would want to actually rename this to simply payload you could keep it as da data you could keep it as a whole lot of things but i actually want to keep it in as payload i find using the term payload to be a little bit better than data data is a little bit too generic for what this kind of data is payload is what's being sent to us right it could be correct data but realistically it's payload just like what we started talking about down here now the other thing is with the put method we could could do the same schema if we wanted to or we could expand it if we wanted to add something different so let's say for instance we came in here and did the event update schema and instead of updating the path maybe we just update the description and that's literally the only field that we allow to be updated in an,358,30,30,tiBeLLv5GJo
93,event update schema and so these are required and the only fields that are changing so what that means then is i've got three fields here that the event could potentially save that is going to be these three fields right here right so we would have id we would have path and we would have description so this is what would actually be stored on the database where the actual endpoints are only supporting these items in here but the actual event schema the final one might have all of them as well so we'll talk about that when we get to the database stuff uh but now that i've got both of these i'm going to go ahead and bring that one in as well and we'll bring it in to the update portion also so let's go go ahead and do that just like that great and this time i'll just go ahead and print out that payload also so i want to see these two events and see what they look like going back into send data to the api this notebook here i'm going to go ahead and run this and what we should get back is we've got an issue here for our input right so we've got page in here and the input saying page field required being path not page so that means that i need to change this to being path or i need to change the other other schema being page so i'm actually going to change the schema because maybe i don't want it to be called path maybe i want it to actually be,358,30,30,tiBeLLv5GJo
94,called page and so in which case i would save it like that and then we go back into the api call in the notebook and now it actually sends it back to us which is great so of course we could always echo the data back as well if we wanted to by sending or updating the event schema payload altogether we'll look at that in just a second but what i actually also want to see is this update view on how that might work so going back and here we'll go ahead and copy some of this data here paste it below this time i'm going to go ahead and grab the endpoint stuff and it's going to change slightly uh which will be this down here and then we'll say this is 12 now with a trailing slash because of how our endpoint is set up and so there we go and i'll call this my detail inpoint so detail endpoint and then detail path and we probably don't need this base url anymore and then we'll go ahead and put this down here now you might be wondering what method do we use hopefully you remember it's simply the put method that's it okay so now that we've got this there's one other thing that i can do in here that's actually really nice so instead of doing data equals to json dumps data and then the headers i can literally just use json itself python requests will then implement all of those other things for me and what we said was description is the value that we need here in,358,30,30,tiBeLLv5GJo
95,which case i'll just say hello world and then now this of course is an http put i now can run this and i should be able to see that it is okay and if i look in the terminal of our application running i see there's the description hello world and there's the page and of course back into the route this is not a dictionary now but it's rather a schema so if i wanted to get this stuff i could go ahead and do payload page and then in here it would be payload uh what did we call it maybe that one was page yeah so that one's page the other one's description so this is payload do page this is going to be payload do description giving us the full advantage of using pantic in our api and so back into the notebook itself if i run that again i see that it says hello world for that description and then the other one should probably say something like uh the path itself so let's go ahead and run that again and there it is cool so we now have the routing and the schemas for incoming validation as well as outgoing validation to ensure all of that stuff's working with pantic we can have optional fields at this point we don't have any optional fields we've got only required fields so let's see how we can actually create these optional fields now before i do i want to actually update my notebook here a little bit so that the create endpoint and the update endpoint are basically one sell by,358,30,30,tiBeLLv5GJo
96,themselves so that there's no issues with it and so the create one is going to be here i'm going to just change this to create endpoint and we'll go ahead and put that there the data will actually turn this back into json data and we'll pass in the dictionary with the page of you know slash test plus or whatever uh this is mostly so we don't have any conflicts with some of the variables that might come through from the different responses and then also we only have to run uh each one of these one time right so get update great so now that we've got this let's go ahead and add in the optional data here so jumping into our schema what's the optional data i want is in this event schema i probably want to have page and description optional i'll start with just simply page and so the way this works is if we put it in as page colon string that is required data if we want to make it optional we bring in the optional class from typing and then we actually bring in the square brackets like this now it's optional or at least seemingly optional so let's go ahead and save everything jump back into our notebook and then run it i get an internal server error so if we actually take a look at that server error what we'll end up seeing is the field is required right so it's now still assuming that that field is required so it's not quite optional yet uh so let's go ahead and add in a default,358,30,30,tiBeLLv5GJo
97,value for it so it's almost optional just not quite there my default is going to be just an empty string here just like that so let's make sure it's saved and once again back into our notebook we'll go ahead and run that post and this time it seems to have worked and there we go okay so what i want to do is actually echo back that response i want to see that data i don't have an error anymore with that field but it doesn't seem to be showing up much right so if we look in here we've got the response coming back let's actually take a look the response coming back is page being an empty dictionary so that's actually kind of nice in the sense that yeah cool the page is there if we look at the other one same thing the page is there it's empty great so what i want to do then is i actually want to return back the page data at least from the first one and the way we do that is by jumping into routing and then putting in the page data in here and we can do that with the page value being added to the response value so payload dopage we save everything like that we go back into our notebook we run it again and now we can see that page values coming through and now it's technically an optional value for this field right for that data and we can do that same thing again with with the description itself and we can come in here just like that,358,30,30,tiBeLLv5GJo
98,and now we've got some optional values in here and if i look at my requests there we go and we can come in and we see all that great so of course i want to echo back the description back here as well so back into our rounding here we've got our payload down here which has our description and then i can do something along these lines where it's pay.,93,30,30,tiBeLLv5GJo
99,description great so this will help us with that echoing and of course we can verify that again and there's that description coming through of course it's just echoing what's ever here and that's that okay so in the long run of course these schemas would be tied into what's happening in the database so in the middle of all this stuff would be sent to the database and then receiving stuff from the database but the other thing about this is the actual payload itself we can actually return it to be a little bit easier on ourselves so now what we can do is do data being payload do model dump and what this does is it takes the payload and turns it into a dictionary which is basically from pantic itself has that model dump right and it used to be something different i think it was like two dick maybe uh like this if i remember correctly but now it's just simply model dump in which case this is now going to be a dictionary in here which i can unpack just like that and i would do it also down here as well so we would unpack it like that and then the data just like that great probably don't need those print statements anymore uh but now we've got the data coming through as we probably want okay so once again i can verify these in my notebook and i should be able to see the echo data coming through in here and one of the questions of course would be oh well could i actually send the description now,358,31,31,tiBeLLv5GJo
100,if i wanted to so if i come in here and do description and set something like you know abc123 will that respond that data back and the answer right now is no hopefully you understand why at this point but i'll let's go ahead and go through it first off the data that's coming through is just this payload right here it's going to ignore that other data if you pass in more data in here it'll just ignore it there's probably a way around that as in it will then say hey you have too much data there's probably a way to do that but for us we really just want to make sure that we're grabbing the correct data and so i can actually come up here and give that same optional description in here and also on the update view if i wanted to be able update the page i can do that as optional as well but there we go so now that we've got the description coming through i should be able to echo that back as well and there it is it's now being echoed and now i have a way to change the responses really quickly and it all has to do with these optional values if you wanted to add in optional values now every once in a while you might want to add in a default value this default would be then using a field so you'd come in here and say something like field default being an empty string or my description something like that in which case you would come back into the api,358,31,31,tiBeLLv5GJo
101,let's go ahead and get rid of the description i passed in and now we'll take a look at it if i run it again it now gives me that default value if you wanted to put one in there um and of course there's more verbose ways and more robust ways we can continue to modify that stuff uh but overall this is actually really nice because i have the ability to change all sorts of things in here for for my schemas and now once again adding to that stability to how we end up using pantic inside of fast api our api now has the ability to handle different http methods like getting post at different api endpoints like our list view or our detail view now once we actually can do that we also see that we can validate incoming and outgoing data now a big part of the reason i showed you schemas in this way like with multiple parts is because the way you use different data pieces might change depending on your needs for your api service when it comes to an analytics api do we really want to ever update it maybe maybe not this is going to be something you'll decide as you build this thing out more and more the point here though is we have a foundation to move to the next level that next level is going to be using sql databases to actually store the data that's coming through and be able to handle it in a very similar way that we've been doing and the way we're going to do that is,358,31,31,tiBeLLv5GJo
102,by using sql model if you look at sql models documentation and you actually take a look at how it defines a database table it looks like this this hopefully looks very familiar to what we just did and that's because sql model itself is based off of pantic it's also powered by pantic and something called sql alchemy sql model is definitely one of the latest cutting edge ways to work with python and sql sql of of course is designed for storing data it designed much better than python would be python itself is not a database sql is and so we're going to use sql model to actually integrate with a postgres database called time scale db and another few packages to make sure that all works really really well let's take a look at how to do that in the next section in this section we're going to be implementing sql model this really just means that we're going to be taking that piden schema validation for validating that incoming or outgoing data and we're going to convert that so that it actually stores into a sql database sql model will do all of the crud operations that's create retrieve update or delete of that data into that sql database now the sql database we're going to be using is postgres postgres is one of the most popular databases in the world for good reason one of the reasons is the fact that it has a lot of third party extensions to really pick up where postres falls flat one of the things that postes doesn't do very well is realtime analytics,358,31,31,tiBeLLv5GJo
103,or just a lot of data ingestion postris doesn't do this natively by itself in which case time scale picks up the slack in a big way so we're going to be using time scale because we are building a analytics api and this is designed for real-time analytics and it is still postgres so at the underlying technology how we're using sql model is going to be the same regardless of which version of postres you use especially in this section where it's really just about using sql model itself we will use the advanced features that time scale gives in the next section this section is really just about understanding how to use sql model and actually storing data into a postc database instead of just using pantic to validate data because that doesn't really provide that much value let's go ahead and dive in on our local we are going to be using docker compose to spin up our postgres database we'll start with the configuration for pure old postgres then we'll upgrade it to time scale and all of this is going to be done using docker compose in just a moment now if you skipped docker compose you can always log in to timescale decom and get a database url and just use the production ready version right in there that process we will go through in the next section but for now i want to just do the post one on my local machine now a big part of this is looking for the postres user and all of the different environment variables that are available so we've got postgres,358,31,31,tiBeLLv5GJo
104,password user and db those are the main ones we want to look at to get our local version working inside of docker compose so i'm going to open up compose diamel here and then we're going to go ahead and tab all the way back make sure it's on the same level as app one of the ways you can figure this out is by just you know you know breaking it down like that and then you can do something like db we'll call it db service in here and then i can go ahead and create those things out with this still broken down having it broken down makes it a little bit easier to know exactly what to do now the first thing is we can come in and decide our image so if i were to use postgres itself i would come into postgres official and then i would want to look for the tag that i'm going to end up using in here and be very deliberate about the tag especially with post cres as a database so maybe you use 17.4 bookworm or just 17.4 the size of them looks about the same so it really probably doesn't matter that much between the two of those so you could do something like this postes 17.4 next up you would want to set the environment which will be all of those environment variables so once again searching for that postcg user in the documentation on the read me let's go backup page here and search for that and you can actually see the different environment variables so you'd come in,358,31,31,tiBeLLv5GJo
105,here and do something like postc user and then the next tab would be postres password and then something like this and then the actual database itself the core values here are basically this right so those are the key ones that we're going to want to have next up what we're going to want to do and this is one we will definitely need is something called a volume this volume is going to be managed by docker compos we don't manage it oursel it's not quite like what we did with our app where we actually mounted the folder here into the container instead we're going to let docker or actual docker compose manage the volume itself so before i even declare that i'll go ahead and do volumes and this one i'm going to call this my time scale db data and just like that that's all you need to put in and then you can use whatever you name this and you can bring it up to your volumes here and then have it go to a specific location which in the case of the postgres location the postes database it's going to be var lib postgres sql and then data and so that allows your postgres instance on docker compose to persist data because you definitely want to have that so if you take down this or run docker compose down all of this will still be up and running so similar to like our app itself we also want to declare the ports in here and i'm going to go ahead and use the default ports which is 5432 and,358,31,31,tiBeLLv5GJo
106,then 5432 and then in some cases especially with what we're going to be doing you might want to expose the database which is going to be 5432 i'll explain that when we start to integrate this into our local project here okay so the idea with this is this is using postes of course right so if we actually scroll back into postgres there's the name of it all that it's all good we could totally go that route but i actually don't want to use postes i want to use time scale so time scale is not a whole lot different but the thing is we grab this right here so now it's going to be time scale that's going to be the repo name or the image name and then we grab the actual image tag which we come in two tags here and we can actually scroll down actually in the overview here we also see the tags the notable ones latest pg17 so in the case of postgres itself you got 17.4 with time scale we can just do latest pg1 17 and everything else is the same which is really nice but of course i'm going to change this these values in here to something just a little bit different and i'm change the user to time- user and then we'll go ahead and say time pd uh pw and then i'll go ahead and do my time scale db in here as the actual database name that i'll end up using so all of those values are going to allow me to do something like this in myv which,358,31,31,tiBeLLv5GJo
107,is going to be my postgres sql plus pyop g which we'll go over again once we implement it but it's going to be our username so it's going to be just like that and then colon our password like that and then at some host value which will come back to then the port value which is going to be right here and then slash the actual database itself so all of this is what we'll need in our environment variables for our app itself so the app we can add it into up here we could say something along the lines of database url equaling to basically that data now in development this is generally okay in production you're not going to want to have your actual environment variables exposing something like this and so in which case you would use an env file as we discussed before but that's when you would come in here and go into env and save it kind of like that now as we saw before as well when we were doing it with our app whatever you hardcode in here that's going to take precedence over the env files themselves which is kind of nice but there's our database url and of course it's based off of all of this stuff but there is one key component that we still need to change in here and that is this host value here this host value is going to be the name of the service itself when you are using it inside of docker compose i'm going to show you how to use it in both places but,358,31,31,tiBeLLv5GJo
108,i want to leave it like this for now we'll save this and then inside of my docker compose all i have right now is my app running so i am going to need to take this down and we're going to go ahead and restart our app itself looks like i'm having a few issues with our app running so i'm not sure if maybe it's related to this docker compose rebuild it seems that it is now with the app down i'm going to go ahead and run docker compose up and i really just want to see that our postgrad is a database is up and working our db service is being created and downloaded and all that no surprise there it's actually grabbing it from time scale the docker hub version of the time scale db of course it's open source so that's really nice and i'll let that finish running okay so it finished running and it looks like everything is working in here we might need to test this out with postgres directly but in our case we're just going to leave it as is and we'll test it directly with python but if you are familiar with postgres you will want to probably test this the other thing i just want to make sure that my app is still running it looks like it is so i think our docker compost stuff is probably good and we're now ready to start the integration process within our app itself self i'm going to be integrating it in two different ways one is by using the docker compos version which is,358,31,31,tiBeLLv5GJo
109,basically this right here another one is just considering our database as docker compos so that our app itself will be able to run with things as well so in other words having basically two environments that can still work with this docker compos instance of our database itself now one of the things that's also important to note that we will be doing from time to time is we'll go into the root of our project and we will run docker compos down- v-v will delete the volume as well which means that it will delete all of the database stuff as well too the reason we run this especially in development is to get things right to get it working correctly then once it is done deleting we can bring it back up and everything is back basically from scratch when it comes to development this is a key part of it is making sure you can bring things down and bring bring it back up and it happens that quickly which is another reason to really like using something like docker compose of course one of the bad things about it is you might delete a bunch of test data that you didn't intend to delete which of course is something you want to think about if you are going to run docker compos down one of the first steps in integrating our database is to be able to load in environment variables now when it comes to docker compose directly the environment variables are going to be injected in other words we can come into src here and let's go into one,358,31,31,tiBeLLv5GJo
110,of our apis into one of our routes i should be able to go in here and go import os and then just come on read events and i'll just go ahead and print out os.,46,31,31,tiBeLLv5GJo
111,"environ dogit of the database url itself and so in which case i should be able to go into uh you know slash api sl events hit enter and i should see that print statement come through and there it is right there okay great so that's being injected into our application because of this right here but unfortunately or fortunately this won't necessarily work if we bring in our application directly from our virtual environment so let's go ahead and do source vv bin activate and then i'll go ahead and navigate into the src here and then i'll go ahead and run this same thing right here but this time i'm going to go ahead and not specify the port because it'll give me most likely port 8,000 so so now i can open that one up and i can go to api and events and hit enter the print statement is going to be none so we need a way to kind of handle both scenarios some of you might want to use docker compos when you're developing i know i do a lot of times but i also know that i also don't do it a lot of times sometimes i don't set up my apps this way so our actual fast api application needs to be able to be ready for both of those things so inside of requirements.txt this this is where we need to change something i'm going to add in something called python decouple now there are other ways to load in environment variables one i think is called python.",348,32,32,tiBeLLv5GJo
112,env but i like using python to couple for a reason as you'll see in just a moment so the idea here is now inside of my api i'm going to go ahead and create another folder in here we're going to call this db inside of db here i'm going to go ahead and create that init method and then i'm going to also going to create config.py and what i want to do of course is using pip install python decouple i'm going to go ahead and do from decouple import config and then we'll call this as decouple config mostly because this module i just created is also called config we don't want to have any weird imports in here now as you notice cursor is saying hey decouple is not installed so we need to make sure that we do have that installed in our virtual environment which i'll do with that pip install there i already added it into requirements.txt so i don't necessarily need to install it again through that but with this running now um i can reload in here and now i've got this decouple config in here so what i like to do is i like to put in our database and url and this is going to be equal to decouple config and that's going to be that same database url and the default is going to just be equal to an empty string here so this is why i like python decouples we've got this empty string here the other thing about decouple config is it can actually load in a env file as in,358,33,33,tiBeLLv5GJo
113,this right here so we've got this host value here so now hopefully if we did it correctly we should be able to use this now the way we use it is going to be from our events we now instead of using os we're going to go ahead and do from.,67,33,33,tiBeLLv5GJo
114,db. config we're going to go ahead and import the database url or better yet i think we should be able to do from api. db config maybe grabbing database url that way but we'll see if that ends up working so let's go ahead and take a look with that print statement now and going back in here we refresh and now we've got none from os. environ but database url is actually working inside of here which is giving me a different host value but that host value is based off of the env file so this actually brings me back to what i would actually do with docker compose i would not put it hardcoded in here but instead what i would do is i bring in another env file like env.,175,34,37,tiBeLLv5GJo
115,compose and use those same parameters in there and i'm going to just go ahead and put them in directly like that and then we would use maybe that same port value um it's completely up to us in that term but we've got this database url here and then in compose yaml the env file itself would be do compose and then i would just go ahead and completely get rid of this environment itself the only reason i'm commenting it out is so that well we can actually bring it in as need be okay so what we see here is we got this you know no module found error i think maybe i didn't save something or whatever but let's go ahead and bring it back up which should actually build everything uh but it's not actually building the python decouple so what i'm going to do then is i'm going to go ahead and stop it um and then whenn in doubt when something like this happens you do uh d d build and that should actually build the application itself for the ones that are going to to be built uh which will install those requirements now for some reason i actually know why this rebuild did not happen and it's because i did not do docker compose watch so that's something else i need to remember to do and that is docker compose and up-- watch so that it does rebuild it when requirements.txt changes so that error goes away but the key part about this is that now i have another place for my environment variables if i refresh,358,38,38,tiBeLLv5GJo
116,on the docker composed version or my local version i should should be able to see the environment variables coming through for either one and now i have support for both places now the reason i'm doing this as well is so you get very familiar with the process of loading environment variables especially as you work towards going into production because you're going to want to help isolate these things going forward now notice that this emv do composed is underlined or it's ready to be going into um you know git so if we actually look inside of our git step is here we can see that env.,143,38,38,tiBeLLv5GJo
117,compose is in there if we look in the github repo the actual sent code we can see thatv is not in here this is an example of i'm okay with sending this environment variables because composed yl is still using them and in this case it's really just for a local test environment one that i consider disposable as then i can delete it any time obviously the usernames and passwords are not great here this also should indicate to you that the actual time scale service if you were to want to use compose in production this these environment variables should also be in an env file and be done differently inside of there if you were using compos and production which we are not going to be using at all we will be using docker files but not docker composed okay so that's loading in the environment variables in both places as you can see docker compose is far easier than what you would do what i just did but the other thing about this is beyond just having environment variables if you have any other static variables you want to set like other settings for this entire project you could also put those inside of this location as well now we're going to convert our pantic schemas into sql models this is before we actually store it into the sql database i really just want to see how the feature of sql model work in relation to the actual pantic models so what i want to do here is grab this schemas and i'm going to go ahead and copy it and,358,39,39,tiBeLLv5GJo
118,make a new file called models so realistically what we're doing here is just updating the previous ones to be based in sql model and it's going to be incredibly straightforward to do i'm actually going to name them the same thing for now we might change them later but for now we'll leave them as event schema and so on mostly for the ports when we go to test this so the idea here is we want to not use pantic but rather use from sql model we're going to go ahead and import the sql model and then also the field okay so base model is now going to be sql model so we go ahead and replace all of these in here like so and i shouldn't have to change the field but overall this is like the main change that i'll need to do with the actual id we will probably have to change that later which you can actually see in the documentation it's going to look like this at some point so we might as well put that in there uh at least as a comment for now and then we'll come back to that in a little bit uh but what we've got here is basically the same thing as we had before so inside of routing instead of schemas we're going to go ahead and change this to simply just do models then we'll go ahead and jump into our notebook and we really just want to test to see if all of the notebook stuff is still working let's make sure our app is running i have,358,39,39,tiBeLLv5GJo
119,it running in two places i think it looks like it is so uh inside of docker of course and then in my local machine let's go ahead and run this all together and everything works as it did prior um so we should have the actual models all set up as we had before okay so the big change from here would then be to change this into a table as in this is what we're going to actually end up storing of course we're not going to do that just yet the point of this was really to see that sql model and pantic are the same and of course if you actually look in the documentation you will see that at the same time it's also a pantic model so it's quite literally a drop in replacement for this as we'll see very soon it will also help us with storing this data now we need to create our first sql table our database table the way we're going to do this is a three-step process the first step is going to be connecting to the database itself using a database engine when i say connecting i mean having python being able to call the sql table right that's it that's the connection that's what database engines allow for us to do after we actually can connect to the actual database we need to decide which of our schemas in here is going to be an actual database table that might be all of them it might be one of them it might be none of them but the idea is we need,358,39,39,tiBeLLv5GJo
120,to decide which one's going to be a table and then make sure that that it's set up the correct way to be a table and then finally inside of fast api we need to make sure that fast api is connected to all that so those tables are actually being created and they're be being created correctly okay so the first thing that i want to do then is actually decide which of these are going to be database tables now you may have never worked with databases before or you have and you just didn't realize how these tables work to make a decision like this so if you think of a database table very similar to like a spreadsheet you're on the right track spreadsheet have all of those columns and you can keep adding columns as you see fit but then when you actually want to store data those columns describe what data you want to store in that particular column this is not that surprising you might have a column for id you might have a column for page you might have a column for description all of those columns in a database need to have data types so in the case of an id the entire column is only integers they're only numbers in the case of a page those can all be strings and every once in a while you might have a datetime object or you might have float numbers or all sorts of different other data types there's a lot of them out there the point is sql tables especially inside of the structure of sql have,358,39,39,tiBeLLv5GJo
121,"the ability to do very complex operations on all this data now when you think of a spreadsheet you might only have a few thousand rows in there at any given time maybe you have up to a million but if you got a million rows in a spreadsheet there's a really low likelihood you're going to be opening up in in excel or something like that you probably need to use something different altogether but the idea here is a spreadsheet has limits to the number of rows sql tables on the other hand basically have no limits or at least in many cases they're designed to have massive amounts of data in there and so having these data types make a huge difference in how we can use those tables so for example if you want to get ids that are greater than 100 or a, the actual sql table itself will be able to do that very very efficiently versus trying to go one by one and looking for that specific id there's a lot of advantages of using a database itself to store data i'm not going to go through those advantages the point that we need to know right now is that our columns need to have specific data types now this is the cool thing about sql model is we've already declared those data types integer string string right so we already have three data types in here which are going to be actual fields that will be in the database itself otherwise known as columns columns fields you can think of them as the same thing in the case",358,39,39,tiBeLLv5GJo
122,of how we use our sql model so the idea here then is we want to decide which one of these is going to be a table now this is actually fairly straightforward if we look at how we're using it by going into routing.,58,39,39,tiBeLLv5GJo
123,pi right you could already have some intuition about which to pick already but let's go ahead and go through each view to see how we're using it and how this might affect what we decide to store in our database so if we look at the very first one we've got an event list schema this is a git we are only extracting data from the database that's it so there's a really good chance when you extract data you're going to not store the extraction that you just did so that kind of is like a circular loop if you were going to store every time you extracted something that doesn't make sense now you might store the fact that it was extracted but you probably won't exore the actual data that was extracted so this one i think the event list schema we can say yeah it's probably not going to be a database table the next one create event now the incoming data is what we end up storing but do we always want to store all of that incoming data or do we want to make sure that the incoming data is flexible in other words this event create schema here if we were to change it let's say for instance we didn't want to store a description anymore on that crit event we want to remove that do we want to remove this field from the entire database itself the answer in that case would be no we would want to keep that field in there it's just when we first get that data we aren't going to want to,358,40,40,tiBeLLv5GJo
124,um you know maybe have that field anymore okay so in that case this one we're probably going to rule out but notice that it actually turns it into an event schema we return back the event schema itself now this is the one we're definitely going to want to store so we get a payload coming in of data that's all validated and all that then we want to turn it into a different data type which means that we're going to end up storing it in inside of the event schema table which will change very soon now another thing about this is we notice that the event schema itself is used multiple times for database like things in other words if we are looking up that database table we see event schemas in there so that means it's probably going to grab data from there and then also when we go to update data um we've got some data coming in that we might want to change and again we can decide which fields we want to actually allow for updating to happen without removing those fields from the database then we can actually return back that event schema so yeah event schema is the only one we really need to st save in here um and of course the event list view is doing a result of the event schema as well so that's the one that's going to be our database table it's going to be this one i actually do not want to call it event schema anymore i now want to call it event model the reason is hopefully,358,40,40,tiBeLLv5GJo
125,straightforward but let's go ahead and actually modify this a couple times we'll call it event model and then i want to go ahead and add in here table being true and then of course if i change this event model i'm also going to delete schemas dopy we're just going to use models.py in this to keep it simple and then inside of routing we're going to go ahead and do some changes in here because it's no longer event schema it is now just event model so all the places that had event schema i'm going to change to event model now the reason i'm calling event model is you might as well think of it as event table or event database table and that's kind of the point here so that's why i'm going through the process of changing all of the places where it said event schema and now also when i look at the schema stuff i can kind of think of those in terms of not being a database table or not being a database model um and that's kind of the point now these could be pantic models but in terms of the documentation from sql model they recommend you use sql model itself there's probably advantages to it that we could obviously look up but for now we'll just leave it as is music now we need to connect python to sql by actually creating a database engine instance and then actually create the tables themselves so inside of the db module here we're going to go ahead and create session.,349,40,40,tiBeLLv5GJo
126,piy this will handle something else later which is why it's called session but the idea here is we need to create the engine itself so engine equals to something and then we need to actually define something that we will initialize the database itself which should be something like creating database okay so before i actually even create the engine itself we need to know when we're going to run this init db stuff now the idea here is in main.py this is where we will run it in what's called a lifespan method for fast api but before i show you the lifespan method i wanted to show you the old way of doing it which is an event so if you actually came in here and did app.on event and something called startup you could then define on startup and then this is where you could actually run the init method for db this is still a valid method it's still works but it is deprecated meaning it's not going to be supported in the long run so we do something a little bit different than this now if you have these onevent stuff you're going to want to either do all on event or none of them before you do the the context method or the context manager method which is what we're going to do right now so what i want to come up at the very top we're going to go ahead and bring in from context lib we're going to import the async context manager this right here and then up here we're going to go ahead and use,358,41,41,tiBeLLv5GJo
127,that async context manager it is a decorator so we'll go ahead and grab it and then we're going to define the lifespan but it needs to be async define lifespan and it takes in the app which is a instance of the fast api app so you could absolutely use the fast api app in here if you wanted to this is we're going to where we're going to go ahead and call in it tb and then we need to yield something and then this last part would be uh any sort of cleanup that you might want to do so basically like uh we've got here let's remove this we'll go ahead and say before app startup then it yields to actually run it and then we actually can clean up anything in here now i think the reason they brought this in was because of ai models and with ai models you might want to run this lifespan thing in here so it kind of gives us before and after inside of our app but the idea here is we can pass this in to fast api so that's what we're going to be doing now we want to actually bring in that in method here so we'll go ahead and do from api.,283,41,41,tiBeLLv5GJo
128,db. session we're going to go ahead and bring in initdb now of course we actually need the database to work in here so we'll go back into our session and create this stuff out so the first thing is going to be our database engine we're going to import the sql model uh itself not sql alchemy but sql model and we'll create the engine by doing sql model. create engine and then passing in the database url string as we see here so that also means that of course i need to go ahead and do from. config i'm going to import the database url and we'll go ahead and pass that in here now the important part about this database url is it does have a default in here of an empty string so i want to actually make sure that that's not the case so we'll go ahead and say if database url equals to that empty string then will raise a not implemented error saying the database url needs to be set now the reason i'm defining the engine here altogether is because later we're going to do something called get session which will handle or use that database engine as well so now that we've got that we need to actually initialize the database itself which is going to be from sql model once again we're going to now import the sql model class and we'll use this class to create everything so it's sql model. metadata.,328,42,46,tiBeLLv5GJo
129,metadata. create all and it's going to take in the engine as argument so now what this is going to do is obviously make the database connect to the database and make sure that that database has all of the tables that we've created in the case of table being true now one of the things that it will not do is it will not search your python modules for any sort of instance of this class right here that's not how sql model works it only actually creates those tables these tables right here when those models are actually being used in our case they are being used and routing.,144,46,47,tiBeLLv5GJo
130,we actually have this the event model in here it's being imported this routing is also being imported into our main application from event router in other words the actual sql model metadata engine thing should actually work now and so let's go ahead and save everything if you do save everything what you might get is an error like this where pyop g is not installed and then in you also might get it in both places actually so what we want to do is make sure that pyop g is installed right now i don't actually have it installed in here so of course we're going to do that in just a moment now if you remember back to environment variables we actually did this plus pyop g in here that's part of the reason that it's looking for it that's why um you know we've got this error in altogether is it's looking for that specifically so what we need to do then is inside of requirements.txt is we need use that i'm going to be using the binary version which is why i put that bracket in there as binary this is scop g3 or version three there is another one called pyop 2 and you would see that and it would be something like this i believe that's the old version that you would end up using so now that we've got this requirements.txt of course if you're using docker which going to end up happening is this should actually rebuild everything looks like we've got a little bit of an issue going on there but if you're not using docker,358,48,48,tiBeLLv5GJo
131,which i have both of them up for a reason you would then just go pip install d r requirements.txt and hit enter this should install all of it using pyop g binary i think is the way to go uh it makes it just a little bit easier to run the integration itself there are other ways to connect it we'll talk about that in a second but i just want to make sure that this loads and it runs it looks like it's loading and running but i get this error in here this is another error that we definitely need to fix and we'll talk about in just a moment but of course on the docker side it actually rebuilt everything and docker looks like it is working just fine so we'll come back to our local python in just a moment the key here though is really this lifespan in here and that it is creating these database tables now we won't know if these are created just yet we need to solve that one error so let's talk about what that error is and how it relates to docker and also using docker locally in that last part we saw an error where psychop g was not available it was not found and the reason that we saw that error in the first place has to do with how we were mapping in our database engine so if we look into our session here we see that we've got this create engine for this database url the pyop g error happened because of our environment variables with this plus pyop g,358,48,48,tiBeLLv5GJo
132,and we can actually see that in both places whether it's in our local python project or in our docker composed version of it so both of these showed us that specific error now the thing about the sql model and more specifically the underlying technology which is sql alchemy is it can use more than just postgrad it can use mysql it can use maria db and there's probably others that it's supports so this create engine here from a database url is very very flexible which is why in ourv we have to specify postgress sql plus the actual package we're going to use from python to connect which is why it raised that error because we never actually installed it once we installed it that error went away but it brought us a new error which is related to connecting to that database now this is not really that clear as to this right here of course if you do a search or if you look at this it will probably show you that there's something with a connection that's failing and of course if we look at our docker composed one that one is actually not failing it is working correctly or at least it seems like it's working correctly so the reason that the python one is failing is because of where our database is coming from it's coming from docker compose this db service right here so inside of the docker composed network which has both of these apps they can communicate with one one another that's one of the great things about using docker compose is there's internal networking,358,48,48,tiBeLLv5GJo
133,inside of docker compose which is why we were able to specify our database url in this way so i could say db service which is literally the name of the service and it would be able to connect it's a little bit different than like going on our local browser right so when we go into our browser and look at our app it doesn't say app here it says 00 z right so if i tried to do app here i would get a connection error because that is not how my local machine can connect to this app it has to be on 0000 or my local host basically i should be able to connect with simply local host or at least i could try to connect with local host and that also seems to be working working just fine so the way we connect to our actual database service though is a little bit different than the way we would connect to our app itself now the reason it's a little different is the app itself is connecting slightly different than our browser so if we actually look in the env here we see that it says host value if i change this to local host i can try that one and see if that ends up working as soon as i save it let me just restart the python application itself and see if that does it that seems to do the trick right and so if for some reason that was not doing the trick i want to show you one other thing we could try as well so let,358,48,48,tiBeLLv5GJo
134,me close this out and inside of docker compose i'm going to go ahead and get rid of this expose here for a second we'll save that and we'll run this again and it's still connecting so we're doing okay in terms of our ports are concerned but every once in a while you might need to go into env and change it from local host to being something based off of docker itself which is host.,100,48,48,tiBeLLv5GJo
135,doer. internal so that might not be another environment variable you would need to run in order for your app to be able to run as well but right now this is still also giving us an issue so that is not one that you're going to want to use you'll want to stick with local host just like our application has been using before so the reason that i wanted to show you the docker host internal one so i'll go ahead and leave this commented out right above here uh the reason that that even exists is because you might want to try it from time to time depending on your host machine itself and how that ends up working so it's host. doer.,163,49,51,tiBeLLv5GJo
136,internal will often be mapped as well so this is of course one of the challenges with trying to use docker compose without putting your app in there as well but to me i think developing this way is fairly straightforward but of course every once in a while you might need to use a database service that's not inside of your internal network itself uh so there's a lot of different things that we could talk about there but the point here is we need to solve that single connection error now this is also more easily solved by using a actual you know database that is definitely going to work like if you went into time scale and created a database you should be able to actually bring in that connection string it might also still need to use this stuff right here because of how we need to connect but the actual host and all that stuff all of these things would then be based off of like time scale an actual production ready one that we could then go off of as well of course we will see that very soon but at this point now that we've solved some of the database issues we can move to the next part which is actually using our notebooks to see if we can actually even store this database stuff which actually takes a little bit more than just using those notebooks let's take a look let's store some data into our database the way this is going to work is first off we need to create our session function inside of the db,358,52,52,tiBeLLv5GJo
137,session module we're going to go ahead and create something called get session here and then what it's going to return back is with session session the session class itself which we'll bring in from sql model we're going to go ahead and pass in the engine in here and then as session this is going to yield the session itself now the reason we have this is because then we can use it inside of our routes that need a database session so the idea then is grabbing that get session method so we'll do from the api.,128,52,52,tiBeLLv5GJo
138,db. session import get session we're also going to go ahead and bring in depends from fast api and then we'll go ahead and bring in from sql model not sql alchemy but from sql model we'll import the session as well so all of this allows us to then come into an event like this and say something like session and this is of the data type session and then it equals to depends and get session that's it so this function right there will yield out the database session so we can do database session stuff down here now the thing about this function now is it's getting a little bit bigger so i'm going to go ahead and separate things out a little bit and i'm also going to put up response model up here just like that so we don't have to use that arrow method anymore you could do that on all of them both methods are supported and work just fine okay great so now we've got this database session i want to actually use my model i actually want to store things in my model so the first thing that i want to do is i'm going to go ahead and say object object and that is going to be our event model then we're going to go ahead and do model validate of the data that's coming through so this data right here we're going to pass into validate so we're basically validating it again but more specifically to the model we're storing it with then we're going to go ahead and do session.,353,53,54,tiBeLLv5GJo
139,session import get session we're also going to go ahead and bring in depends from fast api and then we'll go ahead and bring in from sql model not sql alchemy but from sql model we'll import the session as well so all of this allows us to then come into an event like this and say something like session and this is of the data type session and then it equals to depends and get session that's it so this function right there will yield out the database session so we can do database session stuff down here now the thing about this function now is it's getting a little bit bigger so i'm going to go ahead and separate things out a little bit and i'm also going to put up response model up here just like that so we don't have to use that arrow method anymore you could do that on all of them both methods are supported and work just fine okay great so now we've got this database session i want to actually use my model i actually want to store things in my model so the first thing that i want to do is i'm going to go ahead and say object object and that is going to be our event model then we're going to go ahead and do model validate of the data that's coming through so this data right here we're going to pass into validate so we're basically validating it again but more specifically to the model we're storing it with then we're going to go ahead and do session.,352,54,54,tiBeLLv5GJo
140,add that data so when i say obj i think of this as an instance of this model or basically a row of that class but i like calling it obj it probably is an old habit you can call it what you'd like just make sure that it's consistent throughout your entire project whenever you're doing something like this and then you're adding it into the database so this is preparing to add it then session. commit is actually adding it into the database then if we want to do something with this object as in get the id we would do session.,135,55,56,tiBeLLv5GJo
141,refresh of that object and then we would just return back that object itself because it's going to be an instance of this model class itself and that's how we do it that's how we can add data into our database it's really just that simple now of course it we have to remember that if we want to use a session this is the argument that has to come in there we use the session variable through here if i called this sesh that would be fine you would just want to update it down here that's not common so definitely consider that when you change these names but the idea is of course it's a session class and it depends on that function which will yield out a session based off of our database engine which is of course why i have that database engine up there in the first place so now that we've got this we've got a way to actually add in data which we can verify by going into our notebook and trying this out so i'm going to restart in my notebook here and i'll just go ahead and run it and what we should see is there's some data in here with an id now do we recall whether or not that id was valid before i don't know let's do it again there's another id and we do it again there's another one and we do it again and another one so we'll contin continuously increment this id even though we didn't specify it directly inside of here all we are specifying on that api request is,358,57,57,tiBeLLv5GJo
142,the data that's coming through and so it's automatically incrementing it for us thanks to the features of a sql database in this case it's a postcg database but we're automatically incrementing with this being a primary key as i mentioned before and we can see that result right here when we go through and actually add in more data now of course we need to fill out the other api routes as well which we'll do in just a moment but before we do that i will say every once in a while as soon as you start filling up the database with some of this test data you're going to want to come in here and do docker compose down with that- v to take down the database itself this will delete that database and that network it alog together and then you would just bring it back up with that watch command just to make sure that everything's changing as need be now the database is completely fresh which we can then test again by jumping back into that notebook and running the same commands now we get a connection error with the notebook which doesn't make that it makes a whole lot of sense with how these sessions work within a notebook so i'm just going to restart the notebook and then i'll go ahead and try that again and i'm still getting a connection error so that's probably not good this is probably a little bit of an issue with one of these things so let's go ahead and restart this application which port are we using up here let's see,358,57,57,tiBeLLv5GJo
143,here we've got a 802 so everything should be back up and running again let's see looks like the database is ready let's go back into the notebook here and maybe i just went a little too fast for it and there we go so now it's back into being connected with that one let's try this one here and there we go so we should be good to go in terms of that data and there it is auto incrementing okay so every once in a while you might need to let things refresh and actually rebuild as well so saving data and doing all that will help flush out that process because you're probably not going to bring the database down that often especially not while you're talking about it we're now going to ask our database for a list of items a certain number of items that we want to come back this is going to be known as a database query that will return multiple items so the first thing that we want to do is look into read events this is where we're going to be changing things because well that's our api inpoint to list things out now the first thing i want to do is bring in the session because i'm using the database so i definitely want to have this session in there to be able to use that as well now once again we can use that same response model idea up here as well but this time instead of the event model it's going to be the event list schema which we of course already have,358,57,57,tiBeLLv5GJo
144,format it out right down here so the idea is we want to replace this list right here with the actual results so the way we do this is by doing something called a query so we set it equal to a query now if you're familiar with sql you might be familiar with something like select all um and then from some sort of database table itself which in our case is the event model we're basically doing that as well but we're going to be doing it from the actual event model class in other words the actual sql model om or uh so which means that we need to bring in this select call here which we will go ahead and do select from this event model and so that is a basic query that we can do then from there we can actually get the results by executing this in the database itself which is our database session.,210,57,57,tiBeLLv5GJo
145,execute and then the query itself and then the final thing is we would do all this would allow us to get a bunch of items in here and which case i should be able to return them just like that the reason i should be able to return them is because of this model itself is inside of the list schema also right so going back into that schema itself we see that the results are looking for instances of that model which is exactly what we're doing here we're returning those instances in this request that's what's happening with that so that's at the the baseline what is going to happen and then we can also grab the length of those results and then there we go so this should give us some results in here now if we actually go back into our notebook for the list data types here i added this print statement in here so we can actually see it a little bit better which allows us to see hey there's the ordering that's going on there's all of that data of course if we were to add in some more data we can see this again so i'm going to go ahead and add in a few more items in here and just run this cell several times to just make sure that i have maybe over 10 items in here as we can see by this id or maybe even over 12 so later we can use that 12 number again so now that i've got 12 in here let's go back into my list data types and,358,58,58,tiBeLLv5GJo
146,run this again and we can see there's my 12 in here so this becomes a little bit of an issue is our results here have 12 items but maybe we actually only want to return back 10 items in which case we would come back in a routing and we would need to update our query here by having something called liit and then we can pass in the amount that we might want to limit in terms of the result itself in which case i should be able to go in here and then i can run that limit itself and then there's 10 as a result and we see that there's only 10 items in here now i can also update this so the ordering or the display order is a little bit different as well so back into our routing here we've got this query still we can come in and then do something like order bu and we can add in an ordering in here now how does this ordering work well it's going to be based off of the actual table itself which of course that table is our event model inside of that table we have different fields in there in my case i can use id that field is this one right here we could use page your description as well but those are the only fields we can really order this by and then we can do descending which just changes the order it flips the order in here which we could verify by going back into our api and doing another request and we see now,358,58,58,tiBeLLv5GJo
147,it's flipped and id of 12 is coming up first and two and one are no longer in the results altogether and of course if i wanted to flip it back i can change it to ascending and of course that will flip back that order with different values in here okay so this would be like okay the next level of this would be doing pagination or allowing to have only 10 results coming back but then multiple pages of 10 so you can you can do this request several times to get all of the data cuz realistically you would not get all of the data once you know you would probably never do this command that's probably going to end up being too many things so you would often use a limit you might have a bigger limit like 100 or 500 or a th000 it really is going to be depending on your api capabilities but the idea here though is we do want to have a limit of some kind and by default you may have noticed that the actual ordering was based off of the id it was actually going up like this it was doing that ascending from the id um which is also very nice it's a very easy thing to work with but that's our list view now that we've got that we might want to make it even more specific as in getting a detail view a very specific item that we can work with and then maybe eventually update as well let's take a look using sql model we will do a g or 404,358,58,58,tiBeLLv5GJo
148,error for any given event now what we're going to do here is very similar to what we've done before first off i'm going to use the response model again and this time it is just going to be the event model like that and then now we want to build out the session query itself so i'll go ahead and grab the session data and we'll bring this in as well now what we want to do of course based off of this model we want to look it up for this event here so we need to define the query first which is going to be our select call and it's going to be based off of this model here and then we use something called where and we can grab that model do a field name and we can set it equal to this event id that's being passed through and that's going to be our query now so then from that query we get a result which would be something like the actual data itself which of course is going to be our session.,244,58,58,tiBeLLv5GJo
149,execute and then we're going to grab that query and instead of using all we're going to use first the value here would then be returned and that's pretty much it for our detail view or is it it's clos close it's not quite the whole story jumping into our list data types here we're going to go ahead and copy this git event down here and i want to just change it ever so slightly so that our path is going to be updated to be more appropriate for a detail view so i'll call this detail path and that's going to take in you know something like that 12 and then we'll use the detail endpoint and i probably don't need to put that base view in here anymore okay so now we'll go ahead and grab that detail uh endpoint here for our request and i'll just go ahead and set this to r and then we will do r all across the board and once again pr print this time instead of status r okay we'll do our status code in here and see what that looks like okay so if we look at our list view we should still see uh that there is probably 12 in here right so let's go ahead and just change our list view real quick to descending just to make sure that we have some data in here that is accurate to what're looking up okay so back in here and list view there we go so we got an id of 12 i'm going to go ahead and run this and there we go,358,59,59,tiBeLLv5GJo
150,so we get that data of 12 if i go one higher i get a 500 error that is a server error this should not raise a server error it should raise a different kind of error so this is not quite done and it has or rather this is not quite done and it has to do with this right here this query res returned back a none value and we can actually see the error in here that we get the input is simply none right because we're trying to return back the event model but we're actually returning back none so really what we need to do is say if not result then we need to raise something called the http exception and it's capital htp exception and we want to add in a status code in here of 404 and then some sort of detail saying that this item was not found or event not found right and so we need to bring in this http exception from fast api we'll bring this in like that save it and now we'll go ahead and try out this same sort of lookup and we'll do it again now we get a 404 the actual error that you should have for a item that's not found not found in the database in this case and so the cool thing about this is the same sort of idea if i made a mistake on this lookup and turned this into let's say a string value here and and still did the lookup let's take a look at what happens there 500 error this time so,358,59,59,tiBeLLv5GJo
151,let's go ahead and see what that error is right so it's looking for the parameter with the id of 13 so in this case it's actually not looking up the id uh based off of the string value right it's not going to do that it's going to give us another kind of error which shows us yet another time that hey you want to make sure that you're using the correct data types when you do this lookup now the important part about the actual lookup itself the request that i'm making this is a string it's just fast api parsing it into a number so that's also another nice thing about this um but overall we now have our git lookup uh it's fairly straightforward the next part is using this same sort of concept to actually update the data which kind of puts it all together the delete method i will leave to you it's actually fairly straightforward and there is plenty of documentation on how to delete events which is something i'm not really concerned about at this point but updating one is a good thing to know because it kind of combines everything with we've done so far let's take a look let's take a look at how we can update the event based off of the payload that's coming through keep in mind the payload is only going to have a certain number of fields the way we have it is literally the description that's the only thing we're going to be able to change in this update event which is nice because it limits the scope as to,358,59,59,tiBeLLv5GJo
152,what you might potentially change so the idea here is we're going to go ahead and grab the query and all of this stuff just like we did in the g event i want this to happen before i go to grab the payload data cu i don't need it if i don't have the correct id now of course we still need the session in here as well and then of course i also want to bring in my event model as the response model to make it a little bit easier as well so i'll go ahead and copy that and paste in here there we go and then i will update the arguments here just a little bit so it's also easier to review as we go forward okay so let's let's go ahead and get rid of this now now i've got my session i can still look everything up as we saw before and this is now going to be our object this is really what it is i had it as result up here but it's the same thing it's still an object just like when we created that data so in other words we still need to do basically this same method here just like that and then when it's all said done we will go ahead and return back that object so the key thing here though is the data itself how do we actually update the object based off of this data itself the way you do that is by saying for key value in data.,344,59,59,tiBeLLv5GJo
153,items this of course is unpacking those things we can do set attribute and we can set attribute for that object of that key value pair that's in there and again the actual incoming payload is only going to have a certain number of keys if for some reason you didn't want to support those keys you could set that here so say for instance if k equals to id then you could go ahead say something like continue because you're not going to want to change that field but again you shouldn't have to do this at all it's a bit redundant based off of the actual schema itself but if you were to make a mistake then yeah of course you might want to do it then okay so with this in mind i should be able to run this and get it going so the idea here then is just checking this out or testing it out so if we look into the send data to api we have the api endpoint for this which is basically what we've done before so now we've got the event data here's the description that we want to bring in this time i'm going to go ahead and say inline test and we'll go ahead and run that now we've got inline test in here if i go back and list out that data let's go ahead and do that and see what the description is now and there is that inline test so we were able to actually update that data as we saw uh just like that okay so this process is very straightforward,358,60,60,tiBeLLv5GJo
154,of course if we tried to update data for an item that does not exist we will get a 404 right so going back into the send data here i'm just going to change it to 120 now we get a you know event not found and if of course if i change this response to status code we should see a 404 in here coming through great so yeah the actual api endpoint is now working in terms of updating that data again this one we probably won't use that often but it's still important to know how to update data in the case of an api not just with what our project is doing right and of course the challenge i want to leave you with is really just doing the delete method yourself self which is actually pretty straightforward given some of this data in here as well so at this point we now have update we have create and we also have list so we're in a really good spot in terms of our api the last part of this section what we want to do is adding a new field in this case i'm going to be adding a timestamp to our model to the event itself having an id is nice because it will autoincrement but actually having a timestamp will give us a little bit more insight as to when something actually happens now what we're doing here is we are actually going to delete our previous models and then go ahead and bring things up in other words i'm going to come into my docker compose down and,358,60,60,tiBeLLv5GJo
155,then- v this will of course delete the database so it's a lot easier to make changes to my database table so what you would want to do in the long run is use something like a limic which will allow you to do database migrations it will like quite literally allow you to alter your database table but in our case we're not going to be doing that because once we have our model all well organized we should be good to go and not need to change it very often and if you do running this down command is not really that big of a deal altogether now before i start this up again and this is going to be true for any version of this app i want to build out the actual field that i'm going to do right now so the idea here is we're going to go ahead and do something like created at and this is going to be a date time object so we'll go ahead and bring in from date time we're going to import date time okay and this is going to be equal to a field here now this field takes a few other arguments there's something called a default factory this default factory has to do with sql alchemy that will allow me to do something like define get utc now and then i can return back the date time.,314,60,60,tiBeLLv5GJo
156,now and then we can actually bring in the time zone in here and do time zone do and utc and then i can just go ahead and say um that this value should be replaced for the tz info of time zone. utc okay so just making sure that we are getting utc now this will be our default factory so when it's created it's going to hopefully go off of that function and work correctly now when it comes to the actual database table there's another thing that we need to add in this is going to be our sql alchemy type the actual database type itself which is going to take in from sql model which we need to import sql model as well and oops that's not what we wanted so we'll do sql model here and then i'll go ahead and bring in sql model.,195,61,62,tiBeLLv5GJo
157,dat time and then this is going to be time zone being true so when it comes to sql alchemy you need to define things for datetime objects it's definitely a little bit different than what we have here but the sql alchemy stuff does creep in a little bit with sql model when you're doing a little bit more advanced fields datetime is one of such field and then we're also going to go ahead and say nullable being false okay so doing nullable being false is yet another reason to take down the database and then bring it back up so at this point we should have this field automatically being created for us when we go forward okay so now i'm going to go ahead and run the ducker compos up again and this of course should create the database tables for me and i'll also go ahead and run my local version of the python app as well which is attempting to create those database tables as well so the only real way i can test to make sure this is working at least how we have it right now is to create new data so we'll go back into this notebook that will allow for it and hopefully everything's up and running as it was looks like i've got a connection aborted so maybe i need to do a quick save on one of my models just to make sure that that's up and running looks like it is now so let's go ahead and try that one more time we definitely saw this before so i'll go ahead and run,358,63,63,tiBeLLv5GJo
158,it again and it looks like that time it worked okay great and so now if i go through this process i can see there is a time zone being created in here and of course i could do this as many times as necessary to update future fields if i wanted to which in my case i'll just go and go to five and notice that created at is in there as well now one of the other things that you might consider using this same idea of creat at is updated at so you could add a whole another field that's very similar to this but it won't have a default factory uh necessarily you could still leave a default factory but if you wanted to create it at field or an updated at field in your update you know command here you would then go ahead and set it again so you would say something like updated at and this is where you would go ahead and be able to set that field itself for that time stamp when it may have been updated that's outside the scope of what we wanted to do here but the point is i wanted to make sure that i have a bunch of different fields in here that make a whole lot of sense in terms of actually storing data especially when it comes to a datetime field like this we need to see how we can actually store individual objects itself um at any given time and also have a field that is automatically generating inside of our database which is exactly the point of,358,63,63,tiBeLLv5GJo
159,this whole thing let's actually take a look at how we can do that updated field i'm going to go ahead and come into our event model here we're going to paste this and change it to updated at just like that there shouldn't be a whole lot of changes we would need to that but of course this also means that i need to go ahead and do docker compose down of that actual entire thing so we can bring it down now back into our routing what i want to do is i'm going to bring in the git utc now method here so it's using the same functionality as when it's being created so that it grabs what that value would end up being based off of all that data so then in our update view then we come in here and i'm going to go ahead and bring in object.,200,63,63,tiBeLLv5GJo
160,updated at and then this is going to be the g utc now and then that should actually set it and get it ready for when we go and store it okay great so with this in mind i'm going to go ahead and run this again we'll bring it back up now one of the things the reason that we have it with the same default factory is because when it's initially created we might might as well also set the updated at field at the same time so they're going to be the same value when they're initially created but then in the future they might change if you then update them and let's just make sure that everything is running looks like it's all running we're good to go so let's go ahead and give this a shot so first and foremost we want to just send some data in which i will do with that post method right there and there we go so we've got an id of one in here i'm going to change this to an id of one as well and we should see that updated at and created at are the same time um it might be slightly different because of the microsc but overall the actual seconds are the same as we can see right here and right here great so now i'm going to go ahead and update it and we should now see a change the created at should be the same the updated ad should be slightly different and it's not really that much different but it is different it's different enough to,358,64,64,tiBeLLv5GJo
161,see that that actually ends up working so that's how you end up going about doing that now of course you could also have a field that is completely blank you don't necessarily have to automatically update it uh in the future but that's just a simple way to make that change so you can really see those time differences uh if you wanted to now the other nice thing about this too is then in our list view what we could do is we can come in here and do updated at and go based off of that value instead of we could also do created at of course but now we can go off of a different value alog together in which case we can come back in here and we can see the different data items that are coming through in here which we only have one right now so let's go ahead and send a couple more and of course uh we'll update the first one again just to make these things a little bit out of order and so let's go ahead and try that again and what we see in here is now the different data is coming through on how it ends up looking the updated at is the first one because of course that's how we defined it but i can always re flip it if i wanted to based off of the routing itself so now ascending so the oldest one being first we can go ahead and take a look again and let's list that stuff out and now we've got the oldest one being first,358,64,64,tiBeLLv5GJo
162,the most the the oldest updated one not the oldest created one uh being first and then the most recent created one being last or updated one being last again okay so cool um very straightforward uh i think that would have been really easy for you to do off the video but i just wanted to show you some techniques to do it in case you were curious about it and most importantly to show you this ordering stuff as well as to you can use updated and created app we now know how to store data into a sql database thanks to sql model it makes it really straightforward to use this structured data that is validated through pantic and then actually stored in a very similar way as it's validated which i think is super nice and then extracting that data really looks a lot closer to what the actual sql is under the hood which i think will actually help you learn sql as well if you are trying to learn it because the way this query ends up being designed designed is it's very much like sql itself so sql model i think is a really really nice addition to the python ecosystem and of course i'm not alone in that it's a very popular tool but the idea here is we have the foundation to read and write data into our database now we need to elevate it so we can read and write a lot more data and do so by leveraging time series when it comes to analytics and events specifically we definitely want to turn it into,358,64,64,tiBeLLv5GJo
163,time series data now the difference between it are going to be mostly minor based off of all of the tooling that's out there so actually turning this into time series data is very straightforward so let's go ahead and jump into a section dedicated directly to that we're building an analytics api so that we can see how any of our web applications are performing over time in other words our other web applications will be able to send data to this analytics api so we'll be able to analyze it and see how it's doing over time the key word is overtime there now a big part of the reason that our event model didn't have any time fields until later was really to highlight the fact that postgres is not optimized for time series data so we need to change we need to do something different and of course that change is going to be time scale and specifically the hyper taes that time scale provides now we have been using a time scale database this whole time but we've been only using the postgress portion of it so now we're going to convert it into a hypertable because that revolves around time and time series data it's optimized for it now a big part of that optimization is also removing old data that's just not relevant anymore now it removes it automatically and of course you can build out tooling to ensure that you're grabbing that data if you wanted to keep it in some sort of cold storage long term but the point of this is that we want to optimize,358,64,64,tiBeLLv5GJo
164,analytics api around time series data so we can ingest a lot of data and then we'll be able to query it in a way that we're hopefully kind of familiar with at this point by using postc so at this time we are now going to be implementing time scale and specifically a package i created called time scale python we'll look at this in just a moment but as you can see here this is exactly like our sql model with a few extra items a few extra steps to really optimize it for time scale db now these extra steps are not really that big of a deal as you'll come to see and a big part of the reason they're not a big deal is because the time scale extension is the one that's doing the heavy lifting this is just some configuration to make sure that it's working let's go ahead and get it started right now we're now going to convert our event model which is based in sql model into a hyper table or at least start that process so we can start leveraging some of the things that time scale does super well so the way we're going to do this is we are going to go ahead and bring in from time scale db we're going to import the time scale model and then we're going to swap out sql model for that time scale model at this point it is still mostly the same so if we actually look at the definition of the time scale model by hitting control click or right click you can,358,64,64,tiBeLLv5GJo
165,see the definition of this model it is still a squel model it is based off of that and then we've got id and time in here so two default fields that are coming through and then some configuration for time scale specifically but let's take a look at these two fields we've got an id field hey that looks really really familiar to what we were just doing with our id field it has another argument in here that is related to sql alchemy on sql model if you ever see sa like that that is referring to sql alchemy which is a another part or dependency of sql model um it's an older version of sql model if you want to think of it that way now the other thing here is is we've got a date time field which hey what do you know we've got a default factory of get uc utc now it is a datetime object with the time zone and it's a primary key as well so we actually have two primary keys so the hyper tables which is what we're building is going to rely on a time field absolutely which is part of the reason we declare the time field altogether the other thing is we have a composite primary key which actually uses both of these fields as the primary key so the idea here then is we want to actually use this time field instead of the one we have so if we look back in our event model we had this created at field in here which we can now get rid of as,358,64,64,tiBeLLv5GJo
166,well as this id field now the other thing you'll notice is we've got this get utc now it's in both places and so we can actually use the time scale db version of get utc now instead of having our own now of course you could still have your own but it's not necessary at this point since we have this third party package that we are relying on we can use other fields to rely on it as well so the last real thing about this model is deciding what is it that we're actually tracking here now in my case what i'm trying to track or the time series data that i'm looking for is page visits so how would we think about page visits well that's going to count up the number of visits at any given time right that's the thing that's what we're counting up this is a little bit different than like sensor data for example so if this was sensor data you would have a sensor id instead of a page which would be an integer and then you would have a value which might be a float right and so you're going to then do something like the um you know value or maybe let's say the average value of a sensor at any given time right so that would combine those two fields but in our case we want page visits we want to count up the number of rows that have a specific page on it so the reason i'm bringing this up is mostly so that when we think about designing this model we,358,64,64,tiBeLLv5GJo
167,want to rethink what fields are required in the case of page that one is absolutely required we we definitely need page otherwise it's not an event so we're going to go ahead and say that it is a string and this time we're going to go ahead and give it a field with an index being true now making an index makes it a little bit more efficient to query but the idea here of course is going to be it's going to be our about page or our you know contact page or so on and of course inside of time scale or postc in general we can aggregate this data we'll be able to count the pages we'll be able to do all sorts of queries like that which we'll see very soon but the rest of the actual event model we could either get rid of or we can still use the updated at i'm probably never going to update this data but it is nice to have that in there and then a description in there as well so we're really close to having this as a full on hypertable it's not quite done yet and we still have to modify a couple things those are related to how we are using our session so we're going to change our default engine in here and then we need to run one more command related to initializing all of the hyper tables it's time to create hyper tables now if we did nothing else at this point and still use this time scale model it would not actually be optimized for time,358,64,64,tiBeLLv5GJo
168,series data we need to do one more step to make that happen a hypertable is a postgress table that's optimized for that time series data with the chunking and also the automatic retention policy where it'll delete things later this is a little bit different than a standard postgres model but again if we don't actually change anything it will just be a standard postgres model now we want to actually change things we want to commit those changes now the way we're going to do this is inside of session.,119,64,64,tiBeLLv5GJo
169,create all engine this function here is going to look for all of these time scale model classes and it will go ahead and create the hyper tabls from them if they don't already exist it's really just a conversion process it's still a postc model under the hood and then it actually turns into a hypertable now there are issues that could come up with migrating your tables so i'm not really going to talk about that right now it's kind of outside the scope of this series if you're going to be changing your tables very often but the point is that this right here automatically creates hyper tables the time scale db package has a way to manually create them as well especially when it comes to doing migrations and making those changes that a little bit more advanced now the big reason that i'm talking about that at all is because we want to make sure our model is really good ready set because we probably don't want to be changing our time scale model very often or really our hypertable very often unlike your standard postgres table you might change that more often than something like a hypertable now the idea here then is after we've got our creating hyper tables we have one more step that we need to do and that's changing our create engine because we actually want a time zone in here as well so there is an argument that you can pass in time zone whichever time zone you are using in our case we've been using the utc time zone which of course we see,358,67,67,tiBeLLv5GJo
170,in our model itself where it's get utc now that's going to be the default time zone we use and it's the default one for the time scale model anyway now i recommend just sticking with that time zone because you can always change a time zone later using utc makes it really easy to remember hey my entire project's in utc we can convert it if we need to okay so the idea also with this time zone is we might want to put it into our config.py here which in which case i would go ahead and do something like db time zone and then just paste that in here and then have the default being utc once again in which case i would then import it into my session.,171,67,67,tiBeLLv5GJo
171,and do something like that great so we're not quite ready to run this command and when i say run this command i mean that we need to take down our old database and then bring it back back up so we can make sure that everything's working correctly including any changes that i made to this model but there are a few other things that we need to discuss before we finalize this model and then has to do with some of the configuration that we want to use inside of our hypertable itself there's a couple configuration items we want to think about before we finalize our hyper tables so let's take a look at the time scale model itself and if you scroll down there's going to be configuration items in here maybe some aren't even showing yet the main three that i want to look at are these three first is the time column this is defaulting to the time field itself right so we probably don't ever want to change the time colum itself although it is there it is supported if you need to for some reason in time scale in general what you'll often see the time column that's being used is going to be named time or the datetime object itself so that one's i'm not going to change the ones that we're going to look at are the chunk time interval and then drop after so let's go ahead and copy these two items and bring them into our event model and i want to just go ahead and set these as a default of empty string,358,68,68,tiBeLLv5GJo
172,here so what happens with a hypertable is it needs to efficiently store a lot of data which means that it's going to store it in a chunk each chunk is like its own table itself now you're not going to have to worry about those things time scale will worry about them for us the things we need to worry about are the interval we are going to create chunks for and then how long we want to store those chunks for in this drop after so this is actually very well illustrated inside of the time scale docks by taking a look at a normal table and then taking a look at a hypertable a hypertable is a table and then it has chunks within that so as we see here we've got chunk one 2 and three and then each chunk corresponds to the interval that we set so for example this chunk is for january 2nd this chunk is for january 3rd and so on because the time interval is for one day the entire chunk is a day at a time that's kind of the point here hopefully that makes a little bit of sense but the point of this also with respect to chunks is if you have a lot of data how you handle your chunks is going to be different if you have a little data right if there's very little data your chunks could probably be a lot longer because you're not taking up that much room in any given chunk and also you're probably not going to be analyzing that data much for any given chunk,358,68,68,tiBeLLv5GJo
173,so for example if this is your very first website your chunk time interval might be 30 days and it actually is going to be listed out as interval 30 days like that and so that's how you write it you could say something like 30 days now one of the downsides of having an interval of 30 days or too long of an interval is you will have to wait after that interval passes to make a change so say for instance right now we say hey it's going to be interval of 30 days that means all of the data coming in right now has a chunk time of 30 days if in a day or a week i want to change it down to one day then we have to wait that full 30 days before this new interval would end up changing now these automatic changes right now are not currently supported by time scale db's python package or the one that i created at least maybe at some point it will be but the point of this is we want to decide this early on i actually think interval of one day is pretty good for what we're going to end up doing if we start end up having a lot of data then you might want to change this interval to like 12 hours but more than likely i would imagine one day is going to be plenty for us so the next question comes when do we want to drop this data now time scale will automatically drop this data based off of what we put in this drop,358,68,68,tiBeLLv5GJo
174,after so in the case of our interval here we could absolutely say drop after 3 days but that actually doesn't give us much for time for analyzing our data and what do i mean by drop after i mean quite literally after 3 days this thing would be deleted this would be completely removed from our table it's not going to be there anymore this helps make things more uh optimized and also improve our performance altogether while also removing like storage that we just don't need anymore we might not need that anymore so we could always take that old data and put it into something like cold storage like s3 storage but what we want to think about here though is when realistically would we want to drop some of this old data again it's going to be a function of how much data we're actually storing so this interval can also change as well so for example if i did two months then i would only have at most the trailing two months data if i wanted to have a little bit longer than that like 6 months then again i would only have at most 6 months of data at least in terms of querying this specific time scale model again we can always take out those chunks and store them later if we wanted to in my case i'm going to go ahead and just leave it in as three months i think that's more than enough you might also change it to one month and so on um and you can change these things later but i wanted to,358,68,68,tiBeLLv5GJo
175,make sure that they're there right now so that we have them and we understand kind of what's going on with our hyper tables or hopefully a lot more as to what's going on with our hyper tables so the way we're we're going to be chunking our data is going to be with one day and we're going to keep the trailing one day for 3 months so we'll have at least that much data going forward let's go ahead and verify that our hyper tables are being created first and foremost i'm going to go into session.,128,68,68,tiBeLLv5GJo
176,here and i'm going to go ahead and comment out the process of creating those hyper tabls because we're going to do it in just a moment then i'm going to go ahead and run docker compose down just like that and then we're going to go ahead and bring it back up so that we can quite literally have this working once again okay so we should see creating database regardless of how you end up doing it but the idea here is we now have our database up and running so to verify this what we're going to be using is something called popsql or popsql decom and just sign up for an account it's free especially what we're doing and you're going to want to download it which is going to be available on this desktop version right here and you can go ahead and download it for it your machine once you do you're going to open up and you're going to log in you're going to probably see something like this so the idea here is you're going to select your user and click on connections and then you're going to create a new connection so we've got a lot of different connection types in here that we can use to really just run different queries that we might want to in our case right now we're just going to go ahead and verify that our database is a hypertable database or has a hypertable in it so that means that we're going to be using time scale here now the idea here is we could also use post credits both,358,69,69,tiBeLLv5GJo
177,of them are basically the same right the connection type is basically the same it's just a little bit different so we want to use time scale to verify that it is a hypertable when we look at it and then the next part is going to be our connection type we're going to go ahead and use directly from our computer we want to make sure we do that and of course this is going to be on local host and the port here is 5432 the standard postest port and it's also the same port we defined in composed.,131,69,69,tiBeLLv5GJo
178,yl this port right here the first one of the two great so now we've got that we now need to fill out the rest of it the database name which of course is our time scale db the username which is going to be our time scale user and then also the password time pw and that's it then i'm going to go ahead and save that and we can run connect and now what we should be able to do is grab this data so i'm going to refresh here and we can see there's our event model and i should be able to go to the event model itself and we can see that it's not a hypertable at least not yet so back into our session here i'm going to go ahead and change it to see if it will create those hyper tables from here so as soon as i save this i should see it says creating hyper tables and hopefully i don't see any other errors this is true regardless of what version we're using here uh but ideally speaking we have all of our hyper tables so if i refresh in here this might actually show me my hyper taes it might not uh so the reason that it might not be showing our hyper tables is maybe we need to just start over a little bit in which case i'm going to go ahead and go docker compose down again and then i'm going to go ahead and do docker compose up just to see if that ends up solving it it might be a caching thing,358,70,70,tiBeLLv5GJo
179,"as well so we would also look at that but there we go creating hyper tables and all that let's go ahead and refresh in here right now it's still saying no which it shouldn't so let's go ahead and close out popsql for a moment and see if we can just reopen this up again i think it's a caching thing which is not that big of a deal at all we just need to refresh pop sql itself so that we'll be able to run with our previous connection now there's a chance that we might need to reenter our credentials i don't think that's going to happen uh but here we go and now it shows sure enough it shows a hypertable right away and notice that compression is disabled currently but it does show us the number of chunks we have so what if we actually start making some data the way we're going to do this is by jumping into our notebook here i went ahead and had this one single event right so we had this post event that we did before off the video i went ahead and did this event count where it sort of randomizes stuff but it creates 10,000 events for us so i'm going to go ahead and run that and what we should see in our t uh table at some point it will just be one chunk uh so it's going to take a moment maybe for that to be flushed out completely or maybe we need to do a query alt together i'm not actually sure um in terms of why that's",358,70,70,tiBeLLv5GJo
180,"not showing up just yet but it is bringing in all sorts of data for us which is pretty cool so we will'll test this out a little bit more the key of this was just to verify that it does say hypertable notice that compression is disabled that goes back into our time scale model where the default for compression is false so you can absolutely add in compression to make the data even more optimized for storage uh but for now we're just going to leave it as is and then we'll go through the process of actually seeing all of this data we will use uh popsql a little bit more to run some sql queries or really to verify them but we're not going to run that many sql queries instead we will do the related queries right inside of our notebooks using um you know sql model as well as time scale db the python packages now what would be best is if we actually ran this a number of times over a number of days to simulate the you know actual process of visits but of course i can still run this a lot and we get you know 10,000 visits per time or 10,000 events per time and we'd actually be able to see a lot of data coming through and here uh at any given moment right so 10,000 events and of course this is happening synchronously so it's not quite like what a real web application would be like uh but it is still at least giving us a bunch of data that we'll be able to",358,70,70,tiBeLLv5GJo
181,play around with let's take a look at how we can do some of the querying with the sql model now that we have a hyper table and we are able to successfully send data to it i want to actually be able to query this table and i want to do so in a way that is still leveraging the event model itself before i create the api endpoints now we have already seen how we can query it in fast api but i want to actually design the queries outside of fast api before i make it an api endpoint itself just to make sure i'm doing it correctly now the way we're going to do this is inside of a jupiter notebook so we'll go ahead and do nbs i'll go ahe and do five and we'll go ahead and do sql model queries.,191,70,70,tiBeLLv5GJo
182,ipython notebook and then in here i'm going to go ahead and import a few things the first thing is going to be s the next thing is going to be from pathlib we're going to go ahe and import our path here so when it comes to being able to import something from a model like models.py there's no real clean way to do this i can't just go from source. api. events. models import event model right this will most likely not work because there's no module named src now this is in part because we're inside of the nbs folder here and so there's a number of different ways on how we would be able to import this or grab it but the way i'm going to do it is i'm going to say my src path equals to path of dot dot slsrc and then we'll just go ahead and resolve that right and so if we resolve that and take a look at what it is that is that src folder right there and so what i want to do is i want to pin this to my system path so all i can do is sy. path. append will take that string of that src path here just like that okay so that will append it to our system path which means then i can actually just go ahead and do instead of src i can just use api. events.,319,71,77,tiBeLLv5GJo
183,session we can import the engine as well which of course is going to give us this engine right here which has the database url already on it or at least in theory it does so that's another part of that as well let's go ah a and run that one sure enough it ends up working so next up what i want to do is i'm just going to do a very basic lookup a very basic query for this first part and then we'll come back and start doing more and more advanced queries so what we want to do here i'm going to actually put all of the imports in the same sort of area we'll go ahead and bring in from sql model we're going to bring in the session itself and then run that cell and then to use this session we'll do with session and then we'll pass in the engine in here and then we'll go ahead and say as session and then i'll be able to do my different session things that we might want to this of course is going to be basically the same as what we're doing in our rep it just fast api handles it for us with that session stuff and all that but we can actually do the same exact query in there so let's go ahead and grab that and just verify this in here with our session and i should be able to print out the results in here as well um we also need to import the select so let's go and do that as well and now we'll,358,80,80,tiBeLLv5GJo
184,run this and we should get the same sort of data coming back okay so that gives us a result but what i really want to see right now is before we build out the rest of the queries i want to actually see the raw sql query here now what we do is we bring in compiled query equals to query.,80,80,80,tiBeLLv5GJo
185,compile and then we want to go ahead and bring in something called compile uh keyword rs here which i'll just go ahead and copy and paste but the idea is compile keyword ars the literal binds being true i'll show you why right now so if we print out the compiled query and then if we print out the string of the actual query we see a couple different things so i'm going to go ahead and print this one little line to separate a little bit so the compiled query query is this one right here the other query has a parameter in here that does not have the literal binds in here at all so the nice thing about this is we would be able to change the limit with that parameter uh this is not something we're going to do right now the point is i want to be able to use this compiled query by going into popsql i open up a new tab in here and i can paste that query in and just run that and so what it'll do is it'll run everything that we have in here with all of that data so we've got the pricing data and all the pages and all that so notice that the pricing data looks like we when we import put it in it didn't have a initial slash in there which is kind of interesting uh but overall the data is in there and there's a lot of data going on and this is how we'll be able to verify that our queries are good and so we can,358,81,81,tiBeLLv5GJo
186,continue to refine what kind of queries we'll be end up doing inside of our database now when you're working with database technology it's a really good idea to do some of these queries just to verify that it's working before you were to actually put it somewhere else so now that we've got that i want to jump over into doing time buckets we're now going to take a look at time buckets time buckets allow us to aggregate data over a time interval so it's actually a really nice way to select certain data that falls into an interval in our case we're going to end up counting the correct data for the correct pages but we want to start with just the basics of the actual time bucket itself so one of the things i want to do is grab this session here because we will be using the session itself and we're going to be working based off of roughly this and so the nice thing about these queries here is we can put it into a parenthesis and of course this is just python related but then we can sep create them out line by line now i don't really care about the limit right now so i'm going to go ahead and get rid of that and i'm not actually going to run this query we are going to chunk it down a little bit by using the time buckets so to do this we're going to go ahead and do from time scale db.,339,81,81,tiBeLLv5GJo
187,time now this is where keeping the actual field the time field consistent makes it a little bit easier so if i need to change to a different model time is still that time so that's kind of the idea here so we can actually select it based off of that bucket itself we can come in here and just select that bucket we can also add in our page in there as well so if i do event model and then page i can bring that one as well that of course is an arbitrary field at this point but now what we can do is actually see this so i will leave that compile query because i might look at it in a little bit but for now i'll go ahead and do results equals to session and then execute of the original query and then we'll go ahead and do fetch all to see what that looks like and i'll use prettyprint to actually do this so let's go ahead and import prettyprint so from prettyprint or pr print import pr print okay so i run that and now i can see all of this data coming through based off of time and all that so there's a lot of data i'm actually not sure how much data is in there it's not quite what i want though right so i want to change this a little bit and of course we don't order it by updated at that's not even being selected we've got our time bucket and then our page those are the only things that are being selected and if,358,83,83,tiBeLLv5GJo
188,we scroll down we will see that there is all instances of that so like it's going to have duplicates in here but it actually removed the original time to just being based off of the date right it's not quite the same as what you would see up here which of course we could verify by running the results up there as well so if i came up here and ran those results i should be able to see that there are uh differences in the time itself so the datetime object in here is certainly different than this so already showing the aggregations at work is kind of the point i'm going to go ahead and move these import statements up a little bit uh but now that we've got that out of the way we've got a way to gate this data based off of buckets we want to do one more thing and that is i want to actually count the number of items in that time bucket or based off of that time bucket so the way we do that is we bring in from sql alchemy we're going to bring in a function in here and so this function right next to the page is just going to be function.,280,83,83,tiBeLLv5GJo
189,count and that's it we can give it a label as well if we wanted to we can say something like event count in here and then we could reuse that label later but i'm just going to leave it as simple as possible by using function count if i go ahead and run this we get this other issue now so this is now not necessarily giving us all of the right data so i'm going to go ahead and get rid of this order by here altogether we'll just use the select for now still not giving us exactly what we want now this is happening in part because we need to group the data somehow so how is this count going to work what is it actually counting so the way we do that then is we come in here and do group by and the way we want to group this is with the time and the page so those two things together will give us the grouping that we're trying to count so i can go ahead and run this again now it's showing me the grouping that we're trying to count this is now looking pretty good i think so it's giving us all of this data in here and what if we change this to 1 hour we should be able to see something very similar to that maybe we didn't change very much in an hour let's change it to one minute now we can see those chunkin coming through uh because of all that data that's been coming through so if i were to run and send,358,84,84,tiBeLLv5GJo
190,more data in it's been more than a minute um i should be able to see a bunch of data coming through with these chunks as well we could always take this a little bit further as well but overall what we've got is now a time bucket chunking and we probably also want to maybe order it so we'll do order bu and in this case i'll go ahead and say bucket and then we will maybe also do the page itself so the page model is being ordered and then uh or maybe not even in that ordering we can play around with these things a little bit oh we forgot a comma let's run that again there we go and so now it's in order right and we see and of course it will skip minutes right so if there's no data in there it's not going to have any chunks so it's going to be skipping things which is what we're seeing here now there is a way to do something called a gap fill which is definitely a function that the time scale db package has uh so instead of using time bucket there's time bucket gap fill which allows you to fill in empty spots that are missing that might be missing uh by aggregating the data in my case i don't actually want any empty data coming in here i'm just going to leave it as is because these are the actual data points and the actual counts i'm not going to guess what the counts might end up being now of course the actual analytics for a page,358,84,84,tiBeLLv5GJo
191,would be a little bit different than this if we were actually grabbing web traffic right it's not going to be consistently skipping minutes based off of the ingested data for all of the data it'll just be for some of it okay so another thing about this is we can then narrow this down to the pages we want in here so for example if we go back into when we created this data i'm going to scroll up here i've got a few pages in here so i can go ahead and grab that and we can grab those pages and put them right in here as to like narrowing down the results that i want to have and the way we do that then is by adding a wear clause in here so we'll do uh dot where and inside of here we can then say that this page and then do in with an underscore one of these pages all right so if we run that now it's going to go based off of those pages and so if i were to do let's just say the about page here those are what's happening on that about page over that time over that time bucket so if we do one day not going to be that big of a deal right now but of course over time hopefully we'll see those days come through and it'll end up working based off of this data so it's aggregating i think really well and it's doing so with all the chunks that are available and all that so it's really really nice and we,358,84,84,tiBeLLv5GJo
192,have the ability to do all sorts of advanced querying in here the other part about this is our wear claw we could do it based off of date time as well so if we were to bring in additional imports like from date time we can import date time and time delta and then time zone right and so what we can say something like this where we say start equals to date time.,97,84,84,tiBeLLv5GJo
193,time is less than or equal to finish now that actually filters it down a little bit more but the time bucket is already doing some of that for us so if we were to go into the day and let's say for instance it's doing a time bucket of the day but we actually narrow it down a little bit it's probably going to give us different results so let's see here we've got day the about page is 7500 but that's happening within the last hour for today basically if i were to get rid of those and run it again it's quite a bit more right so it is a the ability for us to filter down if we wanted to this i think is maybe potentially redundant to doing the time bucket because the time bucket already does that really effectively but you will see this from time to time especially when you go into the time bucket gap fill then you might want to fill in the gaps of a specific interval of time now the thing about this bucket here is you could still do a lot of this stuff in standard uh postgres itself so these sql queries can get a lot more complex of course um and then what we have here it just so happens that you know hypertable and more specifically time scale has some of these functions built into it that allow for it to happen efficiently so this kind of aggregation is very efficient and then we can take that compiled query let's go ahead and get rid of the printed results here let's,358,89,89,tiBeLLv5GJo
194,take that compiled query from the bottom which would be pretty big so i'm going to go ahead and actually start from the top and hopefully grab that whole thing and we will bring it into popsql here and we'll go ahead and run that new query which let's make sure that it does finish with page at the end there it looks like it does then we'll go ahead and run this and now it does basically that exact same query but we can just test it right inside of sql itself uh which is exactly why we have that compiled query there in the first place um but of course you might not actually run through the sql stuff very often you might just be running it with python but the underlying sql is definitely still there and it works essentially the same way as sql model does here so that's pretty cool it's a very straightforward way to do these aggregations and be able to start analyzing our data so all we really need to do is turn this into an api route that will allow for us to do basically this stuff so let's take a look at how we do that now it's time to bring in that time bucket querying and aggregating into fast api the way i'm going to do this is i'm going to actually remove our old list view our old events list view to being specifically just aggregations now when we start getting a lot of data it's really unlikely that i'll use the list view very often if at all to see individual items i,358,89,89,tiBeLLv5GJo
195,could always bring it back later if i wanted to but realistically i want to see aggregations here this is going to be that lookup so the way this is going to work is by going into our model here we can grab all of the things related to the query i don't need the compiled query and i will use results in here in just a little bit but the idea is bringing in this query and getting rid of what we had before so i'll go ahead and paste this in this of course means that i'll need to import several things which of course i already have some of the imports on here so i'll go ahead and bring those in as well as well and then we need the time scale db import as well so let's go ahead and do that too okay great so that should give us our new query in here and then the result here i'm going to go ahead and do fetch all and that is actually what i want to return now is just simply those results so that of course means that i need to update my response model so i'm going to leave some of the default arguments in here to start but i do want to change the response model now the response model itself is going to be very uh much based off of what we already saw in here so if we were to run this again we can see that it's a datetime object the page and then some sort of count in here so that's what i want,358,89,89,tiBeLLv5GJo
196,to have as well but i want to be a little bit more specific about that before i create the event list schema or the re response model together so that means inside a select i want to add a label to each one of these this label we can go ahead and call it bucket this one is going to just simply going to be page and then this one right here is going to be labeled as count what do you know basically the same thing as what they are but just adding this label on here which is a characteristic of sql model uh will make it really easy to then have a response model of some kind now let's remember what's happening here this query is going to be searching more than one it's always going to return back a list which means that i also want to bring in from typing we're going to import list here and we're going to go ahead and use a list of some kind so back into models.py let's go ahead and actually create our new schema which is going to be loosely based off of this event schema not quite the same but count will be the same then we want to have page in here which is a string itself and then we're going to have a bucket in here which is a datetime object right and so we already have that imported in here because of updated but the idea is this is now going to be our event uh we can call this our event bucket schema something like that and,358,89,89,tiBeLLv5GJo
197,then we'll go ahead and bring this back into our route we'll use that as our import now and that's what we're going to list back that's what's going to be our new response to this request so not a huge difference now one of the things we could think about too is maybe we change this query to being in its own model or its own field here like maybe something like services.,96,89,89,tiBeLLv5GJo
198,that's not something i'm going to do right now because i really just want to work on what we got here now the other things i'm going to get rid of are this start and finish i don't necessarily need that i'm just going to go based off of the bucket itself um you totally could do the start and finish in there but using just the bucket will allow for me to do these as query parameters which we'll do very soon but now that we've got this api endpoint let's just make sure that our server is running uh so we'll go up here and it looks like we need to maybe save models up pi and there's our oh we got a little little error there let me just resave that there we go so it looks like everything's running both in python as well as docker great so now i'm going to go ahead and make an api call which of course we already have that endpoint in there with list data types i can go ahead and run each one of these i might need to restart the server here or restart the notebook itself um but there's my results right there right and of course these are off of one day so i want to be able to change this i want to be able to come into my request and do pams and say something like duration and we'll go ahead and change that to 1 minute right i want that to be a url parameter that i can change and maybe in here i also want to go,358,90,90,tiBeLLv5GJo
199,ahead and say pages and add in something like slab and let's see another one maybe contact right and so there we go great so those are the parameters i want to bring in now we can do this fairly straightforwardly inside of fast api which uses a query parameter so the query parameter we can bring in as page and this is going to be an st str or rather not page but we called it duration and this is going to be a string of some kind which basically will be a default of one day okay so that query we need to bring in to from fast api we'll go ahead and import query in here and so having a default in there is nice because then i can use that as my parameter here let's make sure we put a comma great okay so the next one is going to be our pages this is going to be a list um but the query itself we'll go ahead and have a default of none or maybe just a default of an empty list let's just try with none for now um and we'll leave it as is okay so this is going to be our default pages now the pages itself i'm going to go ahead and call these lookup pages and i'll have some defaults already in here we could always say something along the lines of our default lookup pages and set that up here so we can kind of readjust that as we see fit but we'll have some default lookup pages in here that will basically be this,358,90,90,tiBeLLv5GJo
200,right here and we'll go ahead and use those inside of our query here for that lookup great so the idea then is inside of here is if there are pages coming with the query we would say uh pages basically and we would then want to check if is instance of a list and pages is greater or the length of pages is greater than than zero else we will go ahead and use this so a nice oneliner for the condition basically if all of these conditions are met it's going to use those pages otherwise it's going to use the default ones and then that's now our lookup and now our api endpoint with that time bucket and all of those aggregations let's just verify that this is working by going back into our notebook and there we go so we've got it at 1 minute here's that one minute we could also verify this by taking a look at the time that is happening obviously if you had a lot more data it'd be even more clear uh then we can also say something like one week right so we can change it as we see fit and it shows only those items in here if i were to use an empty list here it's going to then go ahead and use all of them that i have available in my default ones now in my default ones i'm actually missing one which is when i was playing around myself i didn't actually put a uh leading slash on pricing as well so i did it in two ways so when you,358,90,90,tiBeLLv5GJo
201,actually end up doing this you might want to have some default pages that are in there uh but overall once i actually add that other one and we can see that that one has a lot of data as well now this distribution is probably unlikely i doubt about page would you know be far greater than anything else uh but the idea here or even the contact page being that high right so the analytics here is the data points are actually only because when we created it it just did some pseudo random creation the actual data once you put this into action would look quite a bit different okay so pretty cool now we've got a actual api point to get the aggregations from our analytics we have a way to create the data we have a way to aggregate the data both things are what's critical here and what we will be using once we actually put this thing into a deployed production environment we now want to augment our event model so it's a a little bit closer to extracting real web data so like for example you want to know what web browser people are using to access these pages or you want to know how long they spent on those pages now that is more realistic web analytic data now the point of this is to show you how you can add additional fields and what you would need to do to be able to use the time bucket equation to make sure that you can also include those fields and see how those might play out so,358,90,90,tiBeLLv5GJo
202,the idea here then is inside of our event model we're going to change it now before i make any changes here i'm going to go ahead and run the docker compos down- v to make sure that all of the data has been removed now the reason for this is because i don't have database migrations in place to make these changes you could have database migrations in place i don't so we're going to go ahead and remove some of the fields that are in here by default i'm going to go ahead and get rid of the comments even for them because we no longer need those but i want to add in additional fields the ones i'm going to add in are these right here we've got our user agent otherwise known as like the browser the web browser that's being used their ip address to identify their machine itself um the refer like you know if it's coming from google or from its own website the session id which you might have in there as well and then how long they spend now of course you could always have more data than just this as well so what we're not going to do though is change this data we're just going to ingest this raw data and we will be able to change it later that's more specific to the user agent so that you can see the specific like operating system that's being used as we'll see in just a moment so now that we've got this event model i also want to get rid of some of the schemas,358,90,90,tiBeLLv5GJo
203,that are going to be a little bit different or at least change them so the create schema is no longer going to have this description in here it's just going to have these things the update schema i'm not going to allow updates any longer so i'll go back into my routing and i will get rid of all of the things related to updating any of these events seeing how to update things was important but now we no longer need them so go ahead and get rid of that as well and of course you could always review the old commits to see all of that old data if for some reason you wanted to go back and see it so at this time we don't actually have that much different in terms of our data itself so let's go ahead and bring back docker i'm going to go ahead and do docker compose up watch and then i want to make sure that my models everything's saved and i should see something along the lines of we've got a database in here great so these actual fields themselves maybe you want to change them in the future maybe these aren't actually the ones you'll end up using that's not really the point here the point here is how do we add fields to the data we want to collect and then how do we look at that in terms of our events themselves so this actually means that we need to modify the data we're going to send so inside of our data here i'm going to go ahead and do a,358,90,90,tiBeLLv5GJo
204,"pip install faker faker is a python package that allows for us to have some fake data and i'm going to go ahead and grab some of the data that i have which is going to be just this import here i have additional pages in here now i have 10,000 events that i want to bring in faker can allow for you know fake session ids in here we can have however many that we want in this case i'm just going to use 20 but if you want to use 200 feel free to do it we still have the same ap in points i added a few refers in here and so we can actually start the process of building out this data so what we are going to do here is very similar to what we have up here where we're going through a range of events and we're going to get a random page which we have right there then we're also going to go ahead and get a random user agent which we can use faker for so faker has all of these user agents in here that you can go off of right so just make sure that you have all of that fake stuff implemented then we're going to go ahead and just create a payload based off of this data so the payload is going to be well simply these key value pairs in here it's really just a dictionary but i like to looking at it this way uh just to make sure that it's all working great okay so we've got a duration maybe we",358,90,90,tiBeLLv5GJo
205,do 1 to 300 there's a lot of things that we could do in that realm but the point here is we now have a new payload that i can send back into my database or into my actual api service with all of that data so i'm going to go ahead and run it and there it goes so we can see all of the different data items that are coming through in here uh we see the user agent and all that okay so while that's running i'm going to jump into my queries from the sql model now actually what i want to see is the list data i'm going to go into the api request themselves and we'll just go ahead and run this within that api request there we go we actually see the same sort of data coming back so if i say something like five minutes we should see a lot of that same data coming back as well now the pages are going to be a little bit different because of my original routing how i had it set up so let's go ahead and grab these new pages in here as our new default pages so go ahead and copy this and then we'll go jump into our routing and our new defaults are going to be those right there great now of course you don't necessarily have to even narrow down the defaults you could remove that al together which would allow for all of the pages to show up okay so with that in mind now that we've got that there we're going to go,358,90,90,tiBeLLv5GJo
206,ahead and run this once again in that list data types let's go ahead and run it and there we go so now we have a bunch of different pages in here and we can see the contact and all that stuff okay so now what we want to do is we want to see this a little bit different so going back into the routing we want to add a new field in here and we want to count the number of instances for that field in other words most of this is basically the same so let's go ahead and do that with let's say for instance our user agent so if we came in here and we did event.,158,90,90,tiBeLLv5GJo
207,user agent and then we added a label as something like ua like user agent we could do something along these lines where we're now selecting that data if we go to group that data we totally can which would just be adding in that user agent and then we can order it by that data as well depending on how we see fit our result results now will be slightly different than before so we have to remember the way we are returning our data is going to play in right now so event bucket schema we need to make sure that we are using the label of ua make sure that that's in there as well so the event schema here we'll go ahead and bring it in as simply ua this might have a or not ui but ua this might have an optional value in here as well uh which you just set to an empty string maybe uh but we'll go ahead and save that we'll save this and then we'll go back into our list data here and we'll run it again and now what we see is these things being sort of collected together and right now it's only showing one count for some of these different user agents so realistically this isn't that great because it's not really parsing the data the way i want to because there's different versions and stuff well if you want to get super granular this is going to be great but we don't want to get that granular that's maybe too granular for us so what we want to do is take,358,91,91,tiBeLLv5GJo
208,it a little little further and we're going to change how we actually return back this data so back into our routing here what we can do is we can import another sql alchemy function here so i'll come in and bring in something called case and we can add in something called a operating system case so inside of my read events now i can come in here and say something like this where we've got our event model user agent and then it basically says hey if it's one of these things it's going to set it to something else and then that's what it's going to end up being otherwise it's going to just mark it as other this now i can use as my user agent in here the label i'll leave as operating system so we'll go ahead and paste this in here then we will paste it here and here so basically taking place of our original user agent although it's still the user agent once again i made some changes to the query which will affect the results which means that i need to update the schema itself in which case i'll go ahead and copy the ua this time i'm going to go ahead and add in the operating system in here same sort of idea still optional let's go ahead and leave that as is operating system is the label which is why i named it that way we have all these other labels for that same exact reason so there we go we save it let's go back into our list view here do a quick,358,91,91,tiBeLLv5GJo
209,search and now we've got a much better look at what this data could be we've got android at the homepage android at the about page how many times when all of that and we can keep going through and really see very very robust data so this concept here we could take even further right so we could use something like os case to unpack different ip addresses so we can see the different parts of the world we can do all sorts of that within the query now i'm not going to go through those advanced queries in this one but that is absolutely something you' be able to do at this point let's go ahead and take a look at how we might do the average duration so i'm going to go ahead and copy the operating system thing here and go ahead and do avg duration this is going to be optional float and then we'll go ahead and add in 0.00 or something like that where it's an average duration value now back into our routing here what we can do to add that average duration is just put it into our select what you do that is funk.,265,91,91,tiBeLLv5GJo
210,"avg then you grab the average of what that value would end up being which is the event model duration and then you set a label to it and it's average duration and then there we go so now we have enriched this data with the average duration everything else doesn't need a change because how we're grouping it together is going to be based off of that operating system and that page and the time not the average duration that wouldn't actually make any sense to group it by the aver duration in this place okay so now that we've got that let's go ahead and take a look inside of our list here if i do a quick search here um what i see is that average duration is coming through on each one of these and what we should be able to do is actually modify that to let's just modify how we were sending this data in the first place by going back into send data here we're going to change the average duration up by a lot i'll go ahead and say 50 to uh 5,000 seconds which is definitely a much different look at the durations themselves which should give us a different response back to the actual data that we're getting depending on our our you know duration parameter here so if i said every 15 minutes for our duration we run that our average durations will hopefully change at least somewhat now of course there would be a bunch of ways on how we can modify this but here's another average duration that just popped up a lot",358,92,92,tiBeLLv5GJo
211,that's because of that change that we just did um there was a chance that that wouldn't have happened but it's overall really nice that we're able to completely change how our analytics works and we can do it in a very short amount of time now this is where spending a bunch of time to build out more robust queries might be really useful we now have the foundation in place for a very powerful analytics api where we control all of the data that's coming in and ignore the data we don't want then we can also analyze this data with our very own queries now this data itself the actual sql data that's coming through the table design probably could be done in postres by itself but the real question here is how efficient or how effective will it be as your data set grows a lot and the answer is it won't be the nice thing about time scale is you can just add it whenever you need to so if you want to start with just a sql model you totally can do that now of course the package that we went through won't necessarily automatically do that just yet maybe at some point it will but right now it doesn't do that you kind of have to decide this from the get-go in terms of this particular package but in the long run in terms of time scale you can add this at any time and there's a lot of options for that which is really nice and it gives us a lot of flexibility and the fact that it's,358,92,92,tiBeLLv5GJo
212,open source means that we can just activate it in our postgress database and we're off to the races but what we want to see now is we want to take this to the next level which is actually deploying it so we can see it in production what it might actually function like on our own systems let's take a look at how to do that in the next section we're now going to deploy our application into production using time scale cloud and railway now time scale cloud itself will allow for us to use time scale but not worry about running time scale it's a managed version so all of the performance gains the new releases the bug fixes all of that stuff is going to be in the managed version so we don't have to worry about it at all now do keep in mind that time scale is based on postgres so it's still just a postgres database so yeah we could still use the open source version of time scale if we want but it will be a lot more simple if we just go and use time scale cloud directly so go to this link right here so i get credit for for it and they know that we should make some more videos covering all of these things then we're going to be deploying our containerized application into railway now we've already seen how to build out the containerized application so this process is going to be fairly straightforward as well so we'll be able to integrate the two of them and just have it run and we'll,358,92,92,tiBeLLv5GJo
213,test all of that in this section let's jump in before we go into production i'm going to go ahead and add in some kors middleware this is cross origin resource sharing kors allows us to prevent certain websites from accessing our api this also means certain http methods as well now we're actually going to open up the floodgates on it mostly because we can actually turn this app into a private networking resource in other words the other apps that would access it need to be inside of that same network that is deployed similar to like you can't access my version on my computer because it's in its own private network that you are not a part of that's kind of the same idea when we go into production so for that reason i'm going to go ahead and open up my course here so we'll go ahead and do from fast api.,202,92,92,tiBeLLv5GJo
214,add middleware and it's going to be our cores middleware like this and of course you can go into the fast api documentation and get all of the different arguments you might put in here in our casee we're going to allow all origins all methods all headers we're letting everything come through but you know if you were going to expose this to the outside world you might want to lock down the origins to like your actual website domain you might only want to allow get and post methods in here maybe not even delete right so that's something else that you can think about going forward but this is one part that i wanted to make sure we had before we went into full production so now what i'm going to do is i'm going to push this into github so what we've got here is in my terminal i've got git status i can actually see all of the files that i've changed now i'm actually not going to show you this process mostly because i also have git remote- v i actually have the entire code that you'll be able to use in the next part we'll actually use this code directly make a few changes so we can do the deployment directly i won't do anything else inside of this project at this time other than just adding those cores and that will be on github just a moment now what we'll do is take it from github to deploy it at this time go ahead and log into github or create an account if you don't already have one,358,94,94,tiBeLLv5GJo
215,this account that i'm using is really just for these tutorials then you're going to want to go to cfsh github this will take you to the coding for entrepreneurs profile in which case you'll go into the repositories here and you'll look for the analytics api repository of course do a quick search for it if you need to it's it's going to be this one right here now the point of this is really to just go ahead and fork this into your own project here so you go ahead and create that fork and it's going to bring the code on over now what we need to do is we need to add some additional configuration to our now forked project now that additional configuration has everything to do with deploying fast api so if you actually go to fastapi container.com it's going to bring you to this in which case will allow you to scroll on down and you can see the code directly that's going to be used here now this code is just boilerplate code to deploy fast api into railway that's it so in our case we really just want this railway.,258,94,94,tiBeLLv5GJo
216,json because it's something i've worked out to make sure it works really well for you now i'm going to go ahead and copy the contents of this which you can you know select all of it or just press this button right here then i'm going to go back into my repo the one we just forked from the coding for entrepreneurs profile then we're going to go ahead and hit add file we're going to create a new file here and i'm going to call this railway. json and i'm going to paste this on in here now before i commit the changes i will say that it would have been or it will be a good idea to have it on my local project as well i'm just sort of assuming that you are not using git or you haven't been this whole time maybe you don't know it yet but of course if you do know it you know what to do from here but the idea here is we need to adjust our railway settings to ensure that this thing actually gets deployed now what we see is this build command this build command is looking for a docker file that does not have a period it just says docker file so we need to use docker file.,289,95,96,tiBeLLv5GJo
217,json and i'm going to paste this on in here now before i commit the changes i will say that it would have been or it will be a good idea to have it on my local project as well i'm just sort of assuming that you are not using git or you haven't been this whole time maybe you don't know it yet but of course if you do know it you know what to do from here but the idea here is we need to adjust our railway settings to ensure that this thing actually gets deployed now what we see is this build command this build command is looking for a docker file that does not have a period it just says docker file so we need to use docker file.,174,96,96,tiBeLLv5GJo
218,web because that's our actual docker file path which of course is going to be the same for our watch patterns so we actually seen something like these watch patterns already composed igl has watch patterns what do you know now we actually didn't update our composed yl very well this one probably should say web as well but the point here is we want to actually rebuild this application based off of these patterns here mostly for the src and requirements those are the main ones of course but docker file is another one that's important as well then we also have this deploy stuff this deploy stuff is looking for that health check do you remember when i said we will have a health check well if we actually look in the main.py code and scroll on down here's that health check right there notice there's not a traing slash here but this one is looking for a trailing slash let's get rid of that tr trailing slash we want to make it the same as what's in our code so now that we've got this i can go ahead and commit these changes and we'll go ahead and say create railway.,265,97,97,tiBeLLv5GJo
219,json great our code is now ready for railway there's not really much else we need to do to deploy it so let's go ahead and actually do that going to railway.com feel free to sign up for a free account jump into the dashboard notice that i'm on the hobby account this is a key part of this we're going to go ahead and jump on over into our account settings and we want to make sure that our account integrations are connected to github so go through that process if you haven't what you'll end up seeing is something similar to this when you go through it and then it's going to say hey what repositories do you want you could say all of them or you could select the one that you just forked which is what i'm going to do obviously i have two in here but i definitely want to have the one i just forked so i'll go ahead and save that in here so that now i can go back into railway and i can deploy this so let's go ahead and do new now and notice that the analytics api is there it's ready to go it's ready for me to deploy it which is really cool it's a very straightforward process in terms of the deployment but the key here is we want to look at our settings inside of these settings we should be able to scroll on down and we should see the builder that it says build with a docker file using buildkit mostly that it says docker file.,352,98,98,tiBeLLv5GJo
220,json in here is done incorrectly which might mean that that isn't correct that's kind of the idea beyond that we actually probably won't need to change anything much notice that it has watch paths in there as well we should not have a start custom start command that's not necessary one of the things we might need to change is the actual region but i'll stick with this region for right now and then if you scroll on down notice the health checks in there the timeouts in there all of this stuff is in here things are looking pretty good okay great so what it actually does is it automatically starts deploying this might deploy it might not i actually think it would fail mostly because we don't actually have our database yet so let's go ahead and start spinning up our database and let's combine the two in here assuming that you've already signed up for time scale cloud you'll log in and you'll see something like this we're going to go ahead and create our first database service right now so the idea of course is we're using postgres so we can just go ahead and hit save and continue now we can select what region we want to use now the region you end up using will likely be close to you physically or close to where the most of your users are so if you're doing this for somewhere else you're going to want to put it in that region itself now us west oregon is the closest to me physically but it's also the closest to where i actually,358,100,100,tiBeLLv5GJo
221,have my app being deployed on railway so if i come in here into my settings i can see the deployment is in california now there is one for organ in here as well so i could always change that too now that deployment is going to fail as well but i'm going to go ahead and leave it in as organ because inside of time scale i have the ability to use organ as well so now i'm going to go ahead and hit save and continue how much data i'm going to be using is practically none because we're still early days we're going to use devel velopment here and then we're going to go ahead and hit save and continue and then we're going to create this service and i'm just going to call the service name analytics api and then we're going to go ahead and hit create service now we get a free trial here for 30 days which after that free trial it's still very affordable in terms of the ability what we can do in here but notice we get a new connection string so i'm going to go ahead and copy this connection string and i'm going to bring it locally first so in myv down here i'm going to go ahead and say cra db url and i'm going to paste that in now the reason i did this is twofold number one if i ever want to test my production url locally i totally can now this also means that if you skipped using docker compose this is the route you could do but then you,358,100,100,tiBeLLv5GJo
222,would also use something like this where you actually change the connection string just slightly so realistically you would use something like that you would actually comment out this old one and then use this new one now my case i'm going to leave it as is and i'm going to go ahead and copy the entire new string with it commented out now i'll go ahead and jump back into railway into variables here and i can do this raw editor and just paste in those key value pairs right that's it and then i'm also going to go ahead and update those variables as soon as i do that it's going to attempt to deploy which i'll go ahead and let it do now one other thing that's important to note is how this is being deployed how it's going to be run so if we go back into our pro project a long time ago we created this docker run file which has a run port and a host so this run port is going to either be set by us or by railway so we could set this port in here so inside of our variables here we could come in and do a new variable and we can set it to whatever we want i'm going to use 880 which i believe is the default on railway but i'm going to go ahead and set that port in there as well and then we'll go ahead and deploy that one as well so one of the nice things about railway is the ability to really quickly change your environment variables and,358,100,100,tiBeLLv5GJo
223,then it will go ahead and build those containers for us and then run them for us very very similar to what we were doing with docker compose in that sense now we also have this run host here now at some point in the future we might end up using this a little bit differently so at some point if you want a private access one you might need to put your run host the actual host itself inside of a variable as this value right here so you would do something along the lines of run host equ to that in which case i will go ahead and put this into my sample.,149,100,100,tiBeLLv5GJo
224,emv u you know compos file here just so we have it for later but that might be something if you don't want to expose this to the outside world okay so after i did all those changes in a matter of minutes it was able to deploy the application itself and we can view the logs in deploy logs this will show us that it is running and notice the port in there says 8080 we also have our build l logs in here we could go through this this is building the container for us it happened really quickly and then it was did the health check which also happen really quickly and then of course it finally deployed so now what we can do is since it is fully deployed we need to of course access this you know api itself so inside of our settings here we go into networking we generate a custom domain now sometimes it might ask you what port you want to use sometimes it might not in this case it just defaulted to the environment variable port that we have we might even be able to edit that looks like we can right in there which is really nice as well so the one thing that i want to check though is before i go any further is making sure that the deployment is in that region that i wanted and sure enough it is great so going back into our networking this is our api now this is the actual url that we can use and here's that hello world and of course if i go,358,101,101,tiBeLLv5GJo
225,into sl api events the actual inpoint itself we should have no events which is very very straightforward so if if you are a casual viewer you will notice now this is a production deployment it's fully production and it's also using of course time scale if we go back into our time scale service here and go to the service overview we should see that it has been created right so we've got service information in here um we can see all of the things related to it by doing our explorer in here we can see that there is a hypertable and what do you know there's that event model all of that is working really well now of course we could always open up popsql and bring in that cloud-based version as well which of course is one of the things it's built for and you can always test things out there you could also do these all of this stuff locally as well like we discussed okay so what we really need to do though is we need to test this endpoint the actual production endpoint and see if it's working and how well it's working let's take a look let's go ahead and send some test data to our production endpoint so what i'm going to do here is i'm going to copy the actual url that i got which of course came directly from railway it's this url here of course you could always have a custom domain as well but i'm going to go ahead and use the one that they gave us and then i'm going to jump,358,101,101,tiBeLLv5GJo
226,"into my local notebook here where it says send data to the api and i'm just going to go ahead and change our base url to that exact value now i don't actually want the trailing stuff in here i just want to make sure that i'm using htps and then the actual endpoint that it gave us always use htps if you can so now that i've got this i should be able to send out a bunch of data this one right here is not actually valid anymore we want to use the new data that we have uh based off of the new schemas that we set up and all the new models so here it is right here i'm going to go ahead and run it it is 10,000 events so in theory it should be able to send all of that data just fine it looks like it's working fine now if i go into the production inpoint itself refresh an the air there's some of that data it is now working and it's working in a way that hopefully you have expected now this of course is now a production endpoint it is fully ready you are ready to deploy this all over the world if you want to and have it in all of your applications and just go wild with analytics but there's still one more thing that i want to show you and that is how to use this privately now of course we could always sit here and wait for all of this to finish and have all of that data to come through i'll let",358,101,101,tiBeLLv5GJo
227,you play around with that at this point but let's actually take a look at how we could deploy the analytics api as a private service so we can still use it this way but just not expose it to the outside world now it probably comes as no surprise that you don't want to have your analytics api open to the outside world because then you might get events that aren't accurate they're not real so of course we need to change that we need to turn our analytics api into a private networking service only so you can have private networking in a lot of cloud services what we're doing here is we're really just removing the ability to have public networking which in the case of railway we can just delete the actual endpoint itself and now there's no public networking whatsoever but there is private networking and this is something that's done by default in the case of railway you have to use ipv6 which means that we need to update how we access this now we could spend a lot of time on fast api itself to harden the system add security layers to allow only certain connections but as soon as we turn it into a private networking thing it just adds a whole layer of security right there so we don't have to necessarily add all this additional stuff into our application so what we need to do though is we need to modify our docker run command here and that's going to be by changing the host i think i mentioned before that it was the run host,358,101,101,tiBeLLv5GJo
228,but it's actually the host in here that we need to change and we need to change it to being this right here so that's what we're going to do now is we're going to do host equals to that as our environment variable and it has to be these two brack here this is how gunicorn is able to bind to ipv6 so ipv6 is that by default it's ipv4 which is what this is right here so let's go ahead and grab this and we're going to go ahead and bring it into our analytics api as a variable we're going to go ahead and bring it in here just like this go ahead and update it and deploy it now there's a chance that this won't work the reason that it might not work has to do with how this string is here it's possible that this needed string substitution which we'll see in just a little bit it's also possible that it work just fine so once again we will see that in a little bit as well now it's one thing to make it private and it's another thing to make it private and still being able to access it so the thing about railway that makes it a little bit simpler is if we go into the settings on our application we can scroll on down to private networking and here is the new api endpoint that we can use inside of our railway deployments the big question is going to be how do we access that without building out a whole another application i'm going to show you that,358,101,101,tiBeLLv5GJo
229,in a moment so what we see on our api though is that it looks like it's being deployed and it looks like we're in good shape let's look at our logs here more specifically our deploy logs it looks like it's listening at that actual location so that's actually really good everything else isn't arrowing out we do see those print statements that's a good sign so now we actually want to deploy something that will allow for us to test out these communications so i actually created a tool called jupiter container.com which will take you to this right here which allows you to have a jupyter notebook in a server right inside of railway in your own environment so the way we deploy this is by going back into railway hit create into our project here so the important part of this is you are in the exact same project that you've been working on notice that i have a couple deleted in here this one i want to go in and i want to use it right next to the analytics api i do not want to deploy a new project which may happen if you just go to jupiter container and deploy it directly from there so what we want to do then is come back into our application here we're going to go ahead and create go into template look for jupiter that's with a py and jupiter container you could probably use jupyter lab as well jupyter container works because i know it works that's pretty much it um okay so the next thing is we want to add,358,101,101,tiBeLLv5GJo
230,in a password here i'm just going to do abc123 that's going to be a password this is going to be publicly accessible as we'll see in a moment i'm going to go ahead and deploy this which might take a moment as well so it's going to also have a volume on here for persistent storage which is actually kind of nice when it comes to wanting to use things okay so the next thing here is though going to be our variables now what i can do is i can add a variable to my jupiter container that references this analytics api variable so let's go ahead and do that i'm going to go ahead and hit new variable here and i'll go ahead and say um let's go ahead and call it analytics endpoint something like that and then this one i'm going to use dollar sign two curly brackets and i'll type out analytics api and these are going to be the objects that we have the op and that's going to be specifically the rail ray private domain here so that's the one that i want to have access to inside of this container which kind of connects them together so i'm going to go ahead and add that and we'll go ahead and deploy it now these deployments do take a little bit of time because it is building a container and then it's deploying the image based off of what whatever is going on but one of the main things here is as soon as i create that variable a line is created here showing that they are connected,358,101,101,tiBeLLv5GJo
231,in terms of railway this is important because the actual jupiter container itself should have a public inpoint as we see right here so it already has one as far as the template is concerned without us doing anything else so that's really my goal with this analytics api is to have it so easy that we can deploy it pretty much anywhere we need to as long as we have the necessary configuration across the board which of course would include our time scale db in there as well so now what we want to do is just wait for this to finish so that we can log in to our service once it's up and ready which we'll come back to okay so the jupiter container finished after a couple minutes let's go ahead and jump into the actual url for it there it is i did abc123 to log in of course if you forgot what that value was you can always go into your variable here and just look for this jupiter password this container is meant to be destroyed almost as soon as you open it up so you can delete it at any time as the point so going back into that jupyter notebook here what we can do is we can jump into the volume and i can create a brand new notebook in here and we're going to call this you know connect to analytics or something like that i'll just call it connect to api now i'm going to go ahead and import os here if you're not familiar with jupyter notebooks well you probably are now,358,101,101,tiBeLLv5GJo
232,all of these nbs here these are all jupiter notebooks they're just running inside of our cursor application so what i can do is i can use os.,36,101,101,tiBeLLv5GJo
233,environ doget and i should be able to print out the environment variable that we used for our analytics api endpoint so i'm going go ahead and do that and there it is right there notice there's no htttp on there so that's an important part of this as well so now what i want to do is import something called is basically like python request but it allows us to call the ip v6 endpoints themselves so our actual inpoint our base url is going to be equal to http col slash this value right here which i'll go ahead and set up here like this and just do a little magic here not really magic but you know we'll go ahead and do some string substitution there's our base url let's jump back into our send data to the api thing in here which i'm going to go ahead and copy this whole thing and we're going to go ahead and bring it into our jupiter notebook paste it in like that now i don't have faker in here i don't think so i'm going to go ahead and install it so that installation is going to be a matter of pip install faker and that should actually install it onto the jupyter notebook itself um and then we should be able to run this now of course the base url here is now this one so i can go ahead and get rid of these two right here and i also want to use htps x not um using python requests so there's going to be htbx and i think the api otherwise,358,102,102,tiBeLLv5GJo
234,is basically the same let's go ahead and give it a quick little run and we get a error in here for that one no service known in here so that's a little bit of an issue that we will have to address before i do though i want to just take a look at the base url and what do you know i didn't do some string substitution so let's go ahead and make sure we do the string substitution as well one would so that we have the correct base url now i'm going to go ahead and run this and it still might fail now the reason it still might fail has to do with the port value we've got connection refused so when we actually deployed the public endpoint in here when we went through that process it asked for that port value so this is why i actually mentioned it in the first place is so that we can understand that we do need to know the port value now you could go through the same process as the analytics api endpoint i already know what the port value is off top of my head and it's 8080 so that's the one we are going to go ahead and use and we want to use that port right here this would be the same thing as if we were doing this locally as well and so now we should be able to see the same base url i'm hoping this actually solves the problem it's certainly possible that it still won't solve the problem uh but of course if we look,358,102,102,tiBeLLv5GJo
235,"into our deployment for this analytics api we should see that it is running at that specific point this is the endpoint right here so instead of using whatever this is right here right we are using the actual private ip address name basically which is this right here okay or a dns name rather okay so now that we've got that let's go ahead and see if it starts to work it looks like we've got no response of okay so doesn't do the same thing let's go ahead and actually just print out the data i'm going to go ahead and come back here and get rid of that and we'll just go ahead and run this there's that data coming back and it's quite literally working in a private networking setting pretty awesome if you ask me so the other part about this of course is to verify this data i'm going to go ahead and stop this we don't need to add in 10,000 things we can just do 10 for example we could also just g use the git command to grab that data as well so i'm going to go ahead and grab this response similar thing it's going to be the create endpoint still because it's the same endpoint to do.get and then we can get that data back and we can see what it looks like by just like that we'll get rid of some of these comments run that and there you go we now have that data coming through as well now of course if we were to change the params let's go ahead and change",358,102,102,tiBeLLv5GJo
236,that in the case of the duration i think it was we'll go ahead and say one day and that's all all change and now the data is going to be a bit different right and we can also change the pages let's go ahead and do that i'll come in here and say pages and we go ahead and put in maybe just the uh let's go ahead and do just the pricing page and we'll see what that looks like and now it's only doing the pricing data in there which is not nearly as many because well it's going through one day which is you're grabbing different operating systems over that day right so we saw that as well but the point here is we now have a private analytics api that is well deployed and leveraging a lot of cool technology to wrap up this section we are going to want to unload everything that we just did as in delete all of the deployed resources unless of course you intend to keep them in the case of time scale we're going to come in here and just grab delete service and then we're going to go ahead and write out delete go ahead and delete that service that of course will delete all of the data that went with it which of course is no big deal because we did a bunch of fake data we also want to jump into rail way itself and delete the entire project which we can do by coming in here whatever that project is you go into settings you go into danger and then,358,102,102,tiBeLLv5GJo
237,you can remove each one of these items by typing out their names this of course is just to make sure that you clean up all the resources you might not be using in the future so i'm going to go ahead and delete both of these and there we go and then i'll go ahead and delete the project which i think would also delete those other services but this process has changed a little bit since the last time i did it the idea here is we've got all of these things being deleted including the services we were just working on so at this time i won't be able to access that jupiter notebook at all any longer or our deployed production api endpoint but you now have the skills to be able to deploy it again and again and again because well the repo is now of course open source on github.com so feel free to go ahead and deploy your own analytics api and if you do change it and make it more robust than than what we have here i would love to check it out let me know hey thanks so much for watching hopefully you got a lot out of this now i will say that data analytics i think is going to get only more valuable as it's going to be harder and harder to compete so it's one of those things that i think i'm going to spend a lot more time in what i want to do next is really just kind of visualize the data we built i want to build out a dashboard,358,102,102,tiBeLLv5GJo
238,and i encourage you to attempt to do the same thing after i have that dash dashboard i really want to see how i can integrate it with an ai agent to see if i can communicate or have some chats about the analytics that's going on but before i can even make use of chats or a dashboard i probably want to use this somewhere for real so those things are actually better and will help me make decisions either way it's going to be an interesting technical problem and we'll probably learn a lot and i hope to show you some of this stuff in the near future so thanks to time scale for partnering with me on this one and thanks again for watching i hope to see you again take care music,176,102,102,tiBeLLv5GJo
0,"welcome back to a new machine learning tutorial. in this video, we are going to be talking about how we can create a machine learning api. we are going to use fast api for api creating and we are going to use scikitlearn for creating a quick machine learning model. i'm going to be showing you how we can export the machine learning model you trained and how you can create an api service for getting predictions from it. also, we are going to be talking about taking multiple predictions from the model in this video. this video is great if you are interested in taking your models outside of the jupyter notebooks and you need to learn about the apis if you want to use your models in different apps or on the production systems. let's start coding. okay, so let's start. at the first place, i'm going to create a jupyter notebook like let's say model. and in here, we are going to train our model, then export it, and then we are going to start creating our api. so i will select my kernel something works then i will say import numpy as mp import pandas as pd i will say from scikitlearn linear model i'm going to train a linear regression like this and also i'm going to import pickle because we are going to export our model now we are going to create a data set so let's say we are going to create a house price prediction problem and we need sizes in square feet. so i will say random random integer. i will pass 500. let's say 300 and 100.",360,0,13,dmUtafXB5aw
1,"let's say 300 and 100. so we are going to have something like this for the sizes. and also we need to give something like number of bedrooms. so i will say bedrooms numpy random again random integer and let's say one and six. we need hundreds of data samples. i will say bedrooms. we are going to have 100 data points like let me show you in here. it's going to be 100. and now we are going to create prices. so we also need to add some noise. so i'm also going to do that. let's say prices. and it's going to be sizes multiplied by 0.3 plus bedrooms multiplied by 50. and let's say numpy random and random integer. and in here we can just say like minus 200 200 and we need 100 data points. so we are going to have something like this at the prices right now. now what we are going to do is we are going to create a data frame like data. i will press pendas data frame in here. i will say like size square feet and it's going to be sizes and bedrooms is going to be bedrooms and price is going to be prices. so in here we can just say data head for seeing it. great. we have our data frame ready. okay. now let's split features and target. so x is going to be data. let's say size, square feet and also bedrooms and y is going to be data price and now we need to train the model.",341,13,38,dmUtafXB5aw
2,"let's say size, square feet and also bedrooms and y is going to be data price and now we need to train the model. so model is going to be a linear regression and after actually i have a typo after initializing the model what we are going to do is we will say model fit x and y so we have our linear regression model ready and let's quickly test the model like let's get predictions from it using model.predict predict method. so firstly let's create something like sample x and i'm going to give a two dimensional array like i'm going to say 2003 and for the next one, 1500 square ft two bedrooms. so after that let's say model.predict predict sample x and here we can see that we are getting our predictions from model and we can see the price data. great. now we completed our model training size. i kept this section as simple as possible because this is not the point of the video but you can use the same method for every model every type of data. and after this part, you can just copy with your model and it's going to work out for your model or data too. so let's save this model as pickle. remember that we imported pickle at the top. so i will say with open house price model.picle and i will say write a mode and as file pickle dump i will say model and f. so here at the directory that i'm running my code, you don't see anything as a pickle object.",347,38,49,dmUtafXB5aw
3,"so here at the directory that i'm running my code, you don't see anything as a pickle object. but if i make this run, you are going to see that we are going to have house price model.picle. so this is what we are going to be working with. so i'm going to just close this jupyter notebook. we are done with this and we have our pickle. now we can create our first api application. let's create something like main.py. and in here what we need is you can p install this first api u coin and after installing that we are going to be cool on this side. so let's create our api. at the first place we will say from fast api import fast api like this. and next we will say import pickle and we will say import numpy as np and we will say from pyantic import base model which we are going to use for defining the input data model. and at the first place we need to load the model in here. so i'm going to copy the relative path in here and i will say wait open and i will pass the relative path in here. next i will say mode and as file then model is going to be equal to the pickle.load and i will pass the model.",296,49,62,dmUtafXB5aw
4,next i will say mode and as file then model is going to be equal to the pickle.load and i will pass the model. next up what we are going to do is we are going to initialize the first api app like app is going to be first api and title is going to be let's say house price prediction api and next what we are going to do is we are going to just define the input data how it's going to come like we are going to say class and we will say like house data and we are going to initialize the base model from pentic here. and next what we are going to do is we will say size square feet is going to be float and bedrooms is going to be integers. so actually we have a typo in here so it needs to be uppercase. okay. so let's create the home section. so i will say app and it's going to be a get method. i will say like define home. this is the message that we are going to be showing when users are reaching to our api address.,256,62,70,dmUtafXB5aw
5,so i will say something like return and i will say message and welcome to the house price prediction api and if users reach to the api address and just request it to the home like the base section they are going to see this message and also how we can check this from ui is we can check this regular ui by we can open the terminal and we can say ubicorn music main app and we can pass reload for the developer mode like if you just host this with reload you are going to see that when you make a change on the api file it's going to be automatically updating the app that you have and also you can run this for the workers let's say two or four for the production modes but we are going to be developing this so i will say reload instead of this and when i make this on you are going to see that it says we'll watch for chains in these directories which comes from reload and it says uicorn running on this address this is our local host and now we can reach to this api from this address like i'm going to go to this address just copy it and paste it on my browser and show you how it's looking right now here you can see that we are on this address the local host and we can see our message at the default directory and i'm just going to go to the documents page like where we can see our endpoints and api documentation like for that we need to,358,71,71,dmUtafXB5aw
6,say documents like this and when we do that we are going to see that we only have the home method and we don't have any endpoints so we are going to start defining them but if you're going to be working with api documentation the address that you're going to be using is going to be this address after hosting it locally like you can test your endpoints from here and everything like let's say try it out and when we say execute we are going to see that the code is 200 which means that it worked successfully and now it returns the message like welcome to the house price prediction api which we defined and we can test our api methods like this so let's go back to the code editor and also we can see the tested endpoints from from the terminal.,189,71,71,dmUtafXB5aw
7,so i'm not going to close that. i will just make it like this. and i'm going to be using it. so i'm not going to close the hosting. and now we need to create the prediction endpoint. so what i'm going to do is i will say it's going to be a post method and it's going to be at the predict endpoint. so i will say define predict price and we are going to give the data as house data that we created in here. it's pretty important and firstly we are going to convert input to a numpy array. so i will say features and it's going to be numpy array. i will say data dot here after that we are going to get the size square feet like data dots size square feet and data bedrooms and next up i will say prediction is going to be model predict the features and next up what we are going to do is we are going to say return a predicted price and it's going to be float prediction and the first index of it. so here i created my predict endpoint and what i'm going to do is i'm just going to save this file and i'm going to show you how it changed on the terminal like it's going to update the api. here you can see that stat reload detected change in the main.py reloading and our api restarted. so that i'm going to be recording at the same address on my browser and showing you the new endpoint we just defined.,347,72,84,dmUtafXB5aw
8,"so that i'm going to be recording at the same address on my browser and showing you the new endpoint we just defined. so here i'm just going to refresh this page and you can see that we have the predict endpoint in here. also we have the schemas in here and we have the house data like this. the beauty of the pyantic with fast api is it just automatically defines the schemas in your documentation. so let's use the predict endpoint and test it. i'm going to say try out here and let's say 200 let's say 100 1,500 or something like that and let's say 440 bedrooms. so when we say execute if we say 200 in here it means that our api is working successfully. yeah we see the 200 code and we see the response body here price is 610. so our api is working successfully right now. this is one way that we can test our api endpoints and i'm going to show you the other in the code editor right now. before that let's change this numbers and see the change in the response like let's say two for the bedrooms and increase the size. here we get the new price. okay let's go back to the code editor. here also in the code editor we can see that we have the post product is 200. so how you can test it else is you host it in here from other terminal page use a tool like co or you can use postman or you can even use a jupyter notebook or python script for testing it. so let's do that.",361,84,99,dmUtafXB5aw
9,"so let's do that. i'm going to create a jupyter notebook like testipmbb and in here since this api is hosted i can send requests to it. so i will say like import requests and url is going to be http our local host address like this 8,000 we are hosting on that and we will say predict and next up i will say payload we are going to give the payload like size square feet is going to be let's say th00and and bedrooms is going to be like two. so let's save this up and i will say response. it's going to be request post url and json is going to be payload like this. now we get our response in here and i'm just going to show you what it looks like. if you just call the response like this, you're only going to see the http code like 200. it means that response is successfully retrieved. we can say json for seeing what's inside. and here we have the preded price 375 also let's reach to that like predicted price and we are going to get the price directly. so in your programs after hosting your api like this you can just call it like this and you can start using it in your programs. okay. now let's say that we want to get multiple predictions from our api. let's do that. we are going to define a new class in here in our api like multiple houses and let's say base model in here again for initializing it and it's going to be houses and in here we are going to take a list like house data.",366,99,113,dmUtafXB5aw
10,"we are going to define a new class in here in our api like multiple houses and let's say base model in here again for initializing it and it's going to be houses and in here we are going to take a list like house data. so after that we are going to create a new endpoint like let's say app post and i will say like in here predict batch and define after this predict batch and data is going to be multiple houses and features are going to be like numpy array and we will say again two dimensional house size square feet and house bedrooms like this and after that what we are going to do is we are going to use like for in data houses. so right now we are taking each house separately in here. next up, what i'm going to do is i will say predictions is going to be model.predict features and next up we are going to return predicted prices like predictions to list. so we have predict batch endpoint ready. right now we take the list of house data and next what we do is we extract all of them separately. then we get predictions also in here in this class. we are using the house data that we created in here too. okay, let's check how it looks from the api documentation, but i'm just going to test it from the jupyter notebook again. so let's do that. so here i'm just going to refresh this page and we have the predict batch ready.",344,113,123,dmUtafXB5aw
11,"so here i'm just going to refresh this page and we have the predict batch ready. in here we can see that we have the houses list and in the list we are waiting size, square feet and bedrooms. okay, let's go back to the jupyter notebook for testing it. okay, in here i'm going to go to testpipm and next up what i'm going to do is i'm just going to copy this url in here and i'm going to change the predict white predict and batch. so i'm going to make this run. next up, i will create a payload like let's say houses at the top and i'm going to give a list like let's say size square and let's say 2,000 and bedrooms let's say three. next up i will say size square and i will say,500 bedrooms i will say two. and next up i will say size square ft let's say like 3,200 and bedrooms five. i think it's enough. let's get the predictions for three. i will say response and i will say requests post url is going to be the address that we give in here and the payload is going to be like json equals payload. so here we can get see that we have the 200 response from the terminal also we can get it like this and also we can see the inner data like this. so we have the predicted prices list of 718 525 and165.",322,123,135,dmUtafXB5aw
12,"so we have the predicted prices list of 718 525 and165. so i'm going to stop hosting this like i'm going to just do ctrl c in here and you can see that it says waiting for application shutdown application shutdown complete finish server process stopping reloaded process. so we just stopped our api and in here this easily we can use fast api for creating endpoints for our machine learning models. that was it for this tutorial. let's get to the outro. thanks for watching this machine learning tutorial. i have a playlist named machine learning tutorials where i have more than 40 videos just like this one. you can check that playlist for more videos like this. also, i'm sharing new data science videos every week on my channel. you can subscribe for more. have a great day.",182,135,145,dmUtafXB5aw
0,hi everyone today we will finally learn about docker and we will do it by making a useful machine learning project specifically a translation program that we can easily share with the world now we will build it step by step solving actual real world problems using docker only we are not cloning anything from github and we will of course talk about images containers docker files compose files and we will even see how to publish our own software on dockerhub so what exactly are we waiting for let's music roll so first of all what exactly is docker docker is a platform that helps us build run and share software it uses something called containers to create isolated environments where programs live and even though they use the same hardware as our system the same processor memory and all other components containers are separated entities that operate in a sandbox nothing goes in and nothing goes out which means that all the modules and libraries that our software needs are already pre-installed inside the container so we can think of containers as the perfect set of conditions for our software but why is it so important well let's say we are working on a team project batman has a mac machine i have a linux one and gandalf is using windows our project may have the exact same code but it will probably manifest differently on every system or alternatively even if we all have the same operating system it doesn't mean that our environment is the same i have the newest version of naai but it fails on batman's computer because it has,358,0,0,-l7YocEQtA0
1,conflicts with some other software it happens all the time but why do we need this headache if we can just create a controlled consistent environment that all of us can use and that's exactly where docker comes handy we can all work on three different computers but if we use the same container we are actually using the same environment you will see what i mean shortly so let's quickly install docker and i'll continue explaining it as we go first we will navigate to doer.,113,0,0,-l7YocEQtA0
2,we will select products and we will go for the personal version to download it we will need to sign up but since i already have an account i'm just going to sign in now once docker desktop is installed we will accept the terms and conditions in my case i'm going to skip this lovely form and we will verify that the docker engine is running given this green bar at the bottom left corner if that's the case we can now close the guey and we can use our terminal instead in my case i'll be using the command prompt and i will run it as administrator and for the record this tutorial is using docker version 24.0 point6 great so how exactly does it work well before we go any further we will need to understand the concept of docker images docker images are very similar to github repositories but instead of just storing code they also store the ideal set of conditions for our code so we are not just getting a piece of software but we are getting the environment where our software is already installed now images have a readon format which means that we cannot modify them we can build new images but we cannot change existing ones so we can think of them as this static set of instructions but what are these instructions for well images are instructions for containers so for example if images are blueprints for a house then the container is the house itself and in technical terms a docker container is a running instance of an image we are not just reading containers,358,1,1,-l7YocEQtA0
3,but we are also interacting with them and yes you can modify containers as much as you'd like so in summary we use images to create containers and inside them we are not just running programs but we are running entire environments so let's see it in action so let's search for an image of a nice machine learning library with docker search tensorflow and even though there's quite a few options here we will go for the jupiter tensorflow notebook so let's quickly copy this name with a right mouse click and we will then download this image with docker pool followed by the name of the image where jupiter is the name of the community that maintains the image tensorflow notebook is the name of the image itself and a combination of the two is the name of the repository but what kind of repository are we talking about where exactly are we pulling this image from so let's quickly run this command and let's navigate to hub.,221,1,1,-l7YocEQtA0
4,"do.com as in dockerhub and we will then click on explore which will open a giant collection of images so if we search for jupiter tensorflow we will find the exact same repository we just pulled including the pull command right over here so you can either search here or in the terminal it's entirely up to you now once we finish pulling our image we will turn it into a container with do run followed by the name of the repository and great we will copy one of those urls that jupiter provides us we will paste it in our browser and we get a nasty nasty error but why well if we go back to our terminal we see that we are not communicating with our own operating system my name is not jovian and i am not using linux these are the properties of our container which is essentially an isol ol ated process on our computer and because it is isolated we get an error when we try to access it from the outside so how are we supposed to solve it well first of all let's collapse our notebook with contrl c then we will press the up key to fetch the most recent terminal command and then right in front of our repository name we will add the flag of- p as in ports then we will choose a port from our host system in my case i'll go for port a 8,000 followed by a colon and then the port from our container which is 8,888 and here we don't really get to choose we need to specify the",358,2,2,-l7YocEQtA0
5,exact same port that we got from jupiter great now let's give this command a quick run let's navigate to our browser we'll type local host at port 8000 and beautiful here's our notebook now the last thing left to do is to copy our token from jupiter we will paste it as our password and boom we are in so we basically created this corridor where docker takes all the actions that we perform in this lovely browser window and it automatically applies them on our container in technical terms we call this process exposing a port great now let's quickly create a new notebook and let's make sure that tensorflow works to test it we will load a very nice data set with from tensorflow cars.,166,2,2,-l7YocEQtA0
6,dat sets we will import mnist which is an image data set of black and white digits to get those images we will call the mist. loore data method and we will assign it to data but the thing is our data is broken into train and test data and each of these is broken into samples and labels data so essentially our data is a nested tuple with this type of structure and it's okay if you're not sure what it means it is not important for this tutorial now let's quickly run this cell and once our data is loaded despite all those warnings we will go ahead and plot one of our images with plt do imshow as an image show to which we will pass our very first training image with xor train in the index of zero and yeah we might as well import the library first before we use it with import met plot li.,209,3,4,-l7YocEQtA0
7,pyplot s plt now let's give it a quick run let's have a look at our sample and okay it looks a lot like five now let's quickly verify it by printing the matching label to our sample with y train in the index of zero and beautiful it is five indeed and tensorflow officially works but what if we don't have the time to learn tensorflow and all the machine learning concepts behind it can't we just skip the understanding part and go straight for the results of course we can we'll just use a library called transformers that offers a very large collection of tools for beginners so it's quickly import it with from transformers import pipeline and look at that transformers is not installed inside this container now usually we'll just install it with exclamation mark pip install transformers but the whole idea of containers is that we never need to install anything and if something is missing from our image we cannot just add it we will need to build a brand new image instead now luckily we don't need to do it from scratch we can use the tensorflow notebook as a base and we can combine it with some new modules for this allow me to introduce you to docker compose an alternative way of defining containers so for example if we'd like to reproduce the same container we ran earlier we will need a compos file that defines it now this file is using the yaml language which is all about indented pairs of keys and values separated by colums where the important points are we are using the,358,5,5,-l7YocEQtA0
8,tensorflow notebook image and we are exposing the internal port of our container to the external port of our host system now let's quickly save this file as compose yml we will then navigate to the folder where we saved it and with a right mouse click we will open a terminal instance in the current directory and this time we are dealing with a powershell terminal now let's quickly make the font larger and at the moment of filming i am using docker compose version of 2.,114,5,5,-l7YocEQtA0
9,23.0 now to run a container with docker compose we will simply type docker compose up then then we will navigate to local host at port 8000 we will once again copy our token we will paste it back in the notebook and we are back in but isn't it a bit silly that we always need to copy and paste our tokens can't we just set a really nice password instead of course we can so for this we'll go back to our terminal we will first shut down jupiter with crl c then we will stop and remove move our container with docker compose down then back in our compose file we will add another key of environment and we will assign it to a value of jupiter unor token in all caps now in my case i will set it to i am batman now let's save it let's go back to our terminal and let's call docker compose up again we will then refresh our browser and now instead of a token we will specify i i am batman and beautiful we are in now the only problem is the notebook that we created earlier is now gone so before we build a new image let's make sure we have a way of preserving our files now the way to do so is with something called drive mounting where we expose a folder from our container to a folder on our host machine just like we've done with the ports so back in our compose file we will create a new key of volumes and we will assign it to a value,358,6,6,-l7YocEQtA0
10,of dot slash which represents the current directory of our terminal in my case that will be this lovely folder where our compose file lives then once again we will separate it with a colon from the directory of our container which in my case is slome sl jovian now let's save it let's go back to our terminal let's shut down our notebook and let's call docker compose up again we will then return to our browser where we see our beautiful compost file in the file tree yay now the original plan was installing new modules inside a container so instead of using an existing image we will build a new one instead now the way to build images is with something called docker file which we will create right away so the build key is where we specify the location of this file in our case do slash or the same directory as our compos file now let's quickly save it let's create a new file and let's save it inside our project folder our lovely mounted drive and we will save it as docker file with a capital d no format no extension just docker file now to make it work we will need to begin with the from instruction where from specifies the parent image on which our container is based in our case jupiter tensorflow das notebook then right below we specify our user which in the case of our tensorflow notebook is a variable named dollar sign and bore uid in all caps and what we really mean here is jovian a user with the right permissions now if,358,6,6,-l7YocEQtA0
11,you are building from a different base image you will probably have a different username as well so if you're not sure what it is please try setting it to root which is the administrator so you will basically have unlimited permissions but since we know our username we will go ahead and use it instead next we will use the run instruction to pip install d- upgrade pip and with the help of a double end symbol as well as a backs slash we will move to the next line where we will pip install transformers next we will pip install a library called pi s rt you will see shortly why and then just as a precaution step we'll go ahead and fix dash permissions of a string of slome sl dollar sign and in a set of carly brackets we will specify n bore user and as you may guess once again what we really mean here is jovian and great our docker file is ready we can now save it we can navigate to our terminal and we can call docker compose up again then we will create a new notebook where from transformers we will import pipeline and let's give it a quick run and despite all those warnings transformers was successfully installed how do we know well let's create a new model we'll call it translator we will assign it to pipeline and inside pipeline we will specify a task in our case a string of translation uncore e ncore 2core frr as in english to french now let's give it a quick run and once the model associated with,358,6,6,-l7YocEQtA0
12,our translation task was done downloading we can then call translator to which we will pass a very nice sentence in my case my name is maria and i am a programmer let's quickly assign it to fr we will then print it right below and it all comes down to awesome but the only problem is we're not really getting a string in return we are getting a dictionary that is embedded in a list which is not exactly what we're looking for so let's quickly focus on the item at index zero to cancel the list and then we will focus on the key of translation text to cancel the dictionary and now when we reun run this cell everything looks much much better awesome now let's do something extra useful with our new set of skills so let's take the subtitles from one of my videos specifically in an srt format which is a sequential file with a bunch of timestamps alongside their text and you can find it in the description we will then paste it inside our mounted directory which will load it into our container and then back in our notebook will first verify that our file was loaded there you go captions english and then we can read it with pi sr t.,286,6,6,-l7YocEQtA0
13,openen to which we will pass the name of the file captions unor english.srt we will then assign this expression to subs as in subtitles and we will of course import pi srt before we are using it then we will print the content of our file just just to make sure that we loaded it properly with for i in subs we will print i let's give it a run and great there you go everything was properly loaded but because we don't really need to translate the timestamps let's focus on the text only by printing i. text awesome now let's translate it to french to do so we will create an inner loop variable called fr fror text and we will assign it to translate to which we will pass i. text now since we are looking for a string output we will once again focus on the item at index zero as well as the key of translation text then we will simply assign i. text to our french translation as in f frore text then once we are done with our for loop we will go ahead and save our new translation with subs dove and we will specify the name of our file which in my case would be captions unor french.,283,7,10,-l7YocEQtA0
14,text to our french translation as in f frore text then once we are done with our for loop we will go ahead and save our new translation with subs dove and we will specify the name of our file which in my case would be captions unor french. srt now let's run it and this might take you a minute or two and once jupiter is done we will then navigate to our file tree where we can find our brand new french captions file so let's click it and holy smokes you guys here's our beautiful beautiful translation we did it now the last task is to convert our entire software into an image not just the environment like we've done earlier but also our code that way we can upload it to dockerhub and share it with the world now before we do so let's quickly rename our notebook let's call it trun slater and there's just one tiny detail we will add to our docker file we will need a copy instruction that takes files from our project folder and stores them directly on our image so for example example we will copy captions english.srt and we will save it at the root directory of our container which is dot slash now we will do the same for our notebook we'll just specify it right after our captions with translator.,304,10,11,-l7YocEQtA0
15,iynb great now let's save it let's navigate back to our terminal where we will first need to stop and remove our current container now we can of course do it with docker compose down just like we've done it earlier but on my end i'll go ahead and remove all the stopped containers not just this one with docker container prune yes and once our container was removed we can finally go ahead and call docker compose up again which will build our new image awesome we can finally navigate to docker hub we will click on repository stories and we will create a new one now on my end i'll call it sr- translator and i'll describe it as english to french srt video subtitles translator let's go ahead and click create and perfect now we have a remote repository name and because it is remote it means that our terminal is not aware of it yet so first let's find the local repository name and we will do this with docker images which will show us all the images that we've pulled and we are currently storing on our system and then right below repository we will find the name of our machine learning project so it's quickly copy it and now we will need to change it so it perfectly matches the name of our remote repository to do so we will type docker image tag followed by the name of the local repository and since it has the tag of latest we will add colon latest to the very end of it now right after we specify the new name of,358,12,12,-l7YocEQtA0
16,the repository which i'll just copy from my browser and i will give it the tag of 1.0 because it is the very first version of our image now let's run it and if we check docker images again we see a new instance of our image but this time with the remote repository name great but this repository still lives on our computer to upload it to dockerhub we will type docker push followed by the new name and the tag of 1.0 once we hit enter we finally load our repository to dockerhub and once we are done we will navigate to our browser we will refresh the page and boom here's our beautiful beautiful beautiful repository which is now 100 publicly available yay and let's quickly verify it works as expected so let's prune our containers once again then we will remove the local instance of our image with docker rmi as in remove image followed by the name of the image as well as the tag of 1.0 now we will verify that this image is gone with docker images and beautiful it is gone indeed now in addition i would like to change the current directory of my terminal just to make sure that it has no files in it i want a completely new folder with nothing inside to do so i will type mech dear test as in make a directory named test we will then navigate there with cd test and now we will go ahead and pull our remote image from dockerhub with docker pull followed by the name as well as the tag of 1.0 now,358,12,12,-l7YocEQtA0
17,"we will run this image and we're not going to use docker compos this time we'll just type docker run- p followed by the host system port of let's say 5,000 we'll be creative this time followed by the container port of 8888 followed by the name of the image as well as the tag of 1.0 now let's run it it let's copy the url from jupiter but we will need to slightly modify it we will change port 8888 to 5000 and beautiful here are both of our files and when we click them we get the exact same content we had earlier including the warnings now the best part is if we navigate to our new test directory it is absolutely empty so those files they never came from our system they came directly from dockerhub now i don't know if you've noticed but we have officially learned how to work with docker you can now continue exploring it on your own so congratulations and thank you so much for watching if you found this video helpful please share it with the world and don't forget to leave it a huge thumbs up if you'd like to see more videos of this kind you can always subscribe to my channel and turn on the notification bell i'll see you very soon in another awesome simplifi tutorial in know meanwhile bye-bye",302,12,12,-l7YocEQtA0
0,so today i'll be teaching you how to convert your machine learning model into an image so the first thing we need is we need your test data train data and your machine learning model in the python program or format within one folder so like i have named all of this in put this all in one folder named and underscore model so okay most of you probably know what a machine learning model is if you're watching the video so basically what a machine learning model is actually uh let me show you on my github for that so okay you're free to use my repository as well as a margin shape right from the modern field you can see my repository here and use these codes as freely as you want okay because i like helping the community out so uh let me show you one of my uh predictive models which i made within a sample data set of housing prices okay predictive models here public okay so so basically uh you can see right this is a notebook so in order to download it you go to rob you save it save as and it will be saved as a python notebook okay so in order to change that change it into a python program you have to go to you can either do a command line which you can search in google or you can directly download it from your jupyter notebook as well as where download as a python okay but i've already done it so i won't do it again okay so the first things first we have,358,0,0,JigSpm6KORI
1,to get our docker store ready okay your docker has to be ready okay this is the one i've done previously okay so to get started let's go and get open your command terminal run as administrator okay okay now start your docker see the dockers already started first you i have to go to my folder because everything i have to do has to be on the folder so cd go to my foldered location downloads a model okay see see now i'm doing everything on the app model now in order to create a docker file okay you have to create a docker file put all the dependencies in it okay so echo create docker file it has to be echo dot twitter than sign talk file okay so now my docker file has been created now we'll see see it's been created now now within the docker file you have to put all the dependencies and you also have to create a text file separately okay now i've done this previously as well so let me open this notepad you can do it with visual studio as well okay so from python you have to import slim buster your work directory has to be your folder okay so this one is m model okay so and all your separate pipe installed require pip install requirements all your python libraries they have to be printed on a separate requirements text file and then you copy dot out and then you command it and install the output within using python within your model which will be installed on docker okay so i've also done that,358,0,0,JigSpm6KORI
2,previously as well okay now first the docker file customs okay so let's copy this into the new docker file open with text editor okay has to be m model okay save okay and i've created a separate text file as well okay so your text file has here to start attack so look these are all the libraries you have to install for your machine learning model to work or if you have any other libraries you can just put that and you have to equate it to the version of the library you wanted like my my skit learn has to be equated to 1.0 at least or if you have used a new version of skit learn then you have to equate it to that if it's 1.11 or whatever okay the same with the rest of your libraries including your machine learning libraries as well like light gradient booster is a separate library not installed alone in schedulered alone so you have to separately install it so it has to be in the requirements file because your docker file will be copying from the requirements file all the dependencies okay so i'll just copy that as well paste i'll delete this one in order to avoid any duplication errors okay okay now we have to run the dot file to prompt engineer the documentation from chat gpt took a bit of effort to get all of this done from chat gpt as well so to now copy your docker to build your docker image after putting all the files together in one place you have to copy docker build dash t your,358,0,0,JigSpm6KORI
3,image whatever the image name you want so i'll name my image ml model okay your image name and then you have to put a full stop on space after that ml model okay now let's put that dot let's build it and you have to make sure that as usual your command directory is the folder where all the files are stored and underscore model okay foreign okay now you can see the image has been picked okay so in order to run the image you have to use docker run your image okay and your image name is ml model okay first let's check on docker if it's been built okay see the image has been built here,156,0,0,JigSpm6KORI
0,hello guys the idea of this tutorial is to do a quick and easy build of local container with a simple machine learning model and run it to complete this tutorial you should be familiar with first of all python programming language and second secret learn package that our machine learning model are based on os model functions like os path environment gets current working directory and something about machine learning models about supervised learning classification problems and terminal operation that we are using for making a local container and how to open your docker in your computer in this tutorial you will first of all create a docker file second build a simple docker container and next thing build a docker container with a host directory build a docker container with environment variables and then you will manage this kind of file structure the first thing you manage to train dot pi and inferent dot pi that controls a train and sdsu files for managing data and then you will use a docker file that describes the behavior in docker container let's go to real example and in here we build our first docker for machine learning model and here we are having two files it's the first one is train.pi and second is inference.pi and the train.pi in just a normalized data in a csv file it is a train dot csv and trained two models to classify data and the first model is using sql and from here linear discriminant analysis lda is coming while the second one is neural networks so we are having two models first and the second then inference.pi,358,0,0,mUnrWn6flfc
1,is perform batch inference by loading two models that has been created previously this will normalize new data coming from csv file it is a test.csv performs inference on the dataset and prints a classification accuracy and prediction that you can see directly on the code itself in inference dot pi no more special about the code right now so open the file explorer and check the file structure here you should see four files in your window so open the terminal and check the file off list in terminal as well to be sure that you are on the project directory you should see four files in this list as well so clear the screen and go next let's create a docker file just by typing touch space docker file and enter here's our file in the file explorer and so just drag into your code editor right here and let's write our first comment in docker file let's create a simple docker file with the jupyter psyche notebook image as our base image this is the first line we are setting this we need to install job live to allow serialization and destination of our trained model and then we copy the train dot cs we test dot csv train dot pi and inference dot pi files into the image for train dot csu and test.csv is almost done and now we going to copy the train.pi and inference.pi and this couple of lines do it so let's finish this line and go next yeah for inference dot bi is done then we run train dot pi which will fit and serialize the machine,358,0,0,mUnrWn6flfc
2,learning models as part of our image build process which provides several advantages such as the ability to debug at the beginning of the process use a docker image id for keeping track or use different versions so with this line we finish this docker file save it and go next so open your terminal window and clear the screen then you can to extend your terminal window to be comfortable with typing your command to build your first docker image so let's do it right now just type docker build minus t and the name of your docker image in my case is docker ml model and minus f stands for file and docker file and don't forget to add the dot at the end of this line so remember this command is very important to build your first docker image for your machine link model so enter and our first image is complete here on the terminal you can see all the steps that being used to create your first docker image for your machine learning model so clear the screen and go next it's time to perform the inference on new data from test.csv for this we need to run a special comment from docker is docker space run space docker machine length model and we need to run python3 and inference.bi that is our python file that is responsible to make a batch performing on new dataset and as you can see here we have some results from inference as output we can see some result let's go to docker dashboard tool and on the image section you can see the most,358,0,0,mUnrWn6flfc
3,recent one that is our image and it's still running and you can navigate into dashboard tool yourself and let's go here just press on this container and here you can see a quite long list of action that is running behind your container and this is quite interesting and i suggest you to explore yourself to better understand how it's forking so go to the next part of this tutorial we can do a few things that can improve our containerization experience we can for example build a host directory in the container using work there in the docker file that we are going to do right now in docker file let's add a new line and write our work there variable and it is equal to my data that means that within the image we will save our data files into this directory and it is equal to my data so let's say with our docker file and let's go to make it more understandable i suggest to insert a new line in our inference.bi file and in this print statement i suggest you write some directory path and it is equal from os model and we are getting current working directory and let's print it in the another print statement right here i'm using f string statement in python code and i'm printing a directory path in this line so from this i can to have a output path and it is equal os dot path dot join and i'm joining a directory path and output.csv file and that is going to be right here and this variable can be used to save,358,0,0,mUnrWn6flfc
4,our test data set at the end of this code and let's create a pandas data frame from a training set i'm doing it right here in panda data frame function and i'm saving it to csv file and for saving directory i'm using output path that is going to be from direction pap so this is how it's working and i suggest now to run our docker file so let's say with our inference.pi file and the last changes what we have done just before and let's open again our terminal and clear the screen and now it's a good time to build another docker image by the same function as we used before and it will be a model 2 the image is building up is still running and it takes some times and in our example it will take approximately 20 seconds and let's wait for this and as you can see it is consumed about 16 seconds and you see on the screen that it's being finished now we are ready to run our second docker image by using the same command from the terminal as we used before and in here we need to change the number of the image and it will be the second one not the first one and we need to run our inference dot bi file from this command and as you can see we are seeing directory path and this variable is equal to my data and the result as we saw before in the first image it is as we expected and is going good so we can to check how it look like,358,0,0,mUnrWn6flfc
5,in the docker tool as you can see the most recent one is here and if you go deeper and to navigate by clicking here you can see the action that is going in the inside the terminal so it's working fine and congratulations to achieve this step you can invest your time in navigating in this docker tool and let's go to the next part of this lesson so now we can close the docker tool and come back to our project files for future of your development it can be necessary to set environment variables from the beginning the advantage of setting environment variables is to avoid a hard code of the necessary paths all over your code and to better share your work with hours on the agreed directory structure for this let's create a new directory on our image is mymodel and now we are ready to create our first environment variable in the docker image and the first one is model there and it is equal to home slash joe one it is a standard notebook user in the docker image and my model it is coming from the new folder we created yet before i recommend to avoid any spaces in typing your environment variables and now we are ready to create a second environment variable and it will be for model file lda and it is equal to clf its mean classifier and underscore lda and the and its job lib do the same for the next environment variable and it will be for neural network file so replace only this part is to nn and do the same,358,0,0,mUnrWn6flfc
6,in wall way it is and in here so we have prepared three environment variables in this docker file so it is a good time to save our docker file and keep going on working with the codes so let's start with train dot pi and in this code we are going to take this environment variables and it is quite simple procedure we are using os model from python and let's define the variable it's model underscore dir and it's equal to os dot environment and in the brackets we are going to define this environment variable from the docker file so we can do the same for the next environment variable it is uh for model file it's for lda and it's equal also os dot environment and in the square bracket we define a wearable name from the docker file so do the same for the model file neural network it is also os dot environment in square brackets we define a wearable name from the docker file also next create a environment variable for model paths and the first one is dedicated for model lda and in here we should define environment variable that is combining model path for lda and model name itself and in this line i am combining these two wireless and yeah it is a model file for lda it's a second argument in this line so it's complete and let's do the same for neural network model and yeah i'm pasting this line and this is not ld this is neural network it is an n and replace the end of this line yeah it's looking good,358,0,0,mUnrWn6flfc
7,so scroll down a little bit in this file and now we are ready to replace these two wireless in dump function and this code so let's do like this just copy and paste it firstly for lda model just pasting it here in dumb function right now and the second one is for neural network let's copy and paste again in this line because we having environment variable instead of this hardcore line so it's done and it's successful so is a good time to save train dot pi file and go next and next we need to do the same with inference.pi we need to use the same environment variables that we used in train.pi so copy and paste it into inference.pi right here you just remove the lines that we are not longer necessary and copy it in here so we need to use it in infinite.bi in similar way as we used in drain.pi so for make it more understandable let's print some variables wallace in here and in here it is a simple print statement so is for better debug what is happening inside and remove the slide because we don't need it anymore and now we are ready to use the same environment wireless in inference.bi so let's do like this just grab the modal path for lda and replace it in load function right here and just copy and paste it right here in this line in load function because we are using environment variables instead raw wireless do the same for neural network and so paste the environment variable name in here and it is done so far,358,0,0,mUnrWn6flfc
8,so good so we need to be sure that we pasted environment variables in the correct places and as you see from the code that is good and i save it i am double saving also the rest files to be sure that everything is on the last changes and ready to use now we are ready to build the new image from the last changes that we made and for this we are using the same docker build function in our terminal just replace the image name from second one to third one because we now we are having that image number three until we finish this line with docker and dot at the end just press enter and we are seeing that the docker is just building up and in few seconds we gonna see the our image will be finished and this is right now you can see the finish on the top of the information window and then we can declare the screen and go to the next step and the next step is to run our new image by typing this comment from terminal and we are using the same command as we used before and just the only one thing what we need to change is the image name to model three and we are going to run inference.pi so we can see some output in our terminal and what we can do highlight here is it is printed out the location of job live model file in here and in here and this is what we expected to see and this is some result from our classifiers for both,358,0,0,mUnrWn6flfc
9,models and now we can go to our explorer to see that files are okay and go to docker tool that you're gonna be installed on your computer and in here you could play with the docker tool and to select the most recent one in here you can check the all action that is going behind your image because it's still running and to going to analyze some information what is happening it is very good tool to get a better understanding of what is going on and to get a deeper knowledge about docker about docker tool docker hub and you can navigate to different sections and compare different image and to stop or run or do something with your image immediately by selecting a specific comment so this is highly recommend to play around this and this is what i'm doing and learning and suggest you do the same so this is the finish line of this video tutorial and we made a big job if you are with me so congratulations with that and suggest a new topic for the next video so see you there and never stop learning so see you on the next video music,262,0,0,mUnrWn6flfc
0,hello everyone and now in this video we will talk about how we can create the docker file for the python image okay so here you can see the different uh keywords i have written for you all okay so these are the some keywords which we use for creating the docker file actually so in this particular video basically we'll be will be creating the python image okay so uh here you can see actually the uh the description is also written for these keywords you can see the from keyword is used to use the base image okay what are the base image which we are going to use here so mostly in the from keyword we specify the base image for python we use java for sorry for python we'll be use python base image for java we will be using open jdk for the ubuntu will be us ubuntu image okay so these are all the base images which we get from the docker we will be using those kind of images okay and run command is also used to run some kind of commands which we which you want to uh run while creating the container and all and copy used to copy the files from the local file system to the docker image work dr used to set the working directory for the container actually okay and cmd is we use to cmd is basically used to specify the what are the commands you want to run or when that particular container runs or something like that happens okay so these are some of those uh i think three to,358,0,0,KUECJHlV1LE
1,five uh commands or keywords which will be using in our docker file okay before starting the video or you should have uh vs code installer you can use any uh any uh id not id it's just what do we say or something like that okay or editor you can say so here i have this docker docker tutorial folder and i'll be creating one python image folder okay so let me just cancel all those things and now what we will be doing we will just create one docker file okay you can do like this docker file and keep it keep in mind like you should use only one capital d here not the name should be like this only okay so you here is the docker file has been created and let me create some app.pi as well okay so this is the python file and suppose you want to just print like this this is my first image okay cool so this is done suppose till uh now we just want to print this in our container like when our container will run okay now what we will be doing we'll be creating the docker file how we can do it you have to just use form and where from where i from which uh base image you want to use i want to use simple python okay you can use from python after that you can specify the working directory for that particular container so i just want working directly to be app okay and also you want to copy all the files okay to the app here okay and,358,0,0,KUECJHlV1LE
2,next we'll be using the cmd okay now currently we don't want to run any kind of command okay that's why we are not using run here so in cmd what we want uh we want to use this line python 3 and you can specify in a comma you want to run which file i want to run app.pi file okay so this one is done okay that much is fine to create the python image okay so now the further step is just to use to build that build image okay actually from the docker file so you have to just open up the terminal here you can click on this and click on this here talk a terminal and you have to go in that particular directory so we are currently in python image so we are here in the directory and for building the uh image the command which we use is docker build and one hyphen t flag is used to define this some kind of tag like the default tag which you get is the latest so here you can define any tag suppose like my first python app and in that also it should be only in lower case you cannot give the combination of a percussion lowercase in the tag name okay and then we will be using dot dot will be used to just uh get the docker file and then creating that image from that okay just hit enter and here you can see our image has been created actually and this one you can see exporting layer writing image this one okay naming my first,358,0,0,KUECJHlV1LE
3,python app this is done okay now i will just clear the screen i will just run that particular so image name was my i think first python app so it's running so here you can see it is printing this is my first image okay and if you go on the docker desktop so here also you will see the name is this one it is just specifying that random name it specifies and the image name in my first python app and image also here you can see the my first python app okay so in this way you can create the python image now let's do uh some what we can say uh some kind of experi not experiment just i write some commands actually here import os okay and i want to just show you the actual uh directory in which directory that docker is running okay so here we can write current dir is and here comma separator os dot get cwd and here we will be using this one and again i want us to run the build image and i will build it okay and the container name i want to provide i want to provide name of container is iphone iphone name and it should be like uh uh what should be the name uh first pi first pi c means first python container just hit enter and here you will see the this is my first image printing and currently i already slash okay and here if you will see uh here also like okay none it's showing we will see why okay tag is none okay,358,0,0,KUECJHlV1LE
4,okay no problem in containers if you see yeah engineer her name is first pi c okay the container name actually okay so uh now what we will do uh again if you want so i just want to create one file here suppose i create one file hello dot txt here and i just print hi here and i want to print the list of directories or files which are present in that documentary because we are using the copy here okay so it will copy all those things in the docker image so for this we can use os dot get uh not get i think it's list dir yeah again we will just uh here i use some different tag uh list list i just do list image okay okay and now i run this using list image and the second pi container enter and here you can see it is listing the all the uh what you can say um all the files and the current directory and the print statement is printing okay so in this way you can create the uh any python image which you want and suppose you want to uh send this file or something like that suppose there is some kind of project which you have created in that file and you want to send to the other person so you can just create the image of that file and you can send it okay so in this way it works so i hope you like this video and that's all for today's videos guys and if you have any queries you can ask us in,358,0,0,KUECJHlV1LE
5,the comment section okay so that's it for today we'll meet you in the next video thank you for watching,26,0,0,KUECJHlV1LE
0,one of the leading causes of imposter syndrome among developers is not knowing docker it makes it hard to go to parties where everybody's talking about kubernetes swarms shuffle sharding while you hide in the corner googling what is a container we've all been there at one point or another in today's video you'll learn everything you need to know about docker to survive as a developer in 2020 we'll take a hands-on approach by containerizing a node.js application i'll assume you've never touched a docker container before so we'll go through installation and tooling as well as the most important instructions in a dockerfile in addition we'll look at very important advanced concepts like port forwarding volumes and how to manage multiple containers with docker compose we'll do everything step by step so feel free to skip ahead with the chapters in the video description what is docker from a practical standpoint it's just a way to package software so it can run on any hardware now in order to understand how that process works there are three things that you absolutely must know docker files images and containers a docker file is a blueprint for building a docker image a docker image is a template for running docker containers a container is just a running process in our case we have a node application we need to have a server that's running the same version of node and that has also installed these dependencies it works on my machine but if someone else with a different machine tries to run it with a different version of node it might break the whole point of,358,0,0,gAkwW2tuIqE
1,docker is to solve problems like this by reproducing environments the developer who creates the software can define the environment with a docker file then any developer at that point can use the docker file to rebuild the environment which is saved as an immutable snapshot known as an image images can be uploaded to the cloud in both public and private registries then any developer or server that wants to run that software can pull the image down to create a container which is just a running process of that image in other words one image file can be used to spawn the same process multiple times in multiple places and it's at that point where tools like kubernetes and swarm come into play to scale containers to an infinite workload the best way to really learn docker is to use it and to use it we need to install it if you're on mac or windows i would highly recommend installing the docker desktop application it installs everything you need for the command line and also gives you a gui where you can inspect your running containers once installed you should have access to docker from the command line and here's the first command you should memorize docker which gives you a list of all the running containers on your system you'll notice how every container has a unique id and is also linked to an image and keep in mind you can find the same information from the gui as well now the other thing you'll want to install is the docker extension for vs code or for your ide this will give,358,0,0,gAkwW2tuIqE
2,you language support when you write your docker files and can also link up to remote registries and a bunch of other stuff now that we have docker installed we can move on to what is probably the most important section of this video and that's the docker file which contains code to build your docker image and ultimately run your app as a container now to follow along at this point you can grab my source code from github or fireship io or better yet use your own application as a starting point in this case i just have a single index.js file that exposes an api endpoint that sends back a response docker is easy then we expose our app using the port environment variable and that'll come into play later the question we're faced with now is how do we dockerize this app we'll start by creating a docker file in the root of the project the first instruction in our docker file is from and if you hover over it it will give you some documentation about what it does you could start from scratch with nothing but the docker runtime however most docker files will start with a specific base image for example when i type ubuntu you'll notice it's underlined and when i control click it it will take me to all the base images for this flavor of linux and then you'll notice it supports a variety of different tags which are just different variations on this base image ubuntu doesn't have nodejs installed by default we could still use this image and install node.js manually however there is,358,0,0,gAkwW2tuIqE
3,a better option and that's to use the officially supported node.js image we'll go ahead and use the node version 12 base image which will give us everything we need to start working with node in this environment the next thing we'll want to do is add our app source code to the image the working directory instruction is kind of like when you cd into a directory now any subsequent instructions in our docker file will start from this app directory now at this point there is something very important that you need to understand and that's that every instruction in this docker file is considered its own step or layer in order to keep things efficient docker will attempt to cache layers if nothing is actually changed now normally when you're working on a node project you get your source code and then you install your dependencies but in docker we actually want to install our dependencies first so they can be cached in other words we don't want to have to reinstall all of our node modules every time we change our app source code we use the copy instruction which takes two arguments the first argument is our local package json location and then the second argument is the place we want to copy it in the container which is the current working directory and now that we have a package json we can run the npm install command this is just like opening a terminal session and running a command and when it's finished the results will be committed to the docker image as a layer now that we have our,358,0,0,gAkwW2tuIqE
4,modules in the image we can then copy over our source code which we'll do by copying over all of our local files to the current working directory but this actually creates a problem for us because you'll notice that we have a node modules folder here in our local file system that would also be copied over to the image and override the node modules that we install there what we need is some kind of way for a docker to ignore our local node modules we can do that by creating a docker ignore file and adding node modules to it it works just like a git ignore file which you've probably seen before okay so at this point we have our source code in the docker image but in order to run our code we're using an environment variable we can set that environment variable in the container using the env instruction now when we actually have a running container we also want it to be listening on port 8080 so we can access the nodejs express app publicly and we'll look at port some more detail in just a minute when we run the container and that brings us to our final instruction command there can only be one of these per docker file and it tells the container how to run the actual application which it does by starting a process to serve the express app you'll also notice that unlike run we've made this command an array of strings this is known as exec form and it's the preferred way to do things unlike a regular command it doesn't start,358,0,0,gAkwW2tuIqE
5,up a shell session and that's basically all there is to it we now have a full set of instructions for building a docker image and that brings us to the next question how do we build a docker image you build a docker image by running the docker build command there's a lot of different options you can pass with the command but the one you want to know for right now is tag or t this will give your image a name tag that's easy to remember so you can access it later when defining the tag name i'd first recommend setting up a username on docker hub and then do that username followed by whatever you want to call this image so in my case it would be fireship slash demo app and you could also add a version number separated by a colon from there you simply add the path to your docker file which in our case is just a period for the current working directory when we run it you'll notice it starts with step one which is to pull the node 12 image remotely then it goes through each step in our docker file and finally it says successfully built the image id and now that we have this image we can use it as a base image to create other images or we can use it to run containers in real life to use this image you'll most likely push it to a container registry somewhere that might be docker hub or your favorite cloud provider and the command you would use to do that is docker push,358,0,0,gAkwW2tuIqE
6,then a developer or server somewhere else in the world could use docker pull to pull that image back down but we just want to run it here locally in our system so let's do that with the docker run command we can supply it with the image id or the tag name and all that does is create a running process called a container and we can see in the terminal it should say app listening on localhost 8080.,104,0,0,gAkwW2tuIqE
7,but if we open the browser and go to that address we don't see anything so why can't i access my container locally remember we exposed port 8080 in our docker file but by default it's not accessible to the outside world let's refactor our command to use the p flag to implement port forwarding from the docker container to our local machine on the left side we'll map a port on our local machine 5000 in this case to a port on the docker container 8080 on the right side and now if we open the browser and go to localhost 5000 we'll see the app running there now one thing to keep in mind at this point is that the docker container will still be running even after you close the terminal window let's go ahead and open up the dashboard and stop the container you should actually have two running containers here if you've been following along when you stop the container any state or data that you created inside of it will be lost but there can be situations where you want to share data across multiple containers and the preferred way to do that is with volumes a volume is just a dedicated folder on the host machine and inside this folder a container can create files that can be remounted into future containers or multiple containers at the same time to create a volume we use the docker volume create command now that we have this volume we can mount it somewhere in our container when we run it multiple containers can mount this volume simultaneously and access the same,358,1,1,gAkwW2tuIqE
8,set of files and the files stick around after all the containers are shut down now that you know how to run a container let's talk a little bit about debugging when things don't go as planned you might be wondering how do i inspect the logs and how do i get into my container and start interacting with the command line well this is where docker desktop really comes in handy if you click on the running container you can see all the logs right there and you can even search through them you can also execute commands in your container by clicking on the cli button and keep in mind you can also do this from your own command line using the docker exec command in any case it puts us in the root of the file system of that container so we can then ls to see files or do whatever we want in our linux environment that's useful to know but one of the best things you can do to keep your containers healthy is to write simple maintainable micro services each container should only run one process and if your app needs multiple processes then you should use multiple containers and docker has a tool designed just for that called docker compose it's just a tool for running multiple docker containers at the same time we already have a docker file for our node app but let's imagine that our node app also needs to access a mysql database and we also likely want a volume to persist the database across multiple containers we can manage all that with docker compose,358,1,1,gAkwW2tuIqE
9,by creating a docker-compose.yaml file in the root of our project inside that file we have a services object where each key in that object represents a different container that we want to run we'll use web to define our node.js app that we've already built and then we'll use build to point it to the current working directory which is where it can find the docker file and then we'll also define the port forwarding configuration here as well then we have a separate container called db which is our mysql database process after services we'll also define a volume to store the database data across multiple containers and then we can mount that volume in our db container and hopefully you're starting to see how much easier it is to define this stuff as yaml as opposed to writing it out as individual commands and now that we have this configuration set we can run docker compose up from the command line which will find this file and run all the containers together we can mess around with our app for a little while and then run docker compose down to shut down all the containers together i'm going to go ahead and wrap things up there if this video helped you please like and subscribe and consider becoming a pro member at fireship io where we use docker in a variety of different project-based courses thanks for watching and i will see you in the next one,325,1,1,gAkwW2tuIqE
0,"host thank you for coming along to this third after-lunch session here at pycon. if you are looking for popular terms in the industry at the moment -- containers, dockerizing things, microservices -- you've definitely come to the wrong place -- right place. audience laughs you've come to the right place because our presenter, dorian pula, is going to be talking about all of these at the same time. please make him welcome. applause dorian pula thank you so very much. just before we start, just a quick show of hands. how many people have played around with docker? oh wow, okay. keep your hands up. how many of you guys use it for your development regularly? okay. how about testing, like, testing continuous integration? okay. and how many of you brave souls actually use it in production? okay, wow. more than i expected. awesome. so, my talk, pythons in a container, is essentially -- it's about the lessons that we've learned when dockerizing python microservices, and lessons that we actually learned the hard way because, you know, that's the best way to learn anything. so, quick little introduction. so, who am i? i'm dorian. dorian pula. i'm a software development engineer at points, which means i get to work on developing an e-commerce platform for loyalty programs. so if you're buying, gifting, or transferring points from, like, an airline or hotel, you may be using our system, and that actually translates into a lot of flask rest apis and apps, and i get to work on all of them.",338,0,23,qT0dQ8S7jOg
1,"so if you're buying, gifting, or transferring points from, like, an airline or hotel, you may be using our system, and that actually translates into a lot of flask rest apis and apps, and i get to work on all of them. and, a lot -- basically, the deployment strategy that we have -- development and deployment strategy that we have decided to take at points is actually using sort of a dockerized microservices approach, and i'll be talking about that today. i'm also an open-source contributor. i have my own cms because, you know, we all need one more cms. it's built in flask, though, not in php. i've contributed to fabric, ansible, and a little bit of core python, and because i have a sort of an unhealthy interest in working on deployment and infrastructure processes, i've also come up with a bunch of -- released a bunch of ansible roles for nginx, uwsgi, node.js, and supervisor. and, okay. so, let's jump right in. what is this talk about? so, as i said, this is a sort of the lessons that we learned when actually using docker for flask rest apis and apps, but a lot of the same things will apply for, like, django, pyramid, or bottle. basically, anything that you can have a uwsgi application should basically work as well. i'm also going to be talking about some of, like, incorporating the tools that docker and docker compose provides, which basically enable a better sort of devops workflow.",327,23,34,qT0dQ8S7jOg
2,"i'm also going to be talking about some of, like, incorporating the tools that docker and docker compose provides, which basically enable a better sort of devops workflow. and about also the usefulness of unlearning some of the sort of regular accepted patterns that we do in python development, because some of them don't really apply as well when working with docker also, what is this talk not about? so, it's not an introduction to basic docker or wsgi application development because there simply just wouldn't be enough time to do sort of a tutorial for both. i won't also be talking about docker machine, which is a really cool technology basically letting you link local docker containers with sort of like remote aws instances and things like that, but that's a subject in of its own. and i won't get into really advanced docker wizardry. for that, i recommend actually going to dockercon which is actually next week, and if you guys do you, you may actually bump into some my co-workers there. and finally, i'm not going to try to give you an expose on why you must or must not use docker. essentially, just use the right tool for the job and understand why you want to use it. however, i will say that docker definitely provides some really nice things. so, let's talk about, like, why we want to actually use microservices and docker in the first place. so, to illustrate things a little bit easier, let's take sort of a sample situation.",334,34,44,qT0dQ8S7jOg
3,"so, to illustrate things a little bit easier, let's take sort of a sample situation. so, imagine that all of us in this room are tasked with actually building a new application or service that would provide, like, a loyalty program experience for, like, sprint contributors at pycon. essentially what would happen is that you would earn points per commit or issue resolved and you'd be able to redeem them for essential sprint goods like coffee, pop-tarts, and dogecoin, all the things that you need to get through a coding sprint. and we'll have the following dependencies -- the following components. we'll have a rest api. we'll host the front-end application. we'll have some sort of service for handling the redemption of points, user and project registration and linking, and also a database. so if we were to break that up into sort of an architecture, so why would -- sorry. why would you actually want to -- if you were tasked out to build something like this, why would you want to use microservices in the first place? well, if we went with a microservices architecture, we could divide this up into multiple services and have multiple teams working on this on different services concurrently, and using sort of just technologies that they're good at. so, some of the benefits of actually going with this type of architecture is that you'd get smaller, less complex codebases, right? because each one of the services you're working on has only one specific purpose.",327,44,55,qT0dQ8S7jOg
4,"because each one of the services you're working on has only one specific purpose. you would enable greater independence between codebases and teams, so if one team decided to build an application in flask, that doesn't mean that another team can't build it in tornado, twisted, node.js, or rubio, or whatever. and essentially it would enable us for more sort of flexible scaling schemes, and not scaling just in terms of production, but also organizationally. of course, with any technology there's always a -- or any kind of architecture -- there will be drawbacks. one of them being a distributed codebase is harder to think about, harder to infer. it may contain implicit inter-service dependencies which are not easily recognizable just from looking at a codebase. and we would definitely have more complex orchestration, monitoring, provisioning schemes, just because we have more services. now, if we were to break this out into sort of like an architectural diagram, we could divide up our application into, like, three different rest services. one for, like, the redeem service which would connect to external vendors, one for the user and project registration service, and one for the actual app and api. now, you can build -- you know, you could break it up into various different components, but in essence that's what you're doing. you're taking an application and breaking it up into multiple different parts. so why would we want to actually use docker for this? well, docker uses containers, which are usually -- which are lighter in memory and processing than virtual machines. and that's because containers act more like user space instances, rather than trying to emulate a full machine.",364,55,68,qT0dQ8S7jOg
5,"and that's because containers act more like user space instances, rather than trying to emulate a full machine. so they will definitely take less memory and less space because you don't have to emulate a full physical machine. docker also uses an interesting sort of cached immutable layered file system which i'll talk about a little bit later. also, one of the really nice things about docker and docker compose is just the tooling that it provides. tooling for basically quickly spinning up various containers or environments. it is very easy to do that in docker compose. and also to actually create, share, and publish these images, so that you can collaborate across your teams and with other people. and ultimately it actually just provides a better sort of workflow. a more unified workflow that would replace a bunch of the other tooling that you would normally have. namely, things like chroot, lxc, and vagrant, and so on. so, how would we actually develop this? so, the first thing that we would do is that we'd probably, we want to specify all of our services in one place just so it's easier to reason about, and this is where docker-compose.yaml would come into place. so, we specify all our services and the environment variables, hostnames, its dependencies, all that wonderful stuff in one single place. and then, we would actually basically start up this entire service just by simply going docker-compose up. and then that will bring up all of your services and make sure that everything is nicely connected and working, assuming that you've got everything right the first time around.",353,68,82,qT0dQ8S7jOg
6,"and then that will bring up all of your services and make sure that everything is nicely connected and working, assuming that you've got everything right the first time around. laughs and so, the docker workflow is really nice because it ultimately replaces some of the more conventional workflows. most of you guys have used vagrant, right? in the past for working with, like, complex setups for various applications? well, docker and docker compose specifically, if you put them together they will actually sort of map one to one with the commands that vagrant would have, with ones that you would have in docker compose. so things like vagrant up and vagrant ssh, and then run app_command, it would basically -- it translates into docker run app_command. vagrant halt becomes docker stop. vagrant status becomes docker ps because, yes, just like as you would do ps in unix or a linux system, if you did ps even without the sole docker ps, you could actually see your docker processes and the containers actually running on your system. docker ps just gives you a nice little abstraction for that. you would have vagrant provision which becomes docker build, so, building of your images and your containers. and vagrant destroy becomes docker stop and docker rm to remove the images. and just like with vagrant, how vagrant handles the creation of base images, you also have similar commands in the case of, like, docker images, docker rmi. okay. so, one of the lessons that we -- that people learn when actually working with docker, is the importance of having good docker images.",351,82,95,qT0dQ8S7jOg
7,"so, one of the lessons that we -- that people learn when actually working with docker, is the importance of having good docker images. here you have an example of a very simple docker file just for bootstrapping the flask application using uwsgi. one of the things that you'll notice is that each one of those steps, the from, run, add commands, they all will create sort of new layers in a file system, because docker uses sort of like a layered file system, which basically builds up your image, and then all the layers work together as a sort of a container. the only problem is that each layer takes up a lot of space, right? so what you want -- one of the things that we learned is that we want to minimize, like, the number of steps that have the separate run steps, because instead of having one layer just for updating, like, apt-get, and then one layer for installing, you really don't care about those two separate layers. you just want one layer that says, okay, bringing my system up to date and install all my dependencies, right? rather -- so, in this case we only need to have one command, rather than multiple commands and having multiple layers for each command. and ultimately you also want to make sure that your layers are cacheable. so, what does that mean?",304,95,103,qT0dQ8S7jOg
8,"so, what does that mean? well, that means that docker, when it's building up your images, it uses the cache layer from a previous step, and it will try to reuse that layer if it finds out that there hasn't been a previous step that has changed, or if a particular source file has changed. if they haven't, then it will try to reuse the layers, which means you get a faster setup time for actually doing -- when you're building your containers. and if you find yourself that you have a lot of, sort of, like, basic setup just to get an image up, you want to look at stuff like base images, especially if you have, let's say, three different microservices and they all use the same sort of setup, whether they be like, you have a common flask or you need node to be installed, it's good to actually include the base image, and maintain that. if you're interested in actually maintaining sort of base images that other people can use, then go check out the onbuild command and that will help you build sort of like dynamic base images. and finally, one thing that's -- it's not critical but it's useful when you're actually developing, and that's exposing sort of the ports and the volume maps just so that you can document how your image is going to work. alright. so that's -- okay. so, so far i've really only been talking about, like, you know, basic docker stuff that applies to all things, to all different kind of systems. but what about something that's python-specific and specific to wsgi apps?",357,103,112,qT0dQ8S7jOg
9,"but what about something that's python-specific and specific to wsgi apps? one of the lessons that we learned is web servers. since you want to keep nice, concise docker images, you don't want to include too much stuff, and one of the things that you don't really need to include is actually including a web server for your image. instead you should probably look at using sort of an external proxy, so an external nginx instance or having your own web service container. rather, if you're just using -- if you're just building wsgi applications, then just do the simplest thing and run a simple wsgi app server to actually host and run your python application. two of the more popular examples being uwsgi and gunicorn for that. another thing that is sort of like a common accepted practice when developing complex or just any kind of python applications, is the use of virtualenvs. virtualenvs to separate what packages you install in one place, rather than having a whole system of very conflicting packages, you use virtualenvs. but when you're actually working with docker, actually you don't want to use virtualenvs inside your docker container because you don't need them. ultimately, each container is an isolated system and so you don't have to manage having separate environments. so, just install directly into the system python site packages. it'll also save you a lot of time and hassle. alright. since usually, you know, when you're developing something, you don't always get it right the first time around so you may have to, like, debug containers or services.",345,112,125,qT0dQ8S7jOg
10,"since usually, you know, when you're developing something, you don't always get it right the first time around so you may have to, like, debug containers or services. now, normally, if you have a vagrant instance you would ssh into this thing, but you don't want to include an ssh daemon into your docker image because it's just going to get too large. so, how do you debug a running container in that case? well, you can actually use the docker commands to actually run various commands on the service. so in this case, let's say we want to run bash on a running service. we could do basically going back to our docker compose example, use docker compose, execute the service name, and bin bash. and there's a similar command for just running on pure, plain docker. similarly, if you want to just inspect what an application's logs are doing, then you could do that using the docker compose logs and providing the service name. now mind you, docker only handles standard out and standard in -- sorry, standard out and standard error output -- so, you can't use any sort of complex logging. just have your application log to standard out and standard error and you'll be good. finally you want to actually inspect your -- you may want to inspect your running computer setup. so, you might want to -- you can use the docker inspect command, and it gives you a whole bunch of output. like, it tells you what exposed parts you have, what volume maps, and all that stuff.",344,125,137,qT0dQ8S7jOg
11,"like, it tells you what exposed parts you have, what volume maps, and all that stuff. one of the gotchas i've run into is that you would have docker inspect produce an incredible amount of output, so sometimes you just want to scale it down and just look at just a particular aspect. so you can use the format command and provide a golang format string to actually see those things. and a couple minor tips when coming to persistent configuration and processes. so, one thing with volume maps. you want to use volume maps because you want to -- because when you make a change to a docker container and you destroy it, all of those changes are lost. so, there are two approaches of actually getting around that. one is to have a volume map so you have an external host folder where you can dump all of your stuff for your database and whatnot. another pattern that's often used is actually to have a separate docker data container that won't go away. in terms of configuration you should be using environment variables for configuration. your services are very single purpose, so you should be able -- they should be simple enough that you can just use environment variables to set things up. you can volume map configs as well, but that may be a warning sign you just have a really large, complex configuration setup, and your configuration may need refactoring. if you find yourself that you need to run multiple processes, you can use stuff like supervisord or runit to control multiple processes.",347,137,149,qT0dQ8S7jOg
12,"if you find yourself that you need to run multiple processes, you can use stuff like supervisord or runit to control multiple processes. however, in a microservices approach, the best way is just to have one type of process that you're running. so you may want to -- if you find yourself coming up with complex supervisord setups, then you should probably consider refactoring your service into multiple containers instead. docker and docker compose really make testing a lot easier. it will add consistency into your ci environments so you no longer have to try to build up -- if you're working in a -- with a large build farm, you don't have to make sure that each agent in your build farm has the exact same environment, because it's really simple to set up a docker host, and then you take your environment with you. because, you basically, you control whatever you put into that container. so that means that you can have repeatable workflows and simpler test processes. and there are a lot of cloud ci options with docker support out there now. very quickly, when it comes to tooling, it's probably not a good idea for you to write your own docker tooling. the docker commands usually provide enough things for you to work with. plus, the api, the internal api, has a habit of changing a lot from version to version. if you do decide to actually do something like that, then take a look at docker-py. that's a python client library for working with docker. but again, you probably don't want to do this.",349,149,162,qT0dQ8S7jOg
13,"but again, you probably don't want to do this. so, we've covered sort of, like, the best ways of doing development, so how do we actually deploy this out into prod and how do we scale this up? well, if we go back to the architecture that we selected for application, this -- well, it's a nice way to understand what we're trying to build. this really won't scale in production. rather, in production your setup will probably look like this, right? where you have the multiple load balancers between services. you may have, like -- you may find that the application or front end just takes more, needs to handle more traffic, so you'll be running multiple containers over multiple data centers and things like that. so when you look at -- there's a lot of moving parts that you have to to consider. and i'm not even putting in things like message giving, data warehousing, and all that stuff. this is just our basic application, right? this is how it could potentially look in prod. and, when you think about it, ultimately, what you're really doing is you're setting up a little private cloud of various microservices, and so there's a couple things that you have to think about when you're actually doing that. you have to start worrying about load balancing and network topology so you have to look at using stuff like haproxy and nginx. you also have to deal with provisioning because you may not dockerize all of your systems, but you still want to have an automated repeatable setup for non-dockerized systems.",348,162,175,qT0dQ8S7jOg
14,"you also have to deal with provisioning because you may not dockerize all of your systems, but you still want to have an automated repeatable setup for non-dockerized systems. or you may even want to just have your docker containers controlled separately by a provisioning script. so you should probably look at stuff like ansible, puppet, salt. please don't try to run your own little fabric script to get everything working together. it's just -- there's too many things, too many edge cases that you might run into. you also have to think about the monitoring. you have to know, if you're dealing with multiple services, what is the health of your app, what is the behavior of your application, and the system resources on basically all the systems that you're managing. so, suddenly things like nagios, pingdom, and new relic become important. and logging becomes crucial because in a microservices architecture you're going to have multiple log streams, and you're going to need to aggregate the various logs and correlate the events so that you can be able to debug when something happens -- if something breaks between services, or if basically the bug is not in the first service that you have to deal with but in some lower service along the way. so, tools like splunk and others come in very helpful. so yeah, essentially if you go down the dockerized microservice approach, you have to get into sort of some of the cloud structure, and managing cloud infrastructure is hard. you need a lot of tooling and a lot of automation to get all that stuff into place.",353,175,186,qT0dQ8S7jOg
15,"you need a lot of tooling and a lot of automation to get all that stuff into place. and you really don't want to build out your own tools unless you want to support those tools to the end of time. unless you're a cloud tech vendor, in which case you don't really need to be at this talk. you should be out building awesome cloud stuff for all of us to use. so, instead of building your own you should probably look at using other services. use something like docker swarm or kubernetes. the openstack guys have a pretty interesting compute platform called magnum. and there's also the coreos guys, so if you want an alternative to docker, there is coreos fleet. alright. so, to summarize the lessons that we've learned, microservices and docker can really improve building and deploying a large complex system, but neither one is a cure-all, right? there's a lot of things you have to keep in mind. also, good development and deployment processes matter, because ultimately if you have a good, smooth development employment process you get happy devs, right? makes life so much easier. and docker has a fairly decent workflow that you can use to shape those processes. and you should expect lots of additional infrastructure around microservices, which might make you tempted to actually build your own tooling, but unless you want to support it, don't do so. it's not a fun thing, and even a lot of the cloud tech vendors are actually pooling their resources just not to rebuild the same kind of infrastructure tooling.",345,186,201,qT0dQ8S7jOg
16,"it's not a fun thing, and even a lot of the cloud tech vendors are actually pooling their resources just not to rebuild the same kind of infrastructure tooling. use docker containers to do the actual effective isolation so you don't have to worry about using stuff like virtualenvs or any other kind of virtual environment. and ultimately good app design enables all this. a good app design will be able to work nicely into a dockerized system without a lot of manual rework. okay. so, there's a couple of interesting resources that are included in the slides. i want to thank jared kerim from mozilla, who presented this really great django docker template. if you are working with django applications and you want to deal with some docker compose, take a look at his example. also, if you haven't read 12 factor apps, the guidelines, you should definitely read them. they're really good. i also built a dockerized sort of workflow example for my cms, rookeries. it's on a branch, but if you're interested in sort of a simple setup for, like, building a flask application with a database attached to it, then go ahead and look at that. thank you so very much for everyone for your time and attention. applause have any questions? host okay, if you have questions, line up at either of these two microphones. if you are going to stay in the room or even if you're walking out right now, please keep yourself relatively quiet so we can still hear the questions being asked and answered. thank you. over here. audience member you mentioned there are a couple different options -- host excuse me.",365,201,219,qT0dQ8S7jOg
17,"audience member you mentioned there are a couple different options -- host excuse me. please audience, keep the chatter to a minimum while questions are being asked. this is really valuable to the audience at the moment. audience member you mentioned there are a couple different options for running containers in production. what do you use and how do you like it? dorian pula so, right now we're actually manually managing our containers. we're looking into using kubernetes in the future. it's something that we're still experimenting with. audience member how do you, if it all, handle database migrations in an environment where using docker for the database? dorian pula so, database migrations for the most part are going to be very similar. you still have that whole graduated rollout of databases. you might be able to migrate some of your datas running on a separate container, but otherwise it's a very similar setup. the same rules for a database deployment of a gradual rollout still apply regardless. audience member hi. i was wondering if you have any experience with -- say you have three services, a, b, and c. a depends on b, b depends on c, and you want to test a and not have to spin up the entire world basically to test it. dorian pula yeah. well, you can spin up the entire world. it's a useful end-to-end test and sometimes we do that.",309,219,237,qT0dQ8S7jOg
18,"it's a useful end-to-end test and sometimes we do that. but ultimately i would say that using sort of a stub server to mock out a service b -- if all you want to do is just test service a in isolation and only its dependence on service b, then it's better to just have a stub server, something like mountebank seems to work very well for those kinds of cases, yes. audience member thank you. host we have one more over here. audience member a couple things. one thing that you mentioned was that an alternative for docker was coreos fleet, but fleet is coreos's initial offering for a scheduler. so it's kind of like in the same sort of, like, realm as docker swarm. but coreos's container engine is something called rkt, which is kind of amazing. that's the first one; the second one is that, i'm sorry to say but a lot of these -- host is your second question a question? audience member it's more of a observation. host then can we have somebody else? thank you very much. dorian pula feel free to talk to me afterwards. audience member sure. host this is time for questions. thank you. okay, and with that, everybody please thank dorian for a fantastic talk. applause we also have a real person operating the open captions and we should probably give her thanks as well. thank the stenographer, please. applause",312,237,256,qT0dQ8S7jOg
0,hi guys it's me ub programmer and today the topic of our video is how to doize any machine learning model uh we will see that uh in a bit in practical and i want you to know if you are working on any custom project or you have an idea building a product uh you can reach out to me through these uh whatsapp email or you can fill out this form all the links and in is in the description okay let's get started with uh today's video uh here i have an api a flask api and i am using a machine learning model this one this is a pickle object of that model and i am predicting uh the marks student marks based on study alls uh this is the model i have have trained on some data set from kel right but your use case will definitely be a different one so adopt uh accordingly and uh this can be a great uh example or template for uh adopt right okay uh let's uh doize this app and run it on doer environment i have here docker file and first thing i'm doing is importing a base image of python because obviously i'm using uh flask and it is uh based on python and the machine learning model i have trained is also from pyit l which is python so i have uh found this image uh which is closest to my use case if you are using something like javascript use node right uh they also have their official image you can use doer hub to find the images from,358,0,0,uTgEkNd5378
1,uh their repository okay now we have workd command which will create uh app folder in that doer container uh which is just a fancy name or i guess a small name for uh virtual machine okay and copy uh will uh copy all of this uh files into the app which we have just created right in the container and then it will uh run a command in the terminal of that container uh which will be pip install and requirement.,106,0,0,uTgEkNd5378
2,dxt which will uh install p uh requirements from this uh text file which is floss can job li because i'm using only this one here right uh yours can be or will be always different uh now i have one route which is just simply saying i am live just to see that we have successfully deployed our or uh the api is working the second one is predict uh which will get a parameter study yours and and then it will predict uh using that model uh the predicted marks right then it will return adjacent object to me and it will run on port 8080 uh of the container because all of these will go into a container into a virtual machine and i'm saying use port 880 and then i will map out this 880 port with the port of this uh machine my machine right uh we will see that how we can do that also okay so this is basic uh flask api setup and the doer uh setup and then we are saying expose 880 because we want to use chatport and then cmd command or the terminal command python app.py on this host right uh okay let's uh see now how we can uh doize this first of all let's see how many images we have doer images i have one image uh no three images uh demo vid then this was i just created it for the demo purpose uh and my jango app uh i have doize the jango application which is i think my website now what i will do is doize a new app,358,1,1,uTgEkNd5378
3,for this flask api with the new tag name let's see how we can do this doer build the command to build uh uh image and minus t is for tag or naming and i will name this as yt demo the tag will be v1 version one i will name it as this and then uh dot dot means we have docker file in our current directory so build it let's enter building building now our doer file commands will run it has already uh buil it uh work dpr so here we have running the commands and cmd right uh the run uh pip install right cmd is obviously uh when we use or when we run this container now let's see how many images we have now okay so we have our image here yt demo which is uh v1 right now let's uh run this image doer run yt demo but we will specify a port also because we are we have exposed a port is 8080 so that means the 8080 port of the container is now mapped or interconnected to my local machines port 880 right kind of uh tricky but you will get it doer run this i unable to find okay by that see yt demo we have to give the tag also right v1 okay so now our api is running on flask and we can see that from here i am live right so you have seen how we can doize uh any machine learning model or any flask app uh through the docker uh runtime right and you can now use this runtime to any,358,1,1,uTgEkNd5378
4,deployment service like google cloud run or lambda functions of aws anywhere on the cloud services which will automatically uh manage the container for uh deployment purpose right i hope you like the video uh that's it for now i will see you in the next one thanks for watching,65,1,1,uTgEkNd5378
0,hello guys in this video i'm going to deploy a machine learning application into a cloud server like heroku with the help of dockers and github actions okay now get up actions whenever i say i'm basically talking about ci cd pipeline that basically means as soon as i commit anything from here automatically deployment should happen into the server and before that i'll also make sure that i will try to dockerize this entire application that i've actually created now right now here you can see this is my machine learning application a simple machine learning application and this machine learning application is nothing but boston house pricing data set we have basically used so whenever i dip i probably have created a front end wherever i put inputs and predict it it is going to give me the price of the house so first of all let me quickly run this and again guys i've created many this kind of project video so you can definitely watch that but here i've directly written the code you know probably done the exploratory data analysis and all so if you really also want to refer this definitely refer it from the github okay but i'm not going to implement this because i've done it many number of times so first of all uh if i just go and write python app.py here you'll be able to see how my output will look like okay so it is running let's see so for for the first time i think it will it'll take some time anyhow uh i'm just going to run this 127.0.0 here you'll be,358,0,0,Gs15V79cauo
1,seeing that my application will look something like this okay so my application looks something like this so here i'm just going to put up some information like this okay let's say i'm putting up all the information like this over here and if i probably do the prediction and this is right nine localhost right and i'm going to get the prediction something like this okay now what i'm actually going to do is that i'm going to basically deploy this i'm first of all going to dockerize this and run it as a dockerized container okay and what is dockers and all i've already created a playlist on to that you know just understand that dockers saves a lot of time with respect to configuration setups and all right because entire configuration is basically made within a docker image and that can be run as a container anywhere let it be your operating system or something cloud server wherever you want you can basically do it okay so what i'm actually going to do again i'm going to minimize this let me just do control c now let me just show you first of all this step we will try to create a docker file now in order to create a docker file it's very very simple not that difficult to create a docker file itself all you have to do is that is within this just go and click over here write docker file and make sure that you use this same naming convention then automatically vs code will be able to determine whether it is a docker file or not now with,358,0,0,Gs15V79cauo
2,respect to the docker file whenever we create the first thing is that understand what exactly is docker image here with the help of this file whatever information i am actually writing it actually creates a docker image okay and that docker image can be taken and it can be run within a container which we specifically say it can be run as a docker container in any operating suppose if i also want to run in my local it we can run this entire docker image as a docker container which will be interacting with the kernel of our operating system okay it is not like a virtual machine but you can just understand it is a kind of container which can independently run uh by communicating with the kernel of the operating system now in order to create a docker image first of all there are some commands that we'll be using one is from command okay now what is this from command i will discuss the next command is something called as copy command copy okay the third command is something called as work directory command the fourth command is something like run command and fifth command is nothing but expose command and then finally my cmd command now the firm command basically says that whenever now see why why docker is so super important guys suppose let's say if i don't probably create a docker with this application and if i want to give this same application for my friend to run it right so what my friend will do whatever installation i have done whatever setup i have done whatever library,358,0,0,Gs15V79cauo
3,setup i have actually done he has to do all those steps manually right and because of this what may happen is that he may face some kind of errors or issues so that is a major problem over there like you heard this saying right when when uh q is working right suddenly let's say when developer is working in the developer in in a developer machine suddenly deploy the code into the qa machine when the queue is basically testing they'll say oh something is not working developer says that oh it is working fine in my system so what is the main issue over here there may be some kind of configuration some kind of dependencies come with some kind of hardware issue or some kind of operating systems issues also right because let's say i am running an application in windows machine suddenly i deploy that in a linux machine i may get some kind of issues over there also right so docker helps us to prevent that because here in the docker image we will make sure that we have all the base configurations set up then and there and we'll try to use that same base configuration in every machine we want to deploy it okay so first command is from now this command is basically used to select any kind of base image okay now as i said that see docker container also requires some base image base image means that okay we can have a linux operating system on top of that installed something right so suppose if i write from python uh 3.7 so what this is,358,0,0,Gs15V79cauo
4,going to do is that i'm not going to use alpine okay alpine is again another different version okay of base image now as soon as i write python 3.7 when we are building this docker image it will go and take out the base image from the docker hub wherein it will take probably this python colon 3.7 basically means it will take a linux base image and on top of that it will try to install probably let's say a python 3.7 is installed it is going to take that particular base image and it is going to do the necessary other configuration settings so in short what happens as soon as i write from python 3.7 all it is going to do is that from the docker hub because all the images are present over there it will take that particular base image which has linux on top of it python 3.7 and then it is going to do the next step that is copy copy basically means whatever code i have in this repository see all these four files that i have in the repository i need to copy within that particular base image right all these files within that base image so for that what we will do i'll just say copy from my current location from my current location to a location which i am going to name it as app so that basically means i am going to create an app folder within that particular base image which will be copying all my local content to that particular app folder okay and then the next step is that i'll,358,0,0,Gs15V79cauo
5,create that as my working directory so here also i have to give my same location okay this is super super important three steps from i've taken my base image i made sure that i copied all the content all all my code from here to a app folder inside that particular base image and i'm making that as a working directory now the next thing is something called as run now see guys here also whenever we learn any machine learning application there will be some dependencies in this particular requirement.txt if you are doing a javascript project you probably have to install some of the packages and all so i need to install all these things before going ahead so for that i will be using this run command now inside this run command i will write pip install minus r requirement.txt right so this will do all the installation order dependencies will get installed over here finally i will go and expose now see inside my docker image when that docker image is run as a container right in order to access the application inside the container we have to expose some port then only we'll be able to access that particular entire url right because from that portal you will be able to access that application so we are going to expose a port within that particular docker container and that port i will just write it as a placeholder which is called as dollar port why because this value when we are deploying into the cloud or server right it is going to the server is going to automatically or the,358,0,0,Gs15V79cauo
6,cloud is automatically going to assign this particular port in that container okay so this is the next command and finally i will run my uh command which is basically used to run my web application or in this particular case my entire applications for this i'm going to use g unicorn okay g unicorn actually helps you to run the center python web application inside the heroku cloud itself so here i'm going to assign four workers workers importance is too much right workers what it does is that it whenever a request is coming into the application it will divide based on the instances okay it is let's say a thousand requests are coming so if i'm using four workers it is going to take 250 requests with one 250 another like that parallely different types of just to make that particular process uh easy right then i'm going to also bind now this ip address will be the local address in the hereku cloud and then i'm going to assign with port and this will basically be my app file along with that app file so see over here this app file is basically the file which is where i have to run my application inside this this will basically be my app name right so that is the reason why it is written app colon app app colon app basically means we are just going to take this particular file inside that we are going to run this okay so app colon app will be basically the entire process okay so we are just trying to run the first file over there,358,0,0,Gs15V79cauo
7,and over there app is basically my file name that is present inside this okay so that is what we are going to do so this is what is the entire configuration over here what this binding is doing this is super important guys see understand about this particular binding very very important this port number whatever we have exposed in the container that will be getting binded to the local ip address whatever local ip address we will be getting in the heroku club right so that particular local ip address so it's just like a local host at a 0.0.0 i know it is 127.0.0 in our machine zero point zero zero point zero we can assign it over there and this port whatever port here is uh assigning in the container will be able to access it over here okay very much super clear very much simple g unicorn is definitely required whenever you try to deploy anything in the heroku cloud platform okay simple simple and easy okay now this is done this is my docker file now the next thing that i'm actually going to do since i also need to make sure that i have to configure my github actions okay now in order to configure the github actions whenever you want to configure github actions considering ci cd pipeline two folders needs to be created one is dot github and the other one is something called as dot workflow oh sorry workflows okay workflows now why i have created this file because as soon as i deploy i push push this entire code into the repository github repository you,358,0,0,Gs15V79cauo
8,know when they seize this particular thing github workflow and inside this i will also create a file which is called as main.yaml file this will have the entire process as soon as i commit what all things we need to do first thing is that we need to build this docker file build this docker file basically means the entire image needs to be built and then we have to push this image in the form of a container to the heroku platform okay so the entire configuration over here will be set up in this main.yaml now guys uh this kind of main.yaml we don't uh write everything from scratch they're already available some people have already written this i'm just going to copy and paste it over here but just understand what is my workflow this main.ml will define the entire workflow okay so here i'm just going to say your workflow name this is deployed to heroku for in which branch it is basically on as soon as from here any push command goes on to the main branch then what will happen is that this entire build process will start main thing we are trying to do is build push and release the docker container to heroku okay so it will run on the ubuntu latest it will take a ubuntu operating system and do this entire process over there but here you will be seeing that you require three main information one is here email api api and heroku app name now this three information are super super important now the thing is that where do i get this specific,358,0,0,Gs15V79cauo
9,information obviously from the heroku itself right so heroku here you can see that i have logged in let's say i want to deploy inside this boston house pricing one okay now if i want to get this information this are my secret keys that is available that needs to be provided in the github actions so if i probably go over here in just a second if i go over here and probably let's say i'm going to my hostel boston house pricing now if i go to settings okay in the settings there will be something called a secret because i have to make this as a ci cd pipeline as soon as i push my code to this github repository so i have to add some secret keys now click on secrets click on new repository secret and here i will basically write my new repository secret the first repository secret is basically let's say i want to add heroku api key okay so i will go over here copy and paste hello api key now where do i get my heroku api key just go into this dashboard of heroku go and click on account settings and here if you go down here you'll be seeing api key just reveal this copy this entire thing and paste it over here okay so i'm just going to paste it over here so this will indicate that which hiroku app is my information like i want to deploy i want to sorry this will indicate which account i'm actually using in hiraku okay so as soon as i click this a secret key is,358,0,0,Gs15V79cauo
10,adding added but we need to still add two more secret keys so i'm just going to click on new cpr uh new secret key over here and now my second secret key will be what uh it will be hiroku email the hiroko email will be the same email id that i'm using for my heroku uh web app sorry dashboard so it will be krishna 06 at the rate gmail.com this just to give this configuration so that my deployment will happen successfully okay as soon as i do any push to the github repository so this is my heroku email now similarly coming to the third one it is heroku app name now here app name is super important because in which app i need to display deploy my code right so here i'll be writing europa app name and again i will go back to my account and let's go back let's say i want to deploy it in this boston housing pricing right so i'm going to copy this i'm going to paste it okay i'm going to paste it over here now here i will just go and add this secret so this all information is perfect uh we have added it now what will happen see we have set up the github actions that basically means as soon as now i deploy anything over here right right now you can see there is no github workflows folder only it is present over here right now as soon as this folder has been seen and whatever file is basically written over here all these things will get deployed all these,358,0,0,Gs15V79cauo
11,things all this process will automatically happen in short the build push and release of a docker container to heroku will happen automatically from the github repository itself okay and this is super important this build push and release because there will be many people who will be working in a team right and many people may commit multiple things right and every time if a bullet build push and release actually happens then every time you'll be able to understand what error is actually happening someone may do some kind of errors over there so every time this build will be a very important step to go with right this is what we basically follow in real world industries where we are specifically moving our content from development to staging staging to pre-broad pre-brought to production you know so with this kind of containers we'll try to create and we'll try to push it okay so this is done now let's go ahead and open my terminal now as usual i'll go to my environment and now i will add everything that i have actually done so get ad if i probably see get status you'll be seeing this many number of files has been created okay and i'm just going to push this okay let's see whether i have missed anything as such no no no no no nothing um no no no no no okay uh everything is here perfectly fine okay now i'm going to just push everything oh sorry before that i need to commit the snapshot so i'll write minus m now i'll say dockers and github action uh changes,358,0,0,Gs15V79cauo
12,commit okay because i have initiated this so this is done now all i have to do is get push from origin to i don't have to write to push from origin to main so you can see that everything has got done now as soon as i open github right now see what something amazing will happen okay if i reload this some orange color will come over here see okay now let's go and see the commit so here i will go and click it now you see this entire details automatically this entire deployment is happening okay something went wrong input required and not supplied hiroku app name so something wrong is there so let's go and see what is the thing that has gone wrong here app name i had actually provided it but i don't know what happened okay okay see app underscore name should be there right so let's do it again uh i'll add a new repository secret okay uh new repository secret i will go again back to my optional over here let's see this was the entire app name right i wrote only app underscore name okay so now here app name is nothing but house pricing dot this okay so because of that it failed see this because of that it failed right we will fix it don't no no need to worry we will fix it okay so now i will just copy this and i will add my secret add secret perfect now let's build or rerun this rerun this job rerun all the jobs okay a new attempt of this workflow we are,358,0,0,Gs15V79cauo
13,including all the jobs yes we run these jobs okay either you can rerun it or either you can do this okay so now you can see build has actually started okay so the build has started the job is started here first of all it has taken the ubuntu whatever configuration we have given now see every steps even building of the docker will also automatically happen right every steps will probably happen over here pull commit this download verifying checksum see this copy process is again happening working directory changes is automatically happening this is quite amazing guys this pip install is basically happening and all the steps are happening you can see over here right all the requirements are getting installed see this build process you did not do much anything right automatically these things are happening that is the power of cicd pipeline that is the reason why devops and mls jobs are heavily required in every companies you know this process with respect to every project you have to do right so this is going to take some time anyhow i will just uh wait till all this installation will take place and then we will again start the installation okay sorry we'll we'll automatically see at the end of the day when this entire thing will happen it will get pushed to this particular housetown uh hosting uh sorry boston housing price app itself and then once you open the app they will be able to see the output okay so let's see okay still happening layers already exist pushed pushed pushed that is going to probably take some time,358,0,0,Gs15V79cauo
14,because there are many files many installation will take place this entire process will get run if you get any errors you can again rerun it try to always get errors if you get errors that basically means you will be and if you are able to solve it trust me you are able to learn in an amazing way okay so post post post now i'm just going to pause it and once it is pushed we will try to see the application okay so i will just come back in some time until then i'm going to pause oh see okay releasing container okay releasing container all the steps you'll be able to see and please note down all these steps guys these steps are super important whatever things we have written in docker everything is happening because this is now getting created as a container now finally you can see my job is completed i'll go over here open the app let's see still nothing is working okay you can also check logs if you are probably finding any issues in boston house pricing okay and ta-da it's here and let's now go and execute it okay let's go and execute it here you go here you go here you go here you go and now this entire thing is running in a docker container if you don't believe me i'll show you that also i'll show you that proof once i predict it this is my output it's working absolutely fine if i probably go over here into personal here you'll be seeing now it is running as a container see before,358,0,0,Gs15V79cauo
15,if you just deploy a python application it will look like this but if you are deploying it as a container you will be able to see like this okay so i hope you like this particular video of deploying your data science application into heroku cloud with the help of dockers and github actions try in this particular way and please let me know whether you want a complete series of videos with respect to dockers and github actions i will try to upload it but for that i require 1000 likes for this particular video so yes this was it from my side i'll see you all in the next video thank you bye take care,153,0,0,Gs15V79cauo
0,"there's now an even easier way to run ai models locally other than using oalma. now, docker just released their model runner and this is a complete gamecher for running models locally. so, i want to show it to you in this video. now, just like olama, you can manage, run, and deploy models locally with openai compliant apis. but the real gamecher is that all of this is built right into docker desktop. so you don't need to install cuda, you don't need to install a driver, you can just enable it and start using it right away. now you can pull models directly from dockerhub or from huggingface and then you can run them from the command line or from your containers and deploy genai applications very quickly. so in the next few minutes, i'm going to show you exactly how to set this up and how to build two real applications from a simple python chat app to a containerized streamlit dashboard. this is all going to be using the docker model runner. let's dive into it. now, before we dive in, let me quickly cover what you need or the requirements to be able to run docker model runner. and i'm also just going to quickly disclose that i did team up with docker for this video. but everything you see here is completely free. all right. so, if you're on mac and you're using the newest apple chips, so like the m1, the m2, the m3, etc., then you're golden. this is just going to work perfectly and it's actually able to utilize the gpu, not just the cpu.",356,0,15,GOgfQxDPaDw
1,"this is just going to work perfectly and it's actually able to utilize the gpu, not just the cpu. however, if you're still on the legacy intel max, then unfortunately there's no support for this, so you won't be able to use this. now, for windows users, you can run this on both cpu and gpu as long as you have an nvidia gpu on your system. now, it's actually going to default to use the cpu, but if you have an available gpu, then it will automatically use that. again, as long as it's an nvidia card. now, if you're on windows arm and you're using a qualcomm gpu, then this will work as well. okay? and for linux users, this runs on both cpuon setups as well as nvidia gpu configurations, which is actually really nice, which means you can utilize small models on like a small linux vm on the cpu. or if you want to run big models, then you can of course have an nvidia gpu and that will work on linux. now, unlike a lot of the other solutions out there, you don't need to install cuda drivers. you don't need to mess with any gpu configs. docker handles all of this for you and it will automatically pick the gpu if it's available in your system. okay, so with that said, let me show you how to set this up. let's go to the computer. all right, so i'm on the computer now and i'm going to show you the very simple setup steps to make this work on your computer and then i'll show you how it functions.",358,15,29,GOgfQxDPaDw
2,"all right, so i'm on the computer now and i'm going to show you the very simple setup steps to make this work on your computer and then i'll show you how it functions. so what you're going to need in order for this to work is docker desktop installed on your machine. that's pretty much it. once you have docker desktop installed, then you need to open it up and you need to go to the settings tab, okay? or the settings page. from settings, you're going to go here to where it says beta features, and you're going to see that it has enable docker model runner. here, we're going to select that and we're going to make sure that we enable the host side tcp support so that we're able to actually use this from something like our python code. then, if you want, you can enable the gpubacked inference as well. okay, so that's literally it. once you enable that, just go ahead and press on apply. and then you can use the model runner to interact with models directly from docker desktop or you can do it from the command line. so if we close this here, you'll see now in docker desktop that you should have this models tab popping up once you've enabled the model runner. from here you can press on models and then it should show you various models that you can install. in my case i already have the small two downloaded and you can see it's 300 uh megabytes here or 360 megabytes. but what we can do is we can pull models directly from dockerhub.",360,29,43,GOgfQxDPaDw
3,"but what we can do is we can pull models directly from dockerhub. so you can see there's a few options here and we can download them or we can pull them from hugging face. okay, so pick any model that you want. i just did the small two. so i just pressed pull on that, downloaded to my m machine. it took a second and i'm just using that because it's very small for this video. okay, so once you have one of the models pulled, then what you can do is you can just press run on it and then you'll be brought into this interface where you can just directly talk to it. so hey, you know what's up, right? and we can send this and we can get back a response. obviously extremely fast locally on my own computer and in this case it's using my gpu. okay, so that's how you can do this directly from your docker desktop. you also can inspect and kind of view what's going on with the model here. but if you want, you can of course do this from the command line as well. so if we open up a terminal or let's open up cmd here, then all we have to do is type docker model like that. when you do the docker model command, let's just make this full screen so you guys can see it. you're going to get a list of how to use this command as well as all of the other options. if you've ever used olama before, this is very similar.",349,43,59,GOgfQxDPaDw
4,"if you've ever used olama before, this is very similar. so, if you wanted to pull a model, for example, you can say docker pull or sorry, docker model pull and then you can specify the name of the model that you want. now, again, you can pull these models directly from docker hub. so, you can see here the models are actually represented as oci artifacts inside of docker hub if that means anything to you. if not, don't worry too much about it. you can see there's a bunch of different options and you can run them. now, what's also interesting is that you can push models up and you can package your own models and then reuse them later. okay, so if we wanted to pull a model, we can say docker model pull. we can find one. so, let's go here and let's pull maybe the ai gemma 3. so, we're going to go ai geema 3 like that. it's going to take a second and it's going to pull that model down for us. while that's running, i'll also mention that there's various different versions that you can use. you can specify the number of parameters that you want when you're pulling these models. okay, so the model has been pulled. so now if we want to run this, we can do docker model. and then if we want to list the models that we currently have, we can type list and we can see the options. okay, so here we have gemma 3. so let's go docker model run. and then we're just going to paste the name of our model.",360,59,78,GOgfQxDPaDw
5,"and then we're just going to paste the name of our model. make sure it matches exactly what you have right here. go ahead and press enter. and then you can start talking with it. so we can say, hey, what is the meaning of life or something, right? and we should get back a response fairly quickly here. okay. and then you can go. it's generating the tokens and it will give us the response. all right. so, i'm just going to quit out of that. by the way, after you're in this interactive chat, if you want to leave, you can escape or get out of it by hitting control c on your keyboard or you can type by and that will exit you out of it. now, of course, there's a lot of other commands that you can use here. for example, package, pull, removing, running, status. i'm not going to go through all of them. what i am going to do now though is show you an example of how we can actually use the model runner from code because sure it's interesting to run this in an interactive chat but a lot of times we want our code to be able to interact with the models that we have on our computer locally. i'm first just going to go over a bit of important information and then we'll get into those demos. you'll see the timestamps in the video player. all right. so unlike traditional containerized approaches, the models themselves from the docker model runner are running directly on the host operating system. okay. they're not running inside of a container.",358,78,99,GOgfQxDPaDw
6,"they're not running inside of a container. now, this means that they get direct access to your system resources like your gpu, for example, and your memory. so, you can actually utilize the maximum performance of these models. now, when you pull a model down, it's not packaged as a container image. instead, it's downloaded as a model file and it's stored in your docker directory. now, the models can intelligently use both your cpu and your gpu depending on what's available. now the benefit of this architecture is that you get the speed of native execution while keeping the simplicity of the docker workflow. so the important thing to understand is that this is running on the host machine, not in a container. and if that means nothing to you, don't worry. but for those of you that are a bit more advanced, that's an important thing that you should know. all right. now, i quickly also just want to go over how this compares to lama because a lot of you are probably already using olama and you're wondering what's the point of the docker model runner. now, the biggest difference here is the architecture. so with o lama, your models are running inside o lama's managed service, but with docker model runner, they run directly on your host system. so you end up getting better performance. now, integration-wise, when you use the model runner, it's built right into docker desktop and it works seamlessly with compose and various other development tools. so the entire docker ecosystem pretty much. whereas olama is really more of a standalone tool.",351,99,116,GOgfQxDPaDw
7,"whereas olama is really more of a standalone tool. now both of these do offer openai compliant apis, but they use different ports. now, alama makes its service available on port 1 1434 and the model runner is using 12434. okay, so note that important distinction. now, if you're already deep into the docker ecosystem and you're using it a lot and you want native integration, then this really is just the best thing to use because it will just integrate already into your workflow, right? makes it a lot easier to deploy genai applications. whereas if you were going to use olama for example, typically what you would do is you would spin up your own olama server instance and then you would need to manage that independently compared to your docker containers. anyways, that's the main distinction. that's the difference between olama and docker model runner. if you're already using docker, you might as well just use this and effectively does the same thing as olama except for that architecture difference. it just makes it very easy to access the models. so with that said, let's get into some code demos here. and i want to show you how you can access the model runner locally on your own computer and how you can use it within a container in a docker configuration. so now that we've gone over that, i am back on the computer and i'm going to show you how you can interact with these models from something like python code. now the docker model runner exposes all of the models and the service that it's providing on port 1 2 4 34.",361,116,130,GOgfQxDPaDw
8,"now the docker model runner exposes all of the models and the service that it's providing on port 1 2 4 34. so just like lama, what you can do is you can simply interact with this url. you can go to slashengineeslama.cppvatcompletions. there's a few other ones as well that it provides and you can generate completions based on various messages. now, this is what's called an openai compliant api, meaning it follows the same practice as something like openai or gpt. so, however you've interacted with models before, you can pretty much do the exact same thing except now you're just using this here as your base url. so i just have a really simple script here in python that shows you how this works. we import requests. then i just have some data. so in this case i specify oops let's close that. the model that we want to use and then i specify the messages that we want to send. so we have a system message and then a user message. and then you can see here that we generate a post request. so we say request.post to this url. we pass our data. we wait to get the response and then what we do is we simply print out the content of that response. so from here i'll type uv run and then main.py. this is the script that i'm running right here. you'll see that it will take a second and then it will generate those 500 characters for us. okay. so there we go.",339,130,150,GOgfQxDPaDw
9,"so there we go. we can see that we get the 500 words that we asked it to generate which is what i put in the prompt right here. now, i just want to show you that because this is openai compliant, this also means that you can use other libraries that python has to interact with this uh what do you call it? model runner. so here what i've done is i've actually just used the openai library. so there's a library that you can install in python called openai. and what i've done is i've simply swapped the base url from being the openai base url to be the docker model runner base url. and now we'll start interacting with it just using this library. so that's the advantage of this being open ai compliant. you can see i create this open ai client. i swap the base url. the api key needs to be here, but it can just be anything. doesn't matter what it is. i specify the model. i prepare the message and then i can create a response again using this library rather than sending a manual request. just to prove that to you, we can go uv run and then again open version.py. this will take a second and then it should explain to us how transformers work. okay. you can see that we got the response. and again, i just want to clarify the reason why this is really important is because this is openai compliant. all of the modules typically that you're using in python already know how to interact with this type of api.",356,150,170,GOgfQxDPaDw
10,"all of the modules typically that you're using in python already know how to interact with this type of api. the only thing that you need to provide them in order to start using the docker model runner rather than olama, for example, is just this new base url where you're specifying where the api lives. so again that's a huge advantage and it means you can use it with existing libraries like the openai one like langchain like lang graph etc. so now i've showed you how to do this from your own local computer. now i want to show you how we can interact with this from a docker container. so for this next example i'm going to show you the same thing except in a containerized application using docker so you understand how this whole workflow comes together if you're going to use this in a docker container. now what i've done is i've set up a very simple streamlit application. okay, so this is just a simple ui that allows you to just chat with an llm. that's it. okay, and notice what i've done here is i've initialized a client using a base url that i'm going to get from this backend.env file. now, from this env file, i've specified the base url, the model, and then it just kind of a dummy api key. doesn't matter what the api key is. we just need it for this particular library because if you don't have one, it's going to throw an error. okay. so here what i do is i specify the base url this time equal to host.doccker.in internal.",357,170,184,GOgfQxDPaDw
11,"so here what i do is i specify the base url this time equal to host.doccker.in internal. so last time you would have seen that i was using localhost. so i was using localhost port 1 2 4 3 4 to communicate with the api. in this case i swapped it to host.doccker.in internal. so this is the main change that you'll need to make if you want to interact with the model runner from a docker container. now remember the way that this works is the model runner is going to be running on the host operating machine. hence this host.talker.in internal, right? so this is just what you need to swap the url to so the container knows to go and essentially navigate to the host machine rather than looking for another containerized instance or application. okay, so that's what we've set up. we've just changed that url. and then of course we need to do a few things in our docker compose and docker file. so from our docker compose file, you see that i have a very basic configuration here. and i specify an app. i'm building this in the app directory. okay, i have my environment variable file. this is the port that my application is going to run on, which is 8501 for streamllet. and then i just added this. i said depends on llm. the reason i need this is because we're going to be communicating with the docker runner. so i have to tell it, hey, you know, we need this. we're going to be looking at the llm or trying to talk to it. then i simply specify the llm service.",364,184,205,GOgfQxDPaDw
12,then i simply specify the llm service. so for the llm service i say okay this is llm. we have a provider. the type is model and then the options are this specific model. now this can be loaded from your backend.env file or you can just specify manually what you want the model name to be. this here is just like the backup. so if we don't find a variable for the model name then we would use this model right here. okay. very basic. that's literally all you have to do. and then of course i just have a very simple docker file here. this is like anything you would see just to run my python code. so now what i can do is i can start running this. so i can say docker compose and then build. so this has built the image for me. and then i can say docker compose and then up. and i can put the container up. and then you'll see that it's now running. so what i can do is i can now open this up. so let me just open this in my browser. okay. so you can see i've opened this streamllet application running on localhostport 8501. and then i can send a request here. it's going to get a response and then it should generate that for us using the docker model runner. and you can see we get our response. so main change is that you need to make sure that you specify that you're depending on the llm service. provide the llm service right here.,349,205,231,GOgfQxDPaDw
0,hello guys so we are going to start a series on dockers and in this series we are going to understand how to work with containers uh specifically docker containers we are going to understand what is the difference between containers and virtual machines we're also going to understand what is difference between images docker image and container and then finally once we get that in-depth intuition behind dockers we'll start our practical implementation wherein we'll see how we can dockerize our entire web application or any application and probably run it as a container in any specific cloud uh we'll also see that how we can convert or publish our docker image into docker hub repository so there are many things that is going to get completed but understand docker is super important for any developer who's working in iit company you know because this actually eases the deployment process so let's go ahead and talk about dockers and containers in the next video hello guys so in this video i am going to discuss what are containers and why do we use containers now let me give you an example let's say that i am not using containers at that point of time so what is the problem that i may face while doing the development and while deploying it into any kind of server so let's say i am a developer okay i am probably working in windows machine and let's say i am the only developer in my team right now okay so this is basically developer a okay and i'm working in the windows machine so let's say i'm we are,358,0,0,8vmKtS8W7IQ
1,creating a data science application so this is specifically a data science application that we really want to create this is our project okay and in this project obviously uh to start my development process i need to install lot of dependencies let's say i want to install anaconda i want to install uh different different libraries you know i want to install mysql separately if i'm using any database if i'm using a nosql database i will be installing mongodb let's say so all this installation process will take place and i'll be installing all this exe file and then i'll probably start my i'll create my development environment and i'll i'll start my development process now let's say tomorrow another developer joins my team and i've already started the development okay so now this particular developer let's say he or she is basically using linux machine or mac machine or windows machine so let's say this is developer b okay now when developer b will also join you know so first thing the developer b needs to do again do all this installation of all the dependencies that is required to set up his or her development environment okay so for his or her development environment they also need to do this complete setup now let's say because of some libraries mismatch libraries mismatch or it can be because of some dependencies you know let's say some of the installation does not go well then what will happen whatever application i may have probably developed as a i may have actually developed over here or developer a may have actually developed this will not,358,0,0,8vmKtS8W7IQ
2,work over here why because there is obviously some kind of library mismatch there's some kind of dependencies mismatch over here right now because of this what may happen my application will not run okay let's say anyhow developer a has helped developer b for doing all the installation and now it is working perfectly fine so let's say this development team has now created a devil environment and from this dev environment they have implemented some of the stories and now they need to send this entire application to the qa environment okay so we will basically send this to the qa environment so the testing will start so let's say obviously where if you are sending it to the qa environment it will be a qa server right so let's say over here the qa server is there now in order to set up this qa server again we have to do this installation process okay and let's say there was another team who was setting up the server and they missed some of the installation and the configuration right they missed some of the things let's say this part is basically missed then what will happen this entire application may not work or some of the modules in that application may not work right so some of the modules or application may not work right now suddenly the qa team will start complaining hey nothing is working in my qa qa website right your qa environment nothing is working we cannot test the application then development will say developer will say that hey everything is working in my machine you can see over,358,0,0,8vmKtS8W7IQ
3,here all the stories are working perfectly fine right so like this qa and developer will be on a war right but understand the main issue what is the main issue that is happening basically the library installation has not happened properly so this is the main problem over here but anyhow the dev and the qa team are fighting for anything now this was this used to happen before con before using containers you know this kind of issues were usually happening whenever we really want to move our application or deploy our application in some another environment we had to install all the dependencies and configurations manually right and this is a major major issue because the team who is basically doing this setup right they may miss some of the libraries they may not have that specific knowledge and because of that they may miss lot of important important mergers or not mergers i'll say but important part of that entire web application or any kind of application that they are deploying right now what happens if we use containers and this entire issue is now fixed with the help of containers so first of all we'll try to understand what is the definition of containers okay so containers is nothing but it is a way to package it is a way to package just a second it is a way to package applications with all the necessary dependencies and configuration so very simple definition containers are a way to package application with all the necessary dependencies and configuration okay now once you make this package this package has a very important property,358,0,0,8vmKtS8W7IQ
4,it is a basically a portable artifact what does portable artifact basically means we will be able to easily share easily share and move this package to any environment package to any environment super important that basically means once we make this package with all the necessary dependencies and configuration these are the very super important keywords we will be able to share and move this package to any environment right now coming to the third and the most important point this makes the development and deployment this makes the development under deployment more easy and efficient and let's say easy but at least efficiency will be there easy why i'm saying in terms of making sure that all the configuration are there or not okay so this will definitely makes the development and deployment more easy and efficiently and in sync okay super important sync also is very much important because all the developers will be working and they will be able to do it in an easy way now let me give you a very good example of a container okay let's say you are staying in house a okay let's say you are staying in this house a so this is a rented house and then you suddenly bought a new house which is house b okay now when you bought this house b you really need to shift from house a to house b and you know in house a you know there are a lot of things like furnitures tvs you know so there are like tv furniture washing machine you know uh here you basically also have kitchen utensils many,358,0,0,8vmKtS8W7IQ
5,things as such right many many items are basically present over here now if you really want to shift from how say to house b it will not be feasible that you take each and every item like each item and just go to house b and just put it over there and then come back again and take the other item you cannot take this item separately because there are chances that you may miss some of the items so what we can do in this particular scenario and obviously uh you when you are also shifting first of all you will take all these things okay you will take all this furnitures tv items and you'll put it inside a container let's say a washing machine kitchen utensils okay uh let's say you have bed you have a different different thing clothes right you will put this entirely inside a container container basically means your packeting packaging it completely right you're completely packaging it you're putting it inside this you'll put this container in one lorry or truck or some big vehicle and then you'll slowly ship this over here you will move this container over here and then you will unpack it right you will unpack it and you will again place all the furnitures back to your house right in your house b so you will unpack it over here so where we are packaging it and here you are unpacking it right so this is the same concept with respect to containers also so in containers suppose you are developing an application so let's say you are developing an application over,358,0,0,8vmKtS8W7IQ
6,here this specific application has lot of let's say this is my application whatever dependencies you have in this application you will try to put all the dependents over here let's say i require some python version okay i have some uh tools installation that is that i need to do so all these packages will get installed over will be put up over here in the form of base image in multiple layers which i will be talking how this will be and this will be packaged as a container now once this is packaged as a container let's say from the dev environment it is packaged as a container now we can take the same container go to the qa environment let us say this is the qa environment and i can run this container we can run this container over here with all the dependencies available within this package run run the container so we can take this container over here and just run it and automatically how in the dev environment it used to run it also run in the queue environment similarly we can take it from we can take the same container and we can also do it for the production also in a similar way we can also take this in the production environment and we just have to run this particular container over here also and because of this there will be no issue with respect to dependencies or with respect to any other packages why because we have already containerized this entire application okay so this is the concept with respect to container now you may be,358,0,0,8vmKtS8W7IQ
7,thinking christian what is dockers okay now let me just show you what exactly is dockers by just going to its web page okay so so here what is docker okay so docker is an open platform see it is an open platform for developing shipping and running applications and this applications is nothing but it is just like a container okay so we are going to use this open platform for doing all this process basically packaging the entire web application moving it shipping it to different different environments docker enables you to separate your application from your infrastructure so that you can deliver software quickly uh with dockers you can also manage your infrastructure in the same way you manage your application so in short c docker definitely helps you for speeding up your development process and your deployment process okay or you'll be also able to test it quickly and deploy your code quickly so here it is all about dockers we have uh we have basically discussed about the containers but in short if i talk about dockers it is an open platform which will actually help you to create this kind of containers okay now in the upcoming video we are going to see some more differences between containers and virtual machine we're also going to understand what exactly is docker image you know uh and what is the difference between docker image versus docker container and then finally we'll be doing the installation of dockers and we will be implementing some of the amazing things we will see how we can dockerize or create containers through it okay so guys,358,0,0,8vmKtS8W7IQ
8,we are going to continue the series on docker and in this video we are going to discuss about docker image and containers now let's say that this is my entire application and this obvious basically has lot of dependencies so let us say it has lot of dependencies and configurations so in order to convert this application into a container right what we have to do is that we have to containerize or we can we can also say dockerize this entire application now in terms of container what we do is that we basically create layers of images okay now what are these layers of images i'll just talk about it so let's say i'm going to create multiple layers of images let's say the base image will be one linux base image will have a linux base image and the size will be very very small again you have different different version of linux also which you can probably take it as the base image and let's say uh in this i have one application that is my sequel or mongodb right now this mongodb can also be added as an image over here okay so mongodb image will also be added over here so in short a containers is nothing but it is a combination of layers of images ok layers of images combined so let us say if this is linux and then i may have some more dependencies let's say i may have python 3.7 installed on that so this will be another image another image which will be taking care of installing python 3.7 suppose if i really want,358,0,0,8vmKtS8W7IQ
9,to install anaconda so this can be another image which will also be added as a dependency in this container right and they can be many many different dependencies and configuration we can which we can add inside this particular container and all this is nothing but it is nothing but images right now let us understand what is the difference between image and container so if we take this entire layers of images then this is exactly called as docker image okay super important guys please try to understand the difference uh this difference many people are confused with with respect to image and containers whenever i am creating the containers whatever images i've used all the layers of images with respect to all the dependencies if i combine all these things together then this is basically called as a docker image okay and once i take up this docker image and probably run this docker image okay if i run this then internally what will happen is that it will first of all create a container and this container is nothing but it is an environment okay and within this environment considering all the dependencies it will try to run this application okay it will try to run this application right all the dependencies will make sure that everything will get installed within this environment and then this application will get started okay now let's go ahead and list down some of the important differences between a docker image and a container okay so here first of all let me write it down here i have docker image on the left hand side and,358,0,0,8vmKtS8W7IQ
10,i have container on the right hand side so first of all uh the first basic difference if i talk about docker image is nothing but it is the package or the artifact or artifact and we know that we can move or share this particular artifact or package move or share this artifact right in different different environments okay so whatever environment in your machine in my machine uh wherever you basically want you can basically do it okay now once i take this docker image and the first step if i take this docker image and run it and run it okay first of all uh what it will do it is that it will do is that it will actually start the actual application it will start the application so let's say it is going to start the application and in order to run this application properly what it will do first of all it will create a container install all the dependencies within the container this container can also be considered as an environment which will make sure that all the dependencies is basically created uh in that and then we will be able to run this entire application over here right so in short this is my docker image once i run it it is in short going to create a container with all the dependencies installed over here right all the installed dependencies and this is specifically the environment where my docker image will be running okay so this is the difference between docker image and container i hope you are able to understand this simple thing uh going forward,358,0,0,8vmKtS8W7IQ
11,i know still i have not shown you any practical things but as we go ahead i will try to show you a lot of practical examples before that we really need to do the installation but one important point that we really need to discuss uh is that what is the differences between container and virtual machines because can we can also do the same thing with the help of virtual machine but there is a slight difference okay usually it is docker images are very small okay very small and why it is very small will try to understand the reason going forward you know when we are going to understand the difference between dockers uh containers and vms now before we go ahead and learn more about dockers we really need to know the basic differences between docker and virtual machines as you all know docker and virtual machine both the main task is basically to perform virtualization now with respect to any os like let's say that in this specific os the uh the system the windows machine that i'm working on if i probably install docker how it is going to virtualize my os and when i probably install virtual machine how it is going to virtualize my os so both the things we are going to discuss and we are also going to find out the main differences now before we go ahead and understand the differences first of all we need to understand how the operating system is basically made up of right so suppose i take occupating system it can be a linux windows or mac any one,358,0,0,8vmKtS8W7IQ
12,of them okay so if i probably consider the operating system first of all let's say in the base i have hardware the hardware can be cpu ram anything as such on top of it you will be having something called as os kernel okay that is the operating system kernel and this kernel is basically responsible in communicating with all this kind of hardware so this is basically my layer one and if i consider one more layer on the top of the os kernel we have something called as application layer okay we have something called as application layer now in any operating system and this is probably my layer 2.,146,0,0,8vmKtS8W7IQ
13,okay and this is what it usually happens with different different operating system let us consider linux as an example right now you know that linux we have different variation of linux right we have linux ubuntu we have linux red hat you know what is the basic differences between all this particular linux is that the ui changes so with respect to ui most of the changes actually happens in the application layer right the kernel will be almost same right now if i consider docker or virtual machine which layer of the operating system it will virtualize okay so let's go ahead and let's see with respect to the docker so i've drawn the same diagram over here if i consider docker right this docker will be virtualizing the application layer of the operating system that basically means docker images you know will be able to communicate with the os kernel okay similarly in the case of vms vm basically virtualizes both the application layer and the os kernel layer okay that basically means whenever we install the vms at that point of time whenever we wish to install the vm it will have its own application layer and it is also going to have its own os kernel layer that basically means a vm is a complete separate subsystem you can say which has been installed in the host os right so this will have their own os kernel whereas in the case of docker it just is an application layer let's consider like i have a specific image i pull that image install it this image will be able to communicate with,358,1,1,8vmKtS8W7IQ
14,the os kernel and this os kernel is of the host whereas in this particular case vm has its own os kernel and uh you know it will probably this os kernel will be also communicating with the hardware so whenever you install vms you also need to assign some amount of resources from the system like hard disk let me ram separately to the vm so this is the basic differences between if i talk about the docker and the vms but one important point is that what is the advantages and what is the disadvantages with respect to this first of all if i consider with respect to dockers since we are just using we are just virtualizing the application layer docker image size is usually smaller image size is usually so smaller right this is a super important point okay whereas in the case of vm if i probably consider vm the size will be big because it virtualizes both it has both application layer and the os kernel layer so size will be huge okay probably in gigabytes this may be probably in megabytes okay something like that now similarly if i go with respect to the next thing is that if i talk with respect to dockers right we can dockers container start and run much faster start and run much faster when compared to the vms why this happens just imagine guys because docker just has an application layer application layer it will be communicating with the os kernel of the host whereas in the case of vm it has its more is it has its own application layer and,358,1,1,8vmKtS8W7IQ
15,the os kernel so just to start both of them it will definitely take time so vm is comparatively slower comparatively slower one more important point is with respect to the compatibility okay now this is a super important point with respect to compatibility we can compatibility so we can install any vm any vm we can install let me write it down completely in depth so that you'll also be able to use this notes whenever you go for the interview we can install vm on any os this is super super important okay on any os this is a very important interview question you can install vm on any os so there will never be any compatibility issue but in case of docker images there may be a compatibility issue so we may find a compatibility issue that basically means what let's say i have a windows machine okay let's say this is a specific windows machine in this i cannot install docker images that are of linux let's say right so here if i try to install uh and probably try to communicate with the os kernel this will not be possible yes it will be possible if the os kernel supports so right now if you consider windows 10 and greater that windows 10 right this usually supports even linux images also linux talker images also but if we consider windows version which were less than 10 it were not supporting this okay so the compatibility issue has been fixed uh because the update in the os kernel has happened after windows 7.,347,1,1,8vmKtS8W7IQ
16,okay windows 10 and below windows 10 it will definitely not work so these are the super important points the first point is that again let me revise is that docker image size is usually smaller obviously it'll be smaller because it just virtualizes the application layer that basically means we can just run the application layer away and it will just communicate with the host os the second point is that docker containers start and run very faster very much faster when compared to vms obviously because it does not have its own os kernel and the third part is compatibility issue on vm does not exist but in docker images compatibility issues exist okay so i hope i have covered most of the points with back to docker which is virtual machine now in the upcoming videos we'll try to install docker and we'll try to play with it and we'll try to do many tasks as such so guys we have covered so many topics related to dockers we have understood what is dockers what is the difference between dockers and virtual machine we have understood about containers docker images and many things now let's go ahead and start working on docker so basically in this video i'm going to install the dockers so to begin with docker can be installed in different operating systems like mac or linux or windows okay so first of all just go to docker.com and if you see over here here all the information regarding dockers is actually given so here you can basically search for it you can see dockers mix development efficient and predictable it,358,2,2,8vmKtS8W7IQ
17,takes away repetitive mundane configuration task and is used to out the development life cycle for fast easy and portable application development and everything i've actually explained with a lot of beautiful examples right so first of all let's go ahead and click on get started now once i click on that get started two things you basically require uh one is docker hub that is not compulsory but at least we'll keep docker hub over here and for this if you want to create a sign up create it otherwise even sign up is not compulsory over here but we will go ahead and install docker desktop okay now this docker desktop can be installed and windows mac os and even in linux okay so first of all here you can see all the options with respect to this you can install for mac intel chip you can also install for mac apple chip i think mac intel chip is the older version of mac i guess but now i think you have mac apple chips yourself then you also have download for linux so for different different distribution you go so you can download so now i'll click on for windows here you can see as soon as i click on windows here you'll be able to uh see that the windows installation is basically happening for dockers for windows is basically happening okay installation is happening but before we go ahead and install this okay they are some of the important things that you really need to note down because there are some requirements whenever you need to install dockers let's say for,358,2,2,8vmKtS8W7IQ
18,dockers if i really want to install on windows you know here you'll be able to see that there are some kind of system requirements okay first is wsl2 backend so what i'll things you basically require it shows windows 1164-bit home of pro version of windows 10 64-bit home or pro to a 21 h1 version okay or higher okay or an enterprise or education so only this kind of configuration like suppose if you have windows 10 or windows 11 with 60 fitbit and this is either home or pro version then only you'll be able to install dockers over here and the other requirement is that the following hardware prerequisite are required to successfully run wsl2 on windows 10 or 11 okay so you basically need to have 64-bit processor 4gb system ram bias hardware virtualization support must be enabled in the bios settings that basically means what suppose if i go and open my task manager and if i probably go and click on sorry just a second if i go and click on cpu here you'll be able to see one field which is called as virtualization this needs to be enabled okay if it is not any enabled in your in your system then probably what you have to do is that shut down and restart your machine and continuously press f11 right by that what will happen your bios setting will get enabled and from there you have to make sure that you have to enable your virtualization okay so please make sure that you have this uh because and again if you are using different different processor uh,358,2,2,8vmKtS8W7IQ
19,in your laptops or in your desktops there are different different ways of enabling your virtualization okay so based on the type of processor you have to actually do is i'm having amd ryzen processor so usually like over here in amd ryzen by default virtualization was not enabled so i had to restart my machine press f11 open my bias settings and go over there and enable my virtualization you can definitely search in the google how to do it you will be getting a lot of articles for the same thing okay now the second thing is right uh by hyper-v backend and windows container also you can basically do the installation so these are the system requirements again windows 11 64-bit 1064-bit for windows 10 11 home see the system requirement in wsl ip hyper-v and container windows feature must be enabled now what does this basically mean if you probably go and search for uh let me just see over here here you'll be getting an idea so i will just go and open my windows features just give me a second so i will just go and open it so now if i probably go and search for windows features so here uh here you will be able to see when you turn your windows features on and off here there will be something called as hyper-v so this needs to be enabled okay and when you enable this enabling this that basically means that is a requirement that is given over here right hyper-v and container windows feature must be enabled right so i have actually enabled this part also,358,2,2,8vmKtS8W7IQ
20,okay hyper b okay so please make sure that you also have this particular setting done and following hardware are required to run client hyper-v on windows 10 we'll learn all about this so again there is 64-bit process 4gb ram and biaside leveler and this will be basically and again your buy setting virtualization needs to be enabled okay so but anyhow if you have all these things by using this wsl to backend also we can actually do the entire thing okay so the installation is basically with respect to windows okay now if you have this requirements and again you may be asking krish then what about the people who are having previous version of windows for them how the installation will happen so there was a tool something which was called as docker toolbox now right now the daughter toolbox is not there it has been deprecated it is not getting provided now for the people who have you know less than windows 10 version or who's not having this matching uh windows version uh operating system what you can do is that you can download a virtual machine within your system and then you have to install the docker desktop inside that particular virtual machine again it is a very tedious process guys so my suggestion would be that if you really want to learn docker definitely have this version so again uh with the help of vms also you can do it that basically means you install the vm in your system and inside that vm you have to download the docker desktop okay based on the different different operating,358,2,2,8vmKtS8W7IQ
21,system now this is with respect to windows let's go and see with respect to mac so here mac with intel chip mac with apple chip whatever mac version you have so here you also you can see all the versions with respect to um you know the mac os what version it needs to be at least 4gb ram a virtualbox period to this this is required and then in order to install it just go and click either on this a file will get downloaded like dmg file and then double click on this and all the steps have been given over here okay so just follow this particular step and do the installation and similarly with respect to linux in linux you know that we definitely have different different distribution let's say your distribution is like debian fedora ubuntu aura go and click on ubuntu let's say it is ubuntu just follow the steps first of all the prerequisite is that to install docker successfully you must meet the system requirements check out the system requirements go over here the system requirements is 64-bit kernel and cpu support for virtualization kvm virtualization support this this is there okay some genome or kd in desktop environment uh and again this will depend on various distributions that you are specifically using and at least 4gb of lab okay docker desktop for linux runs a virtual machine uh for any information on why why do our desktops run a virtual machine you can also find out a reason okay so here you can basically find out this okay and as as said you know when we,358,2,2,8vmKtS8W7IQ
22,start this process this requirement this is there then first of all we uninstall the tech preview or beta version of duster based off our linux run this okay uh you need to have a 64-bit of either ubuntu jammy gel fish or ubuntu impeach entry now this particular steps you definitely need to follow to install all the things okay but now let me just give you an example how you can go ahead and do it with windows and again based on various distribution on linux you can basically do it okay now in my system i support i have this windows 10 64-bit hole mod pro so i basically have a pro version so i will be just clicking on docker desktop for windows here you'll be able to see an exe file will get installed will be downloaded just go ahead and click this double click it and just keep on pressing in next next next next next next already all the installation will happen but before that make sure before you do this installation right so virtualization needs to keep needs to be kept as on right so this needs to be enabled over here okay and that is only enabled when you restart your machine in the bias setting and from there you have to enable it okay guys once the installation of the docker desktop will get completed you will be able to see a icon over here which will say docker desktop running okay so this specific icon will be available over here and once you go into the search bar and search for docker desktop and just,358,2,2,8vmKtS8W7IQ
23,open the specific app here you will be seeing that this kind of app will get opened right and in this app you will be having something thing like containers images volumes dev environment what all are this when we run a specific container you will be able to see the list of all the containers running over here whenever you download any images or you build any images all the images will be shown over here right now let's say that i have i have some images that i have actually created so here you can see boston underscore image mysql and alpine so i had created some of the image over here so here you will be able to see that those image lists will be available similarly with respect to dev environments i will be also discussing as we go ahead right now yes if you really want to run a sample container just go and run this but before that let's say whether our docker has been installed correctly or not just open your command prompt and just type dockers okay once you type docker uh you'll be seeing all these commands will be there right so like this builder build this this if this is working fine that basically means your docker has been installed successfully okay so uh with respect to docker if you really want to find out the version it is very simple just write docker minus v and this is the current version of docker that is installed in my system okay so uh i hope you got an idea about how to install a docker in,358,2,2,8vmKtS8W7IQ
24,windows in linux uh in mac if and always remember the first thing is that your virtualization needs to be enabled in windows okay because in previous version of windows initially in dockers we had something called as docker toolbox but now it is not available it is completely deprecated so you have to do this otherwise the other way is that you have to install vms in your machine and in that particular vm you have to install the docker desktop and then probably uh use that but again there'll be a lot of configuration issues there also so it is better that you have a windows 10 11.,143,2,2,8vmKtS8W7IQ
25,similarly i've shown you how to do the installation in linux and mac definitely follow this documentation this documentation is super super important whenever you are learning anything right guys so this documentation will be giving you all the steps that is basically required to do the installation so guys till now we have seen how to pull a docker image from the docker up repository uh we also saw how we can actually run that as a container in our host machine we also saw a lot of commands like docker ps docker images docker pull docker run you know if i really want to see that how many containers are running i can definitely use docker ps similarly if i really want to see that how many images are there i can actually use docker images now what we are going to do is that we are going to create a docker image and i'll take that specific docker image and we'll also try to push it into the docker hub repository so whatever application i'm actually developing over here will dockerize it completely we'll create a docker image we'll run it we'll all i'll also show you how to build this particular docker image and how to run it as a container and later on we'll also try to push up push this entire application into the docker up repository so that you can also download this docker image and run it in your own local machine so let's go ahead and let's see so here i've actually developed a very simple flask application education in this application you will be seeing that,358,3,3,8vmKtS8W7IQ
26,okay this is just returning hello world okay so let me just run this application so here i just have two file one is app.py and one is requirement.txt okay so let's go ahead and let's run this so i'm just going to run it so let me clear the screen first of all and if i run python app dot py so here you will be able to see that okay uh my uh my application is running on 127.0.0.1 and 5000 is support so once i execute it you here you can see that yeah it's it's displaying hello world okay so this is a simple web application over here with the help of flask and for this in order to run this application i require one library that is called as flask itself which i'll be using uh um you know which i'll specifically using in this python web application now let's dockerize this application so i first of all i will do ctrl c i'll come out of this uh now i'll go to my app.py first of all one change that i really want to make is that in this app dot run right i will also try to provide my host so my host will be let's say 0.0.0 and i'll also talk about the importance of this particular host that is 0.0.0 along with that i will also give my port so let's see the port that i'm going to assign is nothing but 5000 so i'm just going to assign 5000 as my port over here okay so two information one is host and one is port i,358,3,3,8vmKtS8W7IQ
27,have actually provided it so once we run this let's check whether it is running fine or not now because we will definitely be requiring a host a ip address now 0.0.0 what it will do is that it can access your local address also it can also access your local host address also so suppose my ip let's say if i go to my command prompt and if i probably search for ipconfig so here you will be able to see that my local address is nothing but 1.92.168.56.1 i will also be able to access my local ip address and i'll also be able to access my local host address okay so let's go ahead and run this and let's see whether it is true or not so here you will be able to see that yeah it is running on 0.0.0.5 colon 5000 so what i'm actually going to do i'll just open my command prompt and i will just copy this ip address over here and let me just open my browser and see whether everything is working fine or not so here i'm just going to use my local ip address with 5000 as a port and if i run it it here you can see hello world is also there and i can also run with 127.0.1 0.1 and i can also run it with localhost right so it is also going to give me the same thing so that is the importance of 0.0.0 ip address this can also be an important interview question for all of you okay now what i'm actually going to do is that i,358,3,3,8vmKtS8W7IQ
28,made this changes now let me go ahead and create my docker image now first to create a docker image i have to create some files the first file that i'll be creating which is called as docker file so this is the visual studio that i'm actually using okay over here so in this docker file what i am actually going to do is that i'm going to create i'm going to use some commands and with the help of this specific commands it will actually help us to create our docker image okay some of the important commands that we will be using is something called as from okay uh here i really need to specify a base image which i'll be discussing about what exactly is the base image as i said that the lower base image should be a linux kernel or linux operating system right so uh first command is from then i'll be using copy then the other command that i am going to discuss about is working directory and then we'll also be using a command which is called as run okay and finally we will be using a command which is called as uh cmd so this is the command that we are going to run in order to run our flask cap okay so these are the basic commands that we will be using one is from copy work directory along with that run command and uh the cmd command okay now instead of the base image i have to and this base image will be the first layer right and it should be a linux uh,358,3,3,8vmKtS8W7IQ
29,linux image in short and one of the linux image that i found out from the docker hub repository is nothing but python 3.8 alpine okay you can also use python 3.7 alpine so this specific base image i'm basically using and what it will do is that it will try to pull this particular image from the docker hub repository next is the comma copy command now this copy command is super important because this copy command uh it is just saying that from my local repository that is the current working location this both the files should be put up inside this base image in one folder let's say this folder is app right so in short what i am doing is that in my docker image i am creating an app folder and i'm making sure that everything all the files that are present over here i'm just going to copy it inside this app folder this app folder will get created in the container or docker image right and this dot basically says about my local repository then after that i also have to set up my working directory obviously if i am copying all my files to this app right then what will happen is that my working directory should be this slash app okay and then after this i will also be installing or running this particular command that is pip install minus r requirement.txt now why do you do this because all the dependencies with respect to all the libraries will be present over here and i really need to install all these libraries so for that i am,358,3,3,8vmKtS8W7IQ
30,writing pip install minus r requirements.txt and finally the command that we are going to run is python app.py now when i do this in short whatever uh you know the files that are present in app in this app folder will be this app.py and requirement.txt right so it will first of all do all the requirements all the installation of all the libraries and then it will run this app.py file by using this command that is python app.py so this is uh a docker file that is created and as soon as i build this uh entire file or build this application you will be seeing that it a docker image will get created with all this information right so inside this image you'll be able to find out this app folder along with that all these files will be present inside it okay so let's go ahead and let's try to build this now my app.eui file is ready now in order to build it please focus on the commands that we are going to use the first command that we will be using for building is nothing but you'll be seeing over here let me just close this first of all this is a powershell so i don't want to use a powershell right now okay let me just close this and then we will try to see it and for this i'll just take a command prompt so here is my command prompt uh the first thing in order to build the images i will write docker build we use a minus t uh as a parameter over here and,358,3,3,8vmKtS8W7IQ
31,then we give our docker image name the docker image name will basically be let's say i will be saying welcome app so suppose let's say this is the app which will basically welcome everyone and then i will be using this dot over here this is the entire command to build the docker file so docker image so once i execute it here you will be seeing the building process has started now everything step by step all the things will happen the copy command will run the pip install will be running right so if i probably see over here everything from the first step right so here you can see copy command has run then the working directory is sent to uh is set to app and then the requirement.txt has been you know this particular our command is basically getting run okay now if you want to see uh whether your docker images has been built or not so what you can do you can write docker images over here and obviously you can see welcome app boston images one uh one docker a docker image that i created long back but right now the new app that we have new docker image that you have created is welcome app okay now i'm just going to clear the screen now it's the time to run this specific docker image as a container now there are two things with respect to this first of all understand if i'm running this docker image two important information that needs to be given is your host port and your container port so that mapping needs to,358,3,3,8vmKtS8W7IQ
32,happen so i'm going to write docker run minus p i'm not running this in a detached mode but instead i will be running this so that i can also see some logs and first of all i'll be writing minus p where i will be mapping all the ports so let's say this host container port is 5000 and uh you know the sorry host port is 5000 and the container port obviously you can see over here it will be 5000 i'm just assigning it to 5000 and i will probably be running this by calling my um you know the docker image itself right so docker image is nothing but welcome app so once i do this here you'll be able to see now my docker will run now here you'll be able to see two ip okay one is 172.17.0.2 colon 5000.,189,3,3,8vmKtS8W7IQ
33,now if i probably run this now what is happening over here you have to probably check it out here as soon as this uh you know this url gets hit you know it will not run why it will not run there is some reason behind it now this ip is nothing but it is the ip present inside my container okay and it is basically running in this 5000 port obviously we cannot access it but we can access it from our host container through a specific port now what will be that specific port so here you can see one address has 127.0.1 colon 5000 so this is with respect to my local host right so if i execute over here and if i see it here you will be able to see that my file my my entire url is running similarly if i do it with my local host also since i have assigned 0.0.0 this should also be running properly fine right even i can also try it my localhost so this will also be working fine so here what is actually happening is that this application is running inside my container okay and with the help of port from my local host uh for my local host or for my local ip address by using this port i'm accessing that this application which is present inside the container okay now what i'm actually going to do over here as soon as i run this here i will again create another command prompt and let me just write docker ps okay now if i write docker ps sorry docker ps,358,4,4,8vmKtS8W7IQ
34,okay uh spelling mistakes not a problem so if i write docker ps here you will be able to see how many containers are running obviously this container is running and one thing about the port also you will be seeing over here is that here you have 0.0 0.0.0 5000 and this is basically a container port right we are accessing from the host port to the container port obviously uh from this particular terminal you can see that this probably may be the ip inside the container but definitely we cannot directly access it we can only access this through our local host by using the specific port because while running the while running the container i have assigned this specific port to it okay so this 5000 is basically the container port this 5000 is my host port and with respect to the host port since i have used in myhab.pui as 0.0.0 that basically means we will be able to access this containers with the help of my localhost or 127.0.0 0.1 or i can also use local ip address for accessing it okay so right now i have actually created my container so i'm sorry my docker image and the container is also running absolutely fine so finally if i really want to stop it all i have to do is write docker stop docker stop and let me just open the docker desktop also so here also you'll be able to see the container is running welcome app similarly here also you'll be able to see the image welcome app okay now what if you really want to stop this,358,4,4,8vmKtS8W7IQ
35,container just write docker stop and just write this container id and it will automatically get stopped now here you'll be able to see that this has got stopped right over here right similarly i'll be able to find out docker images now docker image over here is absolutely fine now what i'm going to do is that the next session will be quite amazing uh in the next section what i'm actually going to do i'm going to deploy this particular image in the docker hub repository okay and once i deploy it then what you can do is that you can also pull this particular image and run it in your run this particular image docker image as a container and you can also access it with your local machine right by using the same same port that is 5000 so that is what we are going to do in the next video we'll see how to push this in the docker the tree so guys in this section what we are going to do is that whatever docker image we have actually created will push that into a docker hardware okay now to do that first of all just go ahead and login into your docker hub so you have to definitely login into it always remember your username and password for this and just by using the command line itself will try to you know push our um you know docker image like how i have pushed over here like ml underscore model and sentiment underscore analysis into the docker hub repository again guys this is not a private repository this is,358,4,4,8vmKtS8W7IQ
36,a this will be a public repository so if i am deploying it uh if i'm pushing it over here you should also be able to pull in your local machine and run it as a container okay so let's go ahead and let's see the step-by-step process first of all uh first of all as usual go ahead and just go and do the docker login so here you can see that once you go ahead type docker login it will first of all ask for a username type your username m then press enter then it will go and ask for your password right now i have already logged in into this so it is just using the existing credentials okay so clear the screen over here now the next step that you really need to do right now my docker images if i go and see it is something called as welcome app right i have to rename this particular repository or sorry this particular docker image okay so how do i rename it there are two ways first of all i can remove this okay and i can again build it so let me just go and show you an easy way first of all docker image just go ahead and just directly remove it by using minus f and just write welcome app okay then once this is removed if you go probably go and see in the docker images here you will be able to see no image is present now again go ahead and build it so i will say docker build minus t my name of the image,358,4,4,8vmKtS8W7IQ
37,but before writing the name of the image this is super super important that you really need to understand stand if you see over here guys see the naming convention should be your username slash the docker image name okay so this is the naming convention that you should follow okay because that will actually help us to understand that which username it is and where we really need to push the docker image right so here i'm just going to write krishna x06 and here will be my image name that is welcome app okay and then i will write dot and automatic to the build process will start again so once the build up will happen the entire image will get created okay so this will be my docker images if i go and see here you'll be able to see the welcome map now let me clear the screen so again let me just go and show you the docker images so in the docker images here you have something like krishnag06 welcome app okay now after this uh this is one of the way the other way is that you can directly use docker tag uh and you can change the name of the image also okay so here i'll be using docker tag and let's say krishnaik06 is your docker name right so it will be welcome app over here let me change this name to something else so first after this you really need to write one more thing like this let's say krishnaik 0 6 i will change this to welcome app one right so let me do this,358,4,4,8vmKtS8W7IQ
38,and let me change it now once you execute it now if you probably try to see docker images here you'll be able to see one more docker image is basically getting created with welcome dash app one okay so this is how you rename it rename your docker image from the older name to the newer name but always make sure that if you really want to push this into the docker up repository you have to put your username on front of it okay but anyhow i'm going to push this krishna06 welcome app to the docker hub repository so first of all i'm going to clear the screen and in order to push it i'm using a command which is called as docker push and this will be my krishnag06 slash welcome app so this should be my uh you know the the docker image itself along with that will be used using a command which is called as latest okay now why i am specifically writing latest this latest will basically show the tag over there if i go and click on images over here this tag is can be provided with different different versions also like it can be version 1.0 1.1 so any new updates are basically coming we can again create the same docker image with a different version okay and that is basically useful for making sure that your docker image keeps on getting updated with some new changes and all okay right now i'm just going to use this tag as latest so let me go over here and this was the tag that we have specifically,358,4,4,8vmKtS8W7IQ
39,used now here i'm going to write docker push krishna x06 and this should be my welcome app that is my docker image name and the version which will be latest one so i'll just go and press enter the push is going and preparing preparing waiting everything is going to the docker hub repository and probably once this entire things happen the welcome app docker image will get created and the most interesting thing will be that you can also download it and you can run it as a container see guys how simple it is really okay so yes this will take some amount of time because obviously this is somewhere around 58.2 mb you know the boston image this is a data science project so that is the reason why it has become 1.8 to 2 gb okay now here you can see that okay perfect all the steps have happened it has been pushed now let's go ahead and see over here now if i probably reload it you will be able to see the docker hub that is welcome sorry the docker image in the docker hub specifically with respect to my username so here it is just go and click on this you'll also be able to see i've used this particular command which is called as docker push okay and uh if i go and see on tags here also you can see this if you also want to pull you just have to use this particular command okay once you use this command you'll be able to use this and run it in your own local machine so,358,4,4,8vmKtS8W7IQ
40,let me do that also if you want so i'm going to clear the screen and probably see my docker images let me uh do docker image remove i'll just remove one of the image let's see uh minus f krishnaik 0 6 slash welcome api um sorry welcome app with latest so if i'm just trying to remove it now let's see my docker images and then we'll try to pull it okay so right now nothing is there let me go to my browser let me copy this and let me just paste it over here so once i execute this command now you'll be able to see my image will again get pulled so here you can see pulling from krishnaik welcome app now if i go and see my docker images here it is so krishna06 welcome app now all i have to do is that run this i know how to run it so i'll be using docker run minus p and let me use this time minus detach mode also so minus d and p okay and i will be assigning the port as 5000 of the host to 5000 as the container port along with that i will just write my krishnaic06 um welcome app and this will be my latest and once i run this i think everything should run see it is in the detached mode so you'll just get this specific information okay and now if i go to my browser and check it just reload this same thing reload it with your local ip reload it with your localhost you'll be able to get this,358,4,4,8vmKtS8W7IQ
41,entire information and that's it that is how simple it is to basically create your docker image and all now any requirements like let's say when you're working in a full-fledged project right any any libraries that you want like pandas number you can just keep on installing over here everything will happen we just need to list down you have to have your html file and all all those things are there machine learning models and all all the most important thing is this particular dockle file now along with this guys you can do many things with dockers even you can install different different databases within the container and you can directly use it okay so guys in our previous session we have already seen how to create a docker image how to run it as a container and finally we also saw that how to deploy that in the docker hub repository so that you can also pull it and run it in your local machine right now in this video we are going to discuss about a very important topic which is called as docker compose now let's understand why do we use docker compose but before that i'll give you a scenario let's say that i have on my web application which is specifically running in a container so all the default configuration that is required with respect to the port and the requirements that is needs to be installed before we run it as a container everything is put up over here in a specific docker file okay so here obviously you will be having a docker file right over,358,4,4,8vmKtS8W7IQ
42,here right now let's consider that if this particular web application wants to use services like mysql redis or mongodb now you know that if you go to the docker hub right you will be finding different different images for mysql for redis for mongodb so it will not be possible just to run mysql inside this container so this needs to be run as a separate container this needs to be running as a separate container and similarly this needs to be run mongodb also needs to be run as a separate container and then this web application can basically interact with mysql reduce or mongodb and basically store or retrieve the data redis is specifically used for the caching purpose which will basically increase your performance of the website now why exactly stopper compose used now see guys docker compose is a tool for defining and running multi-container docker applications now in this particular scenario here you can see that you have multiple containers right you have my mysql container ready container mongodb container so docker compose with the help of docker compose we will define the entire configuration so we will basically create a file which is called as docker compose dot eml file okay and then we can join down all the configuration requirements that is basically used to run all this multi multi-containers and they will also be able to communicate with each other okay so this in short is about dockery ml sorry docker compose and let's go ahead and see a practical example and then you will get an idea let's say that this is my app.py and this,358,4,4,8vmKtS8W7IQ
43,app.py has a code again this code is not that important but you need to understand how do we use docker compose so here here you will be seeing that i am using a redis and this is a python application that i've written so redis usually helps us in caching information and it will be caching within the memory itself in memory caching so here uh what we'll be doing is that we'll be running this entire code and we are making sure that the cache is implemented over here by using this specific code okay so this will be my define hello and whenever i hit this it will say hello crush i have seen this many number of times so this count will basically be saved with the reddish okay now the first thing is that whenever you want to create a docker compose file now here one thing is that redis you are using this radius is present as an image in the docker hub which i will again show you uh and then you'll be able to understand it okay now first of all i will go ahead and create a docker file now this docker file will be for this particular application container okay so here i will go ahead and write my docker file uh in my docker file uh you can again the steps will be almost same whatever things we have actually used we have to use those specific commands okay but today in this video i also want to convert something about environment variables so one command is that from let's say i'm going to use,358,4,4,8vmKtS8W7IQ
44,python the 3.7 and this will be in alpine mode alpine version basically the linux base is this why we use this because it will be very small in size and can be quickly created as a base and run it as a container itself then finally i will go ahead and write my work directory let's say this is my work directory and obviously the work directory that i really want to keep is you can write anything any folder like you want app or you want code any folder that you want so this will basically be my working directory then i will copy my uh from the current location that is this first parameter that you basically indicate is that from the current location whatever file are there i'm just going to push it inside this code folder right so code folder since i have subjected to my working directory over here so i'll be inside that particular code location right so code folder location so here you can basically see that i have written dot okay so next step is basically just copying that specific file to over here now before this also i really want to specify some uh environment files right see in flask because this app.py is a flask file right we can run it through various ways we can directly use a command which is called as flask run but in that we need to set up some environment variables so let me just show with respect to environment variables in case of environment variables we have to write env and we can supply it in the form,358,4,4,8vmKtS8W7IQ
45,of key value pairs in case of flask i have to supply plus underscore app is equal to app.py file which is my file and the other one is that app.py and oops and after i press enter another environment that i usually put up is flash underscore uh not environment environment is not required but i will write run underscore host so i can also specify my host information when i am actually running the flash cap so this will be 0.0.0 okay so this can either be local machine or your local ip address you know anything it can be so uh this i have set up the environment and as soon as i write flask run automatically it will be taking up this app.py file and on which environment it has to or in which host it has to basically run which ip it has to run it has all this information so now this copy command i hope everybody has understood from a local repository we are just trying to copying it to this code folder itself which is present inside the docker image and then i'm going to run the pip install minus our requirement.txt because this is super super important ah so here one environment variable i've spoken about which is which i have not covered in my previous video and one more uh variable that i really want to talk about is something called as expose so here through this particular expose i can specify the port number that will be that will be exposed from the container so let's say if i'm writing 5000 that basically means inside,358,4,4,8vmKtS8W7IQ
46,the container is 5000 port will be exposed to the host environment okay and finally i will write my command which will be cmd this command is basically to run all the just a second so 5000 and finally i'll be having a command which will basically say two things one is flask flask comma run right so this is it uh so now here you'll be able to see that i'm going to just execute this at the end of the day flask run so as soon as this command runs this right first of all in the environment variable it will look for app.py and then it'll look for host and it will just get all the information it will start running it okay so i'm saving this this is what is my docker file that i'm actually going to create now coming to the next one is basically the docker compose file because i need to use redis in my application and this is one example with respect to redis probably you also want to use mysql and all uh i'll show you a way how you can basically use the docker hub for the same purpose okay so first of all i'll create a file which is called as docker docker compose dot yml file now once i have created this docker plot com docker dash compose.yml file understand why it is basically used so that we can run multiple containers and it will also be able to interact with each other so this is a tool that is used for defining all those things so first of all the field that,358,4,4,8vmKtS8W7IQ
47,we are going to use is something like version so let's say i'm going to just specify a specific version like 3.0 okay and again this needs to be followed a key value pairs in that specific way right the key value pairs with some space okay version the first thing is that i will just write services uh this is the syntax that we specifically use now with respect to services i'm going to use web okay um and inside the web uh let's say this web as soon as we write web this is basically going to create a web service and this web service is going to use the docker file of the specific application so here in the web i will the first command that i will probably write is something called as build so here you can see uh build right now from where you're going to do the docker build itself right so docker image build so here you can see that in the docker file we have actually already we have kept the working directory as slash code so inside slash code all this entire file will be present so i'm just going to write build as dot so that the build will happen from the same root folder okay and then i'm going to assign the ports in this ports um understand what all the ports are there and please make sure that you keep this indentation right this indentation is super super important uh if you do not follow this indentation then you may be getting a syntax error when you're getting a building for the when,358,4,4,8vmKtS8W7IQ
48,you're doing the build that time you will be getting a syntax error right so in this particular is let's say i'm just going to keep it as 5000 and this will also be 5000.,45,4,4,8vmKtS8W7IQ
49,so this is nothing by a host port to the container port that is what i'm using i can also use 8000 if i want uh all this information is there right so web build ports so we are going to also even assign the ports now coming to the next one now this is for this entire services is definitely for my docker file current docker file for my application now i can go ahead and write other web services like redis okay so now if i basically write redis and probably write my image name right so let's say i'm going to specify my image name the image name is nothing but redis right now this red is as soon as we write like this right it is also going to create another service or container of specific redis and this image when i'm writing this right it will it is going to pull this particular image from the you know uh from the docker hub repository so here you can see this will be my one service so one one image of web will get created the another image of redis will get created similarly i can go ahead and probably create my own image for my sql also okay so i can also go ahead and create the image for mysql so in this i if i specifically writing my sql and i'm writing like this i can then give my mysql image name like this and let's say this is my sql that i'm going to use so this will be another image that will get created within mysql this image,358,5,5,8vmKtS8W7IQ
50,will get created for redis and this will be uh uh image for the web which will basically using this particular docker file it will try to run all these things right so this is how we combine multiple services for now i will just show you an example with respect to web and redis now let's see how the output will basically come so here are two things uh i've already explained you about it you can also specify the image name if you want okay so like this if you want to specify the image name then also you can actually specify if i'm probably writing image is equal to redis sorry this is my um let's say this is my web web app okay so this is my image name for this particular web service and for redis my image name is this obviously if the web app will be built here itself from the local right so it is going to use the docker file so let's go ahead and execute it now okay guys now let's go ahead and open the terminal and let's see how to run this docker compose uh which will automatically be doing the build and running part so first of all i will just go and check my docker images and see whether any image is present or not so right now no images here you can see it is completely empty so let me clean my screen now i will be using docker compose and let's say i'm writing docker compose up okay so let's say if i'm saying up then automatically what will happen,358,5,5,8vmKtS8W7IQ
51,this entire yml file will get executed anyhow see i was running in powershell so i'm getting this coloring option let me do one thing let me just open something like my command prompt and let me quickly show you okay first of all docker images so here you can see and then i'm basically going to write docker compose and in order to run it i'll just write up so as soon as i do this all the steps will be going line by line so here you can see version this this image everything step by step here you can see redis has been pulled already at his build copies instruction is happening run pip install requirement is basically happening everything is happening perfectly fine and here now you can see the uh link right so right now the link that you can see by default uh uh you know whenever flask runs it will be running at 5000 port but you know that we have exposed it in 1000 puts so i am getting some error let's see what is the error redis attribute has no attribute reset retry okay perfect that basically means i have done some error over here let me just remove this line because i don't require this line okay now just checking it somehow okay not a problem so in order to stop it uh since i got some error i'm just going to open another command prompt and write docker compose stop okay now once i write docker compose stop here you'll be able to see docker will be stopped i'll also be showing you all the,358,5,5,8vmKtS8W7IQ
52,images that will be there okay so there was some mistake in the code so what i'll be doing here i'll again rebuild it and again i'll send it okay so now here if you go back to the command prompt again it has come out okay right now if i go and see docker images right two images will definitely get created one is web app and one is reddish right but we got an error so here you can see web app is there and ready is there this was the image name that i gave right so let me do one thing since i nearly rebuilt it again first of all i really need to remove these images so remove minus f if you don't want to remove this image we have to use the concept of volume which again i'll show you because in this kind of issues also we can actually fix this so again i'm going to use this same thing and volume part will see it in the next video okay and now i'll just going to execute it over here so done this is perfectly done i'm just going to clear the screen now if i probably go and see docker images nothing is there i've saved my file in app.py i made all the issues that fixed so it is already fixed now i'm going to write docker compose up so once i execute it the entire step will happen again now let's see whether it will work or not okay already exist now you can see whichever things are already existing right it will not going,358,5,5,8vmKtS8W7IQ
53,to download it again because somewhere in our local it will be there right now perfectly fine it is working you can see built also continuously happened and after that it is also running now if i probably execute this and come over here you will be able to see one browser which has something like this but you know that we have exposure to 8000 now i'll execute it hello krish i have seen 74 times because i had already implemented this and previously ran and so many number of times i tested it okay now here you can see that uh it's working absolutely fine as you go if you're running it for the first time you'll be getting it one and that is why if you keep on increasing if you keep on just reloading it this count will get increased this is happening to the redis caching okay just to show you how it is so if you keep on doing it continuously it will happen now let me show you one one important issue again now let's say that i want to just change this message hello chris i just want to write hello dockers okay and let me save it now in this particular scenario obviously i know that if i try to run it right it is not going to get updated again over here so for this i have to reload this entire thing again i have to stop the container and again i have to rebuild all my images and then again i have to reload it then only hello dockers will come right so this is,358,5,5,8vmKtS8W7IQ
54,a tedious process right and this thing can be fixed with the help of docker volumes okay so that is what we are going to learn in the next video but i hope you understood that how we are running with respect to uh different different images as a container by using docker compose now one more thing that i really want to show you which can you can basically try and probably i'll also make a video as we go ahead if i search for mysql right just think over it uh before my further videos will come how you can integrate mysql with your image right with your web application or web container right now in this particular case here you can see that whenever you are using a mysql you have to use default environments like this and provide your values right first of all you will pull this you'll pull this mysql and here just see the documentation here all the documentation will actually help you to understand and the same command can basically be used in the docker impose also right docker compose also so i hope you had got an idea about how you can basically run it now the next step you can see over here i'll write docker images so here you'll be able to see docker images web app and redis but if you go and see docker ps here you will be able to see two containers are running web app and redis and both are communicating with each other one is exposed from 8000 to 50 and this is six three seven nine slash tcp,358,5,5,8vmKtS8W7IQ
55,right so by default whatever it is basically giving and then where do we get this this is from the docker hub itself it will assign a port from there itself by default okay so this port is getting assigned by default right so now if i really want to stop it all i have to write is docker compose stop that's it once i execute this all this stop uh this entire thing will get stopped but you'll still have the images over there what i would suggest is that you want to make any changes uh please next watch the next video in the next video we'll discuss about like suppose if i am making any changes how do i deploy this directly and after that whenever i make any changes it should be available over there itself that is what we are going to see so yes uh i will see all in the next video thank you,209,5,5,8vmKtS8W7IQ
0,this hello everyone my name is sara latif and i am a machine learning engineer and i'm a pathway mentor at stemway so today i'm going to talk to you about how the world under carries a web app of your machine learning model so i will get started with some key terms so we can get familiarized with them machine learning then i will explain a typical machine learning workflow i will talk about the process workflow like what i did overall in this process then i will do a quick demo and we will move on to the questions if you have any so in the machine learning pathway what we did in level one was the following so we try to collect data we scrape data from the discourse hub community forums we explored our data by performance of exploratory data analysis so we can understand our model so sorry so we can understand our data and then try to model them we train some simple machine learning classifiers so we can classify a post into its appropriate category we then trained the bird model one of the super classifiers in the natural language process thin field we evaluated our models and choose the best approach then to wrap up we built a simple web app using flask or streamlit which i'm going to show you today all right so first things first what is docker because as i told you today we're going to actually build the web app and actually dockerize it so what is actually docker so simply docker is just a platform for developers and system admins to develop,358,0,0,Wl74HKLNUgk
1,deploy android applications with containers which is often described as containing as counterization so now in simple words what is docker so docker is just like the perfect environment for your app it has the libraries needed by your app the dependencies and any kind of file you your app may need but then it is lightweight like for example a lot of people confuse virtual machines and containers because they kind of like look this similar way but it's different as you can see here a container uses the host operating system like for example if you're deploying this on the cloud it's going to use the cloud resources and the cloud operating system why virtual machines have hypervisors so they can use any kind of systems that is going to be installed in on them for example if you install your virtual machines you have for example a mac system and you want to put in a virtual machine a windows system or windows os then you will be able to use it in virtual machine but containers aren't like that like you can't install a system in them you can't install a windows or a linux or whatever you want in them they actually directly use the system that you have in your laptop or wherever you are going to deploy them so that is the huge difference between these two and of course containers are lightweight while virtual machines are extremely heavy because they like they require a lot of computing power so how does the documentation process work so what we do is just we build a docker file this is,358,0,0,Wl74HKLNUgk
2,a simple file that has like where you try to install like for example usually when you when you are developing a model when you are developing your own model in your system a good practice is to actually build an environment for it then install the libraries you you will need or along the way you figure out kind of libraries you will need when you are building your model so all of those libraries that you have installed and anything that you have tried to install will actually figure out in that docker file along with of course the environment that your app needs to live in that docker so after you have actually written that docker file you will you'll build that so by when you are going to build it the final output is going to be a docker image that then you can learn on your own system just to set to test that everything is fine it's working your app is working as it should be then uranet of course to check if everything is all right and the final result is going to be called if everything is alright of course a docker container that's it it's as simple as that all right so we'll be using flask and we'll be using streamlet so flask is a web dev is flash framework is a microweb framework okay written in python it's actually classified as a maker framework because it does not require particular tools or libraries not like for example jungle and all the other super interesting web development tools because it does not require of course any kind,358,0,0,Wl74HKLNUgk
3,of libraries it has no database extraction name abstraction layer form validation or any components where pre-existing third-party libraries provide command functions so it's actually just a lightweight web development tool that is what flask is you can either use it to actually build an app like you know with html and css or you can use it to win the simple api which is quite great for the machine learning community because most of the time we do not actually build apps but we build apis so that developers can use them in their apps so there is another tool that has impressed the machine learning community because it's more much more beautiful and easier to will you know like to to build a simple web app that everyone can try and interact with instead of just using the command line which is streamlet and it's an open source application framework for machine learning and data science teams and it creates beautiful data apps in hours networks all in pure python which will be seen in the demo all right so what is machine learning project workflow like a typical one so usually in machine learning we just like collect the data try to apply some pre-processing to it of course there is visualization to try to understand your data and try to clean it so after the pre-purpose system is done you have to you prepare data then you apply you know apply learning algorithm to your data like that is the machine learning part so you have to apply machine learning algorithms to try to learn something from your data model it so,358,0,0,Wl74HKLNUgk
4,that it will be easier for the machine to learn and apply you know the task you wanted to apply to of course then you have to evaluate it and all of that then you deploy the chosen model the best performing one and after that you just you know like put it in an application so that it can be used in production of course this is an iterative process it's any kind of moment you can go back to a certain part and then repeat it until it you know you get the results you need all right so what are we going to do like like what did i do when when i tried to build this app so the first things first obviously is the build the machine learning model then i wrapped this machine learning model in an api for flask and docker is that but for example for streamlets i didn't use any api i directly like i directly coded the streamlet part which really it's not mesh lines well at least in comparison with the flask one i had occurs this as well then i run the app locally so for now i didn't deploy it in in actual you know like in in the cloud or anything like that not because i can't or that it is difficult it's pretty much easier actually you just have to put all the code in your github and push it to heroku for example we take care of everything even that if i can do that for you but because here i'm using the bert model because you will find many,358,0,0,Wl74HKLNUgk
5,machine learning examples like using simple algorithms but very few examples using like advanced algorithms like birch or any of the super state-of-the-art models so i try to use bird but the thing is for example if i try to put in heroku it is too heavier like the wrapped up model i'm going to show you that you need to generate to put in your app to do inference it's just like over 500 megabytes which is the limit for the free heroku for example part yeah so i didn't deploy it but you can like for example change the model like from a bird one to logistic regression decision tree or any of the simple ones then deploy it to heroku and you can play around with it and give it to other people to play with all alright let's get to the demo part so i put everything in my github i can put the project in my github account so you can go clone it like using this link or you can just download the zip and then play around with it so i already have this code inside my own laptop i will try to first show you the flask version then the streamlet version but before that remember that first things first we need to actually build a model so remember to will the model so you can get like the the wrapped up model the one that will be actually put in a pick alpha in a pickle file so you can use it for inference because i didn't put it here in github seeing as it is really,358,0,0,Wl74HKLNUgk
6,too heavy for github to load quickly and i really have a bad internet so i can't afford to put it out there for you sorry okay so i'm just waiting for for the you know the notebook to load so i can show you around like how to train a bird model um i hope it's gonna open up quickly otherwise we can just open it in collab all right okay so for you you can directly open it in collab then select you know like chandran time select gpu so you can train bird faster otherwise you will stay the whole night or something like that because birch usually takes a lot of time to run all right so the library i use here is simple transformers which kind of takes what hugging phase have already done and makes it like hides all the complicated parts and helps you train quickly the model you know like for classification or any kind of task you're interested to do remember here we're trying to actually classify a certain form post into a certain category that belongs to a searching forum so here the data i am using has been screened from the amazon forum in the like it was scraped in june in june 2020 so when i go back to the forum and pick a new example that has been generated in 2021 to test the model you like you will know it's a new layer it hasn't seen before all right so i'm going to use simple transformers nothing really too complicated i just installed the actual library and one of the things you,358,0,0,Wl74HKLNUgk
7,will have to do is that you have to install the tokenizer so usually it is installed by default but the version because simple transformers has been updated sorry i mean the tokenizers has been secondary library has been updated by hacking face so we can't actually use the new one as simple transformers use the 0.9.4 version so just install this one and after you do this pipe install make sure you restart your actual you know like run time okay so i'm just getting the data you can do the same from our github level 1 machine learning repo then after this we will load the data that was you know download and we will try to do simple pre-processing minutes so for example here i choose to keep only the title leading comment and reply comments so i can try my you know train my data on it you can choose to put all of them together and see how your accuracy is going to either go down or go you know go up so we have if i remember correctly 11 categories in that um e-commerce amazon e-commerce forum as you can see we have some serious data imbalance but i didn't try to do anything about it because that wasn't really the interest in in this like in this part so now what i'm going to do is just to do some simple very very simple cleaning like data cleaning because usually bird actually like can take care of all kinds of data even if there are emojis burnt in the stands then so there is not actually a very pressing,358,0,0,Wl74HKLNUgk
8,need to clean the data or do any kind of data formation but i choose to like clean up any kind of signs just to stay safe and have a better you know better looking text for the model to easily train on i of course choose to remove any kind of html tags if they are left from the scraping even though they shouldn't be they should be known but you never know so i lowered the text as well and as i told you try to get rid of all of these signs that could pop up in the text okay so after i cl i applied this function that will clean my text i just applied the this kind of string thing like try to transform all the text in string because the you will be finding some numbers like for example here two boxes and when you try to train the model or do anything on it it's going to complain that is why i apply this here and i of course like renamed the columns into person category you can either rename it like for example test label whatever you should it's going to stay the same all right so after that i transformed my actual categories into numbers because this library doesn't really take care of that part like there are other libraries that quickly take care of it like for example tensorflow they can do that like quickly of course i could have used tensorflow like they wouldn't have been a problem but then what we were interested to use was spiders so yeah here we go okay so,358,0,0,Wl74HKLNUgk
9,after transforming these categories into numbers as you can see like mapping them more likely so after mapping them i just tried to split the data so i used the scale learn model selection one where i split the data into ninety percent ten percent so ninety percent for trained data and ten percent for the test data as you can see we have seventy thousand ninety three row for uh the train and 789 for the test it should be pretty good so we will build the model by first some training arguments if you wanna you know fine-tune more ones you can just add the ones you want to fine-tune i just put for the number of train epochs for like usually that is the best one like four is the maximum you can go for a pre-trained bird if you want to do transfer learning that is the most advised do not try to go over four you won't it won't be like it may even go bad like your accuracy may drop the performance may get better instead of better so four is good but you can fine tune the other parameters like the sequence length and any anything apart from the effects three to four is great over that is too much all right we create the classification model from the simple transformers library and i will be training bird model and most especially i'm going to focus on bird base in case because as you saw before in the pre-processing step like when i was cleaning the actual text i actually lowercased this so there's no need for me to,358,0,0,Wl74HKLNUgk
10,use the cases version that is why so this here number of labels is actually just a number of classes that i have like i told you i have 11 categories and then of course i'm batson to add to train arguments all right let's go into download the bert model and then we will train the model so like i told you we are trying for for apex as you can see you simply just call mode of the train model and on your training data thanks to simple transformers it's made easier otherwise it would have been a long long class that you would have to find okay here because like f1 sorry uh the simple transformers library doesn't have f1 already defined like the f1 score already defined so we are just defining it like nothing too magical we just calling it out from sklearn and then passing it as a function then we will pass it to our model so as you can see f1 we call out our f1 function we just defined then for the accuracy seen as this is already defined we just call it out and of course we're going to evaluate our model on the testing data so we pass all of these things to result model outputs and drop predictions so rook to the projections if you were interested to know what has gone on like where did your model come it's like some mistakes you can check it out all right so in results we will have the usual like the ones that simple transformers library provide by default which is the accuracy evaluation loss,358,0,0,Wl74HKLNUgk
11,then the mcc the mcc metric and f1 score usually mcc is very very like popular these days with multi-class classification even binary one you can use for you can use it like yeah it usually gives a better like better sense of what is happening with your model performance than all the other metrics but you can of course try all of them as you can see for example here our f1 score is 74 percent but our mcc is 69 you can still fine-tune like the the model and even get more data so it can perform better but here the more like the problem that is that if you could solve would give a better performance is the actual casting balance i talked to you about before all right so now we train the model and everything but we need to save it so we can use it in our app so this is what we're going to do we're going to save it as a tar file okay so we will like the actual will like where you will find everything that is needed by the model is put in outputs like that is by default programmed in simple transformers library so save everything that is in output into some kind of folder like i call this amazon forum birth category classifier guest all right so then you remove of course just so you can keep everything clean you can remove the outputs folder as you already saved everything in your compressed folder so now we will test the model on a real example great now as you can see usually all,358,0,0,Wl74HKLNUgk
12,of these libraries we have already defined them before but treat this part as if it's a new notebook like some new like yeah a new notebook or a new py file just so we can test and make sure that everything that we need like when we are building for example the api is like is really there all the libraries we need we define them all the things we need from default model we defined so we can make sure that everything will work out when we build our api okay so we import of course the interesting libraries that we need so here we're just going to unpack the model so we can take everything that we have saved before from the outputs folder we of course will again redefine the actual model like here the the class that has the model with the parameters we had before were used in training so we would use them again obviously we performed pre-processing i mean cleaning we have to clean out the text that we will be passing to our data great now it's time to test oh remember to add categories of well of course as remember we had changed them to numbers so you need for your own self as your user to know the actual name of it not the number all right so here is a pretty interesting mistake actually did when i was testing at first like i only gave it the body of the text so as you can see so this is actually a flowster specific like it was labeled as floater specific but then my model,358,0,0,Wl74HKLNUgk
13,actually said it was selling on amazon and here i was just playing around with the impact one like for example if i asked my actual user to fill in the text what it would be like when would it be and of course the model did another mistake it was actually account healed um he predicted it as soon as amazon and that's all because actually i didn't ask for the title and then the act do you know like the the the text of the actual post because remember i trained my model by using the title of the post the actual leading comments and the reply comments well here the reply comments like won't be needed as what we want to know is for example we just wrote a searching like we were thinking about writing a searching forum post and you want to know what category it belongs to so yeah there is no need to actual reply comments but the title and the body i need them because i trade my model like like that so he remembers that kind of shape of the data so if we can provide them we should provide them that is why i provided this time the forum passed title and the actual body of the you know of my form of my post so i'm concatenating them as i did in you know like when i was training my model and i asked my model to perform the prediction and it is getting them right yeah it's finally performing so i tried another example as well and it's got it right again isn't that,358,0,0,Wl74HKLNUgk
14,great all right so now that we built our model and got the actual corporate app in a folder so we can use it when we are going to build the api let's go and will that actual api all right so i'm going to just show you directly what i did so i just started with the flask version here you go all right so the api so to build the api what do i need remember i need the model i need to load the model and all the things i need from the model and then i need something to call that model so i can perform prediction and of course my app has to take from the user the title of the post and the body of that post so it can perform the prediction great now what i did was wrap like usually this is what we do we prop up the model in a 13 py fight alone and then we develop the part sorry about that great so here is the model i actually didn't do anything magical like i kept like the part where i tested the model where i told you like this is where i'm going to do inference so i took it as it is and reproduced it here like i took all that code and put it here i didn't change anything apart from perhaps call like i just changed the predict part i came in it's wrapped it up in a function just for it to look nicer when i'm calling it in the api all right now the api part or the,358,0,0,Wl74HKLNUgk
15,app part okay so here i go so this is flask like it's not too much difficult so for example when i'm going to launch my my app it's going to directly take me into the home.html so i try to actually build a wall up like one with the front and and this one and this part is going to be the back end so we can play around with it but you can like you can completely just use flask as an api standalone and then you will like not need at all this helm stuff and all of that you will just need to call out to your actual you know model the predict so you can perform prediction using the post method and because actually in my app i am taking stuff from a form the post title and the body that i need from the user that is why i have here requested another dispose and i am taking stuff like recuperating them getting them from the form like for where the user filled it all right and then of course like i did before i'm just concatenating that thing because they didn't concatenate them when i was actually putting it in the in the model.p file i just asked the model to get the post like already everything should be pre-processed in the api and that is usually what we do like any kind of reprocessing that is going to happen in the front-end part just put them in the api and model leave it alone try to make the model as lightweight as you can it has only model,358,0,0,Wl74HKLNUgk
16,stuff all right now the prediction of course i'm going to call the model so it can perform the the prediction on the post remember i called my py the the py file that has the model stuff i call this model.py that is why we are importing it as a module called import model great now i am going to send my you know my result from the model to an actual html page so i can show the results to my user and i called it result.html and of course i'm going to pass it the prediction that was done by the model great we're running the app in uh localhost format or wherever you are it's going to be localhost for them but we don't use 127 etc we use this 0.000 so that is can be like localhost for anywhere it can be either an ip address or anything like that and the usual part is always 80 or 80 and i am putting here the book true just to see if any kind of thing is happening when i was building it so i can debug and then you know like try and get rid of any kind of errors i'm going to run this for you before i show you how we wrap how it wrapped up everything in docker file okay so let's check out readme oh this is strange i guess the readme got changed i will change the readme file for you later on but for now like usually the readme should contain all the details so let me just see if the readme i have in,358,0,0,Wl74HKLNUgk
17,my oh sorry my folders okay read me oh yeah so i guess i didn't change the readme i guess i didn't change the readme for for the flat version yeah i didn't change it sorry about that okay all right all right no problem i'm gonna change it later on for you but for now me just turn this here okay so i'm already like i'm already changed into the forum post classifier that remember you can clone and just turn the following like usually now what we should be doing is just let me take out everything is already in here okay so because i'm going to run the fl the flat version i'm going to work into the flask version so everything you can use everything there okay so this is my model the one that i saved into a tar file this is the app.pui i'm going to discuss the docker file later on with you and this is the model wrapped up in here this is the static part like you know styling for the home.html and result.html great let us run the flask up so class i hope this was the command because i usually tend to forget these things oh yeah all right it is working right now it's just way too bad takes a lot of time music um trend okay let me just test if it's already working 127 let's see you let's see that's one normally this is where i'm going to oh wait i guess it's still running all right it took its sweet time okay all right so here we have like the front,358,0,0,Wl74HKLNUgk
18,and app so remember we need the title of the post that you want to you know to be categorized for you by the model then the body or content of your post so i'm going to go back to the oh all this course her forum so this course have a community great and i'm going to directly like sorry about that seems like my internet is acting up again yeah my internet is pretty bad sure all right no problem i'm going to directly go to the amazon forum why my internet takes its sweet time to figure out what is happening okay so amazon here we go okay so i'm going to visit the site let me just check i didn't get disconnected okay i guess everything is still right okay um let's wait for the arm to load great so i'm going to check out um you know like the lazy stone okay so perhaps we can try this one guess it didn't get like it says something new so very few texts but we can try let's try alrighty so i'm going to fill in for the title we have selling okay did it get copied great okay let's copy the other part the body and this everything okay here is the body okay great okay so you can see like what is happening i'm gonna put this here for you and you reduce the size of this okay great that is why the back is great okay so i'm going to click on predict button okay let's wait so it's going to first get that model and load it then,358,0,0,Wl74HKLNUgk
19,call it out so it needs to extract it because actually the format that the model is in the picklify is just like a kind of compressed form of the model and usually actually the model loses a lot of a lot of you know like accuracy when it is kind of wrapped up in this kind of compressed versions but it still performs a little bit it super forms good but yeah not as much not like when you are when you have trained it in your notebook it's not going to be the same well it actually loaded the model pretty fast as you can see and now it give us the actual inference of prediction so it is saying that my pulse falls into the senior on amazon category of the amazon ecommerce forum and as you can see it indeed is true as this is part of the selling on amazon so perhaps the word like that helped it make the prediction is this selling thing or perhaps you know the numbers like you could try and figure that out as well when you are when you try to understand what is happening in your model but as you can see actually this slash predict is just the result.html page as i will show you so this is the like what was coded in the actual page so we have this kind of text and then we have to we are expecting the prediction from the model as for the home throw the home one it will stay the same as the one i have seen you because it doesn't expect anything,358,0,0,Wl74HKLNUgk
20,it's just pure html but here we need something from the model so we coded it as well and let's check out the source just so you see how it's coded great so this is the source so this is of course we are looking style css and this is just like what you saw the hater i'm sorry the huge title h1 this is the heater part okay and then the results like everything in the div class of course we are calling out the prediction like this as you have seen printed and then i'm just like pointing you out to the forum if you are interested to check it out yourself that's it okay so now that we have run our you know like our api and everything is working great with the api what we're interested in right now isn't the actual api so we're interested to wrap this model in an actual docker file right all right let's go check that ap like the api to talk as well how are we going to dockerize it okay so when i was trying to install my model you know like uh when i can make it work you know i use it the virtual environment and the virtual environment i use is called postclassifier installed pi charge as you can see here and start fire torch and i installed the simple transformers library the tokenizers the one i told you to specify the version and of course i installed flask because like it's the one i'm wearing my app with i did call all like i call these are all the libraries,358,0,0,Wl74HKLNUgk
21,i needed for my model that's what i'm doing here like first thing first i'm telling my docker file that i'm going to use python 3.8 version oh sorry yeah like the version 3.8 and in this one like it's better to use this flambuster not just 3.8 one like that because it seems like it's but it's in some kind of error not sure from where it comes but yeah okay and then i'm putting this line here which prevents python from writing out pic files an outer line so i can keep python from buffering and i'm installing the system dependencies and this is this part especially good for the docker one like it's similar to what happens in linux usually you have to update your libraries that is why we're an app.get like app again update yeah you're of course updating the stuff that you need from the cc plus plus library to make things faster remove anything will install the libraries that we need remember i told you i start by touch that is what i'm doing in this one so there is no actual pipe installed python directory like that no so you have to get like the like the one from the repository that is why we have such a huge line and then i'm telling my my my locker to keep like to keep nothing in cash to get rid of everything that is going to happen like if there is anything that is going to be stayed in cash to get rid of it i don't need it it's going to take more space and make the docker,358,0,0,Wl74HKLNUgk
22,slower all right i am putting all the code i have in flask version folder i am putting it in the flask app which means the requirement that texted it actually contains all the other libraries i need to install remember there is still the simple transformers this flask the flask you know library and all of that so i put them in a requirements that takes the to not water with them but the torch one like when i put it in in requirement that takes the something seems to happen like i still don't understand what is the problem but yeah that is why i try to split them so this one i'm wearing it like for myself just to make sure that everything that has been in flask version that needed to be transferred to the darker has been transferred and mainly it's going to be the model that i compressed and that needs to be decompressed when i am learning you know like when i'm wearing the app all right and this entry point is just like for example if you were running it inside your own laptop you are going to click python app the py even though for example here is going to be flask run but you can of course either call flash current or you can cause call python upd1 actually that's why i forgot the command because usually i'm just using python.p1 great so that was the dockerfile all right so one of the things i want to say about dockerfiles please be careful about them so when them make sure they run well and make sure,358,0,0,Wl74HKLNUgk
23,they have got all the files that are needed for your application and all the libraries were installed correctly and everything is learning as it should by testing everything by yourself so that you can make sure it is running well and there are no problems before pressing it in a production environment okay now let's say i want to run that docker file all right just give me a second i'm not sure what happened to my okay feel like yeah okay okay so this is for streamlet like this is my my readme for screamlet until i find what happened to the readme for flask version and put it out for you i'm just going to steal some comments from here even though they aren't the same music anyway so music okay so what we're going to do right now like just if you want to test it by yourself you can try it out you will have to build the docker remember so we can have the docker image sorry and after building it you will have to run it to make sure everything is working all right and then you can deploy it you will have your container and you can deploy it okay so i'm going to do this for you for the streamlets part so it doesn't take much time the flask version is the same you're going to repeat the same things it's just what you like what you're thinking exactly is going to be different you know like you can name it differently make sure like you provide the interesting name or something you can remember what you,358,0,0,Wl74HKLNUgk
24,did with okay so let me go and try the you know the streamless version for you okay so let me first open the code for stream for the stream net version then i will run the docker for it so you can see it it's going to be pretty much similar so before that i just want to show you all right okay i want to show you the docker font okay as you can see i didn't change anything up above nothing i just entered the name uh the way i wrote the name of the of the you know the folder where i put everything here in the streamlight version folder just remember just going to be the model i'm loading the model i am loading the model.py like the model the one we compress it the model.py the one that contains you know like the one that is going to call out the class of our model and prediction and prediction method and requirements.txt i'm going to show it to you like what is in it so these are the things i need i am copying all of them inside docker and there is actually no real difference between this one and the other one apart from this part and here that is normal because for example the flask uh the flask one next flask api has uses the port 5000 but for example for streamlight the usual part you can change it feel free it's 8505 as 500 watts sorry as you can see here and when you're running it in your terminal it's going to be streamlined run then the,358,0,0,Wl74HKLNUgk
25,name of your p5 file that contains the whole app or the streamlet code the front end code okay so quickly what's in the front and code it's similar to what was in html okay so i first things first i of course import the necessary libraries the model that is going to perform the prediction then the streamlet you know the stream at library i am of course predicting like to predict uh sorry i am calling the prediction function i just wanted to practice wrap it up in a function again but of course you can directly call it out without all of these fancy stuff all right i'm just putting the title just so that we understand what is happening oh sorry seems like i didn't change the title here let me do it quickly while we are here so it's oh all the resources i used to build these stuffs are referenced in the readme files if you want to check them out so you can use them like later on as a reference your author so this is a post forum classifier great that's it all right we're commenting the changes great oh yeah i'm just telling like the user what is happening like what they need to fill in and what i'm doing here then i'm asking my user to fill in the title as in the html part then i'm asking them to provide me of the the body like everything is the same as html just now it's nicer here as you can see i just need to like write it there is no need for me to,358,0,0,Wl74HKLNUgk
26,code a lot of stuff behind the scenes i am of course concatenating the post title with the post body and then i am of course submitting you know i'm creating a submit button called predict so that when my user clicks on predict i'm calling the model out here that's it and of course i'm bringing out the results see pretty easy with streaming it great so before i run the streamlet version for you so you can see it and it's going to be prettier than the one i will choose in html so it was too lazy to to make it more beautiful in the css stuff so here's the requirement.txt file the one that has the libraries i need so like i told you i need tokenizers i need simple transformers i need beautiful soup because i used it so i can you know get rid of the html tag you can remove it if you want it's not really that necessary and i need this lxml library for the beautiful soup for because i guess something happened in this in the library already provided by python so i needed to add this one you know to remove the air and then of course we need streamlight as we are using streamlight we need to install it of course all right the dockerfire as i showed you didn't change mass and this is this post category is the actual act of purely the one that has a stream and it's got great great great great okay so i'm not going to run the docker file now what i'm going to run though,358,0,0,Wl74HKLNUgk
27,is of course the actual wall application the stream network and then i will learn the docker file for you so one of the things i want to say here of course you have to run the pipe install requirements the text inside your environment of course here if you are going to run it without the docker5 make sure you are installing pi short if you don't i have it great i have by george i have everything installed in post classifier so i am going to directly run the streamlet part so you can see the result oh look at this oh sorry i forgot to actually change the directory oh my god i always forgot these things okay now i can run the actual streamlets out music now what i look at streamlets is that it's always reactive but the thing is when i actually put it in the docker like when i dockers to the wall app when i'm trying to like when i click on predict it takes years i'm exaggerating of course it takes a lot of time and i still like i still like even after wall hours i still don't get the prediction and that is actually why a lot of people aren't using streamlight directly because it's not really that optimal when it comes to running models it still needs to be updated behind the scenes so each its engine can become faster and actually like many people even with the flask api people have changed too fast fast api because fast api uses the univ corn engine anyway it has a strange name but the the,358,0,0,Wl74HKLNUgk
28,flysky api use another engine that is slower than that engine so yes i do recommend you to use the fast api when you are building machine learning models and wrapping them up it's pretty similar to flask api no changes like no real huge changes but it turns of course faster but the coding is going to be pretty similar okay let's try the same example okay so the title here cheers the body as well and as you can see the streamless app is better looking and didn't even take me much time as the other one all right i'll click on predict and we will wait for the result so it's running as you can see let's wait um running running running and as you can see the flask for the flask api is faster than the streamlet one okay i guess we will just have to wait so while it is running let's check out what is happening behind the scenes so as you can see okay let me just reduce this one for you so we can keep both of them here it is actually you know like decompressing the model because it was compressed in a tar file so it is decompressing its first then it will call it to perform the prediction and of course it will give us the result as you can see here is give us the result which is your forum post falls in the selling on amazon category of the discourse hub amazon e-commerce forum isn't that great yeah so our model performs well and everything now to the dreaded part let me just,358,0,0,Wl74HKLNUgk
29,quit this so ctrl c and they clear all of this and then i am going to call out the docker file all right first things first my docker file has already been built i am just going to run it for you so you can see it let's go back to the readme all right i'm going to use this part running it great here we go i'm running the darker one building it is actually easier if everything is all right like you shouldn't worry it's going to be it takes a bit of time actually to build the music docker fight that is why i didn't run it because it will take a lot of time but running it takes less time usually and i already gave you like all the trouble shooting like this part here or the trouble shooting you may ever need i guess these are the ones i used to troubleshoot with docker just remember to be very careful when you are running it and see if everything has been actually put into your docker the libraries needed the files needed when you are building it like these are my actual advice when you are pushing them so for example here i didn't deploy my model as i told you before which would have taken much more space and even if i like we could of course succeed in putting it into into haircut but we won't be able to run the model like we won't be able to see any prediction i already had a bad yeah i already had a bad experience with an older model that,358,0,0,Wl74HKLNUgk
30,was like 500 megabytes pretty similar to the one i have now and yes we succeeded in deploying it in heroku but when we tried to run it we got no music like we got no prediction that is why we ended up putting like the docker app wrapped up in like we put it in of course we have deployed this already in docker hub like that's where we can make your docker public and everyone can install it and build it and learn it of course inside their laptop i mean earn it no building as you how they have already put that and when we put it in docker hub with button then in azure like we deployed in the cloud so we can get the the you know like the actual prediction from the model all right so here as you can see i have launched my app by running the docker that has everything that is needed for the app it's taken a lot of time as before and i'm not going to test the prediction for you as i am sure it's going to take a lot of time and we're not going to get any results because it's too too too slow to to you know like to to use streamlights or these kind of things so remember if you wanna do like if you want something like if you want to wrap up your app somewhere please do not use trim net if it's not for demonstration and just to play around with if you're you know like serious about your deployment please use something more professional like,358,0,0,Wl74HKLNUgk
31,like flask api or fast api whatever you want but please don't use the other one it's going to take a long lot of time turn of course it's not the best one for it is not really the best one for for production of course usually in production environment you always want something that is really really fast okay let's see it's still running as you can see okay so that was it like for the part of deploying your you know like deploying not deploying like building the web app and then dockerizing recognizing it and of course making it you know available at least locally after you have wrapped it up let me see if you have any kind of questions i can address sorry let me stop my sharing my screen first and then i will check out your questions okay feels like you have no questions i guess to send to me if you have some oh right let me share my you know the the project help with you i will try to actually fix some things like putting the actual readme for the flask api the real readme go blank account copy all right great here we go can we have two results in the same page for flask yeah sure you can you can you can do that nothing it's like stopping doing it i just like coded each one alone but you can have the same results in the same flask api but usually like people use flask api just flash just to build the api part and not the actual you know like not the actual,358,0,0,Wl74HKLNUgk
32,um like the actual front end of the model because like in production people want to use either mobile app or some other advanced framework for it like you know angular or whatever you want to use for to to serve to your users and if it's testing like if you are just testing whatever you are doing and you just want to share it with your internal team like developers all of that just use streamlit and when you wanna like push it then of course wrap it in a flask api or wrap it in an actual you know fast api which is better and faster i actually did wrap it in fast api later on when i was playing around with the streamlet but because i already like prepared the wall thing with flask i shared the flask one with you as it was really from end engine do you have any more questions okay so thank you everyone for being here and i hope you enjoyed the session and learned something new see you,231,0,0,Wl74HKLNUgk
0,what's up guys i hope you're having a great day and from this video we're going to dockerize an application we're going to look at a simple python code and we're going to containerize it using docker we're going to go over the basics of docker and it's going to be awesome and let's just jump right in and check it out so today we'll be looking at awesome technology called docker and docker is good for something called containerization i'll explain that in a bit what that means but first of all google docker and check out docker.com this is where you're able to download uh docker which will allow your um computer to do the dockerization so you know this is the website just get started and go down here and download the docker desktop depending on what you have i will pop up or you know you pick it manually so once you have that set up we could go to our terminal and confirm that we actually have docker installed so we call docker version we will see that we have this version of docker installed and we we just confirmed that it's installed so we have it and it's working now what is docker so i made a diagram explaining kind of the basics of what it is so typically the way you used to have applications run on servers before containerization or using docker is that you have a server and then you run an application on it the problem with this is each server is different it has different versions of libraries and technology installed let's say you have,358,0,0,mwquAI5BpK8
1,a server with python 3.7 installed and this application works with 3.8 and above so you would have to configure this server to to work with that application let's say you also want to install this application another server you each time you would have to configure that server specifically for the application that you want to want to run it on now if we use something called docker which is a containerization technology which means we containerize our application so we could put it on any server so in this case we would have a server which all we need to do is make sure it's configured to run docker but once we have that we have something called docker images which are kind of recipes on the technologies you want to use for an application that we will run on that server which th which runs the docker container so if we would say we have the server we have docker installed and we want an application running python 3.8 then we would configure our docker image to run with 3.8 would build the docker container from the image and then put it on our server and then the application will be able to run with docker now let's see how this actually works in practice because that's the best way to see and to learn how to use docker so here we just have a python file so the easiest demonstration i could do is make a simple python script run within a docker container so simplest thing we could do is say hello world from docker container and what we'll do is,358,0,0,mwquAI5BpK8
2,we'll place we'll containerize this um python script and then run it from a docker container so to containerize it we need to make something called a docker file so dockerfile is essentially a recipe on how to specify the container how what kind of code it will run how what exactly are the commands that it will run and we use the docker file to make the docker image which will then use that to create a docker container so first thing is to specify what kind of technology we want to run on this container and we do that by specifying with the from keyword python and then the version and it's good practice to specify the exact version in this case we want to use python 3.8.5 next thing is we want to create a directory within this container and to do that we use a linux command called make there and we create the app directory and in order to do this within a docker container we use the run command which basically allows us to run uh linux commands uh within the docking container and a note here is that all docker containers are linux based so that's something to note so the next thing here is to copy our code from our computer to within the container and in order to do that use the add command and we specify where this is on the computer and this is here app.pi and since this this knows that we're in this uh directory we just specify app.pi we want to place that into the app uh directory next thing we want,358,0,0,mwquAI5BpK8
3,to specify this app directory as our working directory so anytime we execute uh code it knows that we need to use that um app directory so that's that so we have this specified the version we want to we made the directory the app directory we put our app.pi within that directory and we specified that this is our working directory and this is what we want to basically execute code from this is this is where our starting point and essentially it changes into their extra in this directory so work there could be thought of as cd uh within linux but it's within docker we have to use work directory to specify that we're going into this um app uh folder from now on so next thing we specify an entry point which means what kind of technology or how are we executing this container and this is a python container and next thing is the command which specifies that we want to which file basically we want to run in the container and that's the app.pi so once we have that this is our docker file this is basically the recipe on how we want to create this uh container the next thing we got to do is actually build our container and we do that using docker build command specify it uh we give it a name we give our container a name with the tag the tag key argument right here and we want to name this let's just say um yeah python doctor and then we put a dot specifying where our docker file is so since it's in,358,0,0,mwquAI5BpK8
4,our directory we just put dot so it knows that it needs to pick up this docker file within this directory and then build that docker container from there so we run this this essentially builds our container and now it is built let's clear that and let's run this container and in order to do that use docker run and specify the name of the container which was python docker and now you can see it ran the code that we placed into that container using uh docker so as you can see this is the code that we ran and it successfully ran this from the container it didn't run from necessary from our computer but the container within our computer and that's and that's this uh diagram reflects that so in this case let me just put this down in this case my computer is the server we created a docker container and we ran the application with end docker container on on the computer on the server so that's really neat we basically containerized this uh mini application and this is um this is the most basic uh docker file that you can create a docker container but this is a good way to start um if you want to learn more about docker there are some really good uh websites one of them being actually on docker.com if you go to play with uh docker and docker get started go to play with docker and you will find a really nice interactive way of learning docker so if you want to learn more uh go to that plate play with doctor,358,0,0,mwquAI5BpK8
5,and the tutorials lab environments uh or community training another really awesome website to learn docker is called uh catecodum.com and this has really nice interactive courses as well so you could deploy your first uh doctor container which we essentially did in this tutorial and you could continue and deploy your static website and so much more so check those check these out if you want to learn more about docker and i'll also be making more tutorials and going more in depth into docker next thing we could do is um actually containerize a flask application which will then run within docker so i hope to like this introduction to docker if you have any questions leave comments below and thank you for watching and i'll see you next time so thank you for watching the video i hope you liked it i hope you got a basic understanding on how to use docker a docker is a bit more advanced technique and software development it goes more towards the devops field but it's a really good skill for developers to learn also make sure to comment if you want to see other kind of technologies like what technology should i do a tutorial on and also feel free to dm me if you have any questions about being a developer i could give you some tips or we could hop on a call and i can help you out again thank you for watching and i'll see you next time,328,0,0,mwquAI5BpK8
0,in this video we're going to go through an end-to-end machine learning pipeline we're gonna start from raw data go to feature engineering register those features with the feature store train and model from training data generated from the feature store were then going to use scikit-learn to train that model we're gonna save the model to hdfs our phone system we're then going to run that model over the network so we're gonna use model serving to make that model available and we're gonna have an application that will make predictions using that model or gonna do all of this in a single jupiter notebook believe it or not okay let's get started so easy admin password to login in this case we're going to use the deep learning project it was created using this deep learning tour over here so let's go straight into that project tell my deep learning project now this case we're gonna use the features door and normally what happen is that the feature store would appear as a service here in the left hand side but we can see it's not visible and what that basically means is it has it being selected yet so i'm gonna go to the settings tab and we can see in here that there are a number of services that haven't been enabled for this project yet this is when you create a project you can option enable a bunch of services and the tour didn't select these services so i'm gonna enable the hive which the feature store bills on and the features store itself we don't need air flow so i'll,358,0,0,xU-53XnS96k
1,just leave that unchecked okay so the feature store is being enabled we can click on the feature store button here just to see if there's anything there there's not much it says we can create feature groups or new training data sets but let's go and run our notebook in jupiter so now i've clicked it's like the jupiter in this case i don't need gpus so i'm gonna go to spark static and we can see it's a it's giving me a driver with four cores and an executor with four cores and twelve gigs of memory and that's fine by me so i'm clicking on the start button now this is going to launch a new tab you may need to approve the top being launched by your browser i've done that already that's okay and then i'm gonna go to the serving folder down here because that's where scikit-learn is we've already in a previous video we did model serving for tensorflow in this case we're going to go to the sk learn folder and you can see there's two files here one is the iris flower pacifier python file we're going to use now later on to make predictions and we're gonna open the notebook file the ipython notebook file first so our ipython notebook file here is quite a long one it starts out by at telling us what it's gonna do i'm just gonna run the first style just to get the spark application started and then we'll go through briefly what's gonna happen so we're gonna load the iris for our data set which is a very well,358,0,0,xU-53XnS96k
2,known data set for machine learning it's a data set that contains a small number of flowers with four different features and then the class of flower so we're gonna do a little bit of feature engineering on that data set save those features to our feature store we're then going to generate training data from the feature data in the feature store trainer model using scikit-learn and we're then going to launch a serving instance to serve the train model and send some prediction requests to it now this particular notebook is running 5 min 36 i'm gonna skip the last part of monitoring operations to traffic but predictions will be loved to the top again cactus so you can run that code separately and typically would run as a separate a long live processes streaming application such as my spark okay let's see what's happening so the spark session is being created we can now read in our data set it's in we can see this telling us it's in our project inside test job data iris our csv we can go back and have a look at that data so we go to the data sets folder here you can see the test folder and inside data we have iris we can see ric it's not a very large data set you can see there's there's some labels for each of the columns sepal length with that length that's a width and then the class the variety set tosa be my name there okay just come back to our notebook so we read up the data set we can have a look at,358,0,0,xU-53XnS96k
3,the schema and they was there the four features that we mentioned and the classical variety and this variety as we saw was a categorical variable we're going to use a string in bexar to encode that and what we'll end up with we can see below is basically a nice integer label for the for the class and then we're also going to take our features a sample length with length and width and make them all doubles they were already but we can go ahead and do that when they have a data frame we'll have just a numeric values and so we can see this is our data frame and it has just numeric features and we can just show up the first three examples we have this is the look of data sets lookup dataset to find early rom it's basically saying give me the distinct varieties that are available in this data frame as telling us there's three distinct varieties virginica versicolor and setosa so the next more interesting step is how can we make these features available to other people who'd like to make use those features and models and we can do that by creating a feature group with this data frame iris of underscore pf3 and that will basically store the four features in our iris data frame into the feature store in if something called a feature group and that feature group will be called iris features myers underscore features it also computed descriptive statistics correlation matrix histograms cluster analysis for the features so there is some metadata about the features that we can now go back,358,0,0,xU-53XnS96k
4,and look at so let's also run the next one it's gonna create a feature group for the artists labels look up we left that runner we'll go back to our feature store so now we're not looking at our feature store what we should see is that we can see the artists features feature group has been created so we have our feature groups here and then we have the individual features here and training sets that are created from feature groups from sets of features and feature groups that come from feature groups will we haven't cut we haven't created any yet so there shouldn't be any there so if we look at training data sets we can see there are none but there should be four features we can see we have the four features in our iris data set we also have them the label which is the type of flower now if we go back to our feature group on we're also able to see is the computed statistics a better feature group so if we click on the green button the red one will delete this feature group so we won't come that we'll click on the green button well you can see is a number of different statistics for our future group we can look at the correlations of the features you can see some of their we can look at some clustering analysis for the features and there's some clusters there we can look at descriptive statistics the counts of the different values that the mean value standard deviation mins and maxes for them we can see them,358,0,0,xU-53XnS96k
5,oh them all here that's kind of interesting so this is what a data songs will typically do when they start addressing a problem they look at the available features they look at they will try to understand the data distribution and the type of data in those features and before they actually go ahead and design models so if we go back to our notebook what we can see is we now have tons of feature engineering register the features are available in the future store and what we can do is we can read the training data set from the feature store so what we can see because we have a feature group called iris features we can say i'd like that feature group artists features and give it back to me as a pompous data frame so we're gonna ask for upon the state of frame of this train df will be upon the state of right we got a box so let's just run these bits out and then we can describe it when the that's wrong what we're gonna do is we can see now there's some statistics over the founders data frame but what we want to do now is we want to generate some with a training data set under test data set so at the moment we can see we've got their training data set here this is the data frame xdf and my df as their labels that the classes that the flowers in this case let's run this and what we can do now is we can use the keener's neighbors classifier to try and train,358,0,0,xU-53XnS96k
6,the model to predict based on the features what class of flower we have so we've done that now and this iris underscore k n is our is our model what we can actually do is we can serialize our pickle that model into a file iris own is broken and decal and copied into our file system hdfs so we've done that now we can go back to hdfs just to check and make sure that father's there it should have been copied into the resources folder here and we can see that it is here so iris underscore k and then doc pol has been copied here if we go back we and we continue what we now have is a model that's available in our file system can be used applications to make predictions about virus flowers given some feature data what we want to do now is make that model available over the network you want to serve it so what we're going to do is we're gonna take the pickled file this rs cannon we're gonna call serving duck exporters and we're gonna give the model a name which we've done up here so we have the name artists therapist firing we have serving version one cop will basically run this one here fifteen this will basically export our model and make it available so to serve this model we have a python script that downloads the hs model in the constructor and saves it as class variable and implements the predict method in this particular class and classify and regress so this is very similar to tense for a model,358,0,0,xU-53XnS96k
7,serving so you can read more details on this in your own time so if we go back to our datasets here and if we go to our models folder we can see that the model was exported in here is our third classifier it's version number was one and this is our model so it's now officially available as a model in our models data set so so we can actually then just show that it's here this is doing what i did it from these interface and in this case we're gonna run the next line which is basically going to call the serving duck crater update and that's gonna basically yeah update the the model file that was all it already existed so after the serving has been created we can we'd like to make available as a as a model over the network by the hops rest api so what we're gonna do is use the using the rest api for hops works we're just gonna get some meta information about that particular model file we can see that's not being served right now we call serving pastels the name of the file and it's god says it stopped so all we need to do is call serving dot start on the name and it will start this model being served so we can actually track this in the user interface they're going back to our model serving service here and we can see that the model in fact is being served and we have a bunch of options over here so we can look at the logs for the model as,358,0,0,xU-53XnS96k
8,it's being served it's gonna pull them out of cabana so we can see them in this is that the psychic learning server model logs and we can kind of filter them and if you want to even graph them or something you can do that in cabana and but what we can also do is more interesting li as we can get the endpoint for the model and an external application outside pops worse could take this endpoint and then create an inference token and this is our kwd token which will have a very long expiry time and then and it can be invalidated at later stage or just by removing my user from the project will invalidate our token and then at the later stage we can now use that model over the network so so in this case it's okay so what we're gonna do now is are gonna sense prediction across to the model that's being served over the house or expressed api and what we're gonna do first is we're gonna get a topic name for for this particular model so the topic name hasn't printed here let's have a quick look so that the topic name we can go back and find to that if we go to casca so the kafka option here will show us the topic that was created to store the logs for the predictions and there's a schema for it as well if you care to look the scheme will actually be available in here in schemas you see it's in for schema the contents - it's not formatting very well but we can,358,0,0,xU-53XnS96k
9,see that there's a model named model version and so on okay so let's go ahead and make some predictions so these predictions are making service inference requests on the model again some predictions back and then that's basically it so we've now made some predictions from the notebook over the network or serving the model over the network and we trained it from feature data and training data generated from the feature store and we registered those features in the feature store so we can let this run or we can stop the model from being served but they don't stop and you can also have a separate spare extreme application which will monitor the prediction of price and responses using kafka and you can see that some example code here but this is written for python 2 7 which won't work in my particular notebook okay that's the end of the video,200,0,0,xU-53XnS96k
0,hi in this video we are going to learn about psychic learn pipeline api now we are going to see like why do we need a psychic learn pipeline api what exactly is the psychic learn pipeline api and what are the components of it and also we will see at cases where you might uh you might it might be better to stay away from the psychic learn pipeline api so let's get started now if you take a typical machine learning model development life cycle this is just part of it and not the complete life cycle so what typically what you do is you get your business and data understanding you frame your problem and then get your business and data understanding you get your data finally based on your data understanding and then you do your explanatory analysis and the data analysis on top of it and finally you start your model development cycle now in the model development cycle there's a combination of data engineering and feature engineering activity right now typically what you do is you do some feature selection based on your domain knowledge or even some analysis that you did and then you enter your data cleaning cycle yeah the data cleaning cycle can be inbuilt psych learn functions or it can be custom functions and then you based on your data type whether it's a categorical value or numerical value you do different uh different transformations on top of it if it's a numerical value you do imputation scaling and if it's categorical value still do imputation but the imputation will be different than what you do,358,0,0,SpirD-FhxQI
1,for your numerical value and then you do one not encoding and finally you fit an ml algorithm to it it can be a tree algorithm it can be a logistic regression or a rinear regression depending on the type of your problem now this is just a sample pipeline now as as your data is uh as your data is complex and you have uh different data behavior different distribution of data you apply different other transformations and also you have your feature engineering done on top of it by combining multiple features or even create additional feutures from one futures right this is how your pipeline looks like now typically what is the challenge with this pipeline now when we think about deploying this pipeline uh we typically have this particular program in a notebook or in id python file we have to the final model that you save typically is just the model alone but you have to go and deploy each and every function separately of this and this creates a challenge of reproducibility when you are deploying that is one part of it second thing is the deployment is pretty complex in this case because you all you have to make sure that the functions that you created during the training is similar to that you are deploying and also when you're are doing like imputation or something like that there are some learned parameters so you have to pickle the imputation object separately you need to pi pickle the scaling object separately and then you also need to deploy so the deployment is pretty complex and the debugging in turn,358,0,0,SpirD-FhxQI
2,is also complex because you have multiple moving parts now how can you make this part particular pipeline pretty reproducible and the deployment make it easy and also enable faster debugging that's where basically the psychic learn pipeline comes into play right so now what is psychic learn pipeline now pyic learn pipeline is nothing but a sequence of transformations the transformations can be your pre-processing or feature engineering uh activity that you do and finally followed by an estimator the estimator can be your algorithm models or anything right you have that you you assemble everything together and execute as a single entity now i will give some example and i will also have a detailed session in my follow-up video but this is how below is how the pipeline looks like you have your data you have pipeline the pipeline initially it will be a bunch of transformers that you have the transformer can be an one not encoder or the transformation can be some uh some data cleaning function or it can be a custom transform that you write for your specific purpose and it can similarly you have multiple transformers for categorical you may have one transformer for numerical you may have one set of transformers and these transformers in this case i have shown execute serious here serially but sometimes you may want to execute it parallely for performance reason right now and then finally you have an estimator so the transformers are nothing but one that implements fit and transform function the estimators just have a fit function fit and predit function right so you put everything in a single,358,0,0,SpirD-FhxQI
3,pipeline and then you finally what you do is this you kind of can run your grid search or random search you can do evaluation and you can deploy the entire pipeline together so basically the psyit lear pipeline gets together all the different components that i talked about in the previous slide into a single pipeline object we can directly go and pickle or save the pipeline object and then deploy it directly every entity that we are using in the pipeline the transformers and estimators are part of a single big object in this case so this is how typically your pipeline looks like if you take a typical ex typically what we do is we just write a jupyter notebook we just take the data we we process it we impute the data then again take the data frame we pass it to a different transformer and everything right but in case of psyit learn pipeline you just take the data once and then create a pipeline object to a typical example over here is i am first creating an a drop column uh pipeline right a column transformer which drops a particular set of column this drop feature is a list i am passing in this right so this will drop the columns then what i'm create doing is i'm taking all the numerical features that i have and i am sending it to the pipeline which does imputing based on the mean and then it does a standard scaling and similarly for categorical i am just doing one not encoding of the variables now these two numerical and categorical transformer i,358,0,0,SpirD-FhxQI
4,am putting it one other column transformer and i'm also including the drop column the numerical processing and the categorical processing right so now i have the transformer object then i create my final pipeline i am putting my uh transformer object the space remover is another custom function that i have written uh i'll go to the detail in my next video but this a custom function to remove spaces in the data right and then this is the transform column that i uh created on the top the column transformer that i created on the top this is the transform column and finally i have a logistic regression so i have all this pipeline fit together and finally what i do is i can just call the pipeline.,169,0,0,SpirD-FhxQI
5,fit so what it's going to do is it's going to uh go take the data pass through the entire pipeline component and then it's finally going to give an fitted pipeline now what i can do is on the left hand side i have the same thing on the right hand side once the pipeline is trained and you are comfortable with the pipeline uh performance and everything once you have tested and evaluated the pipeline you can just uh use jobb or pickle object to directly dump the pipeline all the objects are dumped and finally you can just load back the pipeline uh when you are deploying it and just call the pipeline score method with the same data so all the objects are the pipeline a single big entity that you can deploy but there are some cases where you might not want to use the pipeline right and typically those are the cases where you want better control on the deployment uh say typically when you have an uh pickled object which is a pretty huge object you want to fine-tune each and every pipeline component at that at that time the pipeline object might not be beneficial in case if you want to performance tun it better a typical example can be maybe i have a part of feature enging that i have done that does some calculation on three columns and what i feel is doing deployment maybe i have gpus and i can just take that pipeline component and vectorize it into a huge har and then push it into my uh gpu right now in that case,358,1,1,SpirD-FhxQI
6,maybe you want better control on your pipeline you may want to stay out of the psychic learn pipeline the second part is like if you are deploying some of the pipeline or like even the entire pipeline a different language al together than you are developed say if you have done your coding in python and then you pass it onto your software engineer team who want to reproduce it in java or some other language then maybe you and they want to do like part by part on that then that case maybe you want to stay away from psyched learn pipeline but apart from these two the psyched learn pipeline are convenient function for sharing your model for better reproducibility there are a lot of time that is spent on testing the model and this reduces the testing and debugging cycle and it's very easy to kind of go and get part of the pipeline component and do analysis on top of it which i will cover in my detailed next video uh but this is quickly about psyit learn pipeline thank you very much,244,1,1,SpirD-FhxQI
0,hi everyone george here from date academy today we're going to walk through the process of deploying an scalar model using gcp ai predict from the command line instead of using the the terminal we are actually going to do this in a juvenile notebook what you need to do is simply add the exclamation mark in front of your commands and then you will be able to run them in this environment also in order to be able to follow today's tutorial you will need your gcp product setup and your service account activated if you don't know how to do this please follow my previous tutorial which is called setup gcp for ml deployment all right without further ado let's get started so first of all we need to import the os module and then we need to set up a bunch of variables for example the service account name the product id again you can check out my previous tutorial if you don't know how to get these information and next we need the the bucket name and model directory so what is a bucket so in short a bucket is just a folder on gcp storage that's where you can store your stuff so we're going to call our bucket just the gcp deploy and then the model directory is going to be format formatted this way and then this is the region we want to deploy our model and store the the model files and we are going to provide a model name and version name and also the framework so first of all the framework it is the either sklearn,358,0,0,N83XEvlbEg4
1,or pytorch or keras those are the frameworks because our model is developed using sklearn so we're going to use scikit-learn as the framework and for model and version these are actually the uh the two levels of concepts you need to understand on gcp ai predict so right now if we go to ai platform and then models we can see there is actually nothing in here so first you need to create a model so today we're going to create a model in the name of wine classifier so tomorrow maybe we're going to have another classifier another model okay and for each model we can have different versions so today we're going to deploy a very basic version which is this version number but in the future maybe we have an improved version then we're going to deploy a new version so you see we can have multiple models and under each model there can be multiple versions and note that you can you cannot have the dot symbol in here so you have to replace the dot symbol with the underscore okay so that's pretty much it in terms of uh setting up those variables next we will need to um basically set the environment variable here so that we are able to find the key file okay in here because we're going to use the key file right down below here okay so first we're going to do the authorization basically last time we have set up our service account so this time we are going to use that service account to do stuff what are we going to do we,358,0,0,N83XEvlbEg4
2,are going to store the model files to gcp storage we're going to do the model deployment so all those we need the gcp service account so that can be done by doing this we're going to activate the service account by providing the service account name the key file and also the product id so let's do that and next we can check okay so that is all good so basically we have already activated this service account for this product so that's all good next we are going to do the uh store the model onto gcp storage so first we need to have a bucket so using gseotil mb this command we can create this means make bucket we can create a bucket and uh with the the l flag we are basically specifying the location of uh where to put this bucket and then here we provide the bucket name so once the bucket is created which is already done next we are going to store our model file into that into that bucket so this is the command gs hotel copy okay from my current folder this model file to this remote storage bucket okay so this is the bucket name and this is the the target file name so we're going to do that so once we store our model file onto gcp storage now we are ready to finally do the deployment and just now we mentioned the two layers of concepts so number one is the model number two is the version so we first need to create a model resource so for that we need to use,358,0,0,N83XEvlbEg4
3,gcloud ai platform models create and then we provide the model name here like this and then the region like this by doing this we can create the model so that is done finally we are going to deploy a version under the model we have just specified okay so for that we need gcloud and platform virgins create to create a new version this is a version name and we are going to create a verse the version on top of this model and we are going to grab the model file from here and we are going to use this runtime version and this framework and the python version and the region that's it so now we can run this and this is going to take a couple of minutes but once this is done you can go on to gcp ai predict to verify your model has been deployed that's it thank you for watching,205,0,0,N83XEvlbEg4
0,what's happening guys welcome to the very first episode of code that a new series where i try to code stuff in a ridiculously short time frame in this episode i'm going to be building and deploying a machine learning api using fast api and pyit learn so what are the rules well first and foremost the time limit you guys were super generous on the community tab and gave me 30 minutes to do this well i'm going to try to do it in 15 second and most importantly i'm not allowed to look at any documentation or stack overflow and no github co-pilot if i do it's a 1 minute time penalty but the real stakes and the third rule if i fail to make the time limit it's going to be a 50 amazon gift card to you guys so a little bit of background for the guys that aren't so technical what is an api well api stands for application programming interface think of it as the plumbing between different applications in huge organizations they've got a ton of different apis that connect a bunch of stuff together we could use this api that we're about to build to be able to integrate it into a mobile application or a web application and this will allow those different apps to communicate and use our machine learning models ready to do it let's get to it all right let's do this okay so i've got the existing machine learning model so this is a model that's been trained inside of psyit learn so we are going to be leveraging that so you can't,358,0,0,C82lT9cWQiA
1,open it it's serialized but i'll share it as well what we do have however is this sample.,23,0,0,C82lT9cWQiA
2,json file and this tells us how we should be sending the body of our request to our machine learning api so we are going to be leveraging that now the first thing that we need to do is create an environment so i'm just going to inside of my fast api folder that i'm currently working let's make sure you guys can see that so let's create environment so python dmv env and we'll just call it fast ml for now and we'll let that create so you can see it's creating up there there and then we've got to install a bunch of stuff so we need to install well let's activate our environment so fast ml- scrips das activate and then let's clear that then what we need to do is install our stuff so i believe we need to pip install uv corn which is going to help us actually start our server gun corn which we're going to need when we go and deploy to heroku so we're going to be deploying a heroku as well we need fast api we need pantic and pantic is going to be used to uh structure our or pass through our requests and have them in an appropriate file format um we need psychic learn so i believe we can just type in sk learn and what else are we going to need uh pandas as well to process our request so let's let that install and then while that's happening let's create a new python file we're just going to call it um ml api and then we while those things are still,358,1,1,C82lT9cWQiA
3,installing let's start bringing in some dependencies so bring in really save this as a python file uh rename pie cool and let's just set this particular environment looks like we've installed successfully to pip list double check that cool so we've got pantic psyit learn scipi uv corn and gunicorn yep cool and fast api all right cool we're looking good so let's create a lightweight api to begin with so bring in lightweight dependencies so we're going to bring in fast api so from fast api import fast api and then we need to create an instance of our app so we'll call it app equals fast api and then what do we need to do so we need to create a decorator to tell our api what the route is going to be so it's going to be app.get initially we're going to change this to post later on and then we need an asynchronous api we're going to define our function now so this is going to be what actually gets called when we go to this route so effectively no explicit route so async def uh we'll call it scoring end point and for now we're not going to pass anything through to that uh through to that function it's just going to be a raw function at the moment and then we're going to return just a dictionary so we'll call it hello world right so this is sort of the first phase so how we looking that looks okay all right cool let's try starting that so in order to start we can type in u von and then,358,1,1,C82lT9cWQiA
4,"the name of our file so in our case it's going to be ml api and then the name of our app which is going to be app over here and then we can set hot reloading equal to true so d- reload cool that looks all right so you can see that it's started up successfully let's open up postman so we know that our api is running on port 8,000 so we can go local host 8,000 and hit send all right you can see that that's working down there so you can see it says hello world right we're looking good can we zoom in on that so you guys can see it a bit better nope okay whatever we got to keep going okay so that's fast api what we now need is pantic so we actually want to send some information effectively this our scoring request to our api so we need a way to pass that information and this is the way that fast api works so um from pantic import base model and then over here we are going to create the structure for that so and you can see that our server still running this is the beauty of setting hot reload equals to true so uh class we're just going to call it um what's it called scoring item actually let's just call it item and then we are going to uh subass that pass through the base model and then here you need to structure so this is effectively having a typed item so we need to structure it using this so we are going to",358,1,1,C82lT9cWQiA
5,be grabbing each one of these items or each one of these fields from our machine learning model so in our in the original machine learning model and i'll share that code as well i've got four columns in there so year at company employee satisfaction position and salary so these are the four different columns that we're going to be expecting inside of our psychic learn model now what we need to do is structure this in a format that says what types we're expecting so years a company we already know that's going to be a float so we can say that this going to be a float and let's just comment that bit out and then employee satisfaction it's going to be a float over here we can comment that bit out uh position is going to be either manager or non-manager so what we want that to be is a string st str and then salary i think salary was ordinal which means it can be an integer so i've got a floating point but this can be uh let's set it to int for now so basically what we've done is we've created a structure for what our scoring requests are going to look like so when we make a call to our api we're going to be sending it in this particular structure now we need a way to actually pass that through in our endpoint so first things first we want to change our api method from get to post cuz that's just good practice and then what we're going to do is we're going to capture our item,358,1,1,C82lT9cWQiA
6,and that item is going to be well actually no we should change this so this should be uh let's call it scoring scoring item so we're going to capture our item which is going to be the actual request body so we're going to send a json object through to our api so we are going to send that through and we're going to be able to work with it with the value item now we're going to type explicitly define a type so we want it to be at the type scoring item we can pass that there and then rather than returning hello world let's return back item and what we can do i'm just checking we're still recording and what we can do is let's go test this out now so our method should be post so if i send this it's going to fail cu you can see down there method not allowed so if i change it to post and send so right now it's saying that the fields are missing and that's because we haven't actually passed through a body so what we can do is go to body go to raw go to json and then we're going to send through a request so let's go and grab this initial request here this json object paste that in and let's get rid i think we can leave the comments so now if we send this through uh property name en close in double quotes what are we doing let's just get rid of this set that to an integer and beautiful okay so we're returning our items so,358,1,1,C82lT9cWQiA
7,you can see that we've successfully gone and passed the stuff that we're sending through to our api and our api is returning those values back now what we actually need to do is we actually need to bring in our machine learning model which is currently stored as a pickle file so it's serialized using pickle so what we need to do is import pickle so we can uh do that so we have it installed let's just stop our api to begin with so pip list uh we don't have pickle let's install it so pip install pickle not find a version what python dasm pip install oh no we're going to have to check documentation on how to install pickle can we just from import pickle wait we might already have it in the environment but we don't have it in our custom environment uh i think we're going to have to go to documentation music guys only got 5 minutes left oh no all right sc let's just try this for now um with open rf model.,235,1,1,C82lT9cWQiA
8,pickle and we're going to read it as a binary and then model equals pickle. load um f and then we also need pandas so import pandas ah why is pickles difficulty install okay all right so then what we want to do is we want to pass our item so we're going to create a data frame equal pd. dat frame um we're not going to make this uh so we need to go list no item and then we can access it as a dictionary and then we can grab values to grab just the values out of that dictionary and then we need to specify columns to be able to pass through and effectively create a lightweight data frame columns equals uh items.,163,2,4,C82lT9cWQiA
9,deck do items no do keys what am i saying and then we can make a prediction so y hat equals uh model do predict and then we can pass to our data frame and then ideally what we want to return back is why hat let's put it inside of a dictionary prediction why hat let's convert it into an integer should be just a single prediction all right let's restart our api should be item over here okay no issues uh what is it so uvn ml api and then app d- reload okay no issues send it internal server error what's happened pd is not defined we not imported pandas as pd okay boom we're getting that prediction okay i'm not sure what the pickle issue was but that is successfully we've successfully built the api now what we want to do is actually go and deploy it so in order to deploy we got to create a bunch of files and we're going to deploy using heroku so um we need a proc file and this defines how we're actually going to start up so we want to go web and then uh so gunicorn dw to specify how many workers dk to specify what we actually want to run so it's going to be uv cor do uh what is itw workers.,295,5,5,C82lT9cWQiA
10,uvn worker and then we need to run ml api app so that's our pro file defined then we need to freeze our dependencies i'm concerned with the pickle thing that might cause some issues um pip freeze requirements.txt right so that's now there cool and then we need a runtime so this is just going to tell uh heroku what type of runtime we want to use so uh we can type in python d- version to get that so it's going to be python 39.7 python d 39.7 and then we also need a get ignore file and we just want to ignore _ p b a h e and fast ml we don't want to take in our environment cool so that's now ignored we also don't want uh we do want the model to go up we don't want sample.,187,6,6,C82lT9cWQiA
11,json okay so now we can create or we'll want to commit that so issues that all right so we're going to create a new g repository so get in it get add all get commit dasm initial commit and then we need to log to heroku so heroku log in this might take a little while oh no we've got a minute left um so we need to log in cool we're logged in and then uh what is it so heroku create cool so that's going to create our api and then we want uh get push aoku master fingers cross this works all right i'm going to pause the timer all right so we're uploading right now it's building the source but we can't really do anything else so i'm going to let this run and then music so right now what's happening is we're actually uploading our api like we can't really do very much right now so i figured look i give myself the benefit of the doubt we got 35 seconds l left to test if anything goes wrong we're screw it it's going to be a 50 amazon gift card but uh fingers crossed we can get this deployed in time i'm a little bit concerned that i couldn't install p using p so i don't know i'm going to have to dig into that so right now it's like compressing you can see things are happening got 35 seconds left woo lordy but it was at least running so that's a good sign okay so right now it's actually uploading so this is running a build so over,358,7,7,C82lT9cWQiA
12,here you can see that we've got this link over here that is the link which is going to allow us to test out the deployed model hopefully pickle's just like installed i'm going to have to go check that out after the fact buto that was intense okay it is deployed i'm a little nervous all right time is on let's go so we've got to test it out so we can copy this and fingers crossed we should be able to plug this into here detail not found no that is a app method not allowed all right so that's using a get request we can check our logs heroku logs d- tail wait method not allowed ah so close all right we're clearly got over so let's quickly debug so all right so we got to a post function all right so that's clearly going to be an amazon gift card here you go guys we got close but h so close uh did i have spaces oh had spaces there you go it was actually working okay so i had spaces at the end i'm going to give give you guys benefit of the doubt i'll give away the gift card but that that is it actually deployed so you can see we've successfully gone and deployed it to this link over here which is exactly the link which was from over here so it's successfully deployed and you can see that we've made that prediction so if we actually go and change these values so if i change this to 50 years of the company we're getting a different prediction now,358,7,7,C82lT9cWQiA
13,so zero means the employee hasn't turned so this is a turn model so we can change that employee satisfaction value so i can set that to two uh we can change the position set that to manager and you can see we're actually capturing those different predictions so salary is an ordinal value between 1 to 5 you can see we're making predictions so if we go and drop the years at the company to one you can see that this is saying that it's going to churn in that particular case all right not quite 15 minutes but we got super close anyway the amazon gift cards there first one to get it gets it thanks again for tuning in guys hopefully you enjoyed the first episode of code that i'll catch in the next one peace thanks so much for tuning in guys hopefully you've enjoyed this brand new series if you want to see some more coding challenges or if you want to see the code that series continued drop a comment below and let me know what challenges you think i should be giving a crap thanks again for tuning in guys have peace,258,7,7,C82lT9cWQiA
0,greetings from diazonic labs and welcome back to my channel in this video we'll be learning about sklearn json library whenever you want to deploy any model you need to first convert it or you need to first serialize that particular model so normally we use pickle or job lib library so job is again another library based on pickle itself so here we serialize our model but then there is a problem with pickle whenever you are serializing with a pickle file it actually provides a simple attack vector for malicious users so it can be easily hacked into and there is a link here a very good blog written which actually speaks about the python pickle security problems and solutions so you can just go through this i'll put this particular link in the description below this library sklearn json is actually an alternative for that particular thing so it is considered to be a safe and transparent solution for exporting scikit-learn model files so whenever you create any model using scikit-learn library you can easily you know serialize it with the help of this particular library scala and json and it says that it is 100 safe and it will be converted into json file all right so the version of this particular library is the latest one which was released as a 0.1.0 it was released on november 2 2019.,304,0,0,XwiuEE9314s
1,all right let us see how we can actually use this library so for execution i have considered one common used data set years of experience and salary data set from kaggle it's given here and if you want to see how to download the data set of kaggle using kaggle api i have already published one video on our channel you can go through it i will put the link again in the description below right now i have just uploaded it in my github profile and i will be taking that particular link here so i've opened my google collab here and it is connected and let me just run this thing to see whether our data set is all fine or not and we can see here we have got this data set and this is a typical linear regression problem wherein we know that in our if you want to solve linear regression problem we need two types of variables one is independent variable and one more is dependent variable so basically we have got here two columns one is years of experience and another is salary years of experience is the independent column and salary is the dependent column so here i have run the code for taking the dependent and independent variables separately i have put this in x and y axis x and y variables and then i've created a linear regression model with the help of sklearn library and i have used the method model.fit and now it is uh fitting the model once this thing is done we can just check out the coefficient values and intercept,358,1,1,XwiuEE9314s
2,values so when you run this command you will get here two values the first value what you get is uh the coefficient value and then the second value which you are getting is uh intercept value okay now what we will do is we will actually use uh say this particular library scalar library to serialize the model okay so what i'm gonna do is i will just copy this particular thing pip install sql and json and paste it here with a command exclamatory so that it can be considered as a command line argument and now once it is downloaded uh it says requirement already satisfied and it has installed this particular package okay now once this particular library is downloaded what we can do is we can import the library so i'll write here import sk1 underscore json and i'll import as sk json all right the next thing what i'll do is i will just mention here the file name which i want so i'll write your file name is equal to i can just write one specific file name i'll just write here say abc.json the file type saved will be in json and then the next thing what i'll do is i will call for the two json method from sklearn json okay so i'll write here sk json okay and then i will write say dot 2 underscore json all right and then here i will put the model which we have already defined before and then i will put the file name here and then let me just run this particular cell so you should be able,358,1,1,XwiuEE9314s
3,to see here on the left hand side where i have got the screen of files i have got here one specific file by the name of abc.json and when i open this i will get here the arguments the most important arguments in linear regression so it says that the coefficient value is so and so and then the intercept value so and so all right so basically this is what is stored here uh this is what is actually stored inside this abc.json and we will try to uh you know retrieve this particular file in another say google collab notebook and let us see whether we are able to get the same value of coefficient as well as intercept or not all right so i will open here a new notebook and okay just wait for it to open and once it is open what i'll do now is i will connect this to the python backend and i will import now again i need to import sklearn right so scale on underscore json as skl sorry sk json okay this is what i will import and after that when i run this obviously i'll get an error because this particular library is not installed so i need to install it here right so in this particular environment also i'll do clip install sql on underscore json okay let me run this and it will install all right now i have imported here the model whatever it is there first thing what i'll do is i will just download this particular file let me just click on download here and it will be,358,1,1,XwiuEE9314s
4,downloaded here and then let me upload this particular file here back again so this is the recent file which i have got abc.json i have uploaded it inside my second google collab notebook so this collab and this collab has no relation whatsoever i am just uploading it again here the serialized version is uploaded here now i'll just rename this to something nice so i'll just rename it as again abc i don't want this to i'll close this okay now uh it's time for deserializing so i can write here say d serialized model okay is equal to let me just correct the spelling here be serialized okay and then i will write here say sk json all right and then i will write here say from underscore json so there it was 2 json here it will be from underscore json all right and then i can write here say again abc dot json so i'm importing abc.json here let me just import it and once it is done you should be able to see that it will give you the same coefficient i'll just copy this variable name and paste it here okay now if you want to see what exactly is the coefficient coeff underscore so you'll get the coefficient value here the exact same value which we had got before and let me just copy this for intercept as well so intercept will get the exact same value okay uh two five seven nine two nine four four nine okay if you see here two five seven nine two nine four four nine dot values so this is how,358,1,1,XwiuEE9314s
5,we uh use the serialization method and uh how we can actually export the value and this can be further used for uh you know develop deployment into say web app or you can actually create an android app with the help of this android app or ios app so uh i'm planning to create another video uh wherein we can actually deploy this model on mit app inventor wherein it is a complete graphs programming method so there is no uh as such writing kind of code involved so it will be really helpful for beginners to understand the deployment so i will try to make another video related to mit app inventor how we can deploy our linear regression model there so if you have liked this particular video please do consider subscribing our channel like this video and i'll see you soon,188,1,1,XwiuEE9314s
0,"hello and welcome to this video where we will use fast api and scikitlearn to create an api that will serve machine learning model. i will make a very simple machine learning model using random forest classifier to classify the iris data set. as in all the machine learning courses there are iris data sets but not many not all of them will teach you how to create an api and then connect it to a front end which is the idea with this video and maybe another video where i will put the front end in another video. not sure yet. it depends on the length of this video. maybe i'll put it in one video or i will separate it. we'll see. we will just create a simple machine learning model. then afterwards we will create our simple fast api and then we will test our api out and then we'll connect to the front end. as simple as that. let's move on to kaggle to find the data set. here i am in kaggle and in kaggle i just searched for the iris data set and click on the data sets and then you will find lot of results. i'll choose the one with most downloads here. 200,000. click on this one and then download this and create an account if you don't already have it. let's move on to visual studio code where i'll put in the iris data set. here i am in visual studio code and you can see the iris data set here. and here i also have assets which are basically pictures from wikipedia which i will be using for the front end later on.",367,0,17,CPuyUvnqEi4
1,"and here i also have assets which are basically pictures from wikipedia which i will be using for the front end later on. you can see this is the pictures for iristosa vericolor and virginica. you know that it's three different flowers and the iris data set. we can take a look into the preview. it looks like this. we have sample length, sample width, petal length, petal width and then we have different species. that is it. let's start with creating a virtual environment. uv vin and then source van bin activate and uv pip install ip kernel to work with jupy notebook. we need scikit-learn and we need fast api. we need uv corn to work with fast api uv corn. and then we need pandas. that is it. maybe seedorn as well. yes, that should be enough. yes. then if we need something more, we'll install it on the way. let's start with some explorations or creating the model actually. model develment. i will call it b. then i will also create a folder called models where i'll place in the finished models later on. let's start with here we'll create a random forest classifier. this is the model development stage. we start with say that you're in different teams. you have the data scientists that work with de developing the models. we'll make it very simple here. then afterwards you give it to for example machine learning engineer or data engineer that uh creates the api and then serves the model. that could be one way to work with this. okay let's choose pandas here. import pandas as pd and then we will have df equals pd csv.",366,17,46,CPuyUvnqEi4
2,"import pandas as pd and then we will have df equals pd csv. we'll go into data iris.csv and then the f head. and you can see the data here. okay, first time takes a little bit longer time, but that's fine. we can do other things at the same time. yes, now you can see it here. sle length, sele, petal length, and petal width. then you have species. let's do df.escribe here to see some statistics. tpose it. you can see here sele length it goes from 4.3 to 79 and sele 2.0 to 4.4. these values we will use later on for restricting what our api can take in as value that you don't want to have anyone putting in iris flower or lengths that are totally outside of normal iris iris species. let's take a look into the species here. we'll see how it looks like. okay, it's in in letters or in the in language strings. we can take a look into value counts. we can see which classes there are. yes, there are these three classes. satossa, versol and virginica.",241,46,64,CPuyUvnqEi4
3,"satossa, versol and virginica. okay, then we can move on to see we can move on to let's see i will have import seabor as sns to check out sns.parplot pair plot df and then corner equals to true and we will have hue equals to species just to look if is the data separable we don't we won't make a lot of data analysis here and feature engineering instead we will just create our model soon but i just want to take a look into this one okay here you can see that petal length and sele length are quite separable most of them is quite separable this shouldn't be any problems to create the classifier. and then to create the classifier, we can create here random forest classifier. now it's time for from sklearn.semble import random forest passifier. then we have model selections. so from let's see from sklearn domodel selection import train test split and then x comm y equals to df.drop let's say drop species. so we'll have axis equals 1 df of species. then we'll have i'll just copy this from the lecture note. we'll make it a little bit difficult. we have a test size is 0.5. and then we'll have the classifier here equals a random forest classifier. then we'll have classifier.fit x train y train. and then we will do y prred equals classifier.predict. i'm going through this very quickly but if you want more explanation on how these things works, check out my other machine learning videos. now we have y bread.",339,64,78,CPuyUvnqEi4
4,"now we have y bread. y bread you can see there's we can take a look into colon 10 for example here and then we'll take a look into y test colon 10 you can see that let's see dot value that you can compare them okay now you can see for example here iris versus color is the one we predicted this is the actual value that is the true value iris vica etc note that the random forest we don't need to scale the data okay that is almost it now we will do some metrics and from skarn evaluation sklearn uh metrics import uh classification report confusion matrix confusion matrix display and then we'll print the classification report y test y prred and we will print we will do conf cm equals to confusion matrix of y let's see y test comma y bread pred and then we will have confusion matrix display of cm and then plots. okay, you can see there's some error here. yes, and this looks really good. we're satisfied with this model. if you're satisfied with this model, what you do is that you train on all data. basically is that we train with all data and then what you do is that you want to export the model. these are a few steps. in order to do that, we do a classifier fit x, y and then we want to export this one. we want to import joblib. joblib we use to dump the data.",330,78,88,CPuyUvnqEi4
5,"joblib we use to dump the data. job lip dumped the model basically classifier i call it i will go into models slash iris classifier job liib and i will do some compression equals to this is a simple compression three there's different levels for that then i'll choose a protocol equals 5 basically this is for compressing the model and then you can use it to load the model again. basically, if you look into the models here, you will have this i classifier job liib. let's try it out. to try it out, we do like this. classifier equals to job liib.load. we'll do model irisclassifier joblib. we load this one. and then we need to create new test data. basically for new test data you can do like this. new test data equals to let me just take out yx test and we can take a few of them. we can for example take the first row here. we can take x test x test of zero. you can take a look into y tests of zero and you can see it's iristosa. what we want to do is that we want to for example we want to do a test on this on this sample here. then we can do classifier predict on let's see on this one. let's see. does not have valid feature name was fitted with feature names. okay, we probably need to do let's see i will call it the new test data. new test data equals to or i'll call it sample test data. sample test data equals to pd data frame of this one. then i'll take a look into this one.",366,88,109,CPuyUvnqEi4
6,"then i'll take a look into this one. yes, it looks like this. but we should have it t actually. yes, that's better. then we can put in sample test data. you can see it gives iris versol. okay, the iris versol is actually correct because we take a look into white test.lock zero. then you have iris versus color and you do classifier the predict sample test data and you get vessic color here and you can see here that this is the sample test data and here is the x test okay you have zero you get this one then you send it into the predict which clf is the one that we have loaded now from the models now we have our model finished and it's time to create our backend our api api.py pine to create our api. we start a new terminal. we make let me see from fast api import fast api and then i will take api router as well and i'll import pandas as pd and we'll import job because we need that to load the model. then we will need to have we will also create a constants here constants.py pi from constant we'll have from path lib import path and then we will have assets p we will have different paths here we will have models path equals to path then the file dot parent slash let's see models copy this one and then we will have data path data and we will copy this one. we will have assets path assets path and then we will have assets here. okay.",352,109,121,CPuyUvnqEi4
7,"okay. then we can from constants import we will have what path data path and we will want a models path. this should be it. also we want to have from pantic import base model and field. now let us create our app. app equals fast api and then we will do read data. we need to create our df first. df equals to pd. read csv of data data path slash let's see slash iris dot csv return df.dict. that should be it as a starting point. let's see. uv corn api colon appreload and then we'll go into this url. i will divide this one. make it a little bit bigger here. and read data. let's see. we'll have to have at app.get. we can start with slash api slash iris slash v1. okay, now we have a lot of data and it looks really strange to have it in this way. what you need to do is take the orient equals to records. save this one. and now it looks much better. now you have each record is in a dictionary, a json object. we have an ar json array of json objects. what we have now is that okay instead of doing this i will just move this one into an api router. we will do router equals to api router prefix equals to this one. then we will need to do router instead. router.get and this is how we get that one. then we need to do app.incclude router. it's important that this is after the end points. now we will have a router here. router equals to router. save this one. reload this one. it works as expected.",369,121,155,CPuyUvnqEi4
8,"it works as expected. if we go into just like this. let me see. not found. because this is called router.get. we do api router. we do include router. save this one slash slash docs. you can see here api iris v1. if i get this one, try it out. execute. and you can see, oh yeah, it is this request url. basically, basically we don't write anything here. and we get this prefix by default. you need to go into api iris v1 to get to this point, this endpoint. good. now we can create our prediction. that's quite easy. but for doing the prediction, we need to put in we want to have some validation of the data as well. to get the validation, we have the fantastic library of paidantic. then we need to do what i want to do is that here after app we can do a request response schemas. basically we get the data validation class iris input. i want to have the base model here. bm let's say bm base model colon sele length. and this one is a floating number of fields. field greater than equals to. here it's good that we go into the model development and we can see our values here. for example, here we can take greater than equals to 4 less than equals to a little bit more than i have picked the values here.",309,155,182,CPuyUvnqEi4
9,"for example, here we can take greater than equals to 4 less than equals to a little bit more than i have picked the values here. i will say greater than equal to four less than to equals 5 8.5 then on and fourth i will just copy this one from my lectures from my preparations i've just looked into the model development and saw a little bit of values that looks i've widened the range a little bit but not very much okay now we want to have a response schema as well class prediction output as a base model. this one will have predicted flower. this one is a string. basically, what we want to do now is at router.get and then slash predict. this is the endpoint. for this one, we want to have the response model equals to prediction output that it checks that it's a string. then finally we do def predict flower and we will send in a payload is basically the data that is sent into this endpoint iris input. the payload is of type iris input and we use this to validate the json that comes in. what you want to do here is that you want to take model dump of the payload. when you do model dump of an i'll show you what this means. i will go into not modern development but uh say explorations i pinb here i will show you if we take this class that we created here this class here okay we need to have pyantic from pantic import dm base model and fields okay to create a this is our model iris input.",362,182,193,CPuyUvnqEi4
10,"i will go into not modern development but uh say explorations i pinb here i will show you if we take this class that we created here this class here okay we need to have pyantic from pantic import dm base model and fields okay to create a this is our model iris input. if you look into iris input, it's it's just a a normal class. then we can do iris input of sele length equals to value say 4.2 two sele width equals to 2.5 for example petal length equals to say zero just to make it fail petal width equals to one okay if i run this one you will see ah perfect there's a validation error one validation error for iris input petal length input should be greater than 0.8. pyantic is amazing. thanks to this, it says that this one needs to be larger than it needs to be greater than 0.8. we can 0.9 for example. and you can see this is our iris model or iris input. if we call this payload, i can take payload here and run this one. and then i will do payload dum. and you can see that we get a dictionary back. that is very good. if we get the dictionary back, we can send this right into our we can create a data frame based on this one. i'll show you how to do that. import pandas as pd pd dataf frame payload. this one won't give us or this one will give us a dictionary data frame actually. but we can do it with model dump to get it into json. then we will have index.",365,193,209,CPuyUvnqEi4
11,"then we will have index. we'll need to have zero here. perfect. now you have a data frame. yes, we could have solved it using just payload without model dump but that is fine. we have this data frame and this data frame we can send it into job as we did before in the api we will perform this we want to have data to predict equals to pd dataf frame and here i will place in my payload dot model dump and then i will have index equals zero. when i have this, i can do classifier equals the joblib dot load models path slash iris classifier dot job. now i've loaded my classifier and i can do prediction equals to classifier dot predict of my data to predict then return and we'll make it into a predicted flower colon prediction. run this one. open this one and do try this one here. try it out. execute. we will get back here predicted flower colon string. let me see why. print prediction. okay. and i will run this one. x rerun this one. try this one out. predict endpoint. ah, get we need to have post. of course, we're posting in data. okay. post. try it out. sending this. yes. execute. we can see. ah, there's an error here because let's see. predicted flower. let me see. yes, prediction of zero because we want to get the zero value of it. okay. try it out again. and we get 200. we get response body. perfect. we get predicted flower and iris stosa. this is what we want. okay, let's try this api a little bit. we have how to use this api. we have this request url.",375,209,251,CPuyUvnqEi4
12,"we have this request url. you go in, you choose this request url and what you do is that you send in the data as you send in object or a stream. and basically you can copy this one and go into your terminal and you can see that it will work in in my terminal and just paste this in. and you can see back comes predicted flower aristosa. it works like this. we can try out different values here. for example, i have here five. let's see. execute this one. it doesn't work because of an error because it cannot be. let's see. it should say validation error of course. oh, it was a successful let me see. no, it's not error. yes, error. oh, here input should be less than five. okay, input should be less than five. 4.5. then i can try this out. and you can see it predicted iristosa. okay, good. you can try test this out and you will get different predictions. now i'll show you how to get this data using python. here i need to install uvp pip install okay. then i will import what i want to do is making a request. basically to do that i will have let me close this one. close this one. we have url equals to i'll copy this url here. copy. paste this one. and then i will have a payload. i'll just copy this one here. payload equals to this. let me see. okay, i have this payload and then i want to do with i'll have a timeout.",347,251,285,CPuyUvnqEi4
13,"okay, i have this payload and then i want to do with i'll have a timeout. it will time out after 10 seconds as client and then response equals to client.post post url and i will send in json equals to payload and that is it actually response dot yeah raise for status and then return response or not return response actually because i'm not in a function uh i will print i'll just write out the response here and you can see response 200 okay cool what is the response then response json ah, predicted flower iris satossa. cool. our endpoint works. and now we can place this into a front end. for example, we can place it into uh streamlit. we can place it into typi front end. what we do is that we connect to this api. but i will leave that for the next video as this video has become quite long. in this video, we have gone through how to create a random forest model for iris data set. then afterwards, when we have this classifier, we have trained on all the data and then we have exported a job liib model. then this job model, we use it to in our fast api application. we created a fast api application to serve this. we created an endpoint that was for prediction and then we tried out this predict both in the docs and in the curl using terminal and also in python. we tried it out using you can see that it worked to classify it based on on the json that we sent in as payload.",354,285,299,CPuyUvnqEi4
14,"we tried it out using you can see that it worked to classify it based on on the json that we sent in as payload. and then in the next video we'll connect it to a front end. for simplicity, we will choose streamlit. but maybe in the future, we'll pick also typi and maybe some javascript front end such as react, etc. that will be quite interesting since and when we have this api layer, we can serve whatever model we want and whatever data we want to whatever front end we want. this is super cool and interesting. thank you for watching this video and i hope that you learned a lot. see you in the next one. bye.",157,299,307,CPuyUvnqEi4
0,thank you yes uh hey everyone welcome to this live stream i am aniket maria a developer advocate at lightning ai and i'm josh starmer uh a lead ai educator at lightning ai awesome so in this live stream we are going to talk about how you can build a machine learning pipeline like lego games so let's get started hooray let's do it yes so before we start we would like to hear from you so this is a qr code which you can scan or go to the url mentioned here to fill up a feedback form you can see you can submit what would you like to see more from us and we will make sure to happen that and what would you learn after the end of this live stream you'll learn how you can train a psychic learn model on cloud and how you can use lightningi platform to build machine learning pipelines on cloud you can connect all these machine learning components and build a big platform big um machine learning pipeline for yourself music so why this lego thing if you're wondering we have these small little lego games lego uh plastic pieces which anyone can connect together to build something really big for an example you can dig an airplane just from this little plastic components here we have some components of machine learning components like training component models to model serving monitoring all these components can be connected together to build something big we have built a whole machine learning pipeline just by connecting all these components isn't that cool and the lightning framework yes and with,358,0,0,4iLUKE3TazY
1,the lightning framework you can connect everything to build a platform for yourself so let's get started with building a machine learning component and we are going to use psychic learn to train our model and then we use we use the same code training code to convert that into a lightning application so i am going to use a jupiter notebook i will show you how to train a decision tree classifier and then i will use the same code to turn that into a lightning application and print that on cloud very cool yes and you must have heard about this very famous data set called irish lava classification data set and you can see these are the three flowers and the data set of the features of this data sets are the petal width written length simple width and simple length we are going to use these four features to classify the flowers into these three classes cool and first of all we will import all these libraries and uh all these functions which is going to make our life easier let's run this can you tell us what those libraries do or modules do oh yes so first we import load iris it is a function which scikit-learn provide us to load the iris data set so it has made our lives easier then this is a tree module this stream module defines various uh tree algorithms like decision tree random forest etc and then we have printers brick so whenever you are training a machine learning model it is usually a good practice to split your data set into training and,358,0,0,4iLUKE3TazY
2,test it so that you can evaluate to your model and you can check whether your model is over fitting or underfitting finally we have imported something called matplotlib it is a very useful library in data science that you can use to plot different kinds of charts and histograms etc cool let's let's load the data set and our data set contains 150 rows and four columns that means 150 different uh collection of flowers these are i have printed some of the classes which are three settings are vertical around virginia and these are our features name okay and before we go further i'm going to make sure that uh in this tutorial we are not going to focus on like learning decision tree the aim is to learn how to train the this is a machine learning model on cloud so if you want to learn the internals of decision tree josh has a great youtube video tutorial you can go to his channel stat quest and check that out cool bam bam let's plot our flower data set uh before we do that can you actually go through the code that we used to plot it um because it looks like there's a for loop or something um yes so this is our code for plotting the data set so what i have done here is i have iterated through all the data i have separated the um i think i have yeah i separate metal width and petal length and i have plotted them on these two axis so you can see here i have iterated through all the target names,358,0,0,4iLUKE3TazY
3,which are settings are versicular and virginica i have extracted respective uh petal with petal length and then use the mat.live scatter to plot these data cool finally just some cool stuff to show uh the label in x-axis y-axis and the title in the chat and here it is you can it is very easy to visualize your data set it is an easier data set so you can clearly see uh this blue concentrated color points here are belongs to the cytosa flowers then these rs points belong to the versicolor and then the green points belong to virginia now let's split our data set into training and test set then we build our model from the decision tree classifier class and finally we train our model with the training data set let's run the cell and we have achieved 97 accuracy on test data set that's great hooray that's awesome yeah and one really cool thing that scikitlan provides is you can plot your decision tree so decision tree has been plotted with this tree dot plot tree module and here i have just provided the classifier that i just trained and here it is it might not be much clear i guess but let me zoom in a bit cool so you can visualize how your model is going to make that a particular decision so you can see if the petal length is less than equal to 2.6 centimeter it belongs it goes into the left node and you can see it is it belongs to class sixers and you can confirm that from the chart you just plot it off,358,0,0,4iLUKE3TazY
4,right there yes so if we see petal length is less than 2.6 all points from setosa is concentrated here so it is very easy to visualize all these things your scikitla very cool hooray yeah now we have trained our machine learning model so what next we are going to use the same code to turn that into a machine learning application lightning ai framework provides us a very easy and useful way where you can train this app train this model on cloud so let me go to my vs code here so anakin what you're saying is like we just started off with a really tiny data set and that's fine for running on our laptop but if we had a really huge data set we might want to do that in a cloud and so now you're going to show us sort of like what we need to do if we're going to use lightning what we need to do to do that like what things we need to change and to make it work is that is that with a plan yes so when you have a very small data set it is easier to train on a small device like your laptop which usually have lesser ram but suppose if you have terabytes of data and if you want to process that you cannot really do on your laptop so with lightning ai framework you can basically you can code everything on your laptop you can run that on your laptop and once you are ready with just a simple command you can push whole code to cloud platform and,358,0,0,4iLUKE3TazY
5,your code will be executed in the cloud so i'm just going to show everything how you can take the same code turn that into a lightning application and deploy on cloud cool oh so this is we're not just going to run it in the cloud we're going to deploy it in the cloud this is going to be we're going to create an app uh so that it has an interface a user interface it's got a front end and it's it's this is this is we're not just like training something in the cloud we're training and then we're deploying we're doing like a whole pipeline worth of stuff it sounds like yes so like we said like a lego the first thing to build that model training component then we will build a model deployment component you can connect this to make a machine learning pipeline and deploy the whole pipeline on cloud once the pipeline is deployed you can see the ui on cloud you can share that link with your friends your colleagues they can open it try your application right there in their mobile browser their desktop anywhere i can't wait so as you can see here these are just the same import statements and one difference here is i have imported lightning and um i have imported this just so that i can use some very cool stuff like i'm going to show you here so first i have created a class called sk learn training which inherits from lightning work lightning work is a building block for running long works for example a model training task model,358,0,0,4iLUKE3TazY
6,deployment downloading a very big data set which is any kind of work that is going to take a long amount of time we can use lightning work to execute it okay so our model training might take a very long time that's why i have used lightning fork okay in this case in this case it won't take very long because we've got such a small data set but in practice if it's like a real data set we might spend a lot of time downloading it or or installing that data data set and then we might spend a lot of time training on it is that yeah exactly more realistic okay cool yes so in real scenario you can just uh replace your data loading part and if you have you if you are training a different model you can replace that and in fact you can build a different separate component for data processing for example if you are downloading the data set from a bigquery uh table or something which usually has very long amount of large amount of data you can build a separate component and connect it and connect that here oh cool i'm going to show that later once we have built our training component and the deployment component i'm going to show how you can connect those two to build a pipeline very cool yeah so let's uh see what is here we define the init method and here we have passed something called cloud compute what is that and here i am printing a circuit learn model suppose if i had a deep learning python model,358,0,0,4iLUKE3TazY
7,i would need a gpu so that i can train my model faster so here you can specify different kinds of uh different kinds of machines so i you can specify cpu cpu small your gpu type for instance t4 or a10 and you can also specify disk sizes so i don't need a very large disk for just for example i have entered 10 here so it will be 10 gigabytes of hard disks for my application yeah which should be plenty for our 150 row data set yeah more than enough hopefully and then we have something called a drive this drive is a persistable storage object so once we have trained our model we need to store it somewhere so that we can use it later for deployment or maybe download it and then uh use it on our local system or some somewhere else so this drive is a shared storage which we are going to use to save our model so we have created a model storage object and then we have defined a function called run so this method is exact place where we define all the stuff that is going to run when we start our application okay so you can see here all the model training part exists in the run method download the data set print display initializing the model training the model checking accuracy and here we have some additional steps like the dump our trained model into the disk first and then once the model is dumped into our disk we are going to copy that into the share drive using this drive api so,358,0,0,4iLUKE3TazY
8,what we have done is self.modelstorage.pot it will take the file from the file system and put that in the share drive which we are going to use later so what you're saying is in terms of downloading the data and training the model we're doing exactly what we're doing what we did before the only difference now is that we are going to save the model uh we're gonna write it to a file and we're going to and we're going to move that to some persistable storage place so that we can use it later yes exactly very cool and to run this component we just need to initialize this class and then create a lightning app from our imported lightning library and just pass this inside the lightning app then we can open our terminal and run the school command which is likely run and our file name which is app.pi so if you just enter the command here it will launch this application on your laptop if you want to launch this application on the cloud you need to add the slack called double hyper cloud you can even name this application so let me name this as live stream and then press enter i love i love that that all we had to do to launch this into the cloud is just put dash dash cloud yeah that is very handy and cool yeah as a machine learning engineer one doesn't need to learn s3 aws anything they can just get started so that's really super cool so let me open my dashboard here i have a new application which is,358,0,0,4iLUKE3TazY
9,let me send the logs this dashboard so can you just tell us how you got here um i mean you pulled up a web browser and um yes so i have already logged into lightning ai but if you haven't you need to go to lightning.ai and go you need to first sign up for the platform okay if you sign up you also get three credits for yourself so okay you can use that to run any machine learning application deploy anything for free yeah very cool so we can try all this out no there's not going to be any cost you know we we just sign up we get three free credits and then we can log into this this dashboard that you've got right here so it's pretty straightforward yeah exactly and i am using my free lightning credits to run this application cool right now so uh in the logs you can see i have a work here and my work is basically the sk learn training component if i expand this i can see i i get the same test accuracy which i receive in the jupiter notebook which is 97 so we can verify that we didn't make any mistake and we are ready to go so uh aniket i hate to make you do this but is there any way you can zoom in on this uh oh this window a little bit uh just so we can see the fonts a little better so we see that that at the work so the very first thing there's a check box that says wait for machine availability,358,0,0,4iLUKE3TazY
10,so i guess we didn't have to wait very long which was awesome uh and then it does it installs things for us um did you create that requirements.txt file or is that a standard thing or yes so suppose if you have an application and you need a lot of different python dependencies what you can do is you can define this file here requirement.txt which is usually what we do and here i have defined like some requirements like i need scikitlan i need this create your library fast api and lightning like api access so you can define all the things that you need to run an application in this requirement.txt and lighting will make sure to install all of these for you um anakin also uh there's another person in india named aniket who asks what is the difference between dump and model storage dot put aren't they doing the same thing that's a great question actually so the damp here is if you see i have imported it from job live so the task of this dump is i have my decision tree classifier created here clf is equal to my decision tree classifier it is a python object so i need to serialize it on a file system so that i can copy it to somewhere else and load it from a different python interpreter joblift is used to do the same thing so i have serialized this on my file system okay but so once i have trained my model i do not need this uh machine anymore so i'm going to stop that machine and once i stop,358,0,0,4iLUKE3TazY
11,that machine my file system associated with that machine basically my hard disk for that machine will also stop and i won't have any longer access to the machine my machine learning model so this thing here the strike thing here is a provides an api like put get get for retrieving put for copying so what it has done here is it took my model on my current file system and pushed it to a share drive which i can use later okay so so in a nutshell what dump what dump does is it creates a file out of our um out of our whatever whatever we did in scikit-learn or whatever our model is dump creates the file and then the put the model underscore storage.put what that does is it puts that file somewhere that's not going to get blown away once we turn off that uh that cloud computer that we've been using before it puts it somewhere that's persistent and we can call on it later is that the is that was that a good summary yes exactly so if you see it actually you can access that model in the ui here so okay cool you can come to the ui click on artifact and here you can see i have oh there it is bam yeah great yeah that's awesome okay cool so uh uh so going back so i'm sorry for the interruption i feel like i i'm responsible for it all um but uh so going back so we you did that command line where we ran the program and we put dash dash cloud and,358,0,0,4iLUKE3TazY
12,then we were able to go to our dashboard and we saw that we started up a machine and that we we'd configured it according to our requirements.txt file and then what was the next step can we go through the remaining steps of that process yes uh did you mean the remaining process for uploading it to the tra to lightning platform no because what you've done is you'd gone to the dashboard yeah sorry this is so confusing but you'd gone to the dashboard and there were these there was these four or five check boxes oh this one uh yeah no no he clicked on not that one click on uh what was the one we just did um okay did artifacts but we went to i thought we had like a list of like it's there was a group a purple check box and it said starting up you know looking for a server and then uh yeah these yeah yeah okay and then it said install dependencies for requirement dot text um and then we we hadn't talked about the next step the run initialization so can you talk about that yeah so so first of all what lightning does is it creates a machine i have a cpu machine so it creates a cp motion for me installed if i have any requirements then suppose in usually in python projects we have something called setup.i which defines uh certain aspects of the project it sometimes you can also insert your recurrent dependency and uh some other scripts which you want to be executed by installing your uh repository so can,358,0,0,4iLUKE3TazY
13,you talk about the one for this project specifically like what did you have to do for this project so for this particular project i this was a very simple script so i didn't need any setup.pi and edited and create one so lightning just ignores and like you do not have a setup.pi so it just also moves and move to the next step so if we don't have one it's no big deal it's just if it's if we've got a really complicated project sure we might have something called setup.py but if we're just starting out we're doing something super simple we don't have to worry about that yeah exactly cool so you can basically just start from a single python file like app.pi you can define everything there and even there is a very cool function um in the framework so if you just have a single dependency for example scikit-learn you can create this hash rank like you can create this comment and then just enter pip install psychic learn and it will install that for you oh very cool so that so if you do have complicated packages or whatever that you need to have you can either you can put that in the setup.py can you put that in the requirements.txt yes so you can put that into common.txt but if it is very like just a single library why would you create a new file yeah yeah so we can we can skip that we can just put it right directly into our code i love it bam yeah exactly cool so for running for installing this you,358,0,0,4iLUKE3TazY
14,need this uh command you need to add this double hyphen setup so it will install this for you okay and the usual serve develop iphone cloud and if you want to name something if you do not name lightning will automatically name a very like automatically choose a very cool link for you that's awesome now you got me like wondering about all these cool names like yeah file number one no it's so um i i think it it chooses a very like creative name so you should definitely try it out like without a file name okay so it will create a very cool name for you okay i'm looking forward to the cool file file names um uh i think we have a question uh but i i kind of want to save it for the end because someone someone wants to know how this framework is different from airflow uh and what i think we should do is maybe we should just go through and just see how everything works and then we could talk about that at the end is that okay yeah okay let's keep going cool so once we have trained our model what next we need to deploy it so that we can expose it to some other person who can use our train model for example if someone trains a fraud detection model someone else might in their company might need to use this that as an api to classify if the command request is a fraud or not so what i'm going to do next is i'm going to show you how you can build,358,0,0,4iLUKE3TazY
15,a beautiful model ui for show equation how to demo your model so let me open the next files and here i have okay so just usual stuff import all the things i am going to use something called radio which is a python library to build ui directly from python without coding any react or html css anything at all okay cool the next thing the next new thing that i have imported here is called the serve radio component to import this you need to do from lightning run lightning dot app.com radio lightning has a lot of different components for example the surf component auto scalar component that you can use to scale your api and you can just connect this to build something big so what i'm going to do here is i have created skill and serve ui class which enables the search radio okay class and i have defined inputs outputs and examples okay my inputs are the simple width length width and height and all these are numbers so this is just some greedious stuff you can check the documentation of radio if you want to know what this number does yeah but plainly speaking we need a floating point to enter the simple width so we have used radio dot number yeah so these are just widgets so that the user can enter these values uh so they can search where they can use our decision tree that we've created yes so these are just so when we define this and when they run we run this complete application in the ui we will have multiple input forum,358,0,0,4iLUKE3TazY
16,with these names simple with simple length and we can enter the value there click on enter and my mod i like the way i have defined this server is it will go through the back end and it will classify what does this these entered values represent and very cool classified value will be presented on the screen okay cool those are the outputs yeah so my output is going to be a text what is the flower type okay and i am also going to return the image oh cool if i have a sentosa i am going to show you image of the sentosa cool here i have just entered some examples so that i can just select one of these i i have took this from the test set so that i do not cheat here so you can use those examples to demonstrate how how our app will work is that the idea okay cool and just usual the app define the init method and here again i have the same models to use api and what i did last time was i put my model into the shade right now i'm going to retrieve my model from that share drive okay so the next method we define here is build model okay this build model we first retrieve our model cell dot model storage dot get our model file name it will download the model from our share drive to the current machine file system and then we are using the job live library to load the model and we just return the model i love it next we have,358,0,0,4iLUKE3TazY
17,this method called predict okay and as the name suggests we just we are just going to take all the features simple with length from the ui we are going to make protection from our model here model.predict and finally we are going to return our class name and damage yes an image i have loaded the image from the pillow library and just returned it after you have created all these things just create the component as usual create the lightning app keep the component inside the lightning app and it will create the application for you so let's run this component our sweet little command lightning time app then surf ui.pi which is about filing okay let's launch this on cloud damn so what this will do is it will take my code it will push my publish my code onto cloud and start the machine and just like start executing my code so let's go to foreign i have live stream locks and in the work oh cool it is currently installing the dependencies from my requirement.txt okay and once it is done it will configure everything and start running my application and again we don't have a setup.py yes so again we do not have a setup.pei yeah so meanwhile let me show you our art so if you want to learn more about different uh features of lightning what we have done with lightning we have a block base here and recently we have scale stable diffusion three times so we went from six times six seconds to two seconds that is just crazy so we have made a bunch of,358,0,0,4iLUKE3TazY
18,different optimization centrics like using deep speed good optimization and here is the graph of different optimization that we did and the different speed of inference speed that we got so this particular thing also is uh open source so you can just go to our github repository just check that out clone it and launch it on the lightning ai platform very cool let's come back here cool this is running oh awesome let me go to the web invite to see my so you just click on web ui and it automatically because we are using grad io and work and those so those are the those are the inputs that you created very cool yes so these were the inputs that i had created by gr dot number so these are just some forum and i had specified three examples so if i click on any of these it will auto automatically populate here oh look at that yeah click on the submit button it is a versicolor and here is how it looks oh i love it this is amazing and that was not that hard yes that was just few lines of code and i mean and now you can like you've got a url you can like text it to your best friends and just say hey check it out what put in your favorite petal width and simple width i'm gonna be like oh yeah let's do this um but it's cool i mean it's it's a simple example and it but what i like about it is it is it's you know with the nice thing about the,358,0,0,4iLUKE3TazY
19,iris date is that it's so simple that we can we know exactly what it's doing and and now that we know exactly what it's doing i feel like i have confidence to do more complicated things right this is like it's like the hello world of creating a web app or on webml app we've we've like everything we've done we've verified and we validated that um we're getting exactly what we're expecting we're getting the right output um yeah and i i think this is so cool um so uh so it all just sort of comes together can we talk a little bit more about um how you how these pieces all came together just to give an overview we don't have to go into details but just like let's just can we just have a summary of how we put everything together did you mean how we put everything together this in this component or just combine everything to build a pipeline well for the pipeline so we started off with you know we got the data we built the model and then we saved that model and then we were once we were done we we sort of turned off that cloud computer because we didn't need it anymore but because we'd save that model to that shared persistent resource and then you then you created the code for the user interface and and your and you we loaded our saved model from that shared resource and then all we did was was run it from the command line and it deploys it in the cloud just like here and and,358,0,0,4iLUKE3TazY
20,the pieces just kind of come together yes so let's see uh we have created two components and how we can connect all this together to build the whole pipeline let's do it yes so i have named this pipeline as lego okay oh i love it we're coming back to the legos yes so if you see the code is very simple first thing i have import site sklr training component okay and this skill and training class is exactly what we just created here so what i have done oh i say you you named it sklearn training class okay got it yes got it so i just imported that and similarly i named the my ui thing skill and sub ui and i just quoted it from my module okay then i define a class called ml pipeline and this internet's lightning flow okay unlike last time which was a lightning fork yeah so lightning flow manages long running tasks and by long learning tasks i mean lightning work okay so basically my ml pipeline is going to manage my sql training and sqln serving so the flow is going to manage my box with that and inside the init method i create object for both of my components the training component and the serving component then we define the run method in the run method it is just like a flow you know uh how i'm going to execute all my flow so the first thing i'm going to do is run my training component so just like plain english i have run my uh training component.run and i mean it is,358,0,0,4iLUKE3TazY
21,easier to understand like uh once my model component has succeeded i just i'm just going to deployment like run the deployment component so the api is quite intuitive like if you see the code you try to execute a little bit play with it and you can do a lot of crazy stuff with it like you can build something great using the framework then i am so basically so far i i feel like i'm i'm stepping all over your toes right here but but what i'm thinking is you know how we just did that summary of all those little bits you know we build them we download the data we build the model we create a file from the model we save the model we all those little bits what you've done is you've just streamlined it so that we've just got it in two lines of code uh and this lightning flow is going to take care of everything for us and we don't have to do it manually we don't have to like you know like the first we don't have to like manually hit lightning blah blah blah blah blah and do it for each step we we can coordinate it all right here yes so this flow basically coordinates all the different components for us cool and so we have two components the training and deployment component the training com the training job will happen in one machine and the serving job will happen in another machine so lightning flow is just going to take care of launching different machines terminating these when complete completed all of this,358,0,0,4iLUKE3TazY
22,for us automatically we do not have to do anything manually um yeah i do have one i have a python question it's not necessarily i because i'm i'm gonna be i'm just gonna be honest with everybody i'm not that good at python there's one thing in the code that i'm like i haven't seen that before you've got when you define your run method uh on the right side it you've got an arrow to none what is that can you tell me what that is oh yes python mode for josh so python is a loosely typed language and if you want you can like uh if you want to make the code readable you can add pythons so for example suppose i have something like um flower number of flowers here and i know that it is an integer so for others who is going to read my code i am going to add a python like okay okay so i add a cologne and int yes so this is easier for others to understand and it is a usually good practice to add typhens okay there are some um extensions which also basically um can check if your typhons are correct and oh four digits like vs code yeah but it's automatic suggestions when you like type your code like what should be next or like it will automatically color in your uh editor that you are making some mistake if your code is well you know type in dead cool okay i got it now thank you very much that was a nice python moment for josh starmer i always,358,0,0,4iLUKE3TazY
23,totally should that'd be awesome because i need to learn i mean i'm okay i just need to learn more and i'm learning a lot from you just watching what you're doing which i love oh no i i have learned a lot from you during my college so i used to watch stat quest like i used to winch watch that question well i love that i'm learning from you i feel like the you know the armor is getting all balanced out it's great it was just too humble josh well can we run this i'm ready to can we see the whole thing yes let's spend this in our we use the same sweet shot come on lightning run app lego dot dot bikes ah so let's name this something else like um lego pipeline uh-huh and press enter so this will just like upload my code start the machine and i will be able to see everything so it will come here okay in any moment okay here it is lego pipeline love it and let me go to logs i have the orchestrator which is currently pending waiting for machine availability installing all the different stuff from requirements dot text so earlier we when we launched our application the model training had one component the model deployment had one component in this component in in this application we have two components so we have launched both training and serving at the same time foreign maybe at this point now that we're kind of seeing how everything comes comes together can we talk about the question is how is this framework different,358,0,0,4iLUKE3TazY
24,from airflow yes so airflow is a dag and with lightning framework you can also create something like a dac but it is much more than that like you can launch you can deploy and ui with the lightning ai framework which i don't think you can do with airflow i i'm not an expert in your pro i am just like read about it's it so i don't think uh uh so these are very two different frameworks and uh with the lightning ai you can basically create a whole ml of pipeline like you can connect different components like monitoring and different servers you can deploy even react code on lightning so that is what lightning is i mean i'll be honest i've never even heard of airflow before because i live in a cave um but it does airflow like do you just do dash dash cloud and stuff automatically runs in the cloud with airflow and you have a odd app that you can then share with your friends or i last time i tried to run airflow was i think two years ago so i'm not really sure but i think you need a docker or something to run this okay i i'm not really 100 sure about that okay but with lightning ai you can just like pip install lightning and create the small file just do like internet after pi cloud and it will start executing in your cloud uh can i just pester you with other viewer comments while while we're while we're here um so another uh another uh nishant asked uh about the data set format,358,0,0,4iLUKE3TazY
25,um uh they wanted to know if we could use a json file or is or is there any particular format i'm not um yes can can entire data set send in prediction api any particular format like it's a simple yes um so i guess you can use like for training you can use any kind of format you just need to convert that into a numpy array before you send that into the second model and for using the serving here we have created an ui but you can also launch a rest api with fast api okay and or flask or any uh model or any serving framework of your choice and you can hit the api from a json or like um you can send that as a request body anything the whatever the way you have created your api so it really does i mean the file format doesn't matter as long as you've got some functions that allow you to access the data and convert it into at least in this example because we're using scikat learn we need to learn we need to create it into a numpy array so if we've got tools to do that who really cares what the file format is um is that about right yeah exactly um one last question uh angel asks is this the same as streamlit oh i guess uh the question is about the radio ui streamlit is a different library but it also is used to create ui using python code okay so it is just uh sim very similar to uh streamlit but not exactly simple could,358,0,0,4iLUKE3TazY
26,we have swapped out grad io uh with streamlit if we wanted to is that an easy thing to kind of swap out or yeah in fact uh lightning has a seamless component as well so wow if if you want to like build something with streamlit you can try that as well that's super cool so it's like no pain right there to switch from you know we can do it either way whichever one we're more familiar with i love that yes exactly i am bam so is our is our thing running it can we check it out yeah our thing is running so our model has been trained with 97 accuracy almost perfect the deployment has also the serving is also running so let's go to web ui here and this will look the same as before but it's totally different because this is because now we've combined you know all the steps into one um yeah and bundles it together so if we need to like restart from scratch we don't have to do each piece independently we can just run this one lightning flow to take care of everything yes exactly love it so just like that and we have the ui you can enter anything here and press submit to see what is the prediction of you from your model and if you click this button here you get this url which you can share with your friends they can open it in their web browser their phone and make the model prediction directly on their device i love it this that this is what i'm gonna do for,358,0,0,4iLUKE3TazY
27,the rest of the day i'm just gonna like check out different irises i love it yes yeah um well anakin this is fantastic i i i i learned i myself learned a lot uh it's uh it's it's the code actually looks surprisingly simple i just love it because you know for me like you know if you told me that i had to deploy a machine learning algorithm or deploy a decision tree or something so that people can use it i would i would my hands would start to sweat and i'd be really stressed out because i would think i'd have to do a lot of work and a lot of coding and just like and you've made this look pretty easy very reasonable each little block was like not too much code and i could understand what's going on and i even learned some python while we were at it i just love this yeah i mean just like less than even 20 lines of code so you can like do just crazy stuff you can actually we built a really cool product with the lightning framework called news and news is basically uh stay we defined stable diffusion there and with a beautiful funky ui there you can enter your text prompt and just generate image directly in the web browser and so yeah so we can scale this up to be as fancy and complicated as we want so we've done the most simple thing we've done the hello world of building a decision tree and what i like about that is we can validate that everything's working as,358,0,0,4iLUKE3TazY
28,expected but now that we've got that in place we can go we can start doing state-of-the-art stuff we can put stable diffusion so we can like create crazy pictures of cats jumping off of flying saucers into a a pot of you know spaghetti um and we could you know it'll generate that image for us so we can we can these little things that we did just it's super simple but we can now that we know how to do it and we can trust that it works and we know you know we've we've experimented with running things on the cloud and we're like oh great we can take this the same exact steps to build and deploy the state-of-the-art sort of in artificial intelligence right now which i think is super cool yeah it is amazing and uh if you are training a state-of-the-art model you might need to do like train models on distributed platform so it provide lighting also provide components which you can use to train your model in a distributed fashion on multiple nodes okay you do not need to configure any machine or anything you just need to import a multi-node component so that is really cool and handy for any machine learning engineer that's very cool so what you're saying is that rather than having to worry about all that infrastructure and like how to configure all those machines or even how to distribute stuff i can just import this one component and it'll take care of those details for me is what you're saying yeah exactly awesome i love it so i think we have,358,0,0,4iLUKE3TazY
29,one more question oh i think it's for you josh someone bought the candle but they need the pdf copy how do you get a pdf copy oh it's a good question um uh i don't know um i mean other than buying the pdf copy i think that's really the only option right now which is unfortunate i wish there was some way to bundle it but because there's like they're sold by different entities it's not easy to just bundle these things so i wish i could um but thank you for your support yeah i wish i could do some sort of like you know all access pass to the stat quest library um but that's uh maybe that's something we'll do in the future um also i just want people to know that uh if you uh obviously this is live but it's not going away if you want to review this um you can uh there'll be a url url available immediately after the live so you can you can take this url you can watch it yourself you can text it to your friends so they can watch it too um and i believe uh anakin but i'm not certain but i believe this code is also available and that instead of having to write it all ourselves and squint at the computer screen and make sure we're typing in exactly what you typed that we can just download it is that correct yeah exactly so this code is available on github and let me share it with everyone else yeah let's do it um so yeah you can watch,358,0,0,4iLUKE3TazY
30,the video and you can download the um the the code and also just to let you know we're gonna put this uh lightning has a cool blog with cool stuff that includes the occasional stack quest entry um but we're gonna have uh uh this video and the link to this and the um and the link to the code will be on the lightning ai blog so check that out um so yeah so there you go bam bam we are going to do much more of these kind of sessions and um please fill the feedback form let us know what you'd like to learn from us in the next live stream so see you next time yeah see you next time and and actually because i can't let people go without a triple bam let's just let's do a quick summary we did we created a lightning work to uh load the data and train the model bam we created a lightning work to create the user interface and use the model double damn and then we create a lightning flow to put it all together into one you know big thing that could that could be deployed easily triple back i love it super awesome well thank you very much anakin i learned a ton and i hope everyone had a good time today um until next time josh yeah thank you man,309,0,0,4iLUKE3TazY
0,hello everyone welcome back again my name is jessie and it is a wonderful tutorial we're trying to see how to save our machine learning models as an apa right so be in a very nice frame rate call first api which is a high performance framework that allows us to build eps with simplicity so let's see how to work with it so first of all let's install the switching story just going to go it pip install first epi right and then first epi requires you to use any sgi server which is called ubiquinone hi back on so it's going to be pissed off first api and then uv cone these are the main requirement to install or to work with first it professor let's move on i'm just going to quit my phone which is going to be create a folder called ml up then i'll sit into ml up then you create our first file called up dot pi the last loop in our file and unless start writing some code okay so first of all first api is very very powerful it combines the best of all the various treatment that you have already write as from flags to swagger to all of these things very interesting into one package making it quite interesting so now let's see how to work with it so the basic stuff is that just going to input pacific on so imports ubik on there's going to be the only be easy to save giving you you can't say power up that's the first thing then we need to go it from from fast api,358,0,0,mkDxuRvKUL8
1,imports fast api right the last one supposed to be capped off right that is how to work with it then the next thing that we have to initialize our app so let's initialize our up it up then i'm going to quit it's riley so it's going to be fast api press we can they are to option you can just put debug let's go to through here or you can omit it right so any of them is going to work perfectly so let's go without a twist that is a big idea now we need to be able to initialize or critter it mate so if main then it's going to give it our simple stuff which is going to be our unicorn that's run then i'm going to pass in the app so the app here is referring to this up here right and then it's going to take the nest again which is going to be host is going to be our look how host which is going to be 127.0.0.1 right and then the next one is another thing is our potent eyes to the poor that you want to listen to so by default less good pots which is going to be put it out right that is a big idea so we have created something very simple in pedic so we running it from this particular option now let's create our face also just like just like just like flux right so you also use decorators we're going to put our face the critter it's going to be up but gets tweaked enough it's an api so it,358,0,0,mkDxuRvKUL8
2,supports all the rest api verbs right you can just go with a blood kit then give you a route right so this how to create a simple route so there's going to be our route then it's going to be like this something very basic then from here i'm just going to define my function so let's call it us something very basic so you can call it as indus and then you just do whatever sort of a retain come put it in can't just return something like let's say our test is going to be hello api allow us write api buddhist satellite is it's not the very busy right so that is how to make it so you can just go with this particular function it is going to work or i can just put the asynchronous caption they all should right so any of them is going to make now let's run this simple app that you have created so i'm just going to go back to our location yes there are two means of two menus or vanity the first version option is just go to python 3 then the particular name of the averages under spike then it's going to loop in it and run x because we are using this particular option using the if me write it in the evening so it allows us people to run this particular app well without giving us any arrow so they're going to run give us the particular pot that is running on so let's copy this particular pot completely spot and then you paste it here if i surrounded,358,0,0,mkDxuRvKUL8
3,the face method of runnin our upright using python so it's going to run it is going to give us our test which was a test we found here right hello api body is very very interesting and very simple but they disadvantage of run it in this way that if i change this one to api borders to something like let's say api travis i save it it's good at it identify the change right but if i go with this option so that is a solution right it's not showing stuff until i close it so that is why we have to use the next option of running late so let's screw this one now let's run it with the next option which is going to be uv con uv cone right then i'm going to supply some other stuff with it what argument you be using it are going to go in it so the atom is going to be our up so this up here it's referring to this particle fr right the app is referring to this particular file here tells the name of the file then the next option is going to be our up here this referring to this but cooler up right so in case these powers nima's me this is going to be called me right it's going to be me right but here just leave us up so good to be up then the app again which is referring to this particular up here up right here that is a big idea then i'll just go which lets us reload so this is the only thing,358,0,0,mkDxuRvKUL8
4,we need to enable us to run to achieve account and then it's going to run it's right even if you make changes it's still going to see the changes and reload right as a physical idea so let's go on this little option professor is running on this particular port so if i come back here right and i refresh it again to refresh it now you're going to see our lava stay right to even i can change this one to the old one let's the api masters santa like this api masters i save it and i refresh it go to show the masters de right it's going to take the change here then we find if i refresh the back again it's going to give us the result right that is the basic idea which we have the lavas here the best if i refresh it is what is coming right if etsu that is the basic ideas who is going to identify the changes you take the change here and then shut down and restart again that is a basic idea which they reload so there's a best method of running it right even when you are deployment is one of the best method of running it ok now let's see how to add a machine aspect to it so first epi comes with a lot of things so with this little cookie of quittin if i come back to this place then just go back with backslash doc a group a flash talk i'm going to get the documentation a saga documentation which is very very interesting right we just,358,0,0,mkDxuRvKUL8
5,need to could have written very very cool right so it's loading the swagger or open api documentation very interested and then like i see the real stuff so cannot get here can see the various parameters the real option can let me try it out then give us the option of as keating something right very very interested and then get a lot of things can also do so this is including swagger is all right so in case you're using flux we drive use flash swagger to enable us to do that but first api allows you to be able to do that to get all at once together with this little code right this little code is allowing us to do all of these things very interesting and i don't know a little can also check for every doc if i go with redox don't give us a very nice documentation so dl two options do you have the docs and we have the red doc right you're going to load every doc they're going to be a very nice documentation very interesting and it's going to give us some interesting information about it very very interesting a very simple cool so now let's see some how to add some other stuff to it so first of all let's see how to pass in ultimate i'm just going to go to the same thing i half yet keep on to pattern on ultimate is very simple so it's very simple so i can just copy this one and then let's pass it here and unless you give it a different function so this,358,0,0,mkDxuRvKUL8
6,much and let's call it us let's say get item right to get attempt right or get item something very simple and then we can paste it here let's spit something like i say attempt something else like items they're not passing the query like me that i want to do so i can just pass in a name here so let's give it a speci name right it and then you just pass in the name here if i pass in the name you can just keep you can actually define the particular datatype right wait and this watch will appoint by game blesses strength there's going to identify that this particular stuff is only a string that is one way of doing it but let's go with the simplest method they fit and then what you can do is that we can now passed in our name here right you can pass in the name option here so let's give it our name okay so this is going to be test that's called us name pivot so something very basic so i save it now and i come back to here right we have our doc even if i refer the red rock going to see a whole new documentation for it going to detect the changes detecting the changes that is made right shutting down detecting the changes and if i even with the red dot can see that is giving us some interesting information about it perfect so it's going to give us our get itself right then it's also going to give us the gate items out of the bust right,358,0,0,mkDxuRvKUL8
7,we just let just this little code i've got to get items api documentation with the requirement we just named very very interested and even you can see how to work on it something very simple right these are out now if i go back to the dogs so let's go back to the dogs here it's going to be dog will give out the swagger option right which is very interesting and very cool lutely swagger it's even on the face index which is this place as well as that gets one and then when i get to the miss planet with a get option you can't do some interesting stuff so i can just go with the gates and i'm going to see the various parameters i need to supply the name that is required and then some videos interest and stuff right which the data type in description for the schema so which here i can even go with tryouts then i can pass in my values less personally name like let's say mark right if i great mark and i could execute it's going to pass the mac day you go see that just passed emma and i type here and then consider just pass it here also then it has given us is that our result body and then very interested right so that is something very cool about it the schemas are resolved here right name is mac that is very very interested so we can even run it with this vertical option right the same option that you run here you can run it i so i can even,358,0,0,mkDxuRvKUL8
8,come back to this place and they pass in my item they're not passing less a mock right if i greet mac it's going to give us stay just one in five format very interested so let's change it from this option to something different like let's say george it's also going to change it and give us the john very very aggressive so you can use it in several formats right so the user can i use it in this format or the user can't just go with the normal dogs option right and then it's going to give you get the other option of doing it in a simple way right that is very interesting very cool simple stuff out of the boss now let's see some other stuff you can also do and it's about a dandy machine and an aspect to it so it is going to be our camera last but doesn't mean i did behind this we want to be our amen mmm aspect right so how do we do that so let's first of all inputs job lip so you know to be our ml like it is that you be using civilian input your bleep job lips or the animal package but we didn't to serialize our up alright our model that you already saved and always so i'm just going to be using our famous module that yep interest in a classifier up you'll be right reckon only so i'm just going to go to our doctor writers i'm going to have models right a little bit idea so let's do that right so i'm going to,358,0,0,mkDxuRvKUL8
9,copy the models and put it inside the same location so this is my model so i'm going to copy it and i'm going to paste it here right so ready to be the same play that um so a model is here right so we have all our models here we have our gender transfer models right okay so let's see how to record it so i'll just come back to where we are now here then i'm going to load our victoria so it's going to be our gender doctor either then i'll go to obtain visit open let me expand it to return 0 or pin and i'm going to pass in my mother who's going to be our model so we're going to be our models so we are going into this particular for that we have right is our models folder and i'm going to go with our agenda vectorizer that's pickle that is a first time for you reading and then you're going to open it a sweet bite so that theory right pivot that is all now the next thing we need to do is that we need to be able to load it right we have open our file now let's do it inside a particular format so we're just going with gender and also cv as for a contact riser then i'm going to use your blip to load our stuff so job lipids load i'm going to load our gender victoria that's all that we need to track we are done with so we have new to quit our victories and unless we go on our,358,0,0,mkDxuRvKUL8
10,windows she's going to do something very simple vijender let's call this our envy or i hope this or less for this can be module in our passing same thing i did above so we're just going to be the same thing right this is going to be the same i just copied this one here and i'm going to change this one to is referring to this naive is here so envy motto not read about model you're going to open a sinful then from here i'm going to load it into my classifier gender cls for classifier then i'm going to use your bleep to load our gender now you this although it is very very interesting right so you can make let's make it to the same so it looks more the same so v then this is going to be alright so yeah now in it i've made to load our fact i this ruler motors now let's see how to use our models and on our advertisers within our up in a very simple so disciplines with a dslr aspect you can use poast or i can use guests right any of them go to xls professor for go it get and i'm going to go my route so going to be predicted just like i've been doing always right and then i'm just going to pass in like me just as we did here right this team a committee is really adventurer person is now you're going to pass on our name because a class why fascinate then i'm going to give it a sink then i pass in my,358,0,0,mkDxuRvKUL8
11,function so fragile is going to predict and then this function takes the argument of the name if it so not less work on it so we have a vectorized data let's go to our vectorize name right there now passing my gender classifier or gonna contact writer choose it to transform our name right into practice so we're going to be transformed which i've been doing all the time i'm going to pass inside list then to hurry so to re-write fs we're done with it now we have to send this particular bertrise name to our classifier so let's do us let's go transportation it's going to be very simple she's going to be our gender classifier window don't predict now passing my vectorize that is all that we need right if it is we are going to be getting our result so now i'm going to give it a simple condition to eat my prediction okay so they're getting a face value it's called to zero if that is the case and i want you to see that my result let's go to female right so 0 is for female and then male is for results is for one price so result is code to me right all the variability so we have to do something very simple now from here we can our retainer is also let's return it to return on it here a 10 then look up passing our original newsletter original it now name right which is going to be the name that we have i'm going to go with a predation pressure it is going to be our,358,0,0,mkDxuRvKUL8
12,prediction that we have which is thought to be a result found the variable x we have new to create a simple api the basic idea that we are trying to do the same thing i've been doing all over all over again but in this case we are using first ep a-- so let's see what you have so far so i'm just going to reload it won't identify the changes so if i go back to the redox refresh it hopefully not going to give us any arrows who's finishing the service we're going to restart now if i come back here hopefully is going to identify the changes perfect suit us identify the changes that's restarted now it has given us our prediction option right so we see that you can have our prediction rowdy also right so this is something very simple so i'll just go with the gate then i can open it here and then consider you have our name here then i can try it out like i'm listing the name is who are famously me i'll be even is mary then i'll go execute it's going to analyze it and give us their prediction so mary was sent to this muscular out where were sent to this request url and then because originally must mary and anna prediction as female very very interesting right so you can also do the same thing here so i can just come back to the url and go wait to predict mary just going to make and give us the same results right so total us prediction or journey miss mary prediction,358,0,0,mkDxuRvKUL8
13,is female very very interesting and also give it a name like let's say something different so that is a big idea right behind what you have done so fast if a great schmuck i hope muc is mil precise to mark was giving us a smell very very interested so our app is working well if i come back to my braddock most we're going to get some interest in the commutation right including what you have done so far the new start we added is going to add those two committed shifts to it a very simple way so so which i'll just write in one simple code here right on one once cuckoo birds are getting all of these things out of the post so you see that even as our in this hour gets items and our prediction route if i click on the prediction given us the parameters that are required the response and then the various information about it very very interesting and very very intuitive with just simple code right that is the power of fast ep a-- now let's see another way so this is for this for get right you can also do the same thing for post so you know just only get but kind of good news for post but and just copy the same thing and then let's do it post growth there art for put using post right the same option i'll go eat post now and then good identified changez i refresh it to give us two different options right one for the gate and then one for their post so let's,358,0,0,mkDxuRvKUL8
14,go back here i didn't find any changes so it's finished about process is going to rerun it again and give us our result so in a nutshell the basic idea is that first of all you're just going to import your normal package that is your account and then the first api then you pause me to not even use this you go to import job lip right you luteum what does any of you is right this is for appetizer and this is for our model right then the next thing we need is that we are going to initialize our up then we'll create our first route and our second route which we just did to know something and from the machinima learn aspect you can just follow through the same thing right this is something very busy and this one of the width of doing it the other way that can also define a model can also define class model and then you just pass in your stuff right there's another way of doing it which we will discuss later so that's the basic idea but it's the simplest method right very very interesting so let's see our results now so now if i come back and refresh again refreshing loading our redox purse we still have our in this get get get an end post right and if i click on a post will give us a different result right now i can come back to the docks and i was also going to wait for us so this is the docks then i can see it loading you have our,358,0,0,mkDxuRvKUL8
15,get get get pushed in and filled on a post like also post a resort to try it out post in a name which is jessica i'll go it at the cute because we're going to back right both of them is going to echo it using that's what the result give to us using post right so you can use this protocol option to get your result back and then this is how it's getting you can also get the resort so thank you for watching this tutorial so in case you have any question or contribution can just put aside a comment section below and please don't forget to subscribe stablest seen an assertion stay blessed,153,0,0,mkDxuRvKUL8
0,all right so thank you all for being here i'm rashad i'm a product manager at dado and today we're going to talk about deploying scikit-learn models in production so i should first tell you a little bit about dado even though most of you just sat through like a 20 minute presentation about data so i'll just leave you with the high level overview dado provides a machine learning platform for building intelligent applications and intelligent applications or applications that use machine learning we offer tools to plot to do data engineering data intelligence and deployment so these are the the three tenants of building an intelligent application today so we want you to build deploy and manage your intelligent apps with data so that's it that's all i've got for the company now i want to switch gears and talk about deploying scikit-learn models to production i'd love it if you guys interrupt me with questions or raise your hands with questions let me know if i'm going too fast too slow anything like that so if we think about going to production let's back up and actually think about how all of us got started with ml and i can certainly speak for myself my backgrounds not in machine learning it's in large-scale systems so i'm new to ml so i'm learning data science like i like i imagine many of you are so how everybody starts with ml is pretty simple you get some data it's pretty universal you learn about an ml algorithm and then you learn how to apply that or an ml technique on the data and you generate,358,0,0,AwjeRg1u5VI
1,a plot so sound right does everyone sort of remember this is how they got into machine learning no nods oh i got i got nose interesting okay maybe you'll agree with me as i keep going though so the idea is to learn about machine learning so you want to use the visualization to help you understand the concept so the task that you're doing is you're running a set of experiments as you're learning a new technique and the plots are the results this is how you know whether you've learned the the technique correctly or whether you've applied it correctly but this doesn't tell you anything about how to deploy doesn't tell you how you would take this model and do something with it in a production setting so you don't know how to get this model that you've trained to be consumed by others and this is sort of where the the training in machine learning typically stops and i think this is why you don't see a lot of machine learning models directly deployed into production settings it's because it's not really part of learning machine learning to learn how to deploy machine learning so what ends up happening if this is the state of the world today so after working with a lot of customers i hear this over and over again so you still have data you still have the ml algorithm but then this is what the data scientists so this is what you've built or what i've built as i'm learning to be a data scientist but unfortunately this is not what's deployed what's deployed is a,358,0,0,AwjeRg1u5VI
2,totally different model that's been implemented by a different team maybe data engineers or software engineers and so the output of the machine learning exercise is a specification for another team to implement in a production language it's this this is hugely not optimal this is a this is not a good situation for a lot of reasons so for one it takes a long time to translate from one system to another oftentimes you're oversimplifying so the model does certain things a certain way now you're trying to implement that in a different set of tools and it's different skill set of people that are trying to do that so you're simplifying the model to explain it unfortunately if it takes six months which i've heard directly from customers more than once the state of the world is uh has changed in six months so what you've built is probably irrelevant and then worst of all even if you've gone through this exercise once and now you're working on the six month pace of having any iterative changes to your model you've got two systems and two teams and now your situation is even more fragile so if this is what we're up against today why are we in this place why is it that the machine learning model can't be deployed what's preventing it from being deployed today what's holding it back what's holding back the scikit-learn model so first from my the way i see it machine learning models are very opaque objects they're stateful they're not easy to interact with it's not easy to know how you integrate a trained model into,358,0,0,AwjeRg1u5VI
3,another system so i tried to represent this with this graphic you've got some data it goes into an algorithm which i love this depiction which somehow seems math mystical and magical and what you've learned is a function what a function doesn't seem all that accessible to use directly so there's a lot of effort into trying to export your trained model using a markup language but unfortunately a lot of models don't export and so that that makes that makes it tough and then really i believe that since the goal of a lot of the machine learning libraries is to help you get through conduct experiments quickly the focus is on how quickly can you train the model not necessarily on how quickly can you generate predictions from the model does that sort of make sense so i see this is the the challenges that exist in the current system so what i want to spend some time on now is brainstorming with you guys and going through a design of how we would build the system that would allow us to handle live production traffic and have it directly served from our trained machine learning model so this is our goal this is what we this is the system we want to build so if we were starting from the beginning how would we design such a system so we're switching switching context from what a data scientist does to let's build the architecture let's design the system architecture for this so if we're gonna build a new architecture we should start by looking at some reference architectures and and get inspired,358,0,0,AwjeRg1u5VI
4,by them so if we look at a multi-tiered architecture from the internet today you'll see that it has some interesting properties client machines go through the big bad internet and hit a load balancer which then siphons off traffic between a set of web servers and those web servers are they allow you to scale horizontally you can keep adding them as your load goes up and then those web servers have some sort of this thinks that sharing session but you can think of this as a database or a storage layer that's shared between them so you have this this application serving layer that's horizontally scalable and then your storage layer is by definition typically less scalable do you guys agree with this architecture that this is pretty common in the web today ok so our goal is let's generate requirements and the technical product manager in the back of the room is should be happy about this we're generating requirements if we wanted to build this same architecture but for ml models how would we go about it what are the requirements that are important for building this type of system so the first requirement is pretty straightforward whatever system we build it has to be easy to integrate with so how do you do that let's put a service-oriented architecture in front of it let's make it have rest api so that makes sense so if you look at the architecture diagram you've got now got a web serving layer this is i'm representing it as an api which this is what your users or applications are interacting with directly this,358,0,0,AwjeRg1u5VI
5,makes sense this part of the architecture diagram now if you have a rest api you should have the ability to query this api from a lot of different languages it shouldn't you shouldn't be locked into whatever technology you use to build the this rest api so this way you don't you're not locked into python from the client-side if you've built this api layer in python and if you look at some pseudo code maybe this is how you would interact with it in python you'd you'd want to connect using some url and some authentication key and then you'd want to be able to query by passing in a uri or a service name which is the endpoint you want a query and then the data that you want to pass to that that the input to that query this sort of makes sense what we're doing here we're going through a set of requirements for building this type of architecture so we've gone through one it's got to be easy to integrate the second one is pretty obvious it's got to be high performance or for low latency remember our goal is live production traffic so first and foremost let's put a load balancer in that's a great piece from our sort of our reference architecture that we looked at so so now we've added to this to the system architecture diagram where there's a load balancer and you guys can see the mouse yeah okay so there's a load balancer which is now pointing to the set of api machines and then let's add a distributed caching layer this also makes,358,0,0,AwjeRg1u5VI
6,sense like in many database applications or database workloads the database becomes sort of a bottleneck so you want to introduce a caching layer in front of the database so common queries aren't aren't adding load to your database so i think that that architecture makes sense for this space as well for machine learning models and this last piece of the picture i've called it the engine now this is the the component that's actually hosting the model so we have a distributed cache and i should clarify what i mean by distributed cache a distributed cache it means that there's a piece of the cache running on each node of this cluster but they're they all work together so there's one caching namespace so when you're looking for something in the cache the that item may not physically live on that same machine but it's abstracted away from the client or the guy using the cache in which in this case it would be the api server does this sort of makes sense when i'm saying what what i mean by a distributed cache yeah so oh and i've got pseudocode so in this case i'm trying to do something administrative so i should have some way to administrative lea send an admin sort of command so maybe again i'm using a url to connect to the cluster and i'm using an administrative key and in this example maybe i'm just enabling the cache because it was disabled and i'm turning it on so we've got two requirements this is not sufficient what we have this picture is not sufficient for meeting our goals,358,0,0,AwjeRg1u5VI
7,so what's the next one well the next one that seems pretty obvious to me is we have to be fault-tolerant and so let's get that model running on all the machines so this is a difference than a lot of the database architecture that i talked about before where for many relational databases they are single node machine they're single node products they're single machine products and so you can't do this but for if we're designing the system and we're generating requirements for it the ideal situation would be to put that model on every machine now if a machine goes down your system still operational so what do you need if you have a fault tolerant system well you've got to be able to get status on the cluster and you've got to be able to replace nodes that might be impaired things like that so i still don't think we're there now that we've got this we've got and so i think of each of these blue boxes as one one machine whether that's a virtual machine or a physical machine these are components that are all living on the same machine so in this example i have a three node cluster with a load balancer in front so we're not quite done with our requirements because it's not good enough to just have three nodes your systems got to be scalable you've got to be able to go to four nodes or five nodes or eight nodes so you have to be able to elastically scale the cluster up and down this totally makes sense it's the same reason from the,358,0,0,AwjeRg1u5VI
8,the reference architecture and that we looked at before that based on demand based on the operational characteristics of your of your cluster if you want lower latencies let's add more nodes or you could look at the the surge queue on the load balancer there's a lot of fun ways to find em i as my system overloaded latency is an obvious measure but there's a bunch of others well now that you've got a scalable system you've also got to be able to configure it and that means things like this distributed cache have to do the right thing when you add a node so you've got a re partition keys across the nodes you've got to make sure that the the new nodes continue to use the cache was that a question okay and so uh pseudocode again simple command to add some know and an equally simple command to remove a note so we're getting there i mean these four requirements so far seemed pretty good if we had all of this this would this would be a pretty good system but i still don't think we're there the fifth one to me is that and this is definitely for my engineering background having to carry a pager for many years the system has to be maintainable so when you're doing deployments when you're updating a model so if you go back to the idea of what you want to have when you have this type of system you want it to handle live load but you also want to deploy new models to it or update existing models regularly say every,358,0,0,AwjeRg1u5VI
9,hour or every day or every week depending on your application and you want those deployments to be seamless so zero downtime with rollbacks you need to have metrics logging these are this is a production system so it needs all of these things that you'd use in operating the system and we talked about model management well you need that in this case because you're deploying a bunch of models to each of these clusters and so now you want to know which version of which model is deployed where how if you need to rollback how do you go to the previous version and and when was this version deployed those types of things become important you need to have that that auditability because this is a production system and you want to really really be deliberate about making changes to it so pseudocode for adding and updating models might look like this so you pass in a uri which sort of represents that model and then you can update that model using that same uri so this could be how in pseudocode how we would and define this requirement so we're almost there but i think we we have to be a little bit better than so far we've only been talking about deploying trained machine learning models i think we've got to do more than that we've got to allow arbitrary python because the truth is and all of you data scientists can can agree with me or tell me if you disagree with me you don't just write one trained model you're usually building that using a set of python packages,358,0,0,AwjeRg1u5VI
10,and you're often ensembl together so an application is rarely served by just one model it's usually a handful of models that come together and there's usually some business rules that tie them together so instead of having to maintain a whole other system to just manage those business rules and have that system query into this cluster to find out what's the answer from this model the answer from this model the answer from this model i'm going to tie them together and then return that by making this cluster be extendable by just supporting python you can then put that logic and the ensemble of models into this one deployment so what you see i've done is i've replaced the engine component with something i just called python so in pseudocode if i was to define a service i should be able to define it just as a python function and so this is a really trivial example but i've defined a function called service it has an input called data the i look inside this this dictionary of data looking for a field called user id i then check is this user id new to my system do i know this user id if i do then for this application maybe i returned a fixed set of items maybe this is a recommendation system and so i'm going to return popular items to a user i don't know but if i know this user then i've computed my model has been trained using this user's history so then i'm going to return recommendations for this user instead it's a really simple example of,358,0,0,AwjeRg1u5VI
11,how we've wrapped a model inside a function and we can deploy that as a service so what do you guys think so far if we had this system would we be there would this would this get us to our goal of no more oh i actually think we're pretty close so i'd love to hear what if you guys think there's more requirements there's more okay so to recap what we've covered so far easy to integrate high-performance fault tolerant scalable maintainable and extensible so now that we've sort of defined the what what are the requirements we want let's put on our engineering hats and think about the how how would we go about building this so we've defined the what we did the why in the beginning now we did the what now we're gonna do the how so if we're gonna do this ourselves what do we need well we need a web service layer so if we're in the python ecosystem we could use tornado flask i think it's klein not keen i might have gotten that wrong django pylons pyramid there's a bunch of different application frameworks in python that can help you spin up services so we pick one of those we'd probably evaluate that run some prototypes against them again the key metric here is throughput i think because you want something that can handle live traffic well you need a caching layer based on this architecture that we talked about so you might use redis or cassandra memcache d if you want to be old-school like me or you might use one of these newfangled no,358,0,0,AwjeRg1u5VI
12,sequel databases like dynamo db or berkeley db or you could try and use a relational database which also tries to recently they've been there's been a lot of documentation around how the relational databases can do non-relational things pretty well too so maybe you even want to throw my sequel into this but the key metric when you're evaluating this caching layer is low latency because the whole point of the cache is it's got to be faster to get an answer from the cache than it is to get an answer from the engine okay so now we've got two of the layers in the picture covered so what do we do for logs well maybe we use log back or log stash or a service like splunk or log lea go ahead elasticsearch yes for the caching layer you mean or for the logs for logs as well yep and there's a man that's certainly not exhaustive there's a lot more you could roll your own which is always a fun option well metrics you've got a bunch of options as well maybe you want to publish directly to aws cloud watch maybe you want mixpanel maybe you want liberado there's a whole bunch of other metrics providers out there that i'm forgetting so there's even more components to this than this there's we still haven't talked about how do we get model management how do we get the the right api what's the where the models stored so what's the the storage layer involved in this because the model should be stored in sort of one central place so that you know,358,0,0,AwjeRg1u5VI
13,that with checksumming and all the stuff so that you know that's the same model that got loaded on each node so you can imagine that there's quite a bit to do here so we could go through this and we'd talk for another hour or we could use data predictive services so we took these exam goal the same goal that i covered with you guys the why and the what and we generated the same requirements these same six requirements that these were the requirements we had to satisfy in building the data predictive services product and so i'd like to show it to you guys that sound good all right we're gonna do a demo so all right demo is over here oh oh okay oh boy you know what i want to switch my display so that i'm not i don't have two screens because then i can't see what i'm typing and that's no good arrangement mirror whoohoo okay now let's get rid of some of these toolbars good enough can you guys all read this or should i make it bigger it's okay okay so this is an eye - notebook or a jupiter notebook and this demo i didn't want to go through a whole machine learning exercise so i thought i would just take a tutorial from pycon 2013 on linear regression and adapt from it or augment it and i think this what i like about this example is it covers a linear regression using housing data with this really well understood data set that's actually ships with scikit-learn and it stops with a plot that's the,358,0,0,AwjeRg1u5VI
14,"end of the sort of tutorial so i think that that's a great place to pick up and say okay now let's let's actually deploy this as a service so i'm not going to go through the the details of this but we're doing house price prediction using this boston housing data set that's quite old and you'll understand that when you see the prices so like like everything else in python i'm importing i import scikit-learn i look at my data set i've got five hundred and six rows i want to get some characteristics of the data set so you can see there's a bunch of columns they all mean different things you can see where this data set came from and and sort of the governance of this data let's start by plotting you know this is an important step in exploring your data and getting getting into it so let's build a histogram and matplotlib and this is all coming straight out of that out of that tutorial you'll you can tell when it gets to the stuff that that i wrote because it doesn't have any explanation it just has it commands so here's how you know that this is a pretty old data set the price is in thousands so 20,000 25,000 and you can see the number of houses that fall into this histogram so okay this is the distribution of prices since this is a tutorial or this came from a tutorial it has some exercises which we're going to skip because that's not that's not the fun part for us the fun part is to actually build",358,0,0,AwjeRg1u5VI
15,a linear regression model so that's what we're gonna do here so from scikit-learn we we import linear regression we're going to instantiate the object will name it clf and then we're going to fit or train using our data set so let's do that so now we have a this model has now been trained and because this is a from this tutorial the next thing it did was make a plot after generating some predictions so this we called clf da predict so now we generated predictions for the same training set and now let's plot that so what you can see is the true price on the bottom on the x-axis along with the predicted price so how well did we fit that line so this is basically where most machine learning tutorials stop so let's pick up right from there and say we want to take this trained model and deploy it as a service how would we do that so first and foremost we'll define our web service so this is that python function that takes whatever data we'd like as input whatever inputs we'd like and then calls predict on the trained model remember we named it clf that was what the the variable was called and we call predict passing in whatever was passed in for data we get back the prediction and then we want to take the prediction and wrap it into a json object to return it this makes sense as a python function yeah okay so now we've got our serve we've got the function or the service we want but we don't have the,358,0,0,AwjeRg1u5VI
16,cluster yet so let's build up that cluster so now we're gonna import graphlab create so import graphlab and the reason i should bring this up just to make it clear data predictive services is a product that doesn't require a graph lab create but graph lab create has a client to connect and let you launch predictive services does this make sense you can launch predictive services on their own but you can also launch them from within graphlab so i'm going to show that from graph line i know that's a little confusing and we're working on making that better we want to have a separate python package that's just a connector or client for predictive or predictive services and data distributed but we're not there yet so right now that client's baked into graphlab create okay so all of you that have worked with ec2 or with aws know that there's a bunch of metadata you need to keep things like your access key secret key what instance type you want to launch what region you want to launch things in and where where you want to store your logs and s3 these are things you don't have to type in over and over again so we have this convenience object that we call an environment that defines this configuration of this metadata so this is the one that i have saved on my system you guys all now see my access key but don't ask for my secret key because it's secret and this is uh this is a named object so this is something that's saved away in my home directory,358,0,0,AwjeRg1u5VI
17,so whenever i use graphlab create i can easily retrieve it so the next step is launching the service and so i'm not going to show you that because we have to then wait for ec2 to actually spin up instances and that takes a bunch of time and i don't have that many jokes and so i can't kill that much time but i do want to show you the api it's one command we give a logical name to this cluster to this deployment which is then how we can refer to it we provide this configuration oops cannot double-click we we pass in the configuration the ec2 config we want to use and then we specify where an s3 we want to keep all the state information about this deployment this is really important i didn't cover this in the requirements but you can't build a system for production where all the stateful information about the deployment is sitting on your laptop because then if your laptop gets run over you've now lost your handle to your cluster so the way we built predictive services all of that stateful information is stored in s3 if your cluster is being deployed in aws in one place that you specify the reason that this is so this not only helps you in the case of your laptop getting run over but it also helps you just a sec in the case of you have you all of us work in teams so oftentimes you're sharing deployments this is how you email your your teammates to say hey i've set up the cluster here's how you,358,0,0,AwjeRg1u5VI
18,connect to it to deploy models to it so go ahead what was the question the question was security how do we maintain security really important question so with data predictive services the infrastructure is yours meaning these are your aws credentials these instances launch in your account daito has no access to them no absolutely not that's right so the the okay so this is a very important distinction so the question was what happens to the keys do we send the access key secret key everywhere the answer's no we follow aws best practices what's happening is when you launch this cluster your local machine is making the aws commands to actually call ec2 run instance so that's happening from your local machine so the keys aren't going anywhere except does that answer your question yes and aws helps you with that there's like an 800 number and there's a there's the whole thing so this is at 10 minutes perfect does that include questions or no total oh okay okay so i want to show you how to so here i'm loading the predictive service using that exact same using that state path this path that i'm talking about here so this is an existing cluster that i have and i can show you what's you can see what's deployed when i printed out this the logical name for this is demo lab psych it this is the api key which i'd need to make any query requests there's also an admin key which is what i need in order to make administrative changes like adding nodes removing nodes the cache is enabled,358,0,0,AwjeRg1u5VI
19,and this is the load balancers dns name i also see what are the services or predictive objects that are deployed so right now i only have one it's called funny and it's at version 6 i have nothing pending so what are some of the administrative operations i can do i don't have time to go through all of them but you can do you can change your cache you can actually do more fine-grained cache control how much ram do you want to devote to the cache on each node what's the time to live for all your cache keys per model and for the overall system you can scale up and down by adding and removing and you have all the operational features like getting the metrics if you want to get them programmatically getting the status where you get that back programmatically as a json objects so you can then go and and analyze it with code the logs are periodically shipped to s3 like you would you wouldn't want to take the the i o hit to try and write your logs in s3 all the time so you write them locally and then sync them to s3 periodically but if something's going wrong you don't want to wait for that syncing to happen so you want to be able to flush the logs immediately and if a nodes going bad or is no longer performing well you can you want to just be able to replace it so it's no longer impacting the overall system so this is just a sampling of the administrative things you can do using predictive,358,0,0,AwjeRg1u5VI
20,services so back to our task at hand which is taking this predict function that we define the predict service and deploying it to this deployment so it's deployment dot add i'm giving it a uri this becomes the rest endpoint and now we're in production or we're dealing with our production system so we don't want things to happen incidentally so we make it really explicit you need to call apply changes for all the pending changes to get applied and this is the first time that the model that i had trained in this ipython session running on my laptop has left my machine so what's happened is we've uploaded it to s3 and then notified all three nodes in the cluster that there's a new model i need you to download it so you can get to all this stuff programmatically through our api we also offer it with some visualization i'll zoom out a little so you guys can see this a little bit better whoa okay oh i can't this is what the projector can do so you can see that i've got the the i now have two objects that are deployed funny and this new one house and you can see some operational metrics being plotted and you can always link away to all of cloud watch to set alarms and all to set up your alarms and things that you'd need to measure to operate the service so that your pager can go off at night and you can see this is a three node cluster and each of the instances are in service so now let's query,358,0,0,AwjeRg1u5VI
21,"it and i'm running a little out of time so i'll just go kind of quickly so i want a query i'm going to use the first row from my test set from my training from my data and all this is is a numeric value for each of those columns or those features that that are in the data set so that's my example i'm going to use the python client that's baked into graphlab create query the house service that we just deployed with this example and what you see is i connect it to the predictive services load balancer and i got my response from there the result is for this house given the these features the predicted value is 30,000 you also see a request id for debugging for end-to-end debugging so this is your request id for the request and you know which version of the model returned this response so this is version 1 and that's it i can go back to a couple slides to wrap up look that's my email is that the one yes so see if this works yes so data predictive services lets you do all the things we talked about and more so i'm available for questions oh yeah deploying a new version i'm gonna go right back to the code here why don't i see the window it's it's just deployment update and the name of the uri and then the model so just like we did deployment dot add there's deployment update other questions go ahead absolutely yeah so meaning this is why we queue them up right so you call add",358,0,0,AwjeRg1u5VI
22,add add update update update and now if we look if i look at my deployment you'll see that i have two deployed models and i have a pending change to this to this service and the new version is version two so now when i saw all i did there was i called update instead of add so does that make sense so all the models are cataloged which ones are pending what version they're going to get updated the next time you call apply changes that's right each deployment can have an arbitrary number of models it really varies it depends on the application and this sort of depends on your overall network topology in your organization some some organizations really like having each application have a siloed set of infrastructure that's independent other other organizations really like having a lot of shared infrastructure so it depends on how your organization likes to do things and it also depends on the workload of the models go ahead users well there's kind of two ways to do that i think the simplest i would you could either ensemble it inside one model so really one model is a collection of 15 million models or you could externalize it it depends on how you want to train actually i'll say this way it depends on the velocity that it with which you want to train the personalized models go ahead you jane yeah right right so another way to say that it's context-aware predictions so what is your user interacted with on your website what are the products they've seen there's no point recommending those products,358,0,0,AwjeRg1u5VI
23,if you're looking for recommended products as an example so there's two over here and then i'll come back go ahead yes and no so yes we are pickling when we can but pickle is a pretty fragile so we've wrapped we've wrapped pickle with what we call gl pickle but it's it's a same idea and then we're then for this because we're deploying to s3 because we're deploying this this deployments on aws we are pushing that model to s3 and then notifying all the nodes in the cluster to pull the model from s3 there was another question here yeah that's a great example so great you can do it with an annotation so it's from graphlab deploy import required packages so required packages takes a list of scikit-learn equals equals 0.16 dot 1 so this is disutility lever we deploy this service we'll make sure these required packages are on them on the machine that's correct yeah that's correct there was another question here yeah oh so you watched it it's just this one api called that i didn't run only because it takes ten minutes but it really is just one api call to get the cluster going and the only configuration you need to give it is this ec2 configuration so we've tried we try not to that you shouldn't have to so we've done our job right with the tools you have all the access you want the the the challenge we face about giving ssh access which is the question was ssh access can i ssh to those instances once they're launched and poke around the reason we've,358,0,0,AwjeRg1u5VI
24,been hesitant to do that is for one it makes it tougher for us if you start building tools expecting ssh access and things to be where they where they are in one version we can't go and update that and make changes to it without breaking those tools so that's one of the reasons but if there's a reason that i'd love to hear about reasons you want to ssh or you feel you need to because those are great requirements for us we need to build that tooling in zero minutes ok so one more question here and then i'll stop for reproducibility some version of you absolutely we do so in in the graphlab create models you can always provide a random seed now some of the models are inherently random or inherently probabilistic so even with a random seed you reproducibility is difficult so so it depends on the model so as much as possible with graphlab create we offer reproducibility but some of the models that people are what we implement the model so but some of the models are not you will see yeah okay yep okay - one minute but go ahead oh you that's oh it's a really good point so if you look at this output here and we went through it really fast but you'll see that the version is in this place where we where we uploaded - so you can easily roll back or get to any prior version and load that like load that again it's not oh the function itself is the function is pickled so you can unpick alit yes oh,358,0,0,AwjeRg1u5VI
25,absolutely you can inspect see the source code the the whole thing yeah yeah we capture all of that we need to or else we won't be able to run it on the other end okay thanks,48,0,0,AwjeRg1u5VI
0,tensorflow an open source machine learning framework famous for powering deep neural networks with high-level code it was developed by the google brain team and first released in 2015.,37,0,0,i8NETqtGHms
1,it's most commonly used with python but can run in other languages like javascript c plus plus and java at its core it's just a library for programming with linear algebra and statistics as you know the word tensor describes a multilinear relationship between sets of algebraic objects within a vector space aka a multi-dimensional array what makes it special is its collection of apis for data processing visualization model evaluation and deployment that make deep learning accessible to the average developer it's extremely portable and is able to run on tiny mobile cpus or microcontrollers with tensorflow lite can run in the browser with tensorflow.js while the core library can scale up to multiple gpus or run on tensor processing units ships engineered specifically to run tensorflow at a massive scale it's used in medicine for object detection and mri images by twitter to sort your timeline by tweet relevance by spotify to recommend music by paypal for fraud detection in addition to many other applications like self-driving cars natural language processing and so on to build your own neural network right now create a python file and install tensorflow next we'll need some data like fashion mnist which we can automatically import the goal is to train a model that can predict the clothing type of each image tensorflow has a subclassing api for expert users but also integrates with the beginner-friendly keras library which has a sequential api that can easily build neural networks layer by layer we start with a flattened layer that takes the 28 by 28 pixel image as an input and converts it into a one-dimensional array this input,358,1,1,i8NETqtGHms
2,layer is then fed into a dense layer with 128 fully connected neurons or nodes you can think of each node like its own linear regression as each data point flows through it it'll try to guess the output and gradually update a mapping of weights to determine the importance of a given variable in this case it uses a rectified linear activation function that will output the input if a certain threshold is met otherwise it will just output zero and the behavior of this layer can be customized by tuning as hyperparameters finally we have our output layer which is also dense but is limited to 10 nodes which corresponds to the total number of clothing types in the data set now we can compile the model and tell it to optimize a certain loss function like sparse categorical cross entropy as we train the model for multiple epochs its accuracy should gradually improve the end result is a model that makes a prediction with the likelihood that an image is a certain type of clothing congratulations you just built a neural network this has been tensorflow in 100 seconds hit the like button if you want to see more short videos like this thanks for watching and i will see you in the next one,283,1,1,i8NETqtGHms
0,for anyone who's interested in actually seeing what it's like to deploy them all you can anytime headtube italy slash algo dev al geo de vie and then just click on digit recognition and that repository i've got a full readme there where you can walk along and perform every step that i'm about to and actually deploy a copy of a demo model right away so this is what that actual repo looks like you can see there's that readme and they're telling you everything you need to do but it's pretty simple to actually deploy your model so what i've done here is i've pre-trained a pretty small and frankly pretty poor digit classifier models it's just for demo purposes don't expect it to be it to perform well and don't use this specific model in your production environments you won't be happy with the results but what this model does is this model takes an image which contains a digit so say a handwritten 1 or 2 and then it predicts what the actual number value is inside that image to deploy all we have to do is download the classifier itself the petcul file the serialized scikit-learn model then going to algorithmic comm and we have a hosted data facility where you can upload your models into so if i click on data and then click on my hosted data that gives me basically a directory and file browser now these are all securely hosted files on our servers i can create a new collection here so i'm just going to call this one demo and then i can simply upload any,358,0,0,NLq2gFhoMvI
1,files i want so i'm going to take that pickle file that i just pulled down and i'm going to drop that right in and boom now i've uploaded my model file into algorithm use servers now note that right underneath you get a little file handle and this is simply the identifier for where that file is actually stored so data demo i'll go demo to just classifier alright i'm going to copy that url the next thing i do is i go back to algorithmic comm and i click this plus to create a new grow them this is my entry point for that interface you saw before or a siblings gonna have some algorithm name i'm going to call this digits classifier one i'm gonna be writing in python three today and as it is i'm going to make this one open-source i wouldn't have to i couldn't make it close to the world i'm gonna give it full internet access so somebody could give it the url of a public image somewhere on the internet that they wanted to recognize and i'm gonna give it advanced gpu and hit create algorithm and then again i could get cloned down and use my own ide but i'm just going to step into the web ide for now since it's gonna be some very simple code i'm just gonna make a little comment for myself here to remember where i put that model file and that i've actually got all of the code written out for you though it is pretty darn simple if i look at my actual algorithm here all that it,358,0,0,NLq2gFhoMvI
2,does is it imports the algorithm you libraries imports scikit-learn and pillo the image manipulation library and python and numbi of course and then all i'm gonna do is i'm going to load up that model file from wherever i stored it using job lib and all of this occurs before anyone ever comes in and executes my api so this is simply pre loading giving the model file ready to get which is deserialize in the model and making it ready in memory when somebody actually goes and executes it's my a p i this is where they end up coming in now even though they've sent json i don't have to do any json serialization inside my service function the framework takes care of all of that for me from my point of view whatever comes in is simply going to be a python object in this particular case it's going to move the url of an image somewhere that i am actually going to go and recognize i'm using a little tool here called smart image downloader and what that does is just make sure that whenever i bring in an image if there's something like a javascript protection wrapper around the name image or somebody's accidentally given me the html page that surrounds an image instead of the image file itself smarter than this downloader is just one of those many useful utilities on our system that looks at that and says oh they didn't actually mean the html page they meant the image which is in the html page so it just goes and it grabs that image itself and sends,358,0,0,NLq2gFhoMvI
3,it back to me so this is not necessary if you were just sending it raw image urls which actually went to the image file but i've added this in for safety in case somebody does something like sends me an imagery url which contains the html wrapper this will bury through underneath and get it and we've got lots of useful utilities in our system like that for different purposes and part of the power of our system is of course being able to use those immediately directly from whatever server this function you're creating from there it's really simple all i do is i grab the image file i open it up i resize it to 8x8 and greyscale it because my model was trained on 8x8 grayscale images then i do the prediction and i simply return the integer which the model predicted again the person utilizing this function is sending and receiving json but i don't have to worry about that i just simply return any valid python object and the framework is going to automatically take care of serializing that for me all right so i can simply copy this code i'm gonna paste it right here into my algorithm and the only thing i need to change is just where actually put that model file so i'm copying that url where i saved the model file and i'm putting that in right here from there the very last step is simply making sure that i have the right dependencies file now this is pretty simple i just use pillow and numpy but i've put a requirements text in here just,358,0,0,NLq2gFhoMvI
4,so you can see what those actual dependencies are so we're using the current version of the algorithm yeah library and i've actually specified here specific versions of numpy pillow and scikit-learn it's just a standard requirements file you could of course be using just the most recent versions if you expect your code to always before we're compatible but i'd like to version lock things when i create them for safety i hit dependencies over here and simply paste in those requirements that's it hit save dependencies and everything's ready to go i hit compile and of course pythons not a compiled language but what the compile button does in this case is it simply prepares the serverless function for injection into our system so that it can be run from anywhere it's gonna go ahead it's going to provision whatever it needs in order to make that available and that's going to give me a little notice that it's ready to go all right lastly i'm gonna test this out before i actually publish it so looking at the readme here i've actually just put the url of a image out there so that you can test it out quickly if we look at that actual image it's simply an image of a you know low resolution one drawn out in the image right so i'm going to paste that right in here to test this out it goes ahead it executes smile and it gives me back a prediction so how well did my model perform one excellent very good okay so i'm satisfied that that works and we can see that that,358,0,0,NLq2gFhoMvI
5,first load took a little bit longer but after that it's extremely fast it almost immediately returns each time once i'm happy with that i'm gonna go ahead and hit publish i'm gonna put in some release notes here saying what it is and what i've done i can put in some sample input here so that other people coming in and looking at it know what kind of input to expect and i can set up what kind of versioning i want whether i want it to be public usable by anyone or private only visible to me in this particular case i'm gonna make it public and lastly something you might use if you do release a lot of public algorithms you can if you want to earn a royalty on each call to your algorithm so i'm gonna publish this algorithm out there but if other people come along and run that model i don't pay for the execution of that model they're paying for the execution of that model because they're the ones running it furthermore if i put in a royalty here i get a little credit kickback every time somebody else executes that in this particular case is such a trivial model there's no point in monetizing it so i'm going to put a zero credits per call and i'm gonna publish that out there as soon as i do that i get this webpage which describes my algorithm i can go in and i can edit this turn an image into a number or i can add more complicated documentation here if i want i've got the option of,358,0,0,NLq2gFhoMvI
6,putting in sort of a full markdown file describing what it is but then people have that sample here which they can go ahead and execute it or they can change this and execute it on a different number if they wanted that would be just fine but also down here at the bottom they have boilerplate code that they can cut and paste in any language they want and simply dump it into their code base so i could for example simply call this from curl from a command prompt and it's going to go out and it's going to remotely execute that and return the results right back to my machine so there we go similarly i could grab say my python code i could copy that entire local python script or i could be developing a web page and simply copy this client-side javascript to execute inside my web page all right so that should pretty much wrap us up thanks very much for coming along with me,222,0,0,NLq2gFhoMvI
0,okay we're back with 52 weeks of live coding in molapse the last couple weeks what i did was in the first episode i was able to go through and play around with exporting a model and i made some progress there using tensorflow which is useful it's actually very important to know how to do anything and importing a model was a great step i just really copied some stuff from collab and exported a model the next thing that i did was i tried to figure out what was the default way to serve out a model using tensorflow using their tensorflow serve mechanism really haven't used that quite quite a lot of most of my tensorflow experience has been with putting things onto a device i do a lot of device based mlaps but i tried it out and did actually get it to successfully serve out a model but where i left off was i wasn't able to invoke the model in point because i couldn't figure out how to do it so i'm hoping today that's what i can do i haven't done any prep work for this uh it's possible i'll completely fail but let's go ahead and try okay let's share my screen here i'm gonna make my screen a lot bigger as well which i had a problem that i noticed before okay so we got this and the the screen is way way better here hopefully and i'm going to go ahead and go to the serve tensorflow model example and go back to my code space huge fan of github code spaces you know what in,358,0,0,ZpT0NhrPQgE
1,fact what's kind of cool is that uh github codespace is now available for people in education and there's some really large instances in fact let me make this a little bit bigger too so we want to make the screen really big and i'm going to also use virtual environments which i don't know why i didn't do that before but i i should set up a virtual environments and put it into my um basically into my path automatically so if i do this e and v do i have this kind of funny i didn't create a virtual environment i'll go ahead and create one real quick so let's go ahead and do this we'll say virtual environment and i'll say tilde dot v env there we go once i do this i'm going to go to my bash rc file here and i'm going to edit it i can probably go even bigger yeah there we go let's make this really really big space and then i'm going to go to bash rc do the command shift g which gets me to the bottom i like telling my environment to always source a virtual environment especially if it's a dedicated workspace so we'll go here and we'll say source v e and v we'll do this we'll say source tilde.v env then activate there we go perfect and now if i go through here and i just click plus there we go this virtual environment is sourced i can get rid of the other one so i can say which python perfect and then if i go to the make file that,358,0,0,ZpT0NhrPQgE
2,i have here notice i have the pip install set up uh and i can go ahead and install tools that they like to give me sure i'll do that and let's look at the requirements notice we have tensorflow numpy matplotlib pylon request looks like i have some good stuff in here and let's go ahead and do make install real quick and see what happens hopefully it doesn't blow up while this is installing does it tell me what version of python somewhere i don't i know that they updated python recently to python 310 which has some major issues with numpy which is very annoying so hopefully i don't run into those same issues although this code space is from a while ago so hopefully it's running an older version of python and i don't have issues it is again a little bit annoying to to have your environments always the latest bleeding edge stuff i would prefer to be a little bit behind so let's type in python what do we have three we do we do have 310 well so far so good got that working now let's look at the serve code notice the way it works here which is tensorflow model server rest api 8501 model name fashion model now i don't see this model on disk where is this oh i see because it's in this directory right here which was this uh the metadata location here so if we look at this we have the saved model all this is on disk here is the model uh base path which is right here so we've got this,358,0,0,ZpT0NhrPQgE
3,uh we've got this example now we don't want to call it serve.pi either we we want to call it serve.s8 so i'm going to go ahead and grab this out and i'm going to put it into here because then it makes more sense and we can do this we can say user in env bash that looks a lot better and we can also do a chamod plus x to make it executable and then i'm going to remove the other piece of code which is going to confuse people i'm going to say get rm serve okay we got that working here we can do it there we go okay so we've got tensorflow model server let's go ahead and run this so if i and serve uh it says command not found tensorflow model server command let not not found now i may have to do some kind of installation here so what do i need to do to to play around with this uh what i can do is is take a look at what i did before which tensorflow model server i think i just need to install this which is like an apt-get install is it set up uh here we go set up so i just need to run setup so let's go ahead and do that let's i guess i might as well just change this too i'll do this say user and env bash and then we'll do xmod plus x here setup there we go and so let's run the setup code does this get everything working here let's just do um okay so this,358,0,0,ZpT0NhrPQgE
4,downloads tensorflow model server uh it says denied so we'll do sudo hopefully this works okay so we got this working now if i do serve we have got a little bit closer which is building single tensorflow model config model name and i'm doing something stupid here which is model name fashion model model base path is one so expected model faster model to have an absolute oh i see so i need to make the absolute path here which is fine and if i say echo dollar sign home just out of curiosity it doesn't it doesn't fully give me the path it's kind of kind of annoying i like to make things reproducible but uh sure let's go ahead and do this workspace server so let's try that and we'll do a serve there we go did you it says re-adding um founder base did you forget the name as a number oh i see so what what what we need to do i mean this is a very weird process i have to admit is is it a lot of assumptions here that are that i think are not good but there we go so we got this thing working and and we see so so i just didn't check in my latest changes which is on on me so it says your application is running so let me do that now so i so i make sure that i don't create a mess because i'm going to say get status make sure that i've got everything here and i even put readme that says okay um you know setup would be,358,0,0,ZpT0NhrPQgE
5,first step would be run so step one would be uh let's say run slash setup.sh do that and then and we'll put a note that says something like uh this install tensorflow serving transfer flow serving and then the next thing is that we want to run serve so run slash serve dot sh and this will serve out model in directory named one go one okay that looks good so far and i'm gonna go ahead and add this get status we should have three files git add readme now let me let me also ignore the dot vs code which is annoying let's go ahead and add this i don't want this so if i say get status notice i have add to get ignore now get add this hit add serve hit add setup that's h there we go so we've got get ignored serve setup sh and if i say git status got it all here and let's go ahead and commit this adding working serve i got this cooking here it's all checked in i mean i i guess i could also you know maybe add some build steps and all that stuff but but for now we we have server working so i'm going to go ahead and run this i'm going to say basically serve awesome so what do we have here we see that it's going to run your application on port 8500 and we could even go to the web browser and we can see like a bunch of weird stuff appears so this is at least a reasonable step which is i i know that,358,0,0,ZpT0NhrPQgE
6,it's actually running but what i don't know is what what are the kind of payloads that like what does it behave like we i have no idea actually because i've never used tensorflow serving so i'm gonna need to do a little bit of investigation here so i'm gonna look into tensorflow serving and i'm just gonna read the documentation to make sure i understand how it works and so we see tensorflow serving as a flexible high performance serving system we've got a running and it's got out of the box integration for tensorflow models but can easily extend to other types of models and data what can it do it can serve out multiple models so that is pretty interesting or multiple versions it exposes grc rpc as well as http it allows deployment of new models without changing any client code that's pretty cool you can do a b testing as minimal latency that's really cool so this does look actually very sophisticated and so one way you could do this as well is you could apparently do docker and you do you can do tensorflow serving so that is cool i did not know that so let's put a link here about about this to our documentation and we'll say you know maybe it's this here say like can deploy a serve can't cancer model model out in 60 seconds docker that's pretty cool i like this i'll definitely probably play around with that later or maybe even today aha here we go so query the model using the predict api right here okay so that does look pretty cool here,358,0,0,ZpT0NhrPQgE
7,so where's in this one this one is actually a a saved model here so maybe i need to do this instead temporarily to just get past where i'm stuck and then i can come back to this let's do that let's let's um let's let's make things simple here and i'm gonna i'm gonna stop what i'm doing temporarily to just get anything working so i think this is a good tip to be aware of is sometimes when you're stuck just get anything working and so what i'm gonna do next here is is is follow the instructions so i'm going to say docker pull tensorflow serving in fact why don't i just copy this let's just put this into our readme and say like you know this these steps inside of here we can put this in here so first up what i'll do is i'll say docker pull tensorflow serving so we're going to slightly tweak things and then i'm gonna use docker to do tensorflow serving and then what i'm gonna do is i'm gonna get out of this directory temporarily because i don't wanna mess it up and i'm going to say git clone this i'm going to i'm going to clone the tensorflow serving projects and then i'm going to find the demo models that's what i should have started with and in this particular scenario we can go here this would be the test data there we go and if we echo that variable what do we see echo so test data dollar sign it's a work phase serving tensorflow test data so we can even look in,358,0,0,ZpT0NhrPQgE
8,there what's in there ooh all kinds of saved models so this is promising because it it allows me to to maybe do that without the container so now i just want to do a docker command here and i say docker run on port 8501 and here is where the model is that i care about and and here's the model name and once i've got that set up this should actually invoke a serve so there we go are you sure you want to paste these yes i do let's go ahead and serve this out uh pretty cool just running on port 8501 now i need to do is just open up another terminal here and i just need to run this curl command so if i go ahead and i paste this it should return out a prediction yes success so this is exciting this is the first time i got tensorflow serving to do a prediction i did use docker to do it but this did really give me a lot of of tips here on on how to work in fact i think this is actually very interesting concept which is that maybe it's it could be a good approach is to use docker run itself to be the mechanism to deploy pre-trained models for example as well which is something i definitely would would like to play around with is is to use maybe like a docker container with running tensorflow serve and then go to the tensorflow hub and just download something and serve it out but this is definitely pretty promising here so if i go back,358,0,0,ZpT0NhrPQgE
9,to my readme file here we could say like maybe like the next things that we want to investigate so what's next so we got so what what is next is is we need to figure out how to serve out pre-trained models uh via a tensorflow serve that's really the next thing that i'm very excited about is because the model i had i don't really care that much about the initial one and so i'm gonna i'm gonna actually say docker yes here and we can actually i forget the command to to do uh to to to stop docker so let's say like kill rocker kill i think it's just docker ps kill or something docker kill uh cross kill let's just say kill all containers uh here we go stack overflow stop all containers there we go let's do that docker boom this should unfortunate hopefully stop it let's see if that works and how do we would we know it would work because if i do the curl there we go we know it's not working so i have a little bit of time what i'm curious about is can i do this can i figure out how to do pre-trained models via tensorflow serve and tf hub that's that's really what i what i'm excited to try to figure out and so how would i do this so let's let's keep reading so indian server and training tutorial um that that looks interesting i'll maybe come back to it so the easiest way is to use docker image which i did find out that's pretty good um don't they don't,358,0,0,ZpT0NhrPQgE
10,recommend not using docker which makes sense now that i've tried it without without docker so configure and use tensorflow serving follow tutorial using tensorflow serving models configure a server to make it serve your use case um extend it create a new type of server okay so i think then i need to just read this follow a tutorial on serving tensorflow model so let's see a tutorial so this tutorial shows you how to tensorflow server components to export a trained model and use a standard model to serve it this uses a soft max regression model for imnist which is actually what i was just using imnis fashion data so what do we do train and export a model so this kind of goes through here and it shows basically how to do that which i've done before okay session is a tensorflow session tags a set of tags oh i see this is interesting though so maybe i do need this let's take a look at this so train and export a simple model okay well let's try it out let's let's see if if i copy this code here and and check it in if this will actually give us everything we need so i'm going to go back here and i'm going to call this thing what do i call this thing they call it imnist saved model okay let's do that let's uh say touch mnist saved model got py and let's go in here let's paste this in so what do we got here we have tensorflow import mnist let's try it out so if i just say,358,0,0,ZpT0NhrPQgE
11,python and this save model what happens it's well it it's almo it appears to be doing something so it says okay we don't have gpus which we know about no model no module named imnist input data well let's add this to our requirements so we'll say make install add this in well that's that's interesting so we need to figure out how to actually install that clear glitch hmm so so what this says oh well let's let's do this so i think i think the the the key takeaways which again is this is the inputs this is the outputs this is really where i was lacking i guess i could just try this let's try it out maybe this will work so i will cd up here oh we already have serving so that's pretty cool so let's we already have a serving here it's let's go here cd into serving and what does it say to do it says cd into there which is what we did cop delete this if it already exists which i don't think it does rm the that next step let's train the model this this is where i'm curious if it works okay let's see so so this is a pretty big docker image and this might be why having a very very powerful machine would potentially make sense i do have access to really powerful machines even on github code spaces i just didn't happen to use it today but i would probably use it in the future let's keep going here extracting this forwarded ports warnings boy one point 1.6 gigs is is,358,0,0,ZpT0NhrPQgE
12,quite big all right it's still going here but definitely taking quite a while to to run this so so what it's doing is we're doing this step which is training the model and hopefully it will train it and export it so now hopefully it trains it okay extracting we can even look there we go so now it's downloading the newest uh docker container it's running this and it's saving the model excellent and there's there's where it saved it look see it saved it as temp imnist if we go through here and we look at this and missed it put it into that directory so that's pretty cool and then we can see that in fact this even has the variables it has and this is the serialized tensorflow saved model this is the graph definition and the metadata the variables are the serialized variable of the graph so we can load the exported model with tensorflow model server so we can do that next let's do that and there we go that's running that's a good sign and now we can test the server by doing some kind of load testing and we should we should basically you know make an inference error rate which is which is pretty cool so if i go over here i can i can run that test and oh i need to change into serving and then there we go pulling docker image going through and it's doing all kinds of invocations ah pretty cool so we have this endpoint working and then uh i'm able to actually run that so i would have to,358,0,0,ZpT0NhrPQgE
13,look at this the mnist clients code which tensorflow serving example we would have to go through here total affirming example mnist clients which would be this piece of code and this is actually what goes through and does all of the different inference tests here so you know i've definitely gotten a little bit further down the road here uh i think next steps for next week would be to well first of all i'm going to stop this here so i'll do that same docker command i'll do history and let's just run docker command that kills all of our instances here which where is its history so we want to run pound 37 kill all of our docker instances so basically next week what i will do is i'm going to figure this out i'm going to figure out how to download pre-trained models and serve them out via tensorflow i did get very far down the road here with serving out tensorflow and that was pretty exciting all right i will see everybody next week,232,0,0,ZpT0NhrPQgE
0,in this video we're going to share some advice on how to successfully deploy a machine learning model to music production my name is nemma and i'm a product manager and former mobile and machine learning engineer at a big tech company deploying a model often involves complex engineering challenges in the ml model life cycle there are several decisions to make including whether the model will run in the cloud or on the device how the model will be optimized and compiled what hardware to serve the model with how to handle user trust tr how to make sure the new model outperforms the old model and how to continuously monitor performance designing an effective model is just one part of machine learning system design and this topic will likely come up frequently in your interviews by the way if you're enjoying this video be sure to check out exponent complete machine learning interview course featuring hours of ml mock interviews real world coding practice and machine learning system design deep dives start for free on tri exponent.com let's go over the three main components of ml deployment now number one deploying the model number two serving the model and number three monitoring the model let's talk about deploying a model first only deploy a new model when you're confident that it will perform better than the current production model on real world data beyond picking appropriate evaluation metrics consider how to test your model and production data through ab tests canary deployment feature flags or shadow deployment next start by selecting the hardware deciding if the model will be served remotely or on the,358,0,0,mAvyG9OS4uY
1,edge meaning in the browser or on device serving remotely allows more compute resources but it may suffer from network latency serving on the edge can be more efficient and offer better security and privacy but it may compromise model capacity some trade-offs can be improved using modern model compression or knowledge distillation techniques next you're ready to optimize and compile the model there are many compilers for common ml frameworks and hardware combinations for example nvcc or nvidia gpus with py toward or xlaa for tensor flow with tpus gpus and cpus at this point your code might still need additional optimizations like vectorizing iterative and batching operations to run on the same hardware where the data exists finally decide how to handle different traffic patterns predictions can be batched asynchronously or handled as they arrive this might use computational resources less efficiently but incur less latency for traffic spikes consider using a smaller less accurate model or a single model instead of ensembling predictions from multiple models monitoring the model once deployed you'll need to monitor the model's health and performance performance regressions are common because data and user behaviors constantly shift a model that was once accurate might become obsolete requiring a new model or new features set up infrastructure to detect drift and features data or models and benchmark competing models when you need to to evaluate on real world data you need a source of ground truth do you have a hand labeled data set of gold standard data that's continuously updated or will you rely on less direct metrics like the number of clicks on a recommended post or video,358,0,0,mAvyG9OS4uY
2,determine when the model's performance has regressed enough to require intervention think about what tools you will build to monitor and troubleshoot model serving issues like high inference latency high memory use or numerical instability and that's it thanks so much for watching this video on deploying a machine learning model be sure to check out exponents machine learning interview prep course in the description below and we'll see you in a future video music,97,0,0,mAvyG9OS4uY
0,hello everyone i am siddharthan the topic we have for today is machine learning model deployment and in this video we will be discussing how we can deploy a trained machine learning model using streamlit library in python so we will be taking the diabetes prediction project which we have already worked on in our channel and we will be creating a web page where the user can give in some values and this trained machine learning model can predict whether a person has diabetes or not so this is the web app that we are going to create in this particular video so let's get started so first of all i'll go to my diabetes prediction code in my google collaboratory so as you might know i have already made this video and if you haven't seen that particular video if you don't know how this thing works i'll give the link for the diabetes prediction project in this video description you can check that video out and after watching this video you can continue with this one okay so these are all the basic data processing and all those things so i have already uploaded my data set file which is this diabetes.csv and we have did this uh you know importing the libraries and all other things so the complete code is here so what i'm going to do now is so i'm going to my runtime and i'm running all the cells okay so this is this will run the entire cells so in this the model we are using is a now you can see here it is a support vector,358,0,0,WLwjvWq0GWA
1,machine classifier so we will be uh you know saving this trained model and we will be using it to deploy in our web back so once we have completed all the things so once we have trained our model once we have evaluated our model so finally we have this predictive system where we can give some values in this particular list and it will tell you whether that person is diabetic or not so this is what we have done in that particular diabetes prediction project now we can see how we can save this model how we can save this trained support vector machine model and how we can use that in our streamlit code okay so i'll just make a text here as saving the train model saving the trained model so for this we need a library called as pickle so i'll write here as import pickle so this is one of the libraries that you need for this particular project so and run this so this will import the pickle library and this pickle library as the functions that we need in order to save this model so i'll create a variable here as file name so in this variable i'm going to know give the name of the file that i am going to create so i will name this as trained model trained underscore model dot sav okay so it is basically a saved file and i am going to use the function pickle dot dump so pickle dot dum and within this i need to give the model i need to mention the model that i want,358,0,0,WLwjvWq0GWA
2,to save so here the we have named the model as classifier right so i'll copy this classifier term so you can paste it here so pickle.dum classifier and we need to mention open and file name wb so let's try to understand what we are doing here so these are some file operations that we are doing here so i am naming this particular file as trained model dot sav so this is the name of my file that i am saving and i am storing this in a variable called as file name so in this file i wanted to save this particular model so i'm using this pickle.dump and i want to save this model and this model is loaded in this variable called as classifier so i'm basically opening a file and the name of this file is trainedmodel.cv so i'm mentioning the file name here comma wb so w means write b means binary so we are writing a file in the binary format and what we are writing is nothing but the classifier the model which we have trained using the diabetes dataset so in this we have used a pama diabetes dataset so you can see all the details here so it is basically uh you know helps you to predict whether a woman has diabetes or not so the input features we have are the number of pregnancy a particular woman had what is their blood glucose level blood pressure and so on so this is one of the you know standard data sets that we have in machine learning and finally we have the outcome one being,358,0,0,WLwjvWq0GWA
3,the person is diabetic and zero being a person is non-diabetic okay so this is the data set that we have and uh this model is trained uh with this data set and it is you know this classified term and now we are storing uh this model to this file so it basically contains all the parameter values of each bias and other parameters of this particular model so when i run this this will create a file called as trained model dot sme so i'll run this okay so this will create a file so you can see the file here so train model.sav so this file gets created now let's see how we can load this model and how we can use it so this part is about loading the saved model so this is also very important for you to learn so let's see so we have now saved our model now let's load this so i'll create a variable here as loaded model so in this term loaded model i am going to load my model so is equal to again i am going to use pickle function called pickle.load so this dump function is used to save your a trained model and this load function is used in order to load the saved model that we have seen okay so here also you should mention open and within that mention uh the name of the file okay so the name of the file is nothing but trained model dot sav right so i'll copy this and paste it here so this is it is nothing but this uh particular file,358,0,0,WLwjvWq0GWA
4,you can also go to this option and copy the path and paste it here so i'll just paste the name here and here we should mention rb so rb means reading the binary formatted file so first we have written the file here so hence we have used this you know attribute wb and now we want to read this file and load it to this load and model variable so we you have to mention this rb which is the parameter here so this pickle dot load will unload the saved model that we have so let's run this so now this load and model will contain the model which we have which is actually trained with the diabetes data set that we add and this particular model has an accuracy of around 77 percentage okay so the only difference in the code uh which i have shown here from the diabetes prediction project which i have worked on is that in that particular video i have used the standard scalar function but in this case we won't be using this standard scalar the reason is that uh you know the accuracy is it doesn't change so it is almost the same when you standardize and you know don't standardize the data in this particular project so the reason i didn't include that is in case you are using the standardization function you need to save the standardized you know that standardization function as well to your pickle so that's why i didn't mention it because you know it might be uh too much to handle right now so first of all let's try,358,0,0,WLwjvWq0GWA
5,to understand how we can save the model and we can work on that so afterwards let's see how we can you know save that standardization function and all that so i didn't include the standardization part in this particular code so that part will be missing when you compare it to the diabetes project which we have worked on already okay so that is one difference so we have successfully loaded our model so now what i'm going to do is i'm going to copy this entire thing so i'll paste it here so here we have used this classifier the classifier which we have you know trained using the data set now i'm instead of this classifier and i'm going to use this load and model which uh it's it's basically the same model loaded from the pickle so i'll paste it here so this will also give you the same value so one which basically means the person is diabetic and the new data is this so now what we are going to do is use this saved model okay so this loaded model in order to uh you know predict whether a person is diabetic or not but the only difference is that we will be creating a web page where there will be a user interface where the user can given these details what is the number of pregnancies and what is their bmi what is the blood sugar level and so on and ah you know our web app will predict whether that person is diabetic or not so this is the goal that we are interested in so now,358,0,0,WLwjvWq0GWA
6,i am going to save this model so trained model.sav so you can go to this option and download the file here okay so it is actually a very small file okay so now i'll minimize this and i'll go to my spider okay so the spider is the ide which we will be working on in order to build this uh you know user interface using the streamlight library and in my previous video i have explained you what is uh or you know how to install the anaconda and how to install spider how to install the stream library and so on i'll give the link for that video as well in this particular video description you should watch those videos before watching this one okay so now what we are going to do is so i'll just import the basic libraries that we need first is numpy library import numpy snp and then i'll import pickle so we need this pickle library because we are going to uh you know load the data load the model right so for that also we need pickle and you should also install this pickle library and if you are in python 3.8 version so if you install the anaconda so the default uh python version in the anaconda is 3.8 and for that you need to install the pickle library so for that purpose you need to run pip install pickle mix in so in my previous video i have shown you how you can do this pip installation and so what you basically have to do is you need to go to your anaconda and,358,0,0,WLwjvWq0GWA
7,you can go to this environment and you can go to this play symbol there you will see an option called as open terminal so that you can paste this command called as pip install pickle uh you know iphone mixing so this is the version of pickle which is suitable for python 3.8 so install that library and there are also other libraries like numpy map lip and other things so you know install all the basic libraries that we need so i'll close this now uh what we are going to do is we are going to do the same thing that we have did here so we will be loading the pickled model and we will try to predict this particular data so i'll copy this loading the saved model okay so i'll go to my spider so i'll paste it here now we need we should mention the location of this trained model.sav so where that particular file is saved so i'll go to my downloads and i copy my trained sav okay so this is the model that we have downloaded the train model that we have downloaded so just create a separate folder and work on this particular folder okay so i'll paste this here so i'll just change this numbering okay so the name of the file is trainedmodel.sav so now in this i need to mention the path of the file so i'll go to my directory and i'll copy the path where this trained model is saved and i'll paste it here okay so train to model.sav and you need to mention the slash okay so now,358,0,0,WLwjvWq0GWA
8,the important thing is that uh whenever you are working in python so it should be uh you know a forward slash should be used for this directory location so i'll change all the backward slash into forward slash so modern train model and yeah so now uh the code knows where this particular train model.sav file is file is so it will load this model to this variable and we can use this loaded model in order to make new predictions so i'll go to the next part and now we are going to copy this code let's paste this in spiders okay so now we can run this and see whether this is working in spider or not so this is the same thing that we have worked on google collab so now we are going to do the same in python in this spider ide so once you have pasted this code you know you can go to this run option so this will run this file okay so the label we got here is one and uh one is basically means that a person is diabetic right so we get this particular statement as this person is diabetic so we are going to do this same thing now the only difference is that we need to uh you know showcase this everything in the form of a web page so what we will be doing is we will be creating a function and we will be including all these things in the form of a function so this is the next part so before that we need to save this file and,358,0,0,WLwjvWq0GWA
9,go to this and save my file as so i think it is stored in a different directory file save as i'll go to the folder in which i am working on so i'll name this as predictive system okay predictive system so it is a python file so the extension is dot py okay now uh let's go and you know let's create a new file and this is where we will be using our streamlet library in order to create our web app so this part of the code will begin with you know importing the basic libraries so the libraries that we need are numpy just a second numpy pickle and stream that so i'll give import numpy as empty and i'll import pickle and the last library we need is streamlit okay so streamlit is used for deployment pickle is used for uh you know loading the saved model and numpy is used for you know working with those numpy arrays so i we just need to copy and paste all these folds the only difference is that we need to put this in the form of a function so i'll copy this part of the code okay so let's start with saving this file so i'll save this as on the same folder and i'll name this as diabetes prediction webmap diabetes prediction webweb dot p1 so let's save this now let's create a function so i'll put a comment here as creating a function for prediction so in my previous video i have explained you what all these things mean so why we are converting it to numpy array why,358,0,0,WLwjvWq0GWA
10,we are reshaping it and what all these things means okay so you can refer that video if you are not sure so we are going to create a function now so for that we need to use the keyword def which basically means define and i name this function as diabetes prediction and within the parenthesis let's mention this as input data okay so input data is nothing but the data that the user has to give so you can see the data here so all this uh details the user has to give so whenever we are calling this diabetes prediction so we need an input and that input is nothing but your input data which tells you you know those things the number of pregnancies bmi value and other you know basic things so now let's complete this code so it's basically the same thing the predictive system that we have built we'll copy this and let's paste it here so make sure the indentation is same so the indentation line should be the same same in this case and we just need to you know just make one thing you know one change so instead of this print i'm going to write write it as return okay so return and let's remove this parenthesis and the same thing here return the person is diabetic so what we are basically doing here is so first we need to uh get the input data from the user okay so we don't need this so we don't need this particular line because the data is pre-made here so i'll just remove this okay so what,358,0,0,WLwjvWq0GWA
11,we are doing here is so whenever we call this diabetes prediction function so the user will given the input and once it is done so we are basically converting that input data in the form of a numpy array okay so that's this second line of code so we are creating a numpy array using this input data and after that we are reshaping it in order to make prediction prediction is nothing but this model will predict whether the value is either 1 or 0 where if it is 1 we need to tell that that person is diabetic if the label value is 0 then we we should say that that person is not diabetic so once we convert it to a numpy array we are reshaping it after that we are using this predict function so this predict function is in the spm library okay so the library the classifier which we have trained okay so we are using the load and model so the model which we have loaded using the pickle function so once we load the model and we are using this predict function and this prediction will be stored in this variable called as prediction so label in the sense the value that you get from here will be either one or zero and we are storing that in this uh you know term called as prediction so it will be in the form of a list so this is the term prediction okay so this is nothing but the term we have mentioned a prediction so it is basically a list which contains only one value so,358,0,0,WLwjvWq0GWA
12,the value which is present in this list is one so if the value is zero then we we should return the statement as that person is not diabetic sorry if the value is zero okay so we should mention not diabetic if the else condition so the other condition is the label will be one and if the label is one we should mention that that person is diabetic okay so this is the code that we have worked on now we can move on to the uh you know streamlined part where we will be building some user interface so now let's let's come out of this function and this part is using the streamlet library so for this i'm going to create a main function so there is a reason why we are naming this as main so i'll explain you at the end of this code so for now just remember that we are you know creating a function named as main okay so main function so the first step is giving a title for our user interface a title for our web page so i'll make a comment here as giving a title so for this we should use the function from streamlit so and the other thing is we should we can you know import this streamline st in a short form okay so we are importing streamlet ssd okay so giving a title so st.title so this title is the function within the streamlet library and within the parenthesis give any name so the any name you want so in this i'm going to name this as diabetes prediction,358,0,0,WLwjvWq0GWA
13,webmap so this is the name of my page and now we should create some data fills some you know input data fields where the user can give in some values so all these values so if you come to this data set so the data that you need should be the same as the data that you have here so if your data contains pregnancies glucose blood pressure and so on so the user also should give the same values so same you know it's i didn't mean that it should be the same values that is present in this data set so what i mean is so you should give the pregnancy values glucose value blood pressure value and so on okay so now we are going to create this input data fields so that is the next step so we need to see what are all the different fields that we have so totally we have about one two three four five six one eight when so totally there are nine columns and out of this nine columns the last column is the outcome column so this is the target column which says whether a person is diabetes diabetic or not so we shouldn't mention this so apart from this outcome column because uh the model predicts this particular outcome column so apart from this we should mention all these eight columns from pregnancies to age let's see how we can create those input data fields so this part is about getting the input data from the user okay so let's create all the variables that we need so i'll go to my,358,0,0,WLwjvWq0GWA
14,data set so i'll open so i have my this data set file in my desktop i'll also give the link for this data set file in the video description you can download the dataset from there and i'll open this in my notepad file so we have all these things pregnancies glucose blood pressure skin thickness and so on so i'll just copy all these things except our outcome so we are going to create eight variables so this is just for reference purpose okay so first we should mention this pregnancies okay so pregnancies is equal to so once we are done with this code i will delete this particular line okay so pregnancies is equal to st dot text input so this text input will get the data from the user in our user interface cst.text input so we have this text input function from stimulated library and you can mention some name for it so number of pregnancies and the second variable that we are going to create is this glucose right so i'll just copy this and paste it here and we can change this to glucose so we should do the same thing for all the eight uh you know terms cst dot x this will give us the glucose level okay so blood glucose level and the third thing is blood pressure okay so i'll copy this blood pressure and now we need the blood pressure value which is basically the bp value okay so blood pressure uh level or blood pressure value so what this particular term in quotes represent is so there will be a data field,358,0,0,WLwjvWq0GWA
15,where the user can give in the values and you know above that data field so this particular statement will be seen so above that particular data fill that would be the statement called as pressure value so the user can see that and enter appropriate value so that is the reason for doing this so these are all just variable names for those values so only this thing will be appearing on our web page and the third thing that we need is you know skin thickness right so or else i'll do one thing i'll just paste this eight times so we have four and we need four others okay so and see whether there are eight one two three four five six seven eight okay so we have three terms here pregnancy is glucose blood pressure and the fourth term we need is the skin thickness value so i'll just paste this here and then insulin level insulin level and bmi diabetes predict pedigree function and finally the age value okay so now i can remove this so okay so now we can change the text that we want to see about the data fill so this is skin thickness right so let's write here as skin thickness value so our model will predict whether uh the person has diabetes or not based on these values so here let's write as insulin level and the next is bmi value which is basically the body mass index value so bmi value and diabetes pedigree function so once we complete this you can see how this you know user interface looks like so diabetes pedigree,358,0,0,WLwjvWq0GWA
16,function value and finally what is their age age of the person so let's name this as age of the person okay so these are all the eight terms that we need so pregnancies glucose blood pressure skin thickness and insulin bma diabetes pedigree functional ages okay so now uh we can move on to the uh prediction part where we will be calling this diabetes prediction function so it will predict whether the value is one or zero from the model which we have loaded using pickle okay and then we can say whether that person is diabetic or not based on the labels that we are going to get so now what i am going to do is so this particular part is the code for prediction and all these things should be enclosed in this main function okay so you can see the indentation here so code for prediction and in this i create a variable as diagnosis so diagnosis is equal to so i'm mentioning quotes so two single quotes one opening single quotes and closing single quotes so you can also double quotes it doesn't matter so only is that you should have opening quotes and double quotes so this is basically a null string so a string that contain no values so the reason is that so we will give all these values to this particular function so this input data and this particular function will do all the processing and will try to predict the label which will be stored here and we will try to check whether the label is 1 or 0 and the final output that,358,0,0,WLwjvWq0GWA
17,we get from this diabetes prediction function is this either this line or the second line whether it tells that person is diabetic or not so this string will be later stored in this uh you know variable called as diagnosis so that's why we are creating an empty string here so that's the reason for this now we can create a button okay so what happens is the user can give all these values and can now click a button called as predictive so when when they click that particular button they will get this particular diagnosis okay so that's the idea so this part is about creating a button for prediction okay so for this i am going to create a if condition so if yes t dot button so st being streamed at st dot button and we can name this button as diabetes test results so you can give any names so i name this as diabetes test result okay and let's work on this if condition so if this button is flipped so we should predict uh the output based on the input that we have so diagnosis so we should store the final output so final output is nothing but these two lines either this line or this line okay so diagnosis is equal to diabetes prediction and within that we should mention all the six things here so so this diabetes prediction is nothing but the function that we have created and within this uh list we should mention all the six things pregnancies glucose and all the other values so i'll just copy and paste all the things,358,0,0,WLwjvWq0GWA
18,here so first is pregnancies and glucose level and the second one is blood pressure value skin thickness insulin level bmi so it should be in the same order that we have worked on okay so that is very important diabetes pedigree function and finally the age okay so these are all the eight terms that we need so and so what we are doing here is so we are first of all we are getting the input from the user so that is uh you know done by this particular part of the code so once we do that there will be a button called as diabetes test results so when the user press that particular button so all these things all the data that we got from the user so the input data will go to this function which is diabetes prediction so you can see here so this diabetes prediction will take all these values through that input data and all the processing will be done and the model will try to predict the label whether it is either one or zero and if the label is zero it will tell that that person is not diabetic and if the label is one it will tell that person is diabetic so this is the test result that we are interested in okay so okay so that's it for this and now we can finally say that if this is success so you need you should come out of this if condition indentation and here you should mention st dot success so this is another function and this will give you the final output,358,0,0,WLwjvWq0GWA
19,which is our diagnosis right okay so diagnosis so we have created a null string right so we have created a null string and we have stored the output of this diabetes prediction function into this variable called diagnosis and we should finally print this and the last thing we should mention is if name so you should mention two underscore dimension name again to underscore is equal to so this thing should be exact okay so you can mention code share and again to underscore main and finally you should mention i mean okay so i'll try to explain what we are doing we're trying to do here so this particular file this diabetes prediction web app won't work if you just run this particular scene so this code should be uh you know uh run from the anaconda command prompt okay so we have our unaccount navigator right so we will open our command prompt and we will try to run this particular file from that okay so when we run a file from the command prompt so this you know file will this or this particular code will take uh the name as main and when that happens only this main function will run okay so this main function is nothing but the function that we have created for our you know web page okay so the reason we are doing here is sometimes this file can be uh run on a different file so just like we have imported the numpy and pickle and so on okay so we can you know import this particular python file you know through another python,358,0,0,WLwjvWq0GWA
20,file so in that cases this uh function should not run so that is the reason so this defined function or different main function should run only when we open this from a command prompt and that's why we are running you know using this particular line of code so it will take this value as main only when it is running as a standalone file so when it is running from a command prompt or this is this file is running directly okay so that is the reason we are doing this and this is the general thing that we do in uh you know any python code so when we are trying to run this in a in a command file or you know something like that okay so this is the thing that you should uh you know do as such so what we are trying to do here is so first we are you know trying to import the basic libraries numpy pickle and streamlet that we need so pickle is used for loading the saved model and stream that is used for creating the web page and we are loading the model which we have you know saved from our google collab what that we have done using the pickle.load and in that we should uh you know mention the directory of the file so here the back slashes should be replaced by the forward slashers so sometimes it may not work properly so that's the reason and we are reading the saved file as a binary format so that is mentioned by this rb so w means uh you know,358,0,0,WLwjvWq0GWA
21,writing a file in the binary format rb means reading a file in the binary format and then we are creating this diabetes prediction function where it will take the input and this input is nothing but our input data which will be given by the user and it will try to make that prediction where if the finite prediction is zero it tells the person is not diabetic in other case the person will be you know told they are diabetic and then we are creating this main function where we can give the title from the streamlined library and the title here is diabetes prediction website and the important function that we are using is this st dot text input so this will get the input from the user so after that we are creating a null string and within this mastering called as diagnosis all the you know the result the final result will be stored and if this button so again we are creating a button now and we you can just give any name and here i have given the name of this particular button as diabetes test result and when this button is clicked it will try to call this diabetes prediction function and all the input data given by the user will go to that function and we will get the final diagnosis and when all this is a success so all this has you know executed correctly we want to print the diagnosis so that's the idea of this particular file so i'll go ahead and save this particular file and i know what is the path of,358,0,0,WLwjvWq0GWA
22,this file right so i have the path of the file here okay so now let's run this from our anaconda command prompt so i'll open my anaconda navigator and i'm working on this machine learning environment so i have a base root environment and i have created another machine learning environment here so i have you know mentioned this in my previous video on how to do that so here i can open my terminal okay so make sure to open your spider from the environment that you are working on okay so that is very important so all the libraries should be installed in this another environment that you are working on so go to this machine learning you can see this play button and open the terminal from there and here we can run our code so there is a different way to run this so what you should do is mention streamlid dot run so this is very important okay so this command so stream lid uh space around and you should you know uh create two double quotes one opening double quotes and closing double quotes and within this we should mention the path of the file so i'll go ahead and copy the path of this particular file i'll copy this and i'll paste it here so here it can be the normal backslash the you know the default uh file location that we have here so you don't need to convert that to a forward slash and we should also mention the name of the file okay so the name of the file here is diabetes prediction webmap dot,358,0,0,WLwjvWq0GWA
23,py right so i'll choose this and i copy the entire file name along with the extension which is dot py so here you should paste this so you know include backslash and paste the name of the file okay so what we have done is we have mentioned this streamlit space run space you should mention the path of the file that you want to run okay so if you press the center a web page will be open okay so it will take few seconds to you know get opened so let's wait for it so your default browser will get open now so let's see okay so this is a simple user interface where the user can give in the values and they can get the result as whether the person is diabetic or not so these are the input data files so i'll open my uh data set file let's try to you know use some values and try to predict uh whether the label is either one or zero so i'll have those things side by side okay so let's choose some value okay so okay so i'll choose this particular one so this is the value i'm going to choose so the label here is one so one basically means that person is diabetic right so we should paste all these things so the first value we have is the number of pregnancies right so 2 is 2 is nothing but that person has 2 pregnancies so we should put in that particular value here 2 press enter and the second value is 197 which is the blood glucose level,358,0,0,WLwjvWq0GWA
24,197 and again press enter so press enter once you you know fill in this particular data field and the third value is blood pressure and the blood pressure value is 70 and fourth one is skin thickness value which is 45 and then we have 543 as our insulin level yeah 543 and bmi value will be 30.5 okay and the pedigree function value is 0.158 0.158 and the age is 53 okay here is 53 so i have given all the you know input data and when i run this you can see the button here so this diabetes test result button here so if i click this it will uh tell me whether that person is diabetic or not and i should mention here that you won't get the accurate prediction for all the cases so we know that the you know the accuracy score of this particular model is around 77 percentage so yeah so we have the test date accuracy so 77 basically means out of 100 values you will get the current predictions for 77 values so that's the idea of it so let's see whether we are getting correct prediction or not okay so i'll go to my streamlet and uh now i can press this diabetes test result and now it tells us that that person is diabetic so we can check that out here so the final label is nothing but one so we know that one means that person is diabetic so this is how you can create a simple user interface where the user can given all the values and they can see whether a,358,0,0,WLwjvWq0GWA
25,particular person is diabetic or not so let's try to do the same for a non-diabetic person so we should get the label as zero so it should tell that that person is not diabetic so i will try to reload this and choose this value so you can see this here the label for this value is 0 which means that person is not diabetic so let's try to uh put all these values here so the number of pregnancy is 1 and glucose level is 103 and blood pressure value is 30 skin thickness value 38 insulin level 83 bmi value is 43.3 diabetes pedigree function value is 0.183 and finally the age of the person is 33 okay so the label here is zero right so it should tell that that particular person is not diabetic okay so let's get the diabetes test result and it tells that that person is not diabetic okay so you can see the label here which says zero so this is how you can use this in order to predict uh you know what is the label and what is the corresponding statement whether that person is diabetic or not so you can get this diagnosis and we should uh fill in all the input data fields so that's what we have built now so you can see this a title here right so diabetes prediction web app so that's what we have given in this uh st dot title you know line and then we have created this six functions right so that's what is uh shown in this particular case so you can see all,358,0,0,WLwjvWq0GWA
26,the six data fields here and you can see this names rate so number of pregnancies glucose level and all those things so those are nothing but what you have mentioned in this course and finally we have this button called as diabetes test results so you you can see that particular button here so diabetes test results so when you you know press that you will get the diagnosis which tells us uh you know whether that person is diabetic or not based on the label we have predicted from our saved trained model okay so this is how you can simply deploy a machine learning model using streamlid in a web page okay so i hope everyone is clear up to this particular point so this is actually a very basic one so there are a lot of things you can do so you can have multiple tabs like say for example we have worked on multiple projects like parkinson's detection and breast cancer detection and all that so you can have different tabs here so let's say that we have one tab for diabetes prediction one tab for you know parkinson's prediction and so on so when user clicks that the corresponding data fields will appear and the corresponding you know test result will be shown so there are a lot of things you can do here so you know you can add some images and all those things so you know it's up to you if you can you can change the backgrounds and all that so you can make this look good and all those things so this is a simple,358,0,0,WLwjvWq0GWA
27,way of deploying a model that you have okay so i hope everyone understood up to this particular point and in the future let's try to work more on this streamlet and let's try to you know explore uh a different things that are present in this particular library and what are all the different things that we can do with our saved model okay so that's it from my side and i'll see you in the next upload thanks for watching,106,0,0,WLwjvWq0GWA
0,what is going on guys welcome back in this video today we're going to learn how to actually deploy machine learning models properly on a server so let us get right into it it's not a game it's a all right so we're going to learn how to properly deploy a machine learning model on an actual server in this video today now the only prerequisite for this video is that you know how to train any machine learning model be it with psychic learn be it with tensor flow pytorch it doesn't really matter you just need to know how to train a model how to save a model and how to load a model i'm not going to cover this in this video today now of course we're going to load the model but i'm not going to explain how to train a model and how to uh export it i have a bunch of videos showing how to do that this video today is focused on the deployment now i am going to use a pytorch model however you can do the same thing with psychic learn intensive flow you just need to adjust the code um so that it works with your respective package and your respective model the focus today is on packaging it as a web application and deploying it onto a server so in my case here i have a pytorch model trained net.,313,0,0,oyYur3uVl4w
1,pth and this contains the model parameters for a simple convolutional network which classifies the cipher 10 data sets so 10 classes uh of images and this is what we're going to deploy to a server today again use any model you like you can use a k nearest neighbors handwritten digits classifier if you want to it doesn't matter just know how to load the model and how to then use it everything else is just a web application around it and deploying it um all right so i'm going to start here by creating a python file which i'm going to call network.,136,1,1,oyYur3uVl4w
2,py and then i'm going to use this file in a web application in a flask web application uh to package it as an application so as an api with also a basic html front end and then i'm going to deploy it on an actual server on the internet um all right so i'm going to start here first of all by installing the packages that i need so tip three install i'm going to need torch i'm going to need torch vision i'm going to need um flask and i think uh pillow as well for the image processing now either i already have a video on this channel or i will upload it soon where i'm going to show you how to actually train this model how it works i'm going to explain it in more detail today i'm just going to load it and use it but i'm going to make a video soon or as i said maybe it's already on the channel where i show you how to implement such a convolutional neural network to classify images in pytorch this is the one that i'm using here uh so right now it's installing p torch and once this is done or actually i can already start with a coding here we're going to import uh torch we're going to import torch dot actually i'm missing an r here torch do an n as an n i'm going to import torch do nn. functional as f and then import torch vision.,332,2,3,oyYur3uVl4w
3,transforms as transforms now again i'm not going to focus here on any explanation regarding the model because as i said there's a separate video for that what i want to do here is show that in general all you need to do for a pie torch model is you need to define the same model architecture that you used for training you just have to create the class and then you can load the model parameters in the case of a psychic learn model you can just pickle or serialize the whole model and load it in the case of tensor flow i think you should also be able to just save the model and load it so depending on what package you use um often times you just have to literally load model and in this case here i need to define the architecture first so i'm going to say class neural net and i'm going to speedrun this part here because it's really not important actually i'm going to even copy paste it because i really don't want to focus at all on uh the logic here i just want to copy paste the class so this is exactly the class that i used for training so this is exactly what i used for training the model in my script where i have the whole logic with the epo and so on and i need to define the structure so that i can load the model weights because the model weights are for the structure i cannot just load the model weights without a structure uh so this is what i do i,358,4,4,oyYur3uVl4w
4,define the class exactly the way i define it for training again i'm repeating if you have tensorflow usually it's something like safe model load model if it's psychic learn it's pickle dump and pickle load so you just have to know how to export and import your model and then what we do is we built a function around it we say def classify image and if in your case it's a different task like a regression task you just uh adjust the function accordingly uh and what we do here is we pass a parameter image and we're going to do the following now we're going to say that the net is going to be a neural network we're going to say net.,162,4,4,oyYur3uVl4w
5,load state dict so we're going to load the uh model parameters the weights and biases so it's going to be torch load train net. pth um and then now basically i have my trained network so i have another script where i built this and trained this i export it and now i have the trained model here ready to use all i need to do now is i need to do an inference so i need to make a prediction how i do that is i define first of all the list of classes in the case of the cipher 10 data set it's plain carber cat if you're in any way confused what i'm doing here i recommend watching the other video first if you're really interested in the model itself um so cat deer dog frog horse ship and truck these are the classes and then we're going to just take the image and make a prediction so we're going to say the new transform here um or actually i can call it transform is equal to transforms.,236,5,6,oyYur3uVl4w
6,pth um and then now basically i have my trained network so i have another script where i built this and trained this i export it and now i have the trained model here ready to use all i need to do now is i need to do an inference so i need to make a prediction how i do that is i define first of all the list of classes in the case of the cipher 10 data set it's plain carber cat if you're in any way confused what i'm doing here i recommend watching the other video first if you're really interested in the model itself um so cat deer dog frog horse ship and truck these are the classes and then we're going to just take the image and make a prediction so we're going to say the new transform here um or actually i can call it transform is equal to transforms.,204,6,6,oyYur3uVl4w
7,compose and i'm going to have a pipeline here of transformations the first one is no matter what image i feed into this this it's going to be resized to the size 3232 because that's my model size i'm going to say transforms dot uh to tensor so i'm going to normalize this and turn it into a p torch tensor and then i'm going to um i'm going to actually call the normalized function i'm going to pass here 0.5 0.5 0.5 is a mean for each channel and 0.5 0.5 0.5 as a standard deviation just because this is how i also trained my model um and then i'm going to do image equals new or actually i call this transform image so we feed the image through the pipeline we get a new image we add an additional dimension so we say image is equal to image. uns squeeze we do that to have uh the batch so basically have a layer around it uh then we put our network in evaluation mode and we say with torch. nograd to disable any gradient calculations here we're going to say that the output is going to be equal to predicting what the image is with the net then i get something that i don't need and the prediction predicted is equal to torch.,292,7,9,oyYur3uVl4w
8,nograd to disable any gradient calculations here we're going to say that the output is going to be equal to predicting what the image is with the net then i get something that i don't need and the prediction predicted is equal to torch. max and i want to maximize i want to get the maximum value of the output i want to get just one value here and then i return from class name so from classes uh i return the predicted uh item so again this here depends completely on you actually this whole file here depends on your model again if you're using tensorflow you import from tensorflow kos models i guess i mean actually maybe i can see from tensorflow kos uh what is it probably models oh i don't know you just have to know how it's done tensive flow i think it's load model from from something like that it's a function load model it's a function save model i think um and in the case of pyed learn you just use pickle to dump the model and then you use it here so in py learn you usually have something like model.,258,9,10,oyYur3uVl4w
9,predict and then you pass something to it it doesn't matter you just need whatever code you need to make the prediction so here you put the part where your model does the inference the actual deployment part starts now which is building an application around it now you can do that with fast api or jango as well i'm going to use flask here and for this i'm going to create an appy file i'm going to say import or actually from flask import flask render template request uh what else do we need actually i think that's it and then also from network import classify image so this is from my file here the method or the function um also we're going to need pillow we actually from pill import image and besides that i think i also need the core python package io for the bite handling and then we can create our simple flask application so it's going to be flask uncore name uncore the template folder is going to be equal to template which i think is also the default and then we're going to define our index route the index route will just give us an html file so we're going to say here um def index return render template and i want to render the index html file which i don't have yet and then the interesting part will be um packaging our functionality so again depending on what kind of model you have if you have a handwritten digit recognition maybe you also want to do an image upload if you have something like you pass in parameters,358,11,11,oyYur3uVl4w
10,and you want to classify some tum more or some uh flower or something like that based on parameters you might want to have text boxes and you might want to get them from the form fields in my case here i want to upload an image so i'm going to create a route which is going to be called slash upload it's only going to accept post requests so methods equals post and then i want to have an upload function here and this function will say if image not in request files if that is not present i will say no file past otherwise i'm going to say file is equal to request files and image so now i have the file and all i need to do now is i need to say if there is a file image is going to be equal to image open io byes io file.,200,11,11,oyYur3uVl4w
11,music run and in this case now it doesn't really matter where i run it because or what i pass here because i can just run it on local host and use it directly when i deploy it i need to make sure i use the proper ip address here so for now i'm just going to say debug equals true um and one more thing that i need is i need to have a directory templates and and i want to create a file here the index.html file and here now i'm just going to say image upload and the important thing is i want to have a form where i can actually upload an image so the action is going to be equal to uh sl upload the method is obviously going to be post now it's important to pass an encoding type which in this case it's going to be multiform uh multi-art form data and here now i want to have an input of type file let me maybe zoom in a little bit input type file the name is image and it accepts everything that is an image doesn't matter what kind of image exactly it's required and then i also want to have a button with type submit and i want to upload images text here um all right so let's run this and see if it works locally so this is just packaging our model as an application so right now actually i need to have um let me just open this on a second screen so i don't show too many of my files uh or actually i,358,13,13,oyYur3uVl4w
12,where was this now let me can i go back i don't think so oh there you go okay so actually here i have already a directory where i only have these two examples i have an example of a plane and i have an example of a dog so i can just uh actually i need to open my browser here again there you go browse example one upload image classification plane example two upload image classification doc so this works already on my system locally so how do i get this now onto an actual server now obviously you need to have a server first so depending on whether you have your own computer here whether you have some online instance that you're renting you just need to connect to a server and you need to somehow be able to transfer files if you don't know how to do that it's done easily with scp or with ftp it doesn't matter just find a way to get your stuff onto the server but before you do that you need to first make sure that everything is compatible so the best thing you can do is you can start working with virtual environments so if i go to my uh directory right now here um i have all these files but i also have all my python packages that i'm using i have my full python installation here so what i want to do is i going to say python 3- mvmf dovm to create a virtual environment uh so i have no packages basically um i can then activate it because i can say,358,13,13,oyYur3uVl4w
13,ll i can see there is this vn directory and now i can say on linux at least source and then vnf bin activate and now i have this environment setup and if i now try to start this application appy you're going to see it doesn't know the package pill it doesn't also know the package uh torch and so on so what i need to do is i need to install these packages here for this virtual environment so i need to say pip install torch torch vision and um pillow and what was the other one flask of course so i'm not going to do this now because i have to wait but basically you do that for the virtual environment and then what you can do is you can export whatever is in your virtual environment or actually let me do that let me do that so that i can show you the process so again flask torch torch vision and pillow i'm going to install this this is now going to install it in this environment so this environment literally only has these packages and all the dependencies installed and then you have this clean setup that you can just export to a requirements file and once you export this to a a requirements file you can easily just upload the requirements file and install everything on the server exactly with the same version so right now you can see everything is being installed and once all the packages are installed we can run the pip freeze command or pip 3 freeze command to see all the dependencies and their versions,358,13,13,oyYur3uVl4w
14,so this is uh what is installed in our virtual environment m and we can export that by using the closing angle bracket so pip freeze export this to a requirements.txt file and when i print the content of this file you can see it's the same as running the command so this is the file that i want to have on my server to install the exact same uh packages with the exact same versions on the server side so what i'm going to do now is i'm going to run an scp command so i have a server which i rented uh online and i can connect via ssh to that server you should set up something similar for yourself so you want to have a server you want to have ssh access and then you can use scp which i also made a video about recently if you want to go uh and watch that it's basically just a command that allows you to upload and download files uh via ssh this file now or this command now allows me to upload my requirements txt file uh to the server by saying scp requirements txt and then root at ip address and in my case i have a private public key uh setup actually i need to also specify a path so colon root sl requirements.txt because now actually i just uh created a file with the name root at ip address let's delete that so i uploaded the file now i can connect to the server uh so again root at ip address this should now connect me hopefully so basically what,358,13,13,oyYur3uVl4w
15,you want to you want to have a server that you can somehow access via ssh so you can log into that server be it with a password be it with a public private key which i also have a video uh about on this channel so how to set up an ssh authentication on a server you need to somehow authenticate yourself log into a server and this allows you to also use scp to upload files i'm not sure why my connection is lagging right now so maybe this is just some some server issue right now but you should be able to connect to your server there you go i'm in inside of it and here now i have my requirements.txt file now it seems like my server is a little bit laggy which is unfortunate but it shouldn't be that much of a problem here now what i'm going to do is i'm going to say python 3- mvn and i'm going to create a virtual environment as well and i'm going to use the requirements txt file to do the installation now i'm going to skip this process because it seems like my server is very slow at the moment but you create your virtual environment you do the same thing of source and then vn bin activate and then you just do pip 3 install dasr requirements txd and this is going to install all the dependencies on the server now i'm going to skip that process because it's going to take some time and i'm going to go get back to you once it's done all right so the,358,13,13,oyYur3uVl4w
16,installation is now done the next thing we need to do is we need to upload the rest of the files to the server for this i'm going to open up a new terminal i'm going to navigate to my current directory again uh and here now i'm going to create actually a new direct dory let's call it uh deploy app or something just so i can upload everything as uh a directory so i'm going to just take all this i'm going to actually i'm going to move this so there you go refactor and now i'm going to say scp d r deploy app root at ip address and then root like this and this should now now upload everything to the server and now i should have this deploy app here so i should be able to go into the deploy app and i should be able to run um python 3 appy however i think that i will need to change or i don't think i know i have to change certain things because now this is running on local host and i want this to be running publicly so i need to actually go into my app py file and i need to go to um to the bottom section where i say app.run and i need to say that first of all i don't want to run in debug mode anymore i want to say that the host is equal to uh 000000 0 that is the ip address that i want to be running this on so python 3 app py run this and hopefully this will not,358,13,13,oyYur3uVl4w
17,"take up too much ram because i'm running this on a pretty weak server right now uh but once i have this there you go it's running i should be able to just type in my ip address here in the browser and actually not because this should be running on port 5,000 so we're going to sa 5,000 now and there you go this is my application now let me again just move this to the second screen so i can go to the directory where i have all the files there you go so now here i choose example one jpeg i upload the image and and hopefully the server doesn't crash uh there you go plane classification plane example two upload image and it's a bit slow as you can see but classification should be dark there you go so this is now running on an actual server this is running as a flask application on the server of course when i end this so when i um terminate this this is no longer going to work so it's going to be offline when i run this but um you can see that this ran as an actual application so this is how you can take a machine learning model turn it into an application or at least an api and run it publicly on a server now if you want to know how to do that with a docker container i also have a video on this channel where i show you how to do that i think it's the last part of my flas tutorial series so you can take a",358,13,13,oyYur3uVl4w
18,look at that but this is how you generally do that so that's it for today's video i hope you enjoyed it and hope you learned something if so let me know by hitting a like button and leaving a comment in the comment section down below and of course don't forget to subscribe to this channel and hit the notification bell to not miss a single feature video for free other than that thank you much for watching see you on the next video and bye,113,13,13,oyYur3uVl4w
0,welcome to wide world programming where we simplify programming for you with easy to understand by code videos and today i'll be giving you a brief explanation of all machine learning models so let's get started broadly speaking all machine learning models can be categorized as supervised or unsupervised we'll uncovered each one of them and what all types they have music number one supervised learning it involves a series of function that map's an input to an output based on a series of example input-output pairs for example if we have a data set of two variables one being age which is the input and other being the shoe size as output we could implement a supervised learning models to predict the shoe size of a person based on their age further with supervised learning there are two sub categories one is regression and other is classification in relation model we find a target value based on independent predictors that means you can use this to find relationship between a dependent variable and an independent variable in regression models the output is continuous some of the most common types of resistant model include number one linear regression which is simply finding a line that fits the data its extensions include multiple linear regression that is finding a plane of best fit and polynomial regression that is finding a curve for best fit next one decision tree it looks something like this where each square above is called a node and the more nodes you have the more accurate your decision tree will be in general next and the third type random forest these are assemble,358,0,0,yN7ypxC7838
1,learning techniques that builds off over decision trees and involve creating multiple decision trees using bootstrap data sets of original data and randomly selecting a subset of variables at each step of the decision tree the model then selects the mode of all the predictions of each decision trees and by relying on the majority winds model it reduces the risk of error from individual tree next neural network it is quite popular and is a multi layered model inspired by human minds like the neurons in our brain the circle represents a node the blue circle represents an input layer the black circle represents a hidden layer and the green circle represents the output layer each node in the hidden layer represents a function that input goes through ultimately leading to the output in the green circles next classification so with regression types being over now let's jump to classification so in classification the output is discrete some of the most common types of classification models include first logistic regression which is similar to linear regression but is used to model the probability of a finite number of outcomes typically two next support vector machine it is a supervised classification technique that carries an objective to find a hyper lane in n-dimensional space that can distinctly classify the data points next navies it's a classifier which acts as a probabilistic machine learning model used for classification tasks the crux of the classifier is based on the bayes theorem coming up next decision trees random forests and neural networks these models follow the same logic as previously explained the only difference here is that the output,358,0,0,yN7ypxC7838
2,is discrete rather than continuous now next let's jump over to unsupervised learning unlike supervised learning unsupervised learning is used to draw inferences and find patterns from input data without references to the labeled outcome two main methods used in supervised learning include clustering and dimensionality reduction clustering involves grouping of data points it's frequently used for customer segmentation fraud detection and document classification common clustering techniques include k-means clustering hierarchical clustering means shape clustering and density based clustering while each technique has different methods in finding clusters they all aim to achieve the same thing coming up next dimensionality reduction it is a process of reducing dimensions of your feature set auto states simply reducing the number of features most dimensionality reduction techniques can be categorized as either feature elimination or feature extraction a popular method of dimensionality reduction is called principal component analysis or pca obviously there's a ton of complexity if we dive into any particular model to help you with each i will be publishing new videos so be sure to smash that subscribe button to be notified on every upload next if this video helped you be sure to like it and share it with someone who might need it music,267,0,0,yN7ypxC7838
0,music hello everyone today i want to talk about something that usually doesn't get a lot of attention and this is the model deployments so let's assume that you've created a super nice machine learning model and everyone is excited to use it and want to play around with it how do you supply these people with the predictions of your model and also how do you present the results to answer that question i want to show you how you can build a simple dashboard that serves exactly this purpose the easiest way to build such an application is to create a web app it can simply be accessed from different places via the browser for these web apps there exist many different design patterns but one of the easiest ones is to have a front end a back end and a database the front end defines how the website looks like so all of the buttons and dropdowns and whatever you need typically this is written in javascript and there are some libraries that make the life of a front-end developer easier such as angular react or vue.js the back-end will fill your website with live for example by loading or reading data and also doing a bunch of other things like communicating with other systems and so on for python common back-end frameworks are flask and django finally you have a database that stores your data and when we think of machine learning again this would mainly be the binaries of trained models you could also store other things like the uploaded files therefore this storage totally depends on your data and you,358,0,0,zhgRFBWa6bk
1,could either use things like amazon's as 3 or relational databases like a postgres database data scientists and machine learning engineers typically have decent programming skills but are not experts when it comes to web or software development so the big question is do we have to build all of that by ourselves luckily not there are a couple of libraries that make our life easier at least if we want to build a simple prototype for the front end we can use for example streamlit which is a python-based dashboarding technology this means we don't have to write javascript and can very quickly build a usable frontend here are some examples this website is fully built with streamlit and uses the cyclogen machine learning model to convert images into comics another example is this traffic flow counter where you can upload a video and the model detects cars on it you can also create more complex dashboards like this nfl receiver dashboard which shows descriptive statistics about different players finally i also found this one pretty nice which predicts the sentiment of a text and also applies explainable ai to show which words had the biggest impact under the hood streamlit of course also uses javascript to build this website but you can program all of it with python we will see a simple example later in this video regarding the backend we first need to talk about some concepts around model serving so usually you start your machine learning project with building a model for example this neural network then you train it and save the fitted model in a serialized form as a,358,0,0,zhgRFBWa6bk
2,file so for a neural network this would include the learned weights and the model architecture now the big question is how do you make this model accessible so that it can provide you with the predictions whenever you need them this is usually called putting a model in production to do so you need to run it in a server so basically a program that runs in a loop and waits for your request to respond to them for this the standard in web development is a rest api server so we put our model there it runs in a loop and waits for incoming requests for a rest api server these requests are http requests which use a special protocol to send data back and forth there are typically different endpoints for a rest api and here we would for example send the request to the predict endpoint finally the server passes the data through the network and responds with the predictions we know now that we need to put our model into such a rest api server so that it can respond to our requests but how do we do that again we are lucky that other people already worked on this with a mlflow you can easily convert your model into an api endpoint that can be queried to get the predictions to do so the model needs to be locked with a mail flow let's see how this looks like it all starts with running your training script that somewhere includes an ml flow function call to lock the model by default ml flow will lock the model locally this means,358,0,0,zhgRFBWa6bk
3,it creates a folder called ml runs and stores the experiments and the data inside of it to be able to deploy models however you need to lock the models to a tracking server this server can simply be launched by calling ml flow server with some additional arguments in the codes we can then tell ml flow where we want to lock our experiments this can either be a locally running server like in this example the local host but also a remote server so now all of the models in our training script will be stored on the tracking server this ml flow server comes with a so called model registry this registry allows to control the lifecycle of machine learning models i won't go too much into detail about it but essentially it allows you to pass the model through different stages until it serves in production the important part is that you can register models from specific training runs and also give them a name once a model is registered you can do all sorts of things like versioning downloading it programmatically or and that is what we want to do serve it as a rest endpoint this can be achieved by calling ml flow surf with some additional arguments like for example the model we want to deploy this will package all of the model code into another server and creates an endpoint called invocations that waits for the incoming data at this point it would also be possible to build a docker image that contains the server the model as well as all dependencies the front end also runs in,358,0,0,zhgRFBWa6bk
4,a server which can be started by calling streamlet run and then the filename that contains the dashboard code once we call the model endpoint from the front ends the predictions of the model are returned and we can show them on the ui in this example we have two back-end services running one for the experiment tracking and the other one for the served model and that's pretty much the full architecture of the web app i've built as ml flow comes with a back-end store as well as an artifact store we can also see it as some sort of database it stores the fitted model and additional information about the experiments in those databases so with a mail flow we have a simple option to host our developed machine learning model and with streamlit we can query it and visualize the predictions now let's take a look at the simple dashboard that i've implemented so this is the streamlet application and its main purpose is to provide hiv inhibitor predictions about molecules using a graph neural network that's actually the model i've built in the previous gnn series with streamlight you have a couple of options for example you can deploy the application record a screencast you also have a couple of setting options like choosing a theme i chose the dark theme here and that's something i want to use the white mode in this application you have two options now either you upload a mall file or you specify a smart string so let's drop a mod file here and we can see that a visualization of the molecule appears on,358,0,0,zhgRFBWa6bk
5,the right if we additionally enter a smart string we get the option to switch between the inputs so if i select a smile string for example it tells us this molecule appears to be invalid because i entered nonsense so let's go back to the file here it says this molecule appears to be valid now there's a button get predictions and if we press it we see a message fetching model predictions and then we get the prediction in this case it says hiv inhibitor also we can expand this section and get further details about the model for example the confidence the version the name and that's pretty much the full application so you can use it now and the different molecules and get a prediction if they are hiv inhibitors or not generally streamlit only allows you to build single page applications but you could implement a simple navigation like that so this would only be an if else block in the codes and if you click on this you end up on a new page so that's all for the first part and in the next video i will show you how the code for this application looks like also i want to mention that ml flow's main purpose is to track machine learning experiments and to ensure reproducibility in addition it provides a couple of handy deployment options that make it very easy to put a model into production finally there exists several ways to do things so the stuff i'm showing here is just one of many options with that i wish you a nice day and see you,358,0,0,zhgRFBWa6bk
6,soon in the next part music,7,0,0,zhgRFBWa6bk
0,welcome to viral programming maybe simplify programming with easy to understand by board videos and today i'll be sharing with you what does it mean to deploy a machine learning model so let's get started first off what is model deployment in practice deploying a machine learning model or the process known as model deployment simply means to integrate a machine learning model into an existing production environment where it can take in an input and return and output the purpose of deploying your model is to make the predictions from a trained ml model available to others systems existing in the network further model deployment is closely related to ml systems architecture so if you already know it it'll be easier for you to understand deployment if not no worries i'll be walking you through the process in a few moments but hey hang on a second before you deploy a model there's this criteria that the machine learning model needs to achieve before it's ready for deployment number one portability this refers to the ability of the software to be transferred from one machine or system to another a portable model is one with relatively low response time and one that can be rewritten with minimal effort number two scalability this refers to how large your model can scale a scalable model is the one that doesn't need to be redesigned to maintain its performance next let's discuss the high-level architecture of an ml system at high level there are four main parts of a machine learning system or ml system number one data layer the data layer provides access to all the data,358,0,0,SHyFjJ-tIJE
1,sources that the model requires number two feature layer this is responsible for generating feature data in a transparent scalable and useable manner number three scoring layer which is responsible to transform features into predictions scikit-learn is most commonly used here and is industry standard for scoring number for evaluation layer the evaluation layers check the equivalence of two models and can be used to monitor production models which means it is used to monitor and compare how closely the training predictions match the predictions on light traffic next what are different methods could deploy your model well there are three general ways to deploy your ma models number one one off actually it's not always that you need to continuously train a machine learning model in order for it to be deployed sometimes a model is only needed once or on a periodic basis in this case the model can simply be trained ad hoc when it's needed and pushed to production until it degrades enough to need some fixing number two batch batch training allows you to constantly have an update version of the model it is scalable method that takes subsample of data at a time eliminating the need to use the full data set for each update this is good if you use a model on a consistent basis but don't necessarily require the predictions in real time number three real time in some cases you want predictions in real time for example to determine if a transaction is fraudulent or not this is possible by using online machine learning models like linear regression using stochastic gradient descent moving on further there,358,0,0,SHyFjJ-tIJE
2,are a few factors to consider when determining your method of deployment number one how frequently predictions will be generated and how urgent the results are needed number two whether the prediction should be generated individually or by batches number three the latency requirement of the model the computing power capabilities that one has and the desired sla number for the operational implications and the cost required to deploy and maintain the model with that i hope this video was helpful to you and delivered value be sure to smash that like button and subscribe for future updates music,128,0,0,SHyFjJ-tIJE
0,hey everyone welcome back to my channel where i talk about all things tech and finance and in this video i'm going to be going over each individual step that you should generally take when you are about to deploy your own machine learning model when it comes to machine learning engineering the intricacies sort of become a little bit variety but i'm going to be going over three main steps that you want to take into consideration when you are actually about to deploy your model the very first step is that you want to figure out where your data is coming from this is going to be your data storage capacity and so this is where we are associating ourselves with databases such as postgres mem sql mysql oracle you name it but in this case i'll be using the postgres database in order to query our data from step two we want to figure out how we're actually going to stream our data into our set databases we can use a variety of tools very popular etl tools such as nifi or airflow but in this case i'm going to be using python since it's very very simple to use and it's also very friendly to use as well now that once our foundation is secured and that our data is streaming into our set databases we want to extract that data and that leads us to our third and probably final step when we are creating our model or we can just extract weights from a given model the process is the same but we need to connect our models to the,358,0,0,XBhlgWAtak8
1,front end in this case flask and that is where our models will be deployed with a click of a button so how would we actually combine all of these services together into one lump sum well in this case i'm actually going to be using something called doctor docker is an extremely popular tool to use when it comes to developing machine learning operations and capabilities you don't have to worry about library versions or package discrepancies among different work streams your code can run on any machine so if it works on your machine it will work on someone else's machine this is where finally the mac community in the windows community can finally unite there are a few intricacies that you want to be aware of when using docker and i will be going over the technical details a little bit later in this video a docker file is a text file that builds an image an image is a blueprint of what your system will look like where it includes operating systems software applications you name it the third part is that you need to know what a container is it's essentially a running instance of that said image a volume is where data that associated with the container can be put aside and accessed later docker compose is an excellent way to run multiple micro services and organizes your work very well and it's very easy to understand and read these services can run different languages and one key thing here is that all the services should interact with each other in some way or another and also all these services,358,0,0,XBhlgWAtak8
2,are run in parallel unless there is a service that is dependent on another and the final step is that there's something called kubernetes and docker swarm kubernetes is an infrastructure for managing multiple containers and it takes care of scheduling issues they are also known as kubelets docker swarm is similar to kubernetes but it's just less technically complex than they are also known as docker demons when they are managing multiple containers okay so this is a very bare bones introduction on how to deploy your given machine learning model and according to our very first step in this case our very first microservice is that we are going to be creating our own database our database is going to be postgres of course and we're going to be naming our our postgres database container just postgres container this is where our docker file exists inside of docker storage we're going to be going over that very shortly these are the ports the very standard we have a host to the container these are the default entries so i just leave it as is the environment variables essentially what your username password and database are going to be so of course i just called it username password and database name you know and so if we want to store a given volume our data this is going to be stored into our external volume so whenever our application is done and through you know if we don't delete our volumes then we can easily access our volumes over here when ever we so may choose so we can be going over this real quick,358,0,0,XBhlgWAtak8
3,and over here let's go over data storage we have a docker file and we have a postgres schema so let's go over to docker file this is essentially getting the latest version of postgres i'm exposing a port and i'm going to be running a the initial sql query in order to stand up our given schema in our given table what that looks like is this we have we created a schema called model data and we're going to authorize ourselves uh postgres username over here we can authorize ourselves in order to you know actually work with this database we can set the path and then we set the path to the model data and then we are going to set the path to the model data as well for the search path for our given database over here of course you can comment it and this is going to be our related data when it comes to creating the table and so we are actually going to be predicting a target variable this is related to a crimes data set where you have a zero or a one where zero is that you are not a criminal and one that you have a criminal record something along those lines so this is gonna be our data storage and if we let's actually run our stuff over here so the way to run this first we want to make sure that we are in the same directory as our docker compose file over here the way to do that is just to run dr compose ups dash dash build so this builds your,358,0,0,XBhlgWAtak8
4,image and then also executes your image as well and so we'll be running that it's going to take a few seconds uh let that run the background but we are going to be checking out what that database looks like so it looks like it is good to go database system is ready to accept connections now let's pull up another anaconda prompt or whatever terminal that you are working with make sure that you are in the same directory as well you know just for clarity sake this is our image that we want to go into so to get into this image we just need to run docker exec dash interactive that's what that stands for 403 which is the container id over here you only need the first three or two or three characters end it with bash and then we want to change our rights to postgres and then we want to access the actual database and i have something written over here that i will just be pasting that over and postgres password now we are in so p sql that's postgresql username and this is the name of our database that we are trying to get into over here that is a database let's pull up our prompts here now let's look at our relationships and it looks like our schema has been created so let's see if there's any data which there should not be any data that exists inside of the schema and the table that we have set up over here so that should be an underscore and this is dot ex table yeah select everything,358,0,0,XBhlgWAtak8
5,so here we have zero rows and that is a good sign so we know that our database has been stood up and now we want to go to the next step and that is going to be inserting our given data now that we know our database is up and running and it can be readily accessible whenever we so may choose we can now insert data into our given database and so this is very similar i just named the overall service python insert because i'm using python the container name is called inserting data and then we have the build data insert which is going to be referencing this file over here and this is going to be our docker file that will actually be builded over here and it depends on postgres so this is that this service depends on the very first service in this case postgres in order to run first and this will run thereafter so once we know everything is running let's go to our docker file very similar we are creating a very similar script for the data storage over here let's pull this up over here let's pull that yeah there we go so very similar to the data storage we have over here we are getting the most recent version and in this case i'm creating a working directory over here so what this does is that within the overall container i think of it as a individual computer a linux box so to say it's creating its own working directory and i'm copying all of my files within this working directory everything that's within,358,0,0,XBhlgWAtak8
6,data insert so requirements insert docker file and everything that we have in our data file over here everything in that copy that to this closed off system and then we'll be running our pips file pip install our requirements which all of our given data libraries exist when using panda sql academy and cycop g2 we're going to be doing all that and then we'll be executing this script related to this docker file python insert py so this will finally run the python script that we have here and this is our overall script so essentially very simple what i'm doing i'm accessing the database that we have just created with the same credentials uh postgres username password database name and i'm accessing the data that we have in our data folder over here crime underscore data.csv access that and i'm just going to be inserting this database or this table into the database that we have over here and hopefully our data is actually ingested so see how that runs and let's pull up our thing over here and let's actually exit out here and what we want to do now since we are re-running everything we want to make sure that we are starting fresh and so that's docker dash compose down dash v and over here notice we have the names of postgres container that references the container names that we have over here so cool stuff uh let us rerun it doctor dash dash build and in this case we should now have a database that is up and loaded in data that's inserted into that database and so let's,358,0,0,XBhlgWAtak8
7,see how that is done and it looks like it is good to go so we see that we have inserting data the container name that references inserting data over here and postgres container that references over here and we both know well we know that our container has run successfully and both of the containers have run successfully but exit code zero so let's check out that data and let's get out of here docker ps check out that skeleton docker over here and let's go into that new container it32c bash let's see postpress and let us log in again pc run that over here postgres password dash t and let's select data from this table to see if we have successfully uh inserted data into our pseudo database over here next table and let's limit that to 10.,183,0,0,XBhlgWAtak8
8,bam that is our data set that we are going to be working with also let's check out how many columns there we go 466 observations so not a huge data set but also not a very tiny data set that we are working with so now that we know that our database is up and running with a given data set over here let's now exit down and power down the overall docker instance so let's go to our flask application which is going to be our very next steps minimize this and that's take out insert so deploy model so first let's check out the docker file uh we're going to be working with 3.8 same thing we're going to be you know create a working directory copy everything that exists within deploy model the folder deploy model over here and then we'll be running the requirements file and we'll be executing app.py which is our flask application so what i am doing over here is actually really quite simple essentially i'm going to be training a model this is extracting data from a given well from the given database extracting the data and then it'll be doing a logistic regression function on that data it's been running into a text file and it'll return that text file so that flask can actually read that and this is where we work with you know routes and so on and so forth where essentially each route has a specific function that's associated with it and if you press a button for instance in this case we'll be pressing an output button a specific function will,358,1,1,XBhlgWAtak8
9,run in this case it'll be train model and then we're going to be putting this into our html html code over here and this is just making our output look really really fancy really really nice so let's see how that works and also note everything is exactly the same it has the same build at the docker file and it depends on in this case the python insert making sure that everything is inserted and we have our flask app which is just the name of course we have our ports over here and now let us see what this actually looks like so let's pull up our terminal over here and let's run this again but now with all of our individual components so this will take a little bit of time in order to configure all the app applications and all that and making sure that each individual service has downloaded their associated libraries so this should only take me like two minutes or so in fact it only took about one minute so let's actually open up a uh a server over here a google chrome all the way and let's actually copy this http let's copy that and the 0.000 we need to change this because the zeros only represented the like any type of incoming ip address that is going to this specific port and so we need to change that just put it as localhost and you should be fine and this is our website oh wow how convenient if you haven't taken the time to like this video hit that like button and subscribe it really helps,358,1,1,XBhlgWAtak8
10,out with this channel bam all right so this is where our deployment will actually happen so we have a button over here this button is going to execute our given script related to the logistic regression over here it's gonna be it's gonna execute this train model it's gonna get all that really nice output and it's gonna be outputting that into our next file well our next could be inputting that into our next web page over here i have going on over here so let's actually run that oh look at that it ran that was really really fast and so this is how you actually deploy a model now you just want to associate this with a website you should be good to go you can associate this with corresponding weights with different models that take hours days maybe even years to in order to train so that maybe everyone can utilize this you can incorporate input buttons in order to you know put up upload files in order to predict something maybe an object orientation their options are endless and this is really really cool so this is a diy on how to do or how to deploy a machine learning model with some machine learning pipeline capabilities really really neat stuff well i'm glad that you made it this far into the video if you have any questions make sure you hit that comment button and let me know what you're thinking i'll try my best to get back to you and make sure that you read this like and subscribe it really helps out my channel and thank you,358,1,1,XBhlgWAtak8
11,so much for watching and i hope to see you in my next video thank you again,22,1,1,XBhlgWAtak8
0,goes i think it goes dat according to the portuguese uh name for data but we call it datato ceo is portuguese um so hi i'm krishna i'm a data scientist uh from dat and today i'm going to be talking about machine learning and production so what that basically means is the life cycle of a model after it's been created and all the complexities that go into it um just to i'll just before i get started just a bit of background of where i'm from um i am i come from a background of machine learning research and i'm really passionate about numerical optimization and right now i build uh machine learning tools for data scientists and developers at t and we help a lot of companies deploy machine learning models so over the past year i've learned a lot about how deployment works and i'm hopefully be able to share my learnings with you um so some things that we do that might be interesting to you we're a startup based in seattle we help people build deploy manage machine learning applications uh if you're interested in us definitely check out this thing called s frames it's like a c backend but a python front end data frame it's out of code it's terabyt scale it's really fast uh check it out if you have the time and we also do some machine learning in python um but what i'm here to talk about is what what we've learned over the past year building deployment services and tools for for data scientists u so i'm going to talk about the things that,358,0,0,6TI-gQhsf40
1,i've learned so if i was to summarize my learnings uh in a slide i'm here to tell you that machine learning in production can be fun uh there's lots of fundamental problems to tackle it's an it's an interesting blend of uh statistics applied machine learning and software engineering and the space is absolutely new not a lot of people are doing it so there's a lot of room for for research questions as well as innovations to be done and understanding how production works or how machine learning gets deployed actually helps us make better modeling decisions so we all here know what a machine learning application is but i'll just like to make sure we're all on the same page so it's essentially anything that that consumes or provides predictions for me so um netflix is the canonical example but what i like more is the twitter how to follow so yesterday last night i took a picture of whom to follow is anybody here in the audience one of these two okay never mind so i think one of them is a creat of ipython the other is scipi and the third person is someone who cited my papers so god bless all three people so what what do i mean when i say production well production when i first got into industry i thought production was about season software engineers and data centers and people telling you how to do things but it's really not about any of those things it's about sharing making your models and predictions available to everyone it's about measuring or reviewing the quality of those predictions,358,0,0,6TI-gQhsf40
2,over time and it's about reacting which is improving the quality of your models over time with feedback so it's not about data centers it's about these three things which are completely orthogonal and what this a pipeline would look like is you uh so i'm assuming here the audience is familiar with building machine learning models but for those that aren't typically you have some historical data and you build a model and you and this model is able to make predictions so this is the creation process and this is something i'm many of you have done before but what happens after that might be new to a lot of people which is how do you take your train model and make it into an entity that can provide predictions to a lot of people and what do you do when live data comes in so when live data comes in it goes to this entity and you get predictions out of it so everything in blue here is what we call production and everything in orange is the is the training phase i'll it starts with a simple pipeline and as i go along the slides it'll get more and more complex with more and more components into it but loosely speaking i i think of production as four separate things there's deployment evaluation monitoring and management so deployment is the process of making your predictions available to everyone evaluation is how you what exactly do you do to measure the quality of your predictions monitoring is how do you track the quality of your quality or impact of your predictions over time,358,0,0,6TI-gQhsf40
3,and management is how do you improve an already deployed model with feedback that you collect and metrics that you collect so these four pieces might seem unrelated but they're actually extremely related to each other evaluation typically happens on deployed models and maybe on models that are not deployed monitoring is because of what you're evaluating and management is because of what you're monitoring and your management results in redeployment so machine learning is really a never-ending process it's not something you just learn and then let it go cuz learning itself is an never ending process so i'll try and go over each of them very briefly uh because i don't have time to go over them in detail so let's start with deployment and u that's specifically how do i take my train model and make it into an entity that provides predictions so the most important learning for me is you treat deployment just as you treat code deployment so machine learning is no different from code and when i say what that means is just i have this predict function that say it it it encodes all the feature engineering that i need and finally calls a say a logistic classifier and there's are lots of advantages to thinking of it this way firstly there's flexibility you don't need any abstractions no need for file format uh no need for file format consensus code is just code everybody gets it and software development for code is a very mature field there's already lots of tools out there that you can use and with these kinds of continuous deployment tools you can,358,0,0,6TI-gQhsf40
4,rapidly iterate models very quickly and i'll spend a minute telling you the key challenge here which is a lot of data scientists and de and develop deployment engineers don't speak the same languages uh data scientists are really good at building models and they use things like rsy could learn and and when you get and in a lot of companies i've seen people actually reimplement all the models that are created by the data scientist which is just a huge inefficiency in the entire system and one way to get around it is via a rest api for those of you that aren't familiar essentially it's a simple language language agnostic contract between two parties where they can essentially exchange information and there's a lot there's a lot of research sorry there's a lot of material out there on the internet for it definitely go read it it'll it'll really help you build very systems that you can easily integrate and that's pretty much all that i have to say about deployment and if you want more details you can come get me offline uh evaluation uh it's basically a collection of how do you perform a metric on top of your predictions but then what exactly are you evaluating and where when exactly are you evaluating it that's when all the headiness comes out so for me the key learning is that the model evaluation metric or the metric that you're training your model for is not the measure of impact of your model that's typically totally different and uh so in the twitter how to follow who to follow example uh the,358,0,0,6TI-gQhsf40
5,metric would be something like precision recall which for those you that aren't familiar it's something that tells you how good uh how good how relevant your predictions are and your business metric might be whether i clicked or not on that who to follow link and the two hopefully correlate they may not correlate and you will not know if they correlate unless you're measuring it and you will not know it unless you've already deployed a model so it's important to know that there are two different things and they may or may not correlate so the other important important thing is there's this whole process of offline evaluation which is i'm evaluating my model on a validation set offline and uh and then there's online evaluation which happens on live data so offline evaluation can happen both on the business metric and the machine learning model metric whereas live uh the online evaluation typically happens only on the business metric unless you magically know something which you generally don't so you uh so these two components are are also different and it's important to do them separately i'll now briefly talk about monitoring and management uh as i mentioned before it's about tracking metrics over time and reacting to feedback feedback so there are a couple things there's one which is once in if i go back to my slide you have live data and you're logging it and you that becomes historical data next month and then you have feedback and then you make that into historic data and improve your model so that way you complete a closed loop there are,358,0,0,6TI-gQhsf40
6,a few things about monitoring management that are important for software engineering which i am not going to be talking about which things like versioning logging provence dashboards reports but what's more interesting is how do you update models so the first question is when do i update my model so there are a couple reasons there are a couple reasons why and when you do it uh the first thing is your data is actually changing so i used to like r in the past which is totally not true but now i like python u but the an important tip is to track these statistics over time and monitor it and make sure you have a system that alerts you if something's changed and your uh secondly your model performance has dropped so last month say your ctr went down by 20 and uh here again the most important tip is to monitor your online and offline metric and track correlation and model performance sometimes can have nothing to do with any of these things it could be something as this button changed from green to blue and people don't like green so they don't click so it could be something like that as well and then and then the next question is how exactly do you update and i'm going to go over two things which is ab testing and multiarm bandits so ab testing is this really simple process of i have this new model version two is it better than version one version two is written by this intern who's smarter than me and version one written by me so the,358,0,0,6TI-gQhsf40
7,idea is you split your traffic 50 um and then you measure uh a business metric say click through rate and at the end of a fixed number of u counts or visits uh you determine which was better and then the whole world gets that version and there are about a million ways you can do this incorrectly and i could probably spend an entire hour talking about all the mistakes people make doing ab testing and it gets very it's pretty tricky uh an interesting approach is called is called multiarm bandits i'm going to talk about the two arm banded case here because it's similar to the ab testing situation so it looks fairly similar but the big difference here is say uh 10 of the time you do this thing called exploration and 90 of the time you do this thing called exploitation so in the in the exploration phase you're actually learning which model is better and the exploitation phase you're you're actually giving traffic to the better model so this way you cut your losses and you're only learning a fraction of the time and there are lots of fancy algorithms for doing multiarm bandits that uh mostly focus on how do you learn faster it's a joke so to contrast multiarm bandits versus ab testing uh multi bandits is more like a set and forget approach i have a it'll figure out what the better model is and it'll figure out that this model has to be served more and it helps you minimize your losses and and a good algorithm will converge very quickly and when i mean,358,0,0,6TI-gQhsf40
8,converge i mean it'll ask it'll require to ask very few questions before figuring out that model 2 is better than model one but there's also a goodness in ab testing because it's really easy and simple to set up and you can answer very relevant questions like is the is a blue button on my on my homepage better than a green button and uh sometimes it could take a while before you observe results so if i have an email campaign that takes say a week u you need to actually observe the results before you ask another question multiarm bandit so it's actually tricky in this kind of situation so there ab testing works out better so in conclusion uh hopefully i've convinced you there's lots of fun and interesting problems to solve in the whole lifetime of a model after it's been trained uh and a few things that i'd like to go over again is uh is try and hopefully run the same code when you're built it and when you've deployed it and the business and modeling metrics are cannot not may not be the same and they may not be correlated be really careful with ab testing and multiarm bandits so any anyone have any questions yeah no no all of this is works on the python stack mod yeah pretty much yeah uh the okay the first question was does any of this depend on anything that dat does and the answer was no it everything works with the python stack uh the second question is uh can you give me an example of things that are wrong,358,0,0,6TI-gQhsf40
9,with ab testing and i have backup slides that because i thought my talk was going to be about a rant but here's one example uh when people make an a test they don't separate out the experiences well so in in the on the left side you have uh the homepage has a orange button and the second page has an orange button whereas on the right side the blue button suddenly changes into the orange button so that's an example of an experiment that was actually badly set up um there's also this thing called a shock of newness which is okay i don't like that my button is now green why is it green now it should be yellow and that takes a while to wear off so sometimes it helps to wait a while um and for me probably the biggest thing is that sometimes people you have to predetermine how many clicks you see before you determine whether or not it is statistically better than another and a lot of people don't do that they just say okay it's better now good to go delete version one so that's probably three of the biggest things in my mind any other questions um say okay so an example is i have this model i have i'm training this model on some metric say log loss tomorrow my uh this intern comes and he says this feature is great and the log loss is improved now the question is does that mean that my click throw rate is actually going to improve so uh then the question is can i push that model,358,0,0,6TI-gQhsf40
10,in an ab test run it for maybe a day and then figure out at the end of the day if the business metric is actually better in model version two than model version one and then you update so this is slightly different from maybe the software engineering perspective of let's just update i know you disagree with me but maybe we can we can talk about this after any other questions yeah how do you approach hthis that data might more how do you approach the hypothesis that newer data may be older than uh may be more important in older data that's a hard question and the answer i don't have uh an answer that i can say in a sentence it's always going to be depends on your problem yeah with a compx model of so uh the question was when your model becomes complex and you have lots of components to it how do you handle diagnosis and for me the the number one thing in any kind of good diagnostics is having good logs so you log at several different levels you you say okay this is how the raw data came in model this component of the model returned this particular output etc etc and then you follow the logs for me logging is always the the heart of all diagnostics is there any other questions yeah backup slides yes so um okay let me try and see if i have this architecture slide that we have okay this oops yeah here's the architecture slide so uh essentially we have like a distributed system that's running there and each,358,0,0,6TI-gQhsf40
11,of these distributed systems is is uh running a python process and uh when i say a model i think of it as code it's just i guess pickled binaries and together with some of our own binary calization formats because our formats are like because we we are shooting for more like cross crossplatform data frame kind of thing so we kind of combine combine our format with the pickle format and basically all these prediction servers are loading up these uh engines and you can think of this working with pretty much any language every language has its own nuances to calization so it boils down to that any other questions all right thank you,152,0,0,6TI-gQhsf40
0,model deployment is one of the important steps in a data science pipeline it's getting more attention of businesses and data science teams nowadays as data science and machine learning fields become more mature however it's been found that up to 87 percent of data science projects never make it into production that means that most jupiter notebooks create by data scientists are put on shelves to collect dust and never get used it's getting easier than ever to create a predictive machine learning model but in fact many data science teams struggle to bring a model to real life to create actual impact so in today's video i'll be showing you how to deploy a machine learning model using runway an enterprise machine learning ops platform developed by makino rocks who has kindly sponsored this video this video is also an extension of an earlier video i made about how to learn four stack data science so feel free to check it out here for more detail if you're not yet familiar with it model deployment is the process of making a machine learning model available for use in production environments so what does it actually mean it means to take a machine learning model that has been trained on a data set packeted and stored in a format that can be used by the deployment platform you are using then we integrate this model into an application say a web app or system that can be used by end users users or other stakeholders can now interact with the model for example give the model some data input and get back the model output,358,0,0,tSiS15ubQFQ
1,this is of course a metaphor to help you easily understand the concepts in reality you don't need to physically interact with the model but you can do it remotely through the model api we see how this looks like in a bit there are essentially three steps in model deployments firstly we need to prepare the model secondly we deploy the model to selected deployment platform and thirdly monitor and update the model we need to monitor the model performance to ensure that it is performing as expected over time the model performance will start to deteriorate as data is changed or there is new economic system situation or there are new customer behaviors or preferences when that happens we need to debug the model investigate what goes wrong and then the model should be retrained and redeployed it's good to mention that in big companies they usually have a whole team of machine learning engineers to take care of productionalizing machine learning models using the model codes created by data scientists and the data pipeline created by data engineers in reality though in many smaller or less tech oriented companies this is often not the case and that's part the reason why the model would stay forever as prototypes in jupiter notebooks and never get used there are many machine learning ops platforms in the markets for deploying models they are complex products and it's tricky to compare them in detail now let me walk you through how to use runway by makino rocks it's a very simple and easy to use machine learning ops platform i find it a bit more native and,358,0,0,tSiS15ubQFQ
2,friendly to data scientists because it can work directly with jupyter notebooks but with a clever twist to avoid the limitations of notebooks which i'll show you in a bit in this stylized example we'll be using the auto mpg data set to build a regression model to predict the miles per gallon of a car we then learn how to solve the model via the http api and learn how to interact with the model to get the predictions for a given data input alright let's go to the runway main page and i'll sign in using my credentials currently runway is an enterprise solution so if you want to try out a platform within your company you can contact them for free trial they are very very nice people to work with they are also now working on making the platform available for individual users so let's keep an eye on that after we signed in the platform we see here this is the runway service console and now we can go ahead and create a new project and here we can enter the name of the project and i call this project auto mpg we can optionally put some description of the project here so let's go ahead and click on create now that we have have the project over here let's click on it to enter the detail page of our project at the bottom left of the screen you can see that there's a link area over here and if you click on the three dot button you can select either the cpu or the gpu for our project and i'll,358,0,0,tSiS15ubQFQ
3,use the cpu for now because this is a very small project as a computer finished setting up you can see that the link area now has turned to the blue color let's click on the link area and now we'll be directed to a new web page that's opened up and this is the model development environment which is essentially the familiar jupiter lab environment but this is hosted remotely on the runway platform you can choose to either create a brand new jupiter notebook over here or you can also upload a jupyter notebook that you already have in my case i have already created a jupyter notebook for the model let's now take a look at the model i insert some necessary packages here it's good to mention that this notebook makes use of the link extension which is developed also by makina rocks the idea of this extension is that it allows users to create a pipeline on top of the existing code cells to explain what that means if we click on the link tab over here you can see a diagram showing the whole pipeline of this model so if i zoom it in a little bit more each of these components on this pipeline corresponds to one code cell in this jupyter notebook it's all up to us how we design this pipeline how the components are ordered and how they are connected to each other for example this cell or component load data set has the parent component import packages which means that the component is only run after we import the packages so the purpose of this,358,0,0,tSiS15ubQFQ
4,link extension is to help everyone easily understand the dependency structure of the code cells and the execution order of the cells this can make projects built in jupiter notebook more production ready i've made a whole video on the link extension a while ago on my channel so feel free to check it out if you want to learn more if you want to add a new component into this pipeline it's also very simple let's say we have a cell to print something like hello data nodes you can simply click on add to the pipeline button and enter the component name here and choose the parent components so it is very very simple but let us remove this cell for now so let me quickly walk you through this notebook to show you how this model is created first we import some libraries so pandas numpy and cycle learn and some additional modules from cycle learn then we import the auto mpg data set using this public data source but in practice it's most likely that we have to pull the data set from a database in your company and take a look at the data set we have the mpg column which is the column we want to predict and all the other attributes of a car that could be you use as features and then we can do some data preparation removing some missing values we also define the target column which is the mpg column this step is actually much more complex in reality we may need to do a lot of exploratory data analysis and cleaning the data and,358,0,0,tSiS15ubQFQ
5,transforming the data to get the data ready for modeling and we may also need to iterate several times to get it right and after that we'll split the data set into the training and the test set now we'll declare the runway regressor basically this is a model object where we define the model to be used and the functions for fitting the model as well as predicting output and scoring the model then we actually train the model using the training data set and the training labels after training the model we test and evaluate the model next we'll declare an input sample by sampling a row in the training set in production setting it's often useful to provide model users with an input sample so that they know how the data should look like so let's say everything is good and this is the final model that we have we can now save the model to the runway platform as you can see all the cells in the notebook work correctly we can also verify if this whole pipeline actually works by running all components over here in the link tab if we go back to the runway console we can see that a new model called auto mpg rec motorcycle loan has been created if we click on it we can see all the methods of the model so in this case there's only one method that is predict and the schema for the input data including all the variable names and we also have the output data schema which is simply a numeric number as the output of the model prediction,358,0,0,tSiS15ubQFQ
6,going back to development environment we can click on save pipeline and click on new pipeline because we don't have any existing pipeline yet and and name the pipeline as auto mpg pipeline and click on save now here comes exciting parts we'll create a model api so that other people can also access our model through the api to get the predictions on their data input so let's go to the api menu and create an api endpoint over here and let's give our api a name let's say it's api endpoint for tutorial we can now select the model that we want to attach to this api endpoint so we'll select the model that we have just created and the model method is predict and the model instance type i'll just use a micro one gigabyte memory let's click on deploy now that we have the model api running as you can see the status is set to working if the model service is running on the right panel you can see all the information of this api endpoint we have the content type we have the api endpoint url that we can share with model users and we also have the data sample that we can download let's try to download this sample here and have a look at it this is what the data sample looks like and this input part specifies all the variables together with the data types and also the values based on the data sample that we created just now in the model pipeline one way to interact with the model api is through the terminal if you,358,0,0,tSiS15ubQFQ
7,want to get predictions from the model for our own data input we can use the client url comment using this format we are basically creating a post request to the model api endpoint which i'll copy from the platform over here and we can specify the input data that we want input to the model which is the sample that we have here and some additional arguments over here let's now run this comment in the terminal and tada we have the model predictions for our data inputs as mentioned at the beginning of the video after some time the model performance will almost certainly degrade and this is when we need to retrain and redeploy the model suppose we have already done the retraining part by using new data or adjusting some things in the model code we'll then navigate to the pipeline section on the runway console click on the pipeline and then we can run the pipeline if our model has some parameters we can also input new parameters over here for our model but in our case having a simple linear regression model we don't have any parameters so let's just run the model by clicking on this run button and after the whole pipeline has been rerun meaning that everything is now updated we can go to the model page to redeploy the model so if we click on the model and click on the three dots over here we can redeploy the model we choose the target api endpoint for the model and select the methods which is predict and the deployment instance and we can deploy the model,358,0,0,tSiS15ubQFQ
8,again this is basically how you retrain and redeploy the model on the runway platform at some point in the future we may no longer use this model and we want to delete the model and the whole project so free up the resources to do that we'll just first delete the api endpoint and then the model and the pipeline and finally the whole project this is quite straightforward on the platform we can easily deactivate or delete the api endpoints the models or projects throughout the workflow with a single click so i hope this demo gives you a better idea of how the model deployment process looks like if you want to try out the runway platform to deploy your own model check out the link in the description below and feel free to contact makino rocks for a free trial and if you want to learn more about topics around model deployment and the practical advice on how to design a machine learning system i highly recommend the book designing machine learning systems by chiplin i have another video over here reviewing this book so feel free to check it out for now hope you enjoyed the video and thank you for watching see you next time bye music,276,0,0,tSiS15ubQFQ
0,a portion of this video is sponsored by discover data science powered by wiley more on them in just a moment in this video i'm going to show you how you could build your first machine learning model in python and we're starting right now so we're going to build our first machine learning model in python and we're going to do that using the scikit-learn library and the coding environment that we're going to use is going to be google colab it's free and it's quite powerful and so let's fire it up so typically when i create projects on google codelab one of the first thing that i would do is i would give the notebook a name so we're going to give it a name of first project and the next thing is i like to add documentation or text to the notebook so i'll add a text cell here and move it up or you can also adjust the location of it by using the down button here and then i'll double click here and i'll click on this button which is the equivalence of a hash symbol which will give the text a heading one if you have two of it it will be a hitting two so the great thing about using headings is that it allows you to neatly organize your jupyter notebooks so i'll show you so here we're also going to make the text bold my first ml project and we're going to use two hash symbol or you could click this button twice and then i'll also make it bold so typically we're going to start,358,0,0,29ZQ3TDGgRQ
1,the project by loading in a data set so let's find a data set to analyze and for that we're going to the github of data professor and if you scroll down one of the pinned repository will be called data click on it and then there's a lot of data sets here that i have compiled over the years as a content creator so a reasonably simple and unique data set that i would like to use here is the delani data set and i think it's this one let me have a look okay so this is a data set of the solubility of molecules and they are important in the fact that they are crucial for biologists and chemists in determining whether a molecule is soluble in water or solvent and whether they will be good drug candidates and so let's have a look here you can see that the data set here is in the format of a csv and essentially it is a comma separated value file so if you click on the raw link here you're going to see the native file let me zoom in and you're going to see that the first row will comprise of the names of the columns and each word that you see here is the name of the column and it represents a single cell and then you have the comma to separate it and therefore the first word here and the second word here and the third word and etc are separated by commas therefore hence they are called comma separated values because the comma will separate the values and so here,358,0,0,29ZQ3TDGgRQ
2,how many columns do we have we have one two three four five so we have five columns and then we have correspondingly from lines two until the end of the file they represent our data sets and so typically what i like to do is i normally will put the y variable or the dependent variable or you could also call it the output variable or the y variable so there are so many names for it and so they are the variable that you want to predict as a function of the x variables which are the ones here that are highlighted so you might be familiar with the equation of y equals to f of x right so y is the last column here the y variable equals to the function of x so we have several x here so it is a multivariate analysis okay so what we want to do is we're going to import this particular data set so i'll click here on the raw link and then i'll click here in the address bar right click and click on copy and now we're going to read in the data set into the jupyter notebook so the python library that we're going to use in order to do that is called pandas and so we're going to import it as follows import pandas as pd so pd is sort of a alias for the pandas library so from here on we're going to call pandas as pd as mentioned here and then we're going to read in the data set in the csv format and then we're going to assign,358,0,0,29ZQ3TDGgRQ
3,it to a variable called df and df is an acronym for a data frame so let's do it we're going to type in pd because we want to use pandas and then we're going to use the function from pandas library called read csv and then ask the input argument which is inside the parenthesis we're going to type in the address of the file or you could also type in the file name so you can see here that we could directly within the data set from the url that we had just copied from github and so let's do it and then once read in we're going to print it out by typing in df and i'm going to hit on the play button here to run the cell and so because it is just loading you're going to see that it's connecting so it's going to take a short moment it's initializing and now it's connected and now we're ready now you can see the ram and the disk that are assigned to this particular cloud computing unit that we have here in the notebook all right and so once you have run the cell you're going to see the output which is right here which you could also close if you don't want to see it again or you could play it again to display it again so we're going to see the contents of the csv file in the tabular format here you're going to see here that the first column here that you see is the index number so officially it's not a column so it is the index,358,0,0,29ZQ3TDGgRQ
4,name and here you're going to see the columns moloch p mo weight num rotatable bonds aromatic proportion so these four variables represent the x variables and so when we build a machine learning model to predict the y variable or the log s and therefore log s is equal to the function of all of the x variables here so in other words we're going to use the four variables here to make a prediction on the log s variable okay and so the next thing that we want to do here now is that we want to split the data frame into the x and into the y and so let's do the most simplest thing is to obtain the y variable so let me show you i'll create some text cell i'll make it both data preparation we have here the first level here one hash symbol we have here two hash symbol this is let's make it a two hat symbol let's make this three because we want it to be a sub section of this one and so we're going to call this data split data separation or data separation as x and y okay and so we're going to create the y and we're going to type in df and the name of the last column here is log s so that's how we're going to get the y and let's see okay and these are the y log s and now we want to get only the x variables so we want to remove d log s so we're going to do that type in x equals to d,358,0,0,29ZQ3TDGgRQ
5,f dot drop parenthesis and then we're going to see we want to draw log s and we want to have it axis equals to 1 because x is equal to 1 will allow the drop function to work with the data as column mode however if you use x is equal to 0 it will work it in the row mode let's see if that's correct it is correct you see that the log s now gone and that we have four columns here and prior to that we have five columns so the number of rows remain the same at 1144 so now we have x and y in the separated form so the next thing that we want to do is we're going to split the data set we're going to split it as the training set and the testing set so let's do it so remember how many we need we need three hash symbols here so we're going to add text cell click it three times and then type data splitting and we're gonna use the scikit-learn package for that so you want to type in from sklearn dot model underscore selection and then you want to import the train test split training test split and now we're going to type in x train x test y train y test equals to train test splits x and y and we're going to have the test size to be 0.2 and let me see i want to have the random state to be assigned a specific number so that every time i run the code cell i will get the same data,358,0,0,29ZQ3TDGgRQ
6,split so we're going to have random state equals to let's say 100 and now we're going to run it so we should now have four new variables here and let's have a look at the x screen and we see that we have 915 rows and four columns let's have a look at x test we have 229 rows and also four columns so x tests or x string will come from the x variable so we started out with 1144 and so 80 of thousand one forty four is nine hundred and fifteen and twenty percent of one thousand one hundred forty four is two hundred and twenty nine and so the training set here will have eighty percent of the data and the x test here or the test set will have 20 of the data and i've actually written a blog post about this particular topic of building your machine learning model in python using scikit and i've drawn several illustrations explaining about the data split so let me go and let me show you and it's this article how to build a machine learning model a visual guide to learning data science so here we have the x and y that i mentioned already and i've color coded here as orange and pink for the x and y respectively scroll down and here here's the data split so here you have the initial data set and then you perform data splitting where eighty percent of your data will go into a container that you call the training set and then the remainder or the twenty percent will go to a container,358,0,0,29ZQ3TDGgRQ
7,that you call it the testing center and the typical ratio is 80 to 20 for the training set and the testing set so typically we use the training set to build a model and then we want to use the testing set to serve as sort of a unknown data that you want to test training set for you want to evaluate whether the model that you have built using the training set whether it performed in a robust manner against an unknown data that you simulate using the testing set okay so before continuing further a quick word from our sponsor and so a short message from our sponsor discover data science powered by wiley which is the premier information hub for the field of data science with in-depth guides on careers degrees and industry-leading programming languages discover data sciences goal is to provide accessible resources and materials for prospective students and professionals through discover data science expert driven articles and publications you'll learn more about which data science degrees help accomplish your professional goals the tools and skills that are necessary for a successful career in the field which career paths appeal to your personal interests how to land a job in data science and as you know data science jobs are rapidly expanding on a global scale with a growing need for qualified data science professionals it's never been a better time to earn your degree and pursue a career in this rewarding field you can begin your data science journey by visiting discoverdatascience.org powered by wiley or visit the link in the description below alright and so let's continue with the,358,0,0,29ZQ3TDGgRQ
8,tutorial okay and now we're going to build the model so let me add a text cell and i'll add here to be two hash we'll make it bold let's call it model building and here we're gonna add another one we're gonna say linear regression let's have it as three hash we have two here we have three here so we have it in a hierarchical form so if you click here you're going to see the table of content of your code and so the benefit of organizing your text cells in hierarchical form is that you could see the table of contents here and then you could click through the various sections so actually instead of making load data having two hash symbol i'm gonna make it into having one so it's gonna be the same as the title and then you're gonna see that this one moved to the left a bit and now we're gonna make uh data preparation to be one as well one hash we're gonna make data modeling to be one hash like that and now we're gonna make data separation to be two data splitting to b2 linear regression to b2 okay and now it looks good to me okay and now we're going to continue by populating the code cell underneath the linear regression so we're going to use scikit-learn from sklearn.linear model import linear regression so you're going to see here that scikit-learn has several functions that you could use not only to prepare your data set but also to build a machine learning model and here we're going to build a typical linear regression,358,0,0,29ZQ3TDGgRQ
9,model and now that we have imported the function we're going to create a variable called lr to stand for linear regression we're going to type in linear regression function here which will be represented by lr and then on the next line we're going to run lr dot fits which means that we want to train the empty linear regression model on the following data set which we specify to be extreme comma y train and then we run it you could click here or what i like to do is i like to use the keyboard shift enter which is quicker for me and the model is built and now that the model is built we want to apply this particular model to make a prediction so let me add the text here so that we could annotate it a bit more we could say training the model and make it bold find the model to make prediction and we're going to call it y underscore lr underscore train underscore pred so we're going to apply the model to make a prediction on the training set and the prediction to notify that we're going to use spread and then to make note of the algorithm that we're using to train the model we're gonna specify to be what lr here and then we're gonna start with the y underscore so this naming convention will be helpful when we have several machine learning algorithms that we want to try out and also whether our prediction is made on the training set or the testing set so type in lr.predict and then i'm going to specify,358,0,0,29ZQ3TDGgRQ
10,xtrain to be the data because we want to make the prediction here on the x strain so essentially we're going to do the recall it's going to be making prediction on the original data set that it has been trained on and so that will allow us to evaluate the performance of the algorithm so here we're going to call it y underscore lr underscore test underscore thread equals lr.predicts and as you've guessed why underscore test let's do it let's print out the results y underscore lr train thread y underscore lr test spread actually let's just make it like that okay so these are all of the predictions have a look here so these represents the 80 of the data and there you go the remainder 20 has been predicted and we have the predicted value and the next part here is we're going to compare the predicted value with the original value or the actual value and we're going to call the new section here to be model performance we're going to say evaluate model performance because we want to compare x strain here no not not x string y train with the here ylr train so you're going to notice that they are the actual value and the predicted value okay so in just a moment i'm going to show you a scatter plot of these two values and if they lined up in a diagonal trend line and see whether they have high dispersion or low dispersion so if the dispersion is low we will expect that the performance will be good okay and so now that we see the,358,0,0,29ZQ3TDGgRQ
11,data that we are going to use we're now going to actually perform the model evaluation let me delete it here first lead delete add the code cell and we're going to type in from sklearn.metric import mean underscore squared underscore error and we're gonna use the r2 score function lr underscore train mean squared error equals to mean squared error function y train underscore y now train thread and so these are the two variables that we have taken a look just a moment ago and now we're going to calculate the mean squared error we're going to calculate the squared correlation coefficient using the r2 score function y train and you guessed ylr train thread and so these two blocks are for the training set now we're going to do the same for the testing set mean squared error one test r underscore test red lr test number two equals r2 or and we have y test and the ylr test underscore print run it let's run values here okay they're reasonably similar performance here so we could tidy it up a bit by saying the lr mse and then we say training or to train equal or colon print and then we're going to have this one here we're gonna reuse it r2 that will be our training r2 okay here and now we're going to turn this to be test test this would be test this would be test okay there you go so instead of having four of these we're going to delete them so you could highlight multiple cells just by highlighting it and then you could click on either,358,0,0,29ZQ3TDGgRQ
12,you want to move it or in this case i'm going to delete them okay so we see all of it at a glance here however we could tidy up this particular layout a bit more let me show you lr results and then we're going to create a pandas data frame we're going to call this linear regression lr i mean mse lr test mse lr test underscore r2 and then we're going to transpose it and let's have a look it looks like that and now we need to change the column names here zero one two three four so what we want to do here lr results dot columns and we're gonna rename it we're gonna call the first column to be method second column to be training mse and then we have training our square and then we have test mse and then we have test r square run it there you go it looks much cleaner much cleaner than this in a tabular form and so the great thing about having it in a pandas data frame like this is that if you evaluate more and more machine learning models like random forest k-nearest neighbor support machine neural network then you're going to have a data frame that will allow you to easily compare you could also sort by column the performance and that will help you to evaluate which one was the best so here you have already built a linear regression model and we're gonna try out another one which is the random forest see we have two hash symbol here so we're going to add a text cell,358,0,0,29ZQ3TDGgRQ
13,add two and then we're going to call it random forest random forest and then you can see it here but notice that you don't see the bold text because it needs to be in a hash symbol which will give it a heading one heading two heading three you know like the hierarchical ordering so if you want this to appear here then we need to add more so this is two then we need to make three here add three and you're gonna notice it appears here at three it might be good because you could also you know hop around the notebook like this you know click on the various topic of your choice and then you could skim through your jubilee notebook and also the great thing is that you could take a look at your table of content without you know scrolling up and down to see what's the name of the cell because sometimes your output might be quite long here and it might take some time right but it'll be much quicker to just navigate by clicking on the particular link so we can see here that we have training the model here so we could just add section called training the model training the model and then let's just add the headings and then applying the model to make a prediction then evaluate model performance so we could move this up a bit so we're going to train the model using the random forest algorithm so from sklearn dots ensemble import random forest regressor so a pointy note here is that this particular tutorial video makes use of,358,0,0,29ZQ3TDGgRQ
14,regressor because we're building regression models and it is because the y variable which is called log s let me show you log s right here it's a quantitative value so if the y variable is quantitative we're going to build a regression model whereas if it is categorical then we're going to build a classification model okay so in this tutorial the log f is quantitative therefore we built the regression model because random forest here has two versions random force regressor and random first classifier and here we're using the regressor so we're going to create a rf variable to house the random forest algorithm and we're going to specify some of the parameters for the model here maximum depth of 2 and the random state of what about 100 because in the prior random state we used 100 and now we're going to train the model so we're going to type in rf fits and then we're going to use xtrain and y train and then we run it to train the model and the model is trained we're now going to make the prediction in here so actually we could just copy the code cell above here scroll down and we're going to change this to be rf rf lr to be rf part f and now it looks correct to me and we're going to run it okay and now we're going to do the model performance evaluation i'm going to copy the code here paste it we're going to use the mean squared error and we're going to use the r2 score and here instead of lr we're going to,358,0,0,29ZQ3TDGgRQ
15,replace that to be rf okay so replace all of the lr to be rf and be mindful maybe you might type in wrong like me um just a moment ago to be fr so our f here will be r f now r here will be r f r f and r did i say r f just a moment ago i meant to say lr and now it's rfk let's run it and let's copy the code here which we use to make the table and we're going to change this to brf again and this will be random for rest rf and here r f f show the table okay and now we have two tables we have the linear regression table and we have the random first table so why don't we combine the two tables together okay let me create another level see what level is this random for us with the two hash so one two model comparison and now we're gonna compare it so we're gonna combine the two results table into one and let me see df models equal pd.concats and then i'm going to specify the name of lr results and rf results see do i have x equals to zero because i want to combine it in a worldwide manner let me try if it works all right it worked yeah so x is equal to zero if you want to combine in a row-wise manner whereas if you use axis one it will be in a column-wise manner so here we're stacking them on top of one another okay so you can see now that,358,0,0,29ZQ3TDGgRQ
16,the two are in the same table but then the index number is a bit off so we need to reindex that so let me see if it's as simple as doing this index okay but it also added a new column here we just say draw it true oh and now it worked we could have also added this at the back of here one again and the number is correct okay but i'm just going to separate it so that it looks a bit more tidy and you could see it but you know how to make it into a wine liner you could just copy here and paste it at the end here so here you can see that we have already compared linear regression model and the random force model let's have a look at the scikit-learn okay and if you click here regression and so here you could find other regression model that you like and you could use it to build your own in the colette notebook here and then you could then add the resulting performance into the data frame here to make your comparison and so now we're going to perform data visualization to take the predicted value and the actual value and make a scatter plot let's do it let's say data visualization of prediction results and we're going to make use of the matplotlib library so we're going to import matplotlib dot pi plot as plt let's say plt scatter we're going to assign to the x axis y train and to the y axis be l of train thread and let's make plots okay this,358,0,0,29ZQ3TDGgRQ
17,is our first attempt let's make it a bit lighter you could adjust the darkness of the samples that you see here that are represented by the circles using the alpha option we're making alpha to be 0.3 so that regions that are highly overlapping will be a bit darker whereas those that are not overlapping will be lighter color and you're going to see that the x and y axis is not yet labeled we're going to do that plt y label predicted log f f c f label experimental s okay now we have the labels here why don't we make it have plain width and height make fixed size to be five and five okay why don't we make the dots here to be another color the color option and we're going to use color green and what about a trend line let's add a trendline for that we're going to use numpy get the fidget line creating a z variable np dot polyfit and then the y train and then lr train brad p equals z then we're going to color to it 60.,244,0,0,29ZQ3TDGgRQ
18,okay there we go so we added this red line as trendline that are fitted with the data here so congratulations you built your first machine learning model in python using the scikit-learn library so you can see how easy it is now to build models in python particularly for your tabular data sets and so please feel free to build more models and you could tweak the learning parameters and as i have shown you this api documentation from scikit-learn you could go through the documentation you could click on an algorithm that you're interested in read about it and look at some of the parameters that it allows you to adjust so give it some try let me know in the comments down below what models that you are building and have fun thank you for watching until the end of the video if you reach this far drop a snake emoji so that i know that you're the real one and while you're at it please smash the like button subscribe if you haven't already make sure to turn on notifications to be notified of the next video and as always the best way to learn data science is to do data science and please enjoy the journey,274,1,1,29ZQ3TDGgRQ
0,"hey there! today, we will talk about how one can deploy a machine learning model on kubernetes and since this is a very broad topic i will just show you a minimal example and i will refer you to some external documentation and packages if you want to get some extra features anyway i hope you enjoy the video! here you can see a sketch of the three major steps that one needs to perform. first of all, we need to turn our model into an api server that is able to receive requests run, inference and finally return a response with the result. the reason why we do this is to standardize the way we communicate with our model and to have a layer of abstraction that hides the complexity of the model inference. the second step is to take our api and containerize it. the reason why we do this is to make sure that we can run it on any device and that all the dependencies are available. finally, we take our image and we deploy it as a pod on kubernetes. i guess now it is the correct time to talk about some cool features of kubernetes. first of all, we can deploy a pod on its own, however, it is better to create it with a parent object called a deployment. if you do so the deployment will make it possible to track version history, it will make sure that the pod is always running and it also will allow us to create replicas of the same pod and many other things.",347,0,10,DQRNt8Diyw4
1,"if you do so the deployment will make it possible to track version history, it will make sure that the pod is always running and it also will allow us to create replicas of the same pod and many other things. second of all, we can create another parent object called a service that is going to load balance requests between all of our replicas. in short, the deployment and the service together will allow us to scale horizontally. kubernetes has many other cool features but these are the two main ones that we will be focusing on. here you can see a table of some popular frameworks that can help you with the three-step process and they also add a lot of features and simplifications. just a small disclaimer i'm not familiar with all of them so feel free to write in the comments if you find some mistakes or if you think i left out some important framework. also note that i did not include any cloud service provider services and i also did not include any http web frameworks like fastapi and starlettte. okay so let's start with the hands-on end-to-end example. to make things fast i decided to go for a minimal solution which is an api creation tool inside of the transformer cli. it was the last row of the table i just showed you, however, before i do this let me just set up my environment. pyenv is a tool that enables you to change python versions. here i create a virtual environment. and yeah we are ready to go. so let's first start with installing transformers.",354,10,23,DQRNt8Diyw4
2,"so let's first start with installing transformers. the extras_require equal to serving because it will install things like fastapi and uvicorn. let's inspect this transformer cli a little bit. i actually forgot to install one of the deep learning frameworks so let's get torch. this is the documentation of the cli and let me just quickly show you that there are a bunch of commands that one can use like convert, run and for us not surprisingly the relevant one is going to be serve. so let's see the documentation of it. to be able to run this we need to provide a task which you can see here and in our case i just decided to go for fill-mask and then we can specify things like the host, port, workers and most importantly the model. let's try to run it. as you can see we have a server up and running and now the big question is how do you actually use this server? what are the endpoints? to my knowledge there is not really that much documentation on this so let's just go straight to the source code to try to figure this out. here i'm on the github of transformers and let me go to the src and it should be under commands and in this module called serving. first of all, one interesting thing is that we can look at the imports and we see that we're using fastapi, starlette and uvicorn - these are the underlying web frameworks that we are using under the hood. and if we scroll down again, i'm just looking for the endpoints ...",354,23,36,DQRNt8Diyw4
3,"and if we scroll down again, i'm just looking for the endpoints ... and as you can see here for each of the routes we basically implement some kind of a logic. i would imagine that this root route just represents some information about the model that we are serving. tokenization and detokenization there are self-explanatory and finally forward is the one we are going to be using and it's literally both the tokenization and the forward pass. anyway, let's try out this root endpoint. we just literally sent a get request to it and let's see what we get back. you can see that the server received the request and it returned response with a status 200. the response itself is a json and it's not really readable so let me pipe it into this tool jq and let me make the pane bigger. and as you can see there are a bunch of let's say details about the architecture and about the model that we're using. this is as we expected. cool, let's try to use the forward endpoint now and see what we get. first of all, it needs to be a post request as we saw in the source code. we paste the url and we specify headers both for the type of response body and the type of the request body. not surprisingly both of them are jsons. finally, we need to send over our request body and yeah the way the fill-mask model works we just have this special token called mask and we want the model to suggest the best possible words that could fill in the blank.",357,36,50,DQRNt8Diyw4
4,"finally, we need to send over our request body and yeah the way the fill-mask model works we just have this special token called mask and we want the model to suggest the best possible words that could fill in the blank. let's try to send it over and yeah of course what i forgot was to specify the right endpoint which in our case will be forward. and i think i misspecified it again. let me fix this. it's not here. sorry about this. we managed to get a response the server liked our request and let's again use the jq tool to see what's inside and we get some answers i mean probably this was not a great example so did not really fill in any name but i guess that's just the way the model works. so here i thought that the most likely option was to just end the sentence. let's let's fix the input. instead of this let's just do today is going to be a mask day . let's see. this one is way better. as you can see it thinks that the most likely option is long . the second most likely option is great and so on and so on. that's all we needed, that's all we wanted. we have an api server that receives requests and does tokenization, inference and then sends over a response. so this first step as described in the diagram is done. however, let me just point out that we made a lot of simplifications. there are actually a lot of cool features that one can add to this. for example, things like adaptive batching.",361,50,69,DQRNt8Diyw4
5,"for example, things like adaptive batching. now we are only sending one request at a time and you can imagine if the server was used by a lot of people it might be beneficial to actually batch the requests and only run the forward pass once or twice especially if you have a gpu. that's one of the features. also what's very common for these apis is to have a metrics endpoint that will give us status on how the server is doing, how many requests it received, what were the timings and so on and so on. again, this is a minimal example. frameworks like bentoml they give you way more than this. now the idea would be to containerize our api. i'm going to be using docker for this, however, there are other technologies like podman. anyway, when trying to build a custom image one always needs to specify the base image and since our api is basically using the transformer cli we need to look for an official transformers image on dockerhub. and this one seems to be a great candidate. as you can see it was updated very recently. i believe that there's always a new release or a new docker image being pushed to dockerhub whenever there's a new release on github and yeah it supports both cpu and gpu. here are some of the tags. we're going to get the latest one. we pull and it was pretty fast. i already did this when i was not recording. let's double check that we have it. as you can see it has 17.6 gbs which is quite a lot but it is what it is.",364,69,86,DQRNt8Diyw4
6,as you can see it has 17.6 gbs which is quite a lot but it is what it is. let's now use it as a base image to be able to create our custom image. we start from this one. what we do here is to instantiate our model when we're building this image and it's a hack that will allow us to store the model weights inside of the docker image because this automatically triggers the download. let's do the same thing for the tokenizer. not sure if it's necessary but whatever. and what you also need to do is to get dependencies for the serving logic. so in our case it's just fastapi and uvicorn. let's now expose a port. and finally let us write a custom entry point that is literally going to launch the server. we should be done! let's try to build this image. and let's call it cool-api . it basically was instantaneous. the reason for this is that i already did this before. if you were doing this for the first time this will take way longer. let us just verify that we really have it. do it like this. what's interesting here is that before the base image was 17.6 gbs i believe and now we are on 18.1 gbs. and it's again because of the fact that here we downloaded the model weights inside of the image. let's try to run this. we do port forwarding. we right away get a warning about the platform and this is something specific to my computer.,340,86,108,DQRNt8Diyw4
7,"we right away get a warning about the platform and this is something specific to my computer. i'm on a mac m1 and unfortunately the base image we used did not support my platform and as you can see the server is running. let's figure out what port was exposed. and as you can see here it was 55008 and we can do the same thing as before but instead of sending the request to the port 8888 we need to change the port. we managed to get an answer, unfortunately, because of this platform mismatch this is way slower than having the raw api. but from the functionality point of view it is the same. let me just quickly actually write a separate image where i basically build everything for my platform. and let me just copy paste this from my notes. here i'm using conda but the the logic is the same. let me call the previous server and let me docker build. by default docker build is going to use a file called dockerfile but we can also manually change it. so we can say file, if i'm not mistaken and we can say dockerfileconda. let's not override the old one. let's call this version 2 and again i did this before so that's why it was so fast. and finally let us run it and hopefully it's going to be a little bit faster. i'm not getting the platform warning. let's again check the port. now it's a little bit different. let's do curl. now it's pretty fast - you can even time it.",348,108,127,DQRNt8Diyw4
8,"now it's pretty fast - you can even time it. it works literally the same way as the raw api but now we managed to containerize our application and if you're wondering why it was necessary, why do we need to wrap it? well, docker containers are amazing at capturing all the dependencies and they are very portable so you know i can upload the docker image on some repository or in some cloud platform and chances are i can run them right away without any issues. actually the second image is way smaller. now let's focus on the third step which is deploying our docker image on a kubernetes cluster. for the purposes of this video i'm going to create a single node cluster on my laptop using minikube. however, in real life one would have a kubernetes cluster on premises or one would use a cloud solution. two of the most famous ones are eks from aws and gke from google. anyway first of all let me start the minikube kubernetes cluster. it is done and let me actually show you what this cluster contains. for this i'm going to be using the command line interface kubectl. there are a lot of controllers and other kubernetes specific things. the idea is that we would like to deploy our docker image on this cluster in a form of a pod but before we do so i just need to load the image from my docker daemon into the minikube docker daemon. let me show you what i mean. our cool-api:v2 is not here in the list so we need to load it. this will take some time.",361,127,142,DQRNt8Diyw4
9,this will take some time. it seems like we are done! let's verify this. as you can see we have the cool-api image here. now the idea is to create a deployment which is basically a parent object for pods. in our case each of the pods will contain a single image which will be our cool-api: and we can actually create the deployment in a very simple way using one command. so i will be calling our deployment cool-deploy . we will be using our cool-api image and that's more or less everything we need. and let us now inspect what the side effect of running this command was. we have one deployment object and we also have one pod which was automatically created by this deployment. and what's important is that here there is always random hash. deployments are used in a case where the pods don't have any state so there is no notion of order. and if this given pod dies the deployment will make sure there's a new one with a completely new hash. now we need to create a service which will basically take all the pots we have in our deployment which is for now just one single pod and it will load balance the traffic among these pods. we basically say: hey! look at all the pods that are managed by the deployment 'cool-deploy'. we name our service cool-service and now we just need to specify two ports. this is the port inside of the image that we exposed. you remember this port from the dockerfile.,343,142,160,DQRNt8Diyw4
10,"you remember this port from the dockerfile. and the second port is just any port that we want that will eventually be the one that is exposed on the outside. everything was created. so let's do a small recap of what we have. we get all the objects. internally, there's also a replica set but don't worry about it. under the deployment and the replica set there is a single pod and also we have a service that load balances request to all of the pots. let's try the service out and to do this there are multiple ways. the simplest way with minikube is to just simply do a bit of port forwarding. as you can see it automatically opened my browser, sent the get request to the root if you recognize this info json. but if we go back to the terminal we can basically use this url and this given port to send requests to our service which will eventually end up on the pod. let's give it a try. let me copy paste the curl command. and here let me change the port. that should be it and yeah let's see whether things are working. we received a response from our kubernetes server and up until now we don't necessarily see any benefits compared to let's say the pure docker solution but let me actually demonstrate two very cool features that you get for free if you use kubernetes. and the first one is simply the fact that even if you kill your pod or something happens internally it most likely won't be you . let's say there's an issue and then it crashes.",361,160,177,DQRNt8Diyw4
11,"let's say there's an issue and then it crashes. the deployment will always make sure that there is a certain number of pods running depending on how many replicas you chose at the beginning - in our case right now it is one replica. we haveone deployment. we saw this before and let me artificially kill our pod. let's kill it and let's do kubectl get pods again and as you can see right away five seconds ago the deployment made sure that there's a new image. just note that the hash is different. and related to this that's not something we did we can have liveness probe so even if the container is running maybe something is broken on the inside and kubernetes can periodically query the pod to know how it's doing and if there's something wrong again it can restart it. so that was one of the features and the second feature is using a load balancer. so first of all, before we increase the number of replicas let me just get the standard output of the one single pod that we have. this is the standard output without color from our single pod and if we send the request to it. you can see that we are pinging this single pod. let us now add two more replicas and again we can do this in a single command. we just use the scale command and request three replicas. kubernetes is not going to kill our current single replica and it's going to add two new ones.",338,177,190,DQRNt8Diyw4
12,"kubernetes is not going to kill our current single replica and it's going to add two new ones. there are two new replicas and since we created a service before it will automatically load balance the request between these three replicas. let's verify that this is the case. the way one can do this in kubernetes is to look at the endpoints of each service. our cool-service actually distributes the load among three different pods. let's actually try this out. i'm going to create a separate pane for each of these pods. i already have this first one, let me create a new one so that i can inspect their standard output. and finally this one. and let's rearrange this in a nice way. this should be good enough. here you can see the three pods and here this is the original one where we already sent some requests but let's now actually send a new request.0 as you can see this specific request ended up here on the second pod. let's add a new one. this one ended up on the third one. this one again ended up on the second one. let's do multiple. as you can see the load is being distributed between the three pods and since i am on a single node laptop this load balancing is not really that powerful but in real life one would actually have each of these pods on a separate node and one can really scale horizontally. anyway, i think that's all i wanted to show you.",335,190,207,DQRNt8Diyw4
13,"anyway, i think that's all i wanted to show you. again, kubernetes has many other amazing features that are really relevant but i thought that these two that i showed you are pretty useful for people who want to deploy their machine learning models. anyway, that's it for today i hope you learned something new and if you have any questions or if you think i forgot about something feel free to write a comment and i'll be more than happy to read it and reply to it! and i will see you next time!!",123,207,210,DQRNt8Diyw4
0,hello all my name is krishnaik and welcome to my youtube channel so guys in this video we are going to create an end to end machine learning project with the help of amazon's sage maker so if you don't know about sage maker it is very much popular used in many industries who are specifically using aws cloud using the amazon sage maker you will be able to build train and deploy your machine learning models and even create endpoints and expose that endpoint so that any application can probably use it you know so in this video i'm going to probably show it i'm not going to use the aws console over here instead i will try to show you with the help of coding uh what our libraries are specifically required we'll also take care of that step by step we'll see it this will be an amazing video for every one of you who is really interested into this so please make sure that you watch this video till the end because i'm planning to create this videos with respect to two parts in the first part i will show you with the help of jupiter notebook and in the second part i will try to to probably create an end-to-end project wherein we follow the entire life cycle so uh quickly let's go ahead without wasting any time and let's see this particular thing okay now here is my entire code i have probably taken a data set over here which is called as mobile price classification data set and we will consider this particular data set and then we will,358,0,0,Le-A72NjaWs
1,try to and create some models and this is a classification problem statement we need to probably find out what is the price range of a specific mobile based on all these features that we have so this is the problem statement that we have actually over here right now the first thing what you really need to take care of is that i will be enabling enabling the aws cli so for that what you need to do is that just go ahead and download uh aws cli that is the command line interface so that you can probably write any kind of commands from the command prompt itself right now in order to install this in the windows or whatever machine you specifically have just go and click on this first link okay search for aws cli and then here you will be able to see in windows you will be having this msi file so this is just like an executable file click on this download it double click it and just install it right so through that you know what will happen aws cli will be enabled and installed in your system okay so this is the first step now we we need to configure you know configures considering a specific user right so here if you don't know about here what you can basically do is that you can search for im and here you can probably create your own users right so based on this particular user like over here you can see many users are basically there i can probably click and show you one of the user how,358,0,0,Le-A72NjaWs
2,it is created so this is the sagemaker user and here i will be providing the administrator access okay so in order to create the user just go ahead and click on add user provide the username like i will be showing i will be writing something like sage make a example okay so this will be my user just to show you right and then i will be providing the user access to aws management console click on next okay and this will take you to the identifier or if you also make sure that if you also uncheck it that is also fine then you go ahead and click on next then what you need to do is that go ahead and provide the attach policies directly and here you need to provide the administrator access you know administrator access will give you the access to most of the things that you specifically do everything is not required you can also go ahead and just take the sage maker part also sage maker uh services access also but here i'm just going to give you the administrator access click on next okay and over here also you can see permission boundaries this is there and you can also go ahead and create the users right now before this what i would also like to want is that probably i want to go ahead and create my own uh secret key for this particular problem statement so i will just go ahead and click on create a create user now once my user is basically created so here you will be able to see a,358,0,0,Le-A72NjaWs
3,specific user let me see whether that user is there or not so i will just reload this and uh uh sage maker example is my username right and here what i can do i can go to the security credentials okay and i can probably create my own access keys this access key will be super important and you have to do it for the first time i will go ahead and create this uh and then i will just go ahead and probably click on cli if i want otherwise i just keep an uncheck of this you know by default you can also go ahead and just go ahead and click on the next so let me do one thing let me again do this particular step create the access key create on next so i will select the command line interface click on next and before that just confirm it and no need to create any description tag value and here is my access key what i will actually do download this csv file this is super important because whatever secret access key and this access key i will be having i have to copy and paste it over there okay probably if i open this you will be able to see this right so this is how my access key looks like and the secret access key looks like okay so don't try to use this anyhow because i'm not going to i'm going to probably delete this after my video is basically created you know so i will go ahead and take this now open my anaconda prompt okay and,358,0,0,Le-A72NjaWs
4,here what i'm actually going to do i'm just going to write a aws configure okay and here i'm just going to give my access key id which was the first one which i had actually copied and then again i will go over here and probably create my secret access key this i will just copy and paste it over here right and what is the default reason you will be seeing that us east one is the default region in this aws so i will press enter output format i don't want any default so i will keep it like this okay so this is the first step that you really need to do that is i will just write down in the notepad okay just a second i will just write in the notepad and this specific notepad what i will do the first step create an im user provide the administrative access right administrative access okay the second step is that aws cli i will try to enable it because all the coding i will be doing over here in the vs code and it should be able to uh you know run the instance create an instance in this age maker itself from this particular cli that is the command line interface right so awcli for this we need to configure it right configure it to whatever secret access key you specifically have okay so this is done now what i'm actually going to do is that i will go ahead and open my vs code so here is the entire code over here i will go step by step what,358,0,0,Le-A72NjaWs
5,all things we have specifically done but before that uh let's go ahead and create my new environment so in order to create the new environment what i am actually going to do over here is that i will go to my file or i will just go ahead and open my terminal so new terminal over here and for this new terminal what i will do i will just go ahead and create a command prompt and start my work okay so right now it is going to some development environment so i will just say conda deactivate okay and i will just go ahead and create my i will just go ahead and probably create my new environment so in order to create the new environment i will write conda create minus p let's say my env is my environment name i will give my python version is equal to 3.8 okay you can also have 3.9 it is up to you but right now i'm just going to create this specific environment so here it is you will be able to see that a new environment will start getting created so i will go ahead and create click on y so here what will happen in this same folder location i will be having this my env environment right so every installation that i'm specifically going to do because later on i'll try to convert this as an end to end project i will show you over here the entire project i'll run and show you everything i'll show you over here itself but again to create an end-to-end project you need to,358,0,0,Le-A72NjaWs
6,create the data ingestion layer everything and all that i will do in the part two so this is the first part as we are focusing on understanding things okay so now here is my environment created now what i will do i will write conda activate my env slash okay so now i'm inside my uh my env environment so now whatever installation i did specifically want to do i can probably create a requirement.txt over here so this is my requirement.txt and let's say that i want sagemaker skycad learn pandas numpy and one more library that i can probably say is ipi kernel okay the reason why i write ipi kernel because i will probably require it for my jupiter notebook again not necessary you need to write it so i will say pip install minus r requirement oops just a second i will just clear the screen just a second okay so pip install minus our requirement.txt so done ah minus r okay i do not have to give the space right so minus r requirement dot txt and this installation is basically taking place okay and this will take some time based on internet speed but this is the first thing and here i'm using sagemaker the sage maker and since i've used awcli sagemaker is also a library that we are specifically going to use sky catalan obviously you know because to apply any kind of machine learning algorithm uh pandas numpy and ipi kernel is basically for a kernel for a jupiter notebook right so all these things we are going to probably import or install that is the,358,0,0,Le-A72NjaWs
7,basic libraries that we specifically require perfect then once this is done we will probably start from here okay and we will go step by step so ah every step we will go ahead and see what we are specifically going to do and what things we are not going to do okay every step by step like how we will be starting how we will be going where you have to probably create a bucket in the s3 bucket like how how what naming convention you can specifically use because once a bucket is created it cannot be deleted okay and uh throughout the entire world reason right the bucket name should be very much unique okay now what i'm going to do over here till then i will go over here and search for s3 bucket so s3 bucket if you don't know guys if you probably want to use any storage space specifically for a longer period of time uh you can basically create this kind of buckets okay so i will just go ahead and create a new bucket and my bucket name i will give something like mob okay more bucket mob pocket for sage maker okay something like this i'm just going to give me this naming convention this is ue us east one this is my bucket name don't need to select anything as such and just go ahead and create this particular bucket okay so that bucket name you have basically taken there are a lot of bucket names that we are using and this uh again for some or the other work we have specifically used now,358,0,0,Le-A72NjaWs
8,i'll go back over here till then let's see whether this is done yes this is done so i will clean the screen and i will close it okay now this is the first thing that we need to do here we will go ahead and set up our kernel so this is my python kernel we are actually setting it up now one thing you need to focus on is that here we have to give the bucket name the bucket name will be unique so i'm going to provide more buckets sage maker name right whatever bucket we have specifically created now to begin with we are going to import sage maker we are going to import train test split i hope everybody's no strain test plate this boto 3 is a library which will specifically used to connect the s3 bucket itself along with that i'm using input pandas let's see whether everything works fine or not it may give you an error because i don't know whether i've installed boto3 or not but if it is not installed so here you can see using bucket mob uh mob like this is for the mobile classification right bucket ch maker so this is the name we are basically setting up for the bucket itself here you can see let's go step by step we are creating a client boto 3 client which can actually communicate with respect to the s3 bucket then we are basically creating a session sage maker session then we are writing session.boto session dot region name by default whatever is is the region name we basically take it is,358,0,0,Le-A72NjaWs
9,u.s east so that it will be taken into this is my bucket name and i'm giving the reference of my bucket name with respect to this so anything that i'm probably going to store let's say i want to do a train test split and store that particular data inside that bucket i will be storing inside this bucket itself so that is the reference name that i've actually given over here the next step over here obviously everybody knows this is about reading the data set and this is how my data set looks like i have battery power blue clock speed dual sim 4g into memory everything and the last the output feature is basically a price range so this becomes a classification problem okay now what we are going to do df dot shape uh you can basically see that i can also normalize this just to see the count of value cons some of the feature engineering and all i'm not focusing much on the feature engineer feature selection process you can definitely do that uh two things one i'm connecting it to the s3 bucket and then probably i'll also show you that how i will be training this entire models in the uh sagemaker itself okay even creating an instances part also i'll show you okay then this is my df dot columns so this is my all my all my features that is specifically there then df dot shape then df dot is null dot mean we are just trying to find out the percentage of values missing it is zero no need to worry so this is,358,0,0,Le-A72NjaWs
10,my features all my features over here are in the form of list that i can see over here this pop minus one basically says that remove just the last feature so here is my last feature price range and this i'm just trying to divide that into my independent on dependent feature over here and this is my x dot head right so these are all my features over here here with respect to this right so here it is all my independent features and y is basically having my dependent feature then this becomes my y dot head uh one two categories are there if you probably want to see ah zero low risk one high risk something like this this is just like a price range no need to worry about anything as such so x dot shape basically means the hair you have 200 records 20 features y dot value underscore counts basically means you have this many number of categories 0 1 2 3 and every that number of records for this are equally proportional right 500 500 500 so it is not an imbalanced data set so we don't have to worry about anything now the next step over here again you know this right you need to do the train test split i have explained you so many times with respect to this how to do the train test plates i'm just taking this x and y and doing that okay uh i will tell you where is the most important thing then this becomes my extreme shape extra shape this is there 1700 records in my training data,358,0,0,Le-A72NjaWs
11,set 300 records in my touch data set then i'm actually converting this into a data frame right data frame pd.dataflame and on this same labels uh same label i'm basically going to create my y train data basically i'm just putting a white range data over there okay so here it is and this becomes my uh with 21 features so that basically i'm creating the entire data frame with all those things as my training and test okay then again trends.head i think these all are very simple is null dot sum okay again very simple and this is there okay so these are some kind of any kind of feature engineering that you specifically do handling missing values anything that you specifically do in fee changing till here you can probably do it now what i'm doing i'm probably saving this data set into version one dot csv with respect to the screen and test version one dot csv and x is equal to false why i am saving this file because if you probably see over here there are two files that is created because i want to push this both the file in in my s3 bucket the same s3 bucket that i have actually created now here what i'm doing i am taking this prefix inside my folder like my main folder was what if you probably see over here i had given a name right if you probably see this was my main folder mob bucket second stage maker right now inside this what we are doing over here see this this is important okay i'm creating a prefix,358,0,0,Le-A72NjaWs
12,i am creating a folder sagemaker mobile price classification sql on container so inside this what i will do i will use session dot upload data what is this session this session is the first thing that we made right the bottom the boto3 client session right sage maker session here you can see both a3 dot client to sagemaker i've used and the same sage maker library that i've used this is the session of this right so that will be able to use the specific sage maker so this session only i'm basically going to use over here and i'm going to just upload this particular data right when we upload this data so this is the functionality that is present inside this session variable okay the path i am saying that okay fine this is my file path the file path here you can basically see train v1 dot csv bucket is equal to bucket what bucket you have over here the same bucket name that i have actually given in the initial part right so if you probably see over here this is basically my bucket here you can see this is my bucket name right mob bucket sage maker so that bucket name i have given over here and after this bucket i am saying that you create all these folders inside that bucket sagemaker mobile price classifications container okay and uh this train va1 dot csv will be uploaded and the training path will be there okay that basically means the path over here right so here you can also see if i go ahead and print okay crane path,358,0,0,Le-A72NjaWs
13,okay and if i go ahead and print the test party here you'll be able to see that okay now similarly session dot upload test dot uh test file also i'm trying to upload in the same bucket with the same prefix only the file name will be changing so let's go ahead and execute this now here you'll be able to see it is uploading it is uploading so this becomes my entire path see s3 mobile bucket uh sage maker sagemaker slash mobile price classification scale on container and finally i have my train v1 dot csv now we will just verify whether this file is present over here or not so let me just go and see this oh where is my bucket there's so many buckets created away so this is my bucket so here you will be able to see the object is getting loaded so inside sagemaker you have mobile price classification you have a skill on container sorry you have excellent container and here you have this test v1.csv and test train v1 dot csv right and just now it is uploaded see june 23 this timing and all right so perfect till here you have absolutely done it really well and this is basically a train pattern test part now till here what we have just done this is basically the data ingestion phase so you have taken a data you have done the train test split and you have uploaded in the s3 bucket right so later on then i probably convert this into an end-to-end project this entire instruction of code that i've actually written i,358,0,0,Le-A72NjaWs
14,will specifically write where in the data ingestion part okay and that is where i'm also using boto3 the session that i'm creating and the managing the session uploading the data everything right this uploading the data will be going in my utility uh utility functionality that i'm specifically creating on that project so that kind of end-to-end project i have already created it right so i will do that also don't worry about it okay anyhow i have written the entire code for this also so you don't have to worry now this is super super important okay this entire code write script file script.py so here we are going to create one script.py this script.py what it will be doing step by step and the script.py is available in sagemaker documentation also okay i have not written by my own i've just taken the documentation i have probably changed the libraries over here like random forest classifier and all uh and then i have probably written it right now what this entire script.py file is all about right and whenever we write like this right x in this way write file script.py in short it is going to create a file which is script.py and inside that it is going to have this entire code okay now what this code is all about let's see this okay over here we have created a function which is called as model function and it is saying job lab dot load of this particular model uh whatever model is there from this particular directory right so this that model directory will be the directory because i'll also,358,0,0,Le-A72NjaWs
15,be saving the model in my bucket everything will you will be able to understand okay so this function is basically loading the model okay now you see we'll start the execution from here whenever you are training your model in the sage maker right it requires some by default arguments okay some of the arguments is required by the aws sagemaker amazon sale maker now what all arguments is specifically acquired now see this argument that i've sent right parser arcpash dot argument parser so we are basically taken some argument parser over here this argument parser is basically the arguments of this particular library random forest classifier like what is the n underscore estimator what is random state everything so these two arguments i have given from my side right but by default from the aws sagemaker side we also need to pass this argument what is the model directory what is the train what is the test what is the train file what is the test file okay so train file and test file we know what is the name right by default right so over here you'll be able to see over here you'll be able to the model directory train and test this is what is the file name or what this is the folder name that we specifically specify by default okay so here you will be able to see sm underscore model dot underscore directory so this is basically set up in the environment only in the aws siege maker okay so if you probably see this sm underscore model underscore directory sm underscore channel underscore train at sem,358,0,0,Le-A72NjaWs
16,underscore channel let's go test okay so this is basically set up uh in the environment itself no no changes you don't have to change it because see again i am telling you this entire script.py file i have just copied and pasted it over here okay from the documentation that is given and over here you can see sk1.,78,0,0,Le-A72NjaWs
17,version joblift.version this is there now here you can see we are reading the train file okay we have given the path arguments.train arguments.train file so this is my train path and this is my file we are reading it okay and similarly whatever path over here for test and test file we are given that also we are reading it then again all this same step we are basically doing the train test play everything as such and then we are applying the random forest we are doing the fit and all these things then we are you know dumping the libraries sorry dumping the model after the model is basically trained in some kind of format and then we also are doing the prediction from the x test then we are trying to find out the test accuracy along with that we are also trying to find out the classification report right all these things is that so this is my entire script py file so it is having the name function so it is just going to execute line by line with respect to the end of the function right so here i will just go ahead and execute this as soon as i execute this here you will be able to see script.py file is created see and this file is only going to get used by the sage maker you know how it will be getting used just follow this it will be you'll be able to understand okay now in sagemaker by default you have a library which is called as sk learn capital scalar okay it uses some framework,358,1,1,Le-A72NjaWs
18,version this and all okay and here you have to first of all give the script.py whatever py file that you have the for the entire training purpose and this is specifically the role that we are probably going to give this role how from where it is coming i will show you this role also so here if you probably go ahead and probably search for i am okay iam okay i am the users right whatever users we had we had created right so here you will be able to see because this is the changes that you only have to do okay let's see so i am users okay and here uh let's see let's see where is that sage maker example um mobile okay let's see if any example okay so here i'm just going to click this and groups groups groups yeah so here you can basically see this is the arn right and here with respect to the code also that you are able to see this is nothing but this specific role you need to specify right so this is the role that i'm actually specifying you just need to copy from wherever you are probably taking this uh just copy this and paste it over there right so here we have just pasted it over here with respect to this rules okay then instance count how many instances you want to open you need to provide this inside is sql only see what this sql on estimator is basically doing you can see that it will create an instance in the sage maker like it will assign a,358,1,1,Le-A72NjaWs
19,machine right what the machine type will be like this ml mn ml dot m5 dot large so this is one of the instance type see from sage maker also you can manually do it and i'm showing you from the code if you probably go and check out my sagemaker playlist there i have shown you how to do it from the sage maker but here i'm specifying the instance type at ml dot ml5 m5 dot large okay and then framework version is like this framework version whatever framework version was present in the documentation okay base job name i'm just trying to provide this specific name as my folder after the model is trained hyper parameter is 2 and then stimulators is equal to 100 and random state is equal to 0 use spot instances is true max weight is this much max run is this much so these are some default values that i have actually set it up okay and this is what is basically going to run in the sage maker okay it knows where the script.py file is also so i'm just going to execute this this is done and now all i need to do is that use this sk loan dot estimator which i have actually created over here dot fit on train path and test path and i'll say weight is equal to true now this is where we are going to launch the training job as an unsynchronous call on synchronous call basically means it will go ahead and probably create an instance start creating an instance in the aws sage maker and start okay,358,1,1,Le-A72NjaWs
20,so let's see this i will just go ahead and execute it now now this fit process i have actually started using the provided s3 resources you can see it created a training job with this specific name now till the training will be happening it will take some amount of time okay what it will do is that i will just go and search for sagemaker let's see and i will go ahead and click on this and here i will go and click on training okay training jobs now here you can see this is the name that you can see 23 6 20 23 right right now is the same date that you can probably see over here this has started and it is in progress that basically means the training has basically started so here you will be able to see the same training is basically started it knows where the s3 bucket is it knows each and everything it is now it will try to open an instance of this type right of this particular machine ml dot m5 dot large you know and it will start the training process okay now we will wait for some time till the training is done and once that specific training is done the next step what we'll do i will try to show you that how we can deploy this because the next code is all about that only okay uh this all codes that you'll be seeing over here it will show you that how you can deploy the model i will show you that how where the file is basically created,358,1,1,Le-A72NjaWs
21,and how we move it to this three bucket as a model file and all everything will be discussed so let this happen it will take some time like five to six minutes for this entire training because we are launching an instance also and then we are training a model also okay so just let wait for some time okay so guys the finally the training is done here you can see step by step what is happening see uh the created a training job with this name so where did we assign this specific name here you can see rf custom scale learn and then it started the training job it prepared the instance for training it downloaded the input data from the s3 bucket the training image download got completed here you can basically see no gpus detected if we can also assign gpus from here right in this scale on sk loan right we can also assign gpus uh probably i'll show you that when i probably do a deep learning project okay and then here you can see invoking user training script that is this training script.py okay no gpu is detected invoking user input this is the training environment entirely all the hyper parameters everything is basically taken training seconds and billables seconds is 108 okay so this is what uh we have specifically done and here you'll be able to see that my model will also be ready and if i probably open this here you can see this is also completed right and this is my entire training information right the same thing uh this is the role,358,1,1,Le-A72NjaWs
22,that is there you can see billable seconds every all the information it is basically shown also over here right now you need to understand one more thing here you can see the output data configuration so whatever model is basically created that is also saved in the s3 output right so if i probably go ahead and show you this s3 bucket right uh let's let's go over here so this is my bucket and you can see the name of the three bucket is nothing but sagemaker us is one five six six three seven three okay so sage maker let's see where is the bucket u.s east may 4 2023 not this one let's see inside this and here you have this right so this is one of the file 23 6.,175,1,1,Le-A72NjaWs
23,it's 23 6 yeah this is the file i think yes this is the file uh of 23rd so here you can see inside this you have the output folder and you have the entire model right so what it is done is that the training is basically done from there and then the output in the s3 bucket it has also dumped the entire model right this entirely things is basically done with the help of this justice code you know over here right so and everything is basically done over here and automatically the bucket is also created and it is basically deployed right now you can also use this code to probably see more information regarding your uh entire training you know so here what i'm actually going to do is that i'll just execute this see what is happening over here ah scale on htmeter dot latest training job dot weight that is doing then it is saying that describe this training job with this scale on dot estimator.latesttrainingjob.name model artifacts and s3 model effects okay so it is basically just saying that where all your model exists okay and this is the folder location that is basically given over here right all this information is there and if i probably go ahead and execute this artifact so this is the entire path here you can see the model where the model is exactly so that that entirely code is basically available over here also and if you are very much familiar with machine learning things and all you can definitely know all these things right now finally uh one more thing,358,2,2,Le-A72NjaWs
24,that uh we are specifically going to do over here is that uh how do you now see this in this also what we are basically doing we are uh okay one more thing that has basically happened over here is that when we are preparing the instance for the training the training image download is completed raining is in progress uploaded generated training model and here now you can probably see in this rf custom scale learn i have my model output okay so this is very much clear i hope uh there is no much uh different when when we try to compare that right now here you can see from sk learn from sagemaker.sklon dot model import sql on model here i am going to probably create another folder location which is called as a scale on dot model because i need to also do the deployment okay here i give my model name so whatever with respect to that particular type so that basically means from this particular folder location i'll just i'm just moving that uh into some other folder location that begins with custom sql on model we'll be able to see that okay so let me just go ahead and execute this once okay and this is basically my model over here sk1.,286,2,2,Le-A72NjaWs
25,this now the reason why i'm putting inside this because i want to keep a copy so that i deploy the specific model okay i deploy the specific model as an endpoint okay so now for deploying the specific model as an endpoint see guys this code is fixed almost okay no you don't have to worry much about this because this is also available in the documentation so what i'm actually going to do is that i'll take this end point name use the same folder location which i have actually created over here see over here so if you probably go and see my model underscore name okay so this is my folder location okay in this three bucket i'm going to take this and we are just going to use model.deploy and i'm deploying in this specific instance as an endpoint okay so once i execute this it is basically going to take this as an endpoint name okay and then it is creating a model with this specific name and it will start doing the deployment now this deployment will be super important because this same predictor dot predict when we will do right we will be able to predict for any new input data okay now this particular code is specifically doing the end point okay so here i'm just showing you this so this is my end point and point deployment okay and point deployment right so here you will be able to see that it will take some amount of time again like how it took probably over here somewhere around four minutes of time right that is just,358,3,3,Le-A72NjaWs
26,for training a model this is for deploying the model right and this deployment is basically happening now okay you may be thinking krish why did we specifically use this uh over here in the s3 bucket if you probably see this right i will just show you okay so here you can see this custom folder looks custom scale on the source directory tars.z right this this is basically created i think it is for the same uh 23 6 right so same same thing right so the same model i basically used i've created an additional folder over there and from this only the deployment will happen to the end point okay and what we specifically require only this star directory that we specifically had right that is what we required so from this what we are doing from from this output folder we took this we put inside that particular folder and from there we are putting it as an endpoint just to keep a additional copy that's it okay nothing nothing else so if you probably also execute it you will be able to understand it okay so this will probably take some time because see why this is taking time because we need to start an instance of a machine and then probably do the deployment okay this is more of a deployment kind of thing right and once this endpoint is done then you will be able to see the endpoint name then for any data you just need to write predictor.predict and give that a particular input data in the form of list you will be able to get,358,3,3,Le-A72NjaWs
27,the output okay so we'll wait for some time till the deployment happens again it will take three to four minutes again like how the training actually happened and the main reason is that we are creating an instance and then doing the deployment right so yes let's wait for some time and then i will show you that how we can do the prediction so finally guys the deployment is basically done and now the end point this predictor is basically getting exposed to the endpoint that basically means that from this the help of this predictor you will be able to probably do the prediction so if i probably see what this predictor is right you'll be able to see that it is an sk learn predictor and always remember why we are specifically using this sql on predictor this is basically whenever you create an endpoint from the sage maker this is the return type that you probably see and that is what is basically provided in the ews series maker itself right so now you have your endpoint in this particular variable all you can do is that at any point of time let's say that i have taken this all data right i can take all this data and probably do the prediction with respect to row by row okay so here you can see the first two records i'm trying to predict it and this will become my output right two outputs that i am getting the first record output is three and the second record output is zero right so this is basically the type based on the input,358,3,3,Le-A72NjaWs
28,so this was my first first row input okay and this for this first row input it has predicted three and for the second row input it has basically predicted zero right so this is in short what is basically happening and this entirely thing is basically happening in the aws sage maker how i am actually saying it here you can probably see okay so this is my training jobs and with respect to the end points also you'll be able to see so this is my end point over here right the same endpoint and here is what is my model name everything is there so this is how my endpoint is created this is the url with respect to the invocations right and from this particular endpoint only i'm actually giving the input and getting the output right so uh step by step uh see not everything you have to probably do because in some complex data set you have to perform your own feature engineering the training and the inference part you can definitely use it from the sage maker itself right and this was about the endpoint configuration here you can see endpoint configuration is here and this is your end point now at the end of the day guys remember one thing is that this is all our things are chargeable right so it is always a good habit that that you delete this specific endpoint right just by providing your endpoint name so once i probably use this smo23.delete endpoint you'll be able to see if once i go over there now this end point will get deleted so,358,3,3,Le-A72NjaWs
29,that not much charges should be there right so this was just a clear idea about how you can probably run this uh the only thing that specifically changes is feature engineering and all guys remaining all you can probably how to train a model just by using this as scale on and this is just a machine learning models right i'll also show you deep learning how to set up this role i have told you how to set up the script.py in the script.p why you can also write your any kind of code let's say you want to perform hyper parameter tuning can basically perform you can make this quite generic okay and then if what are things you can basically play with right right your instance type name you know remaining everything will be same right so i hope you got an idea now in my upcoming videos what i'm going to do i'm going to convert this entire notebook code into an end-to-end project where the data ingestion will come like how we create a entire packages right the source folder uh every components from data ingestion to data transformation to data validation to model creation model trainer everything then probably model deployment part also that also i'll show you okay and that is what i'm actually targeting in the next video so i hope you like this particular video this this was it from my side i will see you all in the next day have a great day thank you wonder all take care bye,340,3,3,Le-A72NjaWs
